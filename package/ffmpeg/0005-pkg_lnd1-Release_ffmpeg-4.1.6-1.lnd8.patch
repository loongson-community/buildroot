diff --git a/libavcodec/loongarch/Makefile b/libavcodec/loongarch/Makefile
index 583e3545eb..c1b5de5c44 100644
--- a/libavcodec/loongarch/Makefile
+++ b/libavcodec/loongarch/Makefile
@@ -1,33 +1,33 @@
-OBJS-$(CONFIG_H264QPEL)               += loongarch/h264qpel_init_loongarch.o
 OBJS-$(CONFIG_H264CHROMA)             += loongarch/h264chroma_init_loongarch.o
+OBJS-$(CONFIG_H264QPEL)               += loongarch/h264qpel_init_loongarch.o
 OBJS-$(CONFIG_H264DSP)                += loongarch/h264dsp_init_loongarch.o
-OBJS-$(CONFIG_IDCTDSP)                += loongarch/idctdsp_init_loongarch.o
-OBJS-$(CONFIG_VC1DSP)                 += loongarch/vc1dsp_init_loongarch.o
-OBJS-$(CONFIG_HPELDSP)                += loongarch/hpeldsp_init_loongarch.o
-OBJS-$(CONFIG_HEVC_DECODER)           += loongarch/hevcdsp_init_loongarch.o
+OBJS-$(CONFIG_H264PRED)               += loongarch/h264_intrapred_init_loongarch.o
 OBJS-$(CONFIG_VP8_DECODER)            += loongarch/vp8dsp_init_loongarch.o
 OBJS-$(CONFIG_VP9_DECODER)            += loongarch/vp9dsp_init_loongarch.o
+OBJS-$(CONFIG_VC1DSP)                 += loongarch/vc1dsp_init_loongarch.o
+OBJS-$(CONFIG_HPELDSP)                += loongarch/hpeldsp_init_loongarch.o
+OBJS-$(CONFIG_IDCTDSP)                += loongarch/idctdsp_init_loongarch.o
 OBJS-$(CONFIG_VIDEODSP)               += loongarch/videodsp_init.o
-OBJS-$(CONFIG_H264PRED)               += loongarch/h264_intrapred_init_loongarch.o
-LASX-OBJS-$(CONFIG_H264DSP)           += loongarch/h264dsp_lasx.o  \
+OBJS-$(CONFIG_HEVC_DECODER)           += loongarch/hevcdsp_init_loongarch.o
+LASX-OBJS-$(CONFIG_H264CHROMA)        += loongarch/h264chroma_lasx.o
+LASX-OBJS-$(CONFIG_H264QPEL)          += loongarch/h264qpel_lasx.o
+LASX-OBJS-$(CONFIG_H264DSP)           += loongarch/h264dsp_lasx.o \
                                          loongarch/h264idct_lasx.o \
                                          loongarch/h264_deblock_lasx.o
-LASX-OBJS-$(CONFIG_H264CHROMA)        += loongarch/h264chroma_lasx.o
-LASX-OBJS-$(CONFIG_H264CHROMA)        += loongarch/h264qpel_lasx.o
-LASX-OBJS-$(CONFIG_IDCTDSP)           += loongarch/simple_idct_lasx.o  \
-                                         loongarch/idctdsp_lasx.o
+LASX-OBJS-$(CONFIG_H264PRED)          += loongarch/h264_intrapred_lasx.o
 LASX-OBJS-$(CONFIG_VC1_DECODER)       += loongarch/vc1dsp_lasx.o
 LASX-OBJS-$(CONFIG_HPELDSP)           += loongarch/hpeldsp_lasx.o
-LASX-OBJS-$(CONFIG_H264PRED)          += loongarch/h264_intrapred_lasx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevcdsp_lsx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevc_idct_lsx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevc_mc_bi_lsx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevc_lpf_sao_lsx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevc_mc_uni_lsx.o
-LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevc_mc_uniw_lsx.o
-LSX-OBJS-$(CONFIG_VP9_DECODER)        += loongarch/vp9_mc_lsx.o      \
+LASX-OBJS-$(CONFIG_IDCTDSP)           += loongarch/simple_idct_lasx.o  \
+                                         loongarch/idctdsp_lasx.o
+LSX-OBJS-$(CONFIG_VP8_DECODER)        += loongarch/vp8_mc_lsx.o \
+                                         loongarch/vp8_lpf_lsx.o
+LSX-OBJS-$(CONFIG_VP9_DECODER)        += loongarch/vp9_mc_lsx.o \
                                          loongarch/vp9_intra_lsx.o \
                                          loongarch/vp9_lpf_lsx.o \
                                          loongarch/vp9_idct_lsx.o
-LSX-OBJS-$(CONFIG_VP8_DECODER)        += loongarch/vp8_mc_lsx.o \
-                                         loongarch/vp8_lpf_lsx.o
+LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevcdsp_lsx.o \
+                                         loongarch/hevc_idct_lsx.o \
+                                         loongarch/hevc_lpf_sao_lsx.o \
+                                         loongarch/hevc_mc_bi_lsx.o \
+                                         loongarch/hevc_mc_uni_lsx.o \
+                                         loongarch/hevc_mc_uniw_lsx.o
diff --git a/libavcodec/loongarch/cabac.h b/libavcodec/loongarch/cabac.h
index ea937665b4..e1c946fe16 100644
--- a/libavcodec/loongarch/cabac.h
+++ b/libavcodec/loongarch/cabac.h
@@ -28,104 +28,105 @@
 #include "libavcodec/cabac.h"
 #include "config.h"
 
-#define GET_CABAC_LOONGARCH                                                                   \
-        "ld.bu        %[bit],        %[state],       0x0           \n\t"                      \
-        "andi         %[tmp0],       %[c_range],     0xC0          \n\t"                      \
-        "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"                      \
-        "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"                      \
-        "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"                      \
-        /* tmp1: RangeLPS */                                                                  \
-        "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"                      \
-                                                                                              \
-        "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"                      \
-        "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"                      \
-        "bge          %[tmp0],       %[c_low],       1f            \n\t"                      \
-        "move         %[c_range],    %[tmp1]                       \n\t"                      \
-        "nor          %[bit],        %[bit],         %[bit]        \n\t"                      \
-        "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"                      \
-                                                                                              \
-        "1:                                                        \n\t"                      \
-        /* tmp1: *state */                                                                    \
-        "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"                      \
-        "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"                      \
-        /* tmp2: lps_mask */                                                                  \
-        "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"                      \
-        "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"                      \
-                                                                                              \
-        "andi         %[bit],        %[bit],         0x01          \n\t"                      \
-        "st.b         %[tmp1],       %[state],       0x0           \n\t"                      \
-        "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"                      \
-        "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"                      \
-                                                                                              \
-        "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"                      \
-        "bnez         %[tmp1],       1f                            \n\t"                      \
-        "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"                      \
-        "ctz.d        %[tmp0],       %[c_low]                      \n\t"                      \
-        "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"                      \
-        "revb.2h      %[tmp0],       %[tmp1]                       \n\t"                      \
-        "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"                      \
-        "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"                      \
-        "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"                      \
-        "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"                      \
-        "addi.d       %[c_bytestream], %[c_bytestream],     0x02   \n\t"                      \
-        "1:                                                        \n\t"                      \
+#define GET_CABAC_LOONGARCH_UNCBSR                                      \
+    "ld.bu        %[bit],        %[state],       0x0           \n\t"    \
+    "andi         %[tmp0],       %[c_range],     0xC0          \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"    \
+    /* tmp1: RangeLPS */                                                \
+    "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"    \
+                                                                        \
+    "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"    \
+    "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"    \
+    "bge          %[tmp0],       %[c_low],       1f            \n\t"    \
+    "move         %[c_range],    %[tmp1]                       \n\t"    \
+    "nor          %[bit],        %[bit],         %[bit]        \n\t"    \
+    "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "1:                                                        \n\t"    \
+    /* tmp1: *state */                                                  \
+    "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"    \
+    "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"    \
+    /* tmp2: lps_mask */                                                \
+    "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"    \
+    "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"    \
+                                                                        \
+    "andi         %[bit],        %[bit],         0x01          \n\t"    \
+    "st.b         %[tmp1],       %[state],       0x0           \n\t"    \
+    "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"    \
+    "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"    \
+                                                                        \
+    "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"    \
+    "bnez         %[tmp1],       1f                            \n\t"    \
+    "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"    \
+    "ctz.d        %[tmp0],       %[c_low]                      \n\t"    \
+    "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"    \
+    "revb.2h      %[tmp0],       %[tmp1]                       \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"    \
+    "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"    \
+    "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+    "addi.d       %[c_bytestream], %[c_bytestream],     0x02   \n\t"    \
+    "1:                                                        \n\t"    \
 
-#define GET_CABAC_LOONGARCH_END                                                               \
-        "ld.bu        %[bit],        %[state],       0x0           \n\t"                      \
-        "andi         %[tmp0],       %[c_range],     0xC0          \n\t"                      \
-        "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"                      \
-        "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"                      \
-        "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"                      \
-        /* tmp1: RangeLPS */                                                                  \
-        "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"                      \
-                                                                                              \
-        "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"                      \
-        "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"                      \
-        "bge          %[tmp0],       %[c_low],       1f            \n\t"                      \
-        "move         %[c_range],    %[tmp1]                       \n\t"                      \
-        "nor          %[bit],        %[bit],         %[bit]        \n\t"                      \
-        "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"                      \
-                                                                                              \
-        "1:                                                        \n\t"                      \
-        /* tmp1: *state */                                                                    \
-        "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"                      \
-        "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"                      \
-        /* tmp2: lps_mask */                                                                  \
-        "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"                      \
-        "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"                      \
-                                                                                              \
-        "andi         %[bit],        %[bit],         0x01          \n\t"                      \
-        "st.b         %[tmp1],       %[state],       0x0           \n\t"                      \
-        "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"                      \
-        "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"                      \
-                                                                                              \
-        "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"                      \
-        "bnez         %[tmp1],       1f                            \n\t"                      \
-        "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"                      \
-        "ctz.d        %[tmp0],       %[c_low]                      \n\t"                      \
-        "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"                      \
-        "revb.2h      %[tmp0],       %[tmp1]                       \n\t"                      \
-        "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"                      \
-        "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"                      \
-        "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"                      \
-                                                                                              \
-        "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"                      \
-                                                                                              \
-        "slt          %[tmp0],         %[c_bytestream],     %[c_bytestream_end]  \n\t"        \
-        "add.d        %[c_bytestream], %[c_bytestream],     %[tmp0]              \n\t"        \
-        "add.d        %[c_bytestream], %[c_bytestream],     %[tmp0]              \n\t"        \
-        "1:                                                        \n\t"                      \
+#define GET_CABAC_LOONGARCH                                             \
+    "ld.bu        %[bit],        %[state],       0x0           \n\t"    \
+    "andi         %[tmp0],       %[c_range],     0xC0          \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"    \
+    /* tmp1: RangeLPS */                                                \
+    "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"    \
+                                                                        \
+    "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"    \
+    "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"    \
+    "bge          %[tmp0],       %[c_low],       1f            \n\t"    \
+    "move         %[c_range],    %[tmp1]                       \n\t"    \
+    "nor          %[bit],        %[bit],         %[bit]        \n\t"    \
+    "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "1:                                                        \n\t"    \
+    /* tmp1: *state */                                                  \
+    "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"    \
+    "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"    \
+    /* tmp2: lps_mask */                                                \
+    "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"    \
+    "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"    \
+                                                                        \
+    "andi         %[bit],        %[bit],         0x01          \n\t"    \
+    "st.b         %[tmp1],       %[state],       0x0           \n\t"    \
+    "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"    \
+    "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"    \
+                                                                        \
+    "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"    \
+    "bnez         %[tmp1],       1f                            \n\t"    \
+    "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"    \
+    "ctz.d        %[tmp0],       %[c_low]                      \n\t"    \
+    "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"    \
+    "revb.2h      %[tmp0],       %[tmp1]                       \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"    \
+    "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"    \
+                                                                        \
+    "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "slt      %[tmp0],  %[c_bytestream],  %[c_bytestream_end]  \n\t"    \
+    "add.d    %[c_bytestream], %[c_bytestream],     %[tmp0]    \n\t"    \
+    "add.d    %[c_bytestream], %[c_bytestream],     %[tmp0]    \n\t"    \
+    "1:                                                        \n\t"    \
 
 #define get_cabac_inline get_cabac_inline_loongarch
-static av_always_inline int get_cabac_inline_loongarch(CABACContext *c,
-                                                       uint8_t * const state){
+static av_always_inline
+int get_cabac_inline_loongarch(CABACContext *c, uint8_t * const state)
+{
     int64_t tmp0, tmp1, tmp2, bit;
 
     __asm__ volatile (
 #if UNCHECKED_BITSTREAM_READER
-    GET_CABAC_LOONGARCH
+        GET_CABAC_LOONGARCH_UNCBSR
 #else
-    GET_CABAC_LOONGARCH_END
+        GET_CABAC_LOONGARCH
 #endif
     : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
       [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
@@ -189,7 +190,8 @@ static av_always_inline int get_cabac_bypass_loongarch(CABACContext *c)
 }
 
 #define get_cabac_bypass_sign get_cabac_bypass_sign_loongarch
-static av_always_inline int get_cabac_bypass_sign_loongarch(CABACContext *c, int val)
+static av_always_inline
+int get_cabac_bypass_sign_loongarch(CABACContext *c, int val)
 {
     int64_t tmp0, tmp1;
     int res = val;
diff --git a/libavcodec/loongarch/h264_cabac.c b/libavcodec/loongarch/h264_cabac.c
index d3ea8a884c..d88743bed7 100644
--- a/libavcodec/loongarch/h264_cabac.c
+++ b/libavcodec/loongarch/h264_cabac.c
@@ -26,8 +26,7 @@
 
 #define decode_significance decode_significance_loongarch
 static int decode_significance_loongarch(CABACContext *c, int max_coeff,
-                                         uint8_t *significant_coeff_ctx_base,
-                                         int *index, int64_t last_off)
+    uint8_t *significant_coeff_ctx_base, int *index, int64_t last_off)
 {
     void *end = significant_coeff_ctx_base + max_coeff - 1;
     int64_t minusstart = -(int64_t)significant_coeff_ctx_base;
@@ -38,35 +37,36 @@ static int decode_significance_loongarch(CABACContext *c, int max_coeff,
     __asm__ volatile(
     "3:"
 #if UNCHECKED_BITSTREAM_READER
-    GET_CABAC_LOONGARCH
+    GET_CABAC_LOONGARCH_UNCBSR
 #else
-    GET_CABAC_LOONGARCH_END
+    GET_CABAC_LOONGARCH
 #endif
-    "blt     %[bit],          %[one],            4f                        \n\t"
-    "add.d   %[state],        %[state],          %[last_off]               \n\t"
+    "blt     %[bit],          %[one],            4f               \n\t"
+    "add.d   %[state],        %[state],          %[last_off]      \n\t"
 #if UNCHECKED_BITSTREAM_READER
-    GET_CABAC_LOONGARCH
+    GET_CABAC_LOONGARCH_UNCBSR
 #else
-    GET_CABAC_LOONGARCH_END
+    GET_CABAC_LOONGARCH
 #endif
-    "sub.d   %[state],        %[state],          %[last_off]               \n\t"
-    "add.d   %[tmp0],         %[state],          %[minusstart]             \n\t"
-    "st.w    %[tmp0],         %[index],          0                         \n\t"
-    "bge     %[bit],          %[one],            5f                        \n\t"
-    "addi.d  %[index],        %[index],          4                         \n\t"
-    "4:                                                                    \n\t"
-    "addi.d  %[state],        %[state],          1                         \n\t"
-    "blt     %[state],        %[end],            3b                        \n\t"
-    "add.d   %[tmp0],         %[state],          %[minusstart]             \n\t"
-    "st.w    %[tmp0],         %[index],          0                         \n\t"
-    "5:                                                                    \n\t"
-    "add.d   %[tmp0],         %[index],          %[minusindex]             \n\t"
-    "srli.d  %[tmp0],         %[tmp0],           2                         \n\t"
-    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2), [c_range]"+&r"(c->range),
-      [c_low]"+&r"(c->low), [state]"+&r"(state),
+    "sub.d   %[state],        %[state],          %[last_off]      \n\t"
+    "add.d   %[tmp0],         %[state],          %[minusstart]    \n\t"
+    "st.w    %[tmp0],         %[index],          0                \n\t"
+    "bge     %[bit],          %[one],            5f               \n\t"
+    "addi.d  %[index],        %[index],          4                \n\t"
+    "4:                                                           \n\t"
+    "addi.d  %[state],        %[state],          1                \n\t"
+    "blt     %[state],        %[end],            3b               \n\t"
+    "add.d   %[tmp0],         %[state],          %[minusstart]    \n\t"
+    "st.w    %[tmp0],         %[index],          0                \n\t"
+    "5:                                                           \n\t"
+    "add.d   %[tmp0],         %[index],          %[minusindex]    \n\t"
+    "srli.d  %[tmp0],         %[tmp0],           2                \n\t"
+    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
+      [c_range]"+&r"(c->range), [c_low]"+&r"(c->low), [state]"+&r"(state),
       [c_bytestream]"+&r"(c->bytestream), [index]"+&r"(index)
     : [tables]"r"(ff_h264_cabac_tables), [end]"r"(end), [one]"r"(one),
-      [minusstart]"r"(minusstart), [minusindex]"r"(minusindex), [last_off]"r"(last_off),
+      [minusstart]"r"(minusstart), [minusindex]"r"(minusindex),
+      [last_off]"r"(last_off),
 #if !UNCHECKED_BITSTREAM_READER
       [c_bytestream_end]"r"(c->bytestream_end),
 #endif
@@ -81,9 +81,9 @@ static int decode_significance_loongarch(CABACContext *c, int max_coeff,
 }
 
 #define decode_significance_8x8 decode_significance_8x8_loongarch
-static int decode_significance_8x8_loongarch(CABACContext *c,
-                                             uint8_t *significant_coeff_ctx_base,
-                                             int *index, uint8_t *last_coeff_ctx_base, const uint8_t *sig_off)
+static int decode_significance_8x8_loongarch(
+    CABACContext *c, uint8_t *significant_coeff_ctx_base,
+    int *index, uint8_t *last_coeff_ctx_base, const uint8_t *sig_off)
 {
     int64_t minusindex = 4 - (int64_t)index;
     int64_t bit, tmp0, tmp1, tmp2, one = 1, end =  63, last = 0;
@@ -91,45 +91,49 @@ static int decode_significance_8x8_loongarch(CABACContext *c,
     int64_t flag_offset = H264_LAST_COEFF_FLAG_OFFSET_8x8_OFFSET;
 
     __asm__ volatile(
-    "3:                                                                    \n\t"
-    "ldx.bu   %[tmp0],     %[sig_off],       %[last]                       \n\t"
-    "add.d    %[state],    %[tmp0],          %[significant_coeff_ctx_base] \n\t"
+    "3:                                                              \n\t"
+    "ldx.bu   %[tmp0],     %[sig_off],       %[last]                 \n\t"
+    "add.d    %[state],    %[tmp0], %[significant_coeff_ctx_base]    \n\t"
 #if UNCHECKED_BITSTREAM_READER
-    GET_CABAC_LOONGARCH
+    GET_CABAC_LOONGARCH_UNCBSR
 #else
-    GET_CABAC_LOONGARCH_END
+    GET_CABAC_LOONGARCH
 #endif
-    "blt      %[bit],      %[one],           4f                            \n\t"
-    "add.d    %[tmp0],     %[tables],        %[flag_offset]                \n\t"
-    "ldx.bu   %[tmp1],     %[tmp0],          %[last]                       \n\t"
-    "add.d    %[state],    %[tmp1],          %[last_coeff_ctx_base]        \n\t"
+    "blt      %[bit],      %[one],           4f                      \n\t"
+    "add.d    %[tmp0],     %[tables],        %[flag_offset]          \n\t"
+    "ldx.bu   %[tmp1],     %[tmp0],          %[last]                 \n\t"
+    "add.d    %[state],    %[tmp1],    %[last_coeff_ctx_base]        \n\t"
 #if UNCHECKED_BITSTREAM_READER
-    GET_CABAC_LOONGARCH
+    GET_CABAC_LOONGARCH_UNCBSR
 #else
-    GET_CABAC_LOONGARCH_END
+    GET_CABAC_LOONGARCH
 #endif
-    "st.w    %[last],      %[index],         0                             \n\t"
-    "bge     %[bit],       %[one],           5f                            \n\t"
-    "addi.d  %[index],     %[index],         4                             \n\t"
-    "4:                                                                    \n\t"
-    "addi.d  %[last],      %[last],          1                             \n\t"
-    "blt     %[last],      %[end],           3b                            \n\t"
-    "st.w    %[last],      %[index],         0                             \n\t"
-    "5:                                                                    \n\t"
-    "add.d   %[tmp0],      %[index],         %[minusindex]                 \n\t"
-    "srli.d  %[tmp0],      %[tmp0],          2                             \n\t"
-    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2), [c_range]"+&r"(c->range),
+    "st.w    %[last],      %[index],         0                       \n\t"
+    "bge     %[bit],       %[one],           5f                      \n\t"
+    "addi.d  %[index],     %[index],         4                       \n\t"
+    "4:                                                              \n\t"
+    "addi.d  %[last],      %[last],          1                       \n\t"
+    "blt     %[last],      %[end],           3b                      \n\t"
+    "st.w    %[last],      %[index],         0                       \n\t"
+    "5:                                                              \n\t"
+    "add.d   %[tmp0],      %[index],         %[minusindex]           \n\t"
+    "srli.d  %[tmp0],      %[tmp0],          2                       \n\t"
+    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1),
+      [tmp2]"=&r"(tmp2), [c_range]"+&r"(c->range),
       [c_low]"+&r"(c->low), [state]"+&r"(state), [last]"+&r"(last),
       [c_bytestream]"+&r"(c->bytestream), [index]"+&r"(index)
-    : [tables]"r"(ff_h264_cabac_tables), [end]"r"(end), [one]"r"(one), [minusindex]"r"(minusindex),
-      [last_coeff_ctx_base]"r"(last_coeff_ctx_base), [flag_offset]"r"(flag_offset),
+    : [tables]"r"(ff_h264_cabac_tables), [end]"r"(end),
+      [one]"r"(one), [minusindex]"r"(minusindex),
+      [last_coeff_ctx_base]"r"(last_coeff_ctx_base),
+      [flag_offset]"r"(flag_offset),
 #if !UNCHECKED_BITSTREAM_READER
       [c_bytestream_end]"r"(c->bytestream_end),
 #endif
       [lps_off]"i"(H264_LPS_RANGE_OFFSET), [sig_off]"r"(sig_off),
       [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
       [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
-      [cabac_mask]"r"(CABAC_MASK), [significant_coeff_ctx_base]"r"(significant_coeff_ctx_base)
+      [cabac_mask]"r"(CABAC_MASK),
+      [significant_coeff_ctx_base]"r"(significant_coeff_ctx_base)
     );
 
     return (int)tmp0;
diff --git a/libavcodec/loongarch/h264_deblock_lasx.c b/libavcodec/loongarch/h264_deblock_lasx.c
index 886cc66d95..c89bea9a84 100644
--- a/libavcodec/loongarch/h264_deblock_lasx.c
+++ b/libavcodec/loongarch/h264_deblock_lasx.c
@@ -58,7 +58,8 @@ do {                                                                        \
                 tmp3 = __lasx_xvld(mv_t, 48); \
                 tmp4 = __lasx_xvld(mv_t, 208); \
                 tmp5 = __lasx_xvld(mv_t + d_idx_x4, 208); \
-                DUP2_ARG3(__lasx_xvpermi_q, tmp2, tmp2, 0x20, tmp5, tmp5, 0x20, tmp2, tmp5); \
+                DUP2_ARG3(__lasx_xvpermi_q, tmp2, tmp2, 0x20, tmp5, tmp5, \
+                          0x20, tmp2, tmp5); \
                 tmp3 =  __lasx_xvpermi_q(tmp4, tmp3, 0x20); \
                 tmp2 = __lasx_xvsub_h(tmp2, tmp3); \
                 tmp5 = __lasx_xvsub_h(tmp5, tmp3); \
@@ -132,7 +133,8 @@ void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
     step  <<= 3;
     edges <<= 3;
 
-    H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv1, 1, -8, zero);
+    H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv1,
+                                             1, -8, zero);
     H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(32, 8, mask_mv0, 0, -1, one);
 
     DUP2_ARG2(__lasx_xvld, (int8_t*)bS, 0, (int8_t*)bS, 16, tmp0, tmp1);
diff --git a/libavcodec/loongarch/h264chroma_init_loongarch.c b/libavcodec/loongarch/h264chroma_init_loongarch.c
index 7f1aa4d848..0ca24ecc47 100644
--- a/libavcodec/loongarch/h264chroma_init_loongarch.c
+++ b/libavcodec/loongarch/h264chroma_init_loongarch.c
@@ -20,6 +20,7 @@
  */
 
 #include "h264chroma_lasx.h"
+#include "libavutil/attributes.h"
 #include "libavutil/loongarch/cpu.h"
 #include "libavcodec/h264chroma.h"
 
diff --git a/libavcodec/loongarch/h264dsp_init_loongarch.c b/libavcodec/loongarch/h264dsp_init_loongarch.c
index e02e3d7fa9..37633c3e51 100644
--- a/libavcodec/loongarch/h264dsp_init_loongarch.c
+++ b/libavcodec/loongarch/h264dsp_init_loongarch.c
@@ -38,18 +38,10 @@ av_cold void ff_h264dsp_init_loongarch(H264DSPContext *c, const int bit_depth,
             c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_8_lasx;
             c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_8_lasx;
             c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_8_lasx;
-//            c->h264_h_loop_filter_luma_mbaff = ff_h264_h_loop_filter_luma_mbaff_lasx;
-//            c->h264_h_loop_filter_luma_mbaff_intra = ff_h264_h_loop_filter_luma_mbaff_intra_lasx;
             c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_8_lasx;
 
             if (chroma_format_idc <= 1)
                 c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_8_lasx;
-//            else
-//                c->h264_h_loop_filter_chroma = ff_h264_h_loop_filter_chroma422_lasx;
-//
-//            if (chroma_format_idc > 1)
-//                c->h264_h_loop_filter_chroma_mbaff = ff_h264_h_loop_filter_chroma422_mbaff_lasx;
-//
             c->h264_v_loop_filter_chroma_intra = ff_h264_v_lpf_chroma_intra_8_lasx;
 
             if (chroma_format_idc <= 1)
diff --git a/libavcodec/loongarch/h264dsp_lasx.c b/libavcodec/loongarch/h264dsp_lasx.c
index b6f8b158e9..7fd4cedf7e 100644
--- a/libavcodec/loongarch/h264dsp_lasx.c
+++ b/libavcodec/loongarch/h264dsp_lasx.c
@@ -64,15 +64,16 @@
     q0_or_p0_out = __lasx_xvclip255_h(q0_or_p0_out);         \
 }
 
-void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                int alpha_in, int beta_in, int8_t *tc)
 {
-    int img_width_2x = img_width << 1;
-    int img_width_4x = img_width << 2;
-    int img_width_8x = img_width << 3;
-    int img_width_3x = img_width_2x + img_width;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_4x = img_width << 2;
+    ptrdiff_t img_width_8x = img_width << 3;
+    ptrdiff_t img_width_3x = img_width_2x + img_width;
     __m256i tmp_vec0, bs_vec;
-    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202, 0x0101010100000000, 0x0303030302020202};
+    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202,
+                      0x0101010100000000, 0x0303030302020202};
 
     tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
     tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
@@ -102,14 +103,16 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
                       src, img_width_3x, row4, row5, row6, row7);
             src -= img_width_4x;
             DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
-                      img_width_2x, src_tmp, img_width_3x, row8, row9, row10, row11);
+                      img_width_2x, src_tmp, img_width_3x,
+                      row8, row9, row10, row11);
             src_tmp += img_width_4x;
             DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
-                      img_width_2x, src_tmp, img_width_3x, row12, row13, row14, row15);
+                      img_width_2x, src_tmp, img_width_3x,
+                      row12, row13, row14, row15);
             src_tmp -= img_width_4x;
 
-            LASX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
-                                 row8, row9, row10, row11,
+            LASX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6,
+                                 row7, row8, row9, row10, row11,
                                  row12, row13, row14, row15,
                                  p3_org, p2_org, p1_org, p0_org,
                                  q0_org, q1_org, q2_org, q3_org);
@@ -186,8 +189,10 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
                 p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
             }
@@ -197,15 +202,17 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
                 __m256i control = {0x0000000400000000, 0x0000000500000001,
                                    0x0000000600000002, 0x0000000700000003};
 
-                DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org, 0x02,
-                          p2_org, q1_org, 0x02, p3_org, q0_org, 0x02, p0_org, p1_org,
-                          p2_org, p3_org);
-                DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org, row0, row2);
-                DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org, row1, row3);
+                DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org,
+                          q2_org, 0x02, p2_org, q1_org, 0x02, p3_org,
+                          q0_org, 0x02, p0_org, p1_org, p2_org, p3_org);
+                DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org,
+                          row0, row2);
+                DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org,
+                          row1, row3);
                 DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
                 DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
-                DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6, control,
-                          row7, control, row4, row5, row6, row7);
+                DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6,
+                          control, row7, control, row4, row5, row6, row7);
                 __lasx_xvstelm_d(row4, src, 0, 0);
                 __lasx_xvstelm_d(row4, src + img_width, 0, 1);
                 src += img_width_2x;
@@ -234,13 +241,14 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
     }
 }
 
-void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                    int alpha_in, int beta_in, int8_t *tc)
 {
-    int img_width_2x = img_width << 1;
-    int img_width_3x = img_width + img_width_2x;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_3x = img_width + img_width_2x;
     __m256i tmp_vec0, bs_vec;
-    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202, 0x0101010100000000, 0x0303030302020202};
+    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202,
+                      0x0101010100000000, 0x0303030302020202};
 
     tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
     tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
@@ -259,7 +267,8 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
         alpha = __lasx_xvreplgr2vr_b(alpha_in);
         beta  = __lasx_xvreplgr2vr_b(beta_in);
 
-        DUP2_ARG2(__lasx_xvldx, data, -img_width_3x, data, -img_width_2x, p2_org, p1_org);
+        DUP2_ARG2(__lasx_xvldx, data, -img_width_3x, data, -img_width_2x,
+                  p2_org, p1_org);
         p0_org = __lasx_xvldx(data, -img_width);
         DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
 
@@ -339,8 +348,10 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0Xd8, q0_h, 0xd8, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0Xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
                 p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
                 p0_org = __lasx_xvpermi_q(p0_org, p0_h, 0x30);
@@ -352,15 +363,15 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
     }
 }
 
-void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                  int alpha_in, int beta_in, int8_t *tc)
 {
     __m256i tmp_vec0, bs_vec;
     __m256i tc_vec = {0x0303020201010000, 0x0303020201010000, 0x0, 0x0};
     __m256i zero = __lasx_xvldi(0);
-    int img_width_2x = img_width << 1;
-    int img_width_4x = img_width << 2;
-    int img_width_3x = img_width_2x + img_width;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_4x = img_width << 2;
+    ptrdiff_t img_width_3x = img_width_2x + img_width;
 
     tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
     tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
@@ -388,8 +399,8 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
                       src, img_width_3x, row4, row5, row6, row7);
             src -= img_width_4x;
             /* LASX_TRANSPOSE8x4_B */
-            DUP4_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row6, row4, row7, row5,
-                      p1_org, p0_org, q0_org, q1_org);
+            DUP4_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row6, row4,
+                      row7, row5, p1_org, p0_org, q0_org, q1_org);
             row0 = __lasx_xvilvl_b(p0_org, p1_org);
             row1 = __lasx_xvilvl_b(q1_org, q0_org);
             row3 = __lasx_xvilvh_w(row1, row0);
@@ -431,8 +442,10 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
                 p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
             }
@@ -458,7 +471,7 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
     }
 }
 
-void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                  int alpha_in, int beta_in, int8_t *tc)
 {
     int img_width_2x = img_width << 1;
@@ -482,7 +495,8 @@ void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
         alpha = __lasx_xvreplgr2vr_b(alpha_in);
         beta  = __lasx_xvreplgr2vr_b(beta_in);
 
-        DUP2_ARG2(__lasx_xvldx, data, -img_width_2x, data, -img_width, p1_org, p0_org);
+        DUP2_ARG2(__lasx_xvldx, data, -img_width_2x, data, -img_width,
+                  p1_org, p0_org);
         DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
 
         is_bs_greater_than0 = __lasx_xvslt_bu(zero, bs_vec);
@@ -514,8 +528,10 @@ void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
-                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
                 p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
                 __lasx_xvstelm_d(p0_h, data - img_width, 0, 0);
@@ -565,12 +581,12 @@ void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
     p0_or_q0_out = __lasx_xvsrar_h(p0_or_q0_out, const2);              \
 }
 
-void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                      int alpha_in, int beta_in)
 {
-    int img_width_2x = img_width << 1;
-    int img_width_4x = img_width << 2;
-    int img_width_3x = img_width_2x + img_width;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_4x = img_width << 2;
+    ptrdiff_t img_width_3x = img_width_2x + img_width;
     uint8_t *src = data - 4;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -621,7 +637,8 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         __m256i less_alpha_shift2_add2 = __lasx_xvsrli_b(alpha, 2);
 
         less_alpha_shift2_add2 = __lasx_xvaddi_bu(less_alpha_shift2_add2, 2);
-        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0, less_alpha_shift2_add2);
+        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0,
+                                                 less_alpha_shift2_add2);
 
         p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
         p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
@@ -701,15 +718,17 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
             __m256i control = {0x0000000400000000, 0x0000000500000001,
                                0x0000000600000002, 0x0000000700000003};
 
-            DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org, 0x02,
-                      p2_org, q1_org, 0x02, p3_org, q0_org, 0x02, p0_org, p1_org, p2_org,
-                      p3_org);
-            DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org, row0, row2);
-            DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org, row1, row3);
+            DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org,
+                      0x02, p2_org, q1_org, 0x02, p3_org, q0_org, 0x02,
+                      p0_org, p1_org, p2_org, p3_org);
+            DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org,
+                      row0, row2);
+            DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org,
+                      row1, row3);
             DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
             DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
-            DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6, control,
-                      row7, control, row4, row5, row6, row7);
+            DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6,
+                      control, row7, control, row4, row5, row6, row7);
             src = data - 4;
             __lasx_xvstelm_d(row4, src, 0, 0);
             __lasx_xvstelm_d(row4, src + img_width, 0, 1);
@@ -738,19 +757,19 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
     }
 }
 
-void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                      int alpha_in, int beta_in)
 {
-    int img_width_2x = img_width << 1;
-    int img_width_3x = img_width_2x + img_width;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_3x = img_width_2x + img_width;
     uint8_t *src = data - img_width_2x;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
     __m256i p1_org, p0_org, q0_org, q1_org;
     __m256i zero = __lasx_xvldi(0);
 
-    DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x, src, img_width_3x,
-              p1_org, p0_org, q0_org, q1_org);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+              src, img_width_3x, p1_org, p0_org, q0_org, q1_org);
     alpha = __lasx_xvreplgr2vr_b(alpha_in);
     beta  = __lasx_xvreplgr2vr_b(beta_in);
     p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
@@ -771,7 +790,8 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         __m256i q2_org = __lasx_xvldx(data, img_width_2x);
         __m256i less_alpha_shift2_add2 = __lasx_xvsrli_b(alpha, 2);
         less_alpha_shift2_add2 = __lasx_xvaddi_bu(less_alpha_shift2_add2, 2);
-        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0, less_alpha_shift2_add2);
+        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0,
+                                                 less_alpha_shift2_add2);
 
         p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
         p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
@@ -857,13 +877,13 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
     }
 }
 
-void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                        int alpha_in, int beta_in)
 {
     uint8_t *src = data - 2;
-    int img_width_2x = img_width << 1;
-    int img_width_4x = img_width << 2;
-    int img_width_3x = img_width_2x + img_width;
+    ptrdiff_t img_width_2x = img_width << 1;
+    ptrdiff_t img_width_4x = img_width << 2;
+    ptrdiff_t img_width_3x = img_width_2x + img_width;
     __m256i p1_org, p0_org, q0_org, q1_org;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -937,10 +957,10 @@ void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
     __lasx_xvstelm_h(p0_org, src, 0, 7);
 }
 
-void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *data, ptrdiff_t img_width,
                                        int alpha_in, int beta_in)
 {
-    int img_width_2x = img_width << 1;
+    ptrdiff_t img_width_2x = img_width << 1;
     __m256i p1_org, p0_org, q0_org, q1_org;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -1046,8 +1066,10 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
     tmp6 = __lasx_xvsra_h(tmp6, denom);
     tmp7 = __lasx_xvsra_h(tmp7, denom);
 
-    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3,
+                                  tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7,
+                                  tmp4, tmp5, tmp6, tmp7);
     DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
               dst0, dst1, dst2, dst3);
     __lasx_xvstelm_d(dst0, dst, 0, 0);
@@ -1082,16 +1104,16 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
         DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
                   src, stride_3x, tmp4, tmp5, tmp6, tmp7);
         src += stride_4x;
-        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
-                  0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5,
+                  tmp4, 0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
         DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
                   dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
         dst += stride_4x;
         DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
                   dst, stride_3x, tmp4, tmp5, tmp6, tmp7);
         dst -= stride_4x;
-        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
-                  0x20, tmp7, tmp6, 0x20, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5,
+                  tmp4, 0x20, tmp7, tmp6, 0x20, dst0, dst1, dst2, dst3);
 
         DUP4_ARG2(__lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
@@ -1116,10 +1138,12 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
         tmp6 = __lasx_xvsra_h(tmp6, denom);
         tmp7 = __lasx_xvsra_h(tmp7, denom);
 
-        DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
-        DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                  dst0, dst1, dst2, dst3);
+        DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3,
+                                      tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7,
+                                      tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7,
+                  tmp6, dst0, dst1, dst2, dst3);
         __lasx_xvstelm_d(dst0, dst, 0, 0);
         __lasx_xvstelm_d(dst0, dst, 8, 1);
         dst += stride;
@@ -1177,7 +1201,8 @@ static void avc_biwgt_8x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
     vec0 = __lasx_xvilvl_b(dst0, src0);
     vec1 = __lasx_xvilvh_b(dst0, src0);
-    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1, tmp0, tmp1);
+    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+              tmp0, tmp1);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
     DUP2_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp0, tmp1);
@@ -1219,8 +1244,9 @@ static void avc_biwgt_8x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
               src, stride_3x, tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
     src1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
-    DUP4_ARG2(__lasx_xvld, dst_tmp, 0, dst_tmp + stride, 0, dst_tmp + stride_2x, 0,
-              dst_tmp + stride_3x, 0, tmp0, tmp1, tmp2, tmp3);
+    tmp0 = __lasx_xvld(dst_tmp, 0);
+    DUP2_ARG2(__lasx_xvldx, dst_tmp, stride, dst_tmp, stride_2x, tmp1, tmp2);
+    tmp3 = __lasx_xvldx(dst_tmp, stride_3x);
     dst_tmp += stride_4x;
     DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
     dst0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
@@ -1239,7 +1265,8 @@ static void avc_biwgt_8x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     tmp1 = __lasx_xvsra_h(tmp1, denom);
     tmp2 = __lasx_xvsra_h(tmp2, denom);
     tmp3 = __lasx_xvsra_h(tmp3, denom);
-    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3,
+                                  tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, dst0, dst1);
     __lasx_xvstelm_d(dst0, dst, 0, 0);
     __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
@@ -1334,8 +1361,10 @@ static void avc_biwgt_8x16_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     tmp5 = __lasx_xvsra_h(tmp5, denom);
     tmp6 = __lasx_xvsra_h(tmp6, denom);
     tmp7 = __lasx_xvsra_h(tmp7, denom);
-    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3,
+                                  tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7,
+                                  tmp4, tmp5, tmp6, tmp7);
     DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
                    dst0, dst1, dst2, dst3)
     __lasx_xvstelm_d(dst0, dst, 0, 0);
@@ -1493,7 +1522,8 @@ static void avc_biwgt_4x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
     vec0 = __lasx_xvilvl_b(dst0, src0);
     vec1 = __lasx_xvilvh_b(dst0, src0);
-    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1, tmp0, tmp1);
+    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+              tmp0, tmp1);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
     DUP2_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp0, tmp1);
@@ -1515,14 +1545,14 @@ void ff_biweight_h264_pixels4_8_lasx(uint8_t *dst, uint8_t *src,
                                      int weight_src, int offset)
 {
     if (2 == height) {
-        avc_biwgt_4x2_lasx(src, dst, stride, log2_denom, weight_src, weight_dst,
-                           offset);
+        avc_biwgt_4x2_lasx(src, dst, stride, log2_denom, weight_src,
+                           weight_dst, offset);
     } else if (4 == height) {
-        avc_biwgt_4x4_lasx(src, dst, stride, log2_denom, weight_src, weight_dst,
-                           offset);
+        avc_biwgt_4x4_lasx(src, dst, stride, log2_denom, weight_src,
+                           weight_dst, offset);
     } else {
-        avc_biwgt_4x8_lasx(src, dst, stride, log2_denom, weight_src, weight_dst,
-                           offset);
+        avc_biwgt_4x8_lasx(src, dst, stride, log2_denom, weight_src,
+                           weight_dst, offset);
     }
 }
 
@@ -1618,8 +1648,8 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
         DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
                   src, stride_3x, tmp4, tmp5, tmp6, tmp7);
         src -= stride_4x;
-        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
-                  0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5,
+                  tmp4, 0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
         DUP4_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, zero, src2,
                   zero, src3, src0_l, src1_l, src2_l, src3_l);
         DUP4_ARG2(__lasx_xvilvh_b, zero, src0, zero, src1, zero, src2,
@@ -1632,10 +1662,10 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
         src2_h = __lasx_xvmul_h(wgt, src2_h);
         src3_l = __lasx_xvmul_h(wgt, src3_l);
         src3_h = __lasx_xvmul_h(wgt, src3_h);
-        DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l, offset,
-                  src1_h, offset, src0_l, src0_h, src1_l, src1_h);
-        DUP4_ARG2(__lasx_xvsadd_h, src2_l, offset, src2_h, offset, src3_l, offset,
-                  src3_h, offset, src2_l, src2_h, src3_l, src3_h);
+        DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l,
+                  offset, src1_h, offset, src0_l, src0_h, src1_l, src1_h);
+        DUP4_ARG2(__lasx_xvsadd_h, src2_l, offset, src2_h, offset, src3_l,
+                  offset, src3_h, offset, src2_l, src2_h, src3_l, src3_h);
         src0_l = __lasx_xvmaxi_h(src0_l, 0);
         src0_h = __lasx_xvmaxi_h(src0_h, 0);
         src1_l = __lasx_xvmaxi_h(src1_l, 0);
diff --git a/libavcodec/loongarch/h264dsp_lasx.h b/libavcodec/loongarch/h264dsp_lasx.h
index f3dcc0f3e3..4cf813750b 100644
--- a/libavcodec/loongarch/h264dsp_lasx.h
+++ b/libavcodec/loongarch/h264dsp_lasx.h
@@ -25,21 +25,21 @@
 
 #include "libavcodec/h264dec.h"
 
-void ff_h264_h_lpf_luma_8_lasx(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_8_lasx(uint8_t *src, ptrdiff_t stride,
                                int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_luma_8_lasx(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_8_lasx(uint8_t *src, ptrdiff_t stride,
                                int alpha, int beta, int8_t *tc0);
-void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *src, ptrdiff_t stride,
                                      int alpha, int beta);
-void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *src, ptrdiff_t stride,
                                      int alpha, int beta);
-void ff_h264_h_lpf_chroma_8_lasx(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_8_lasx(uint8_t *src, ptrdiff_t stride,
                                  int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_chroma_8_lasx(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_8_lasx(uint8_t *src, ptrdiff_t stride,
                                  int alpha, int beta, int8_t *tc0);
-void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *src, ptrdiff_t stride,
                                        int alpha, int beta);
-void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *src, ptrdiff_t stride,
                                        int alpha, int beta);
 void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
                                       ptrdiff_t stride, int height,
@@ -62,32 +62,29 @@ void ff_weight_h264_pixels8_8_lasx(uint8_t *src, ptrdiff_t stride,
 void ff_weight_h264_pixels4_8_lasx(uint8_t *src, ptrdiff_t stride,
                                    int height, int log2_denom,
                                    int weight_src, int offset);
+void ff_h264_add_pixels4_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
+
+void ff_h264_add_pixels8_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
 void ff_h264_idct_add_lasx(uint8_t *dst, int16_t *src, int32_t dst_stride);
-void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
-                               int32_t dst_stride);
+void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src, int32_t dst_stride);
 void ff_h264_idct4x4_addblk_dc_lasx(uint8_t *dst, int16_t *src,
                                     int32_t dst_stride);
 void ff_h264_idct8_dc_addblk_lasx(uint8_t *dst, int16_t *src,
                                   int32_t dst_stride);
-void ff_h264_idct_add16_lasx(uint8_t *dst,
-                             const int32_t *blk_offset,
+void ff_h264_idct_add16_lasx(uint8_t *dst, const int32_t *blk_offset,
                              int16_t *block, int32_t dst_stride,
                              const uint8_t nzc[15 * 8]);
 void ff_h264_idct8_add4_lasx(uint8_t *dst, const int32_t *blk_offset,
                              int16_t *block, int32_t dst_stride,
                              const uint8_t nzc[15 * 8]);
-void ff_h264_idct_add8_lasx(uint8_t **dst,
-                            const int32_t *blk_offset,
+void ff_h264_idct_add8_lasx(uint8_t **dst, const int32_t *blk_offset,
                             int16_t *block, int32_t dst_stride,
                             const uint8_t nzc[15 * 8]);
-void ff_h264_idct_add8_422_lasx(uint8_t **dst,
-                                const int32_t *blk_offset,
+void ff_h264_idct_add8_422_lasx(uint8_t **dst, const int32_t *blk_offset,
                                 int16_t *block, int32_t dst_stride,
                                 const uint8_t nzc[15 * 8]);
-void ff_h264_idct_add16_intra_lasx(uint8_t *dst,
-                                   const int32_t *blk_offset,
-                                   int16_t *block,
-                                   int32_t dst_stride,
+void ff_h264_idct_add16_intra_lasx(uint8_t *dst, const int32_t *blk_offset,
+                                   int16_t *block, int32_t dst_stride,
                                    const uint8_t nzc[15 * 8]);
 void ff_h264_deq_idct_luma_dc_lasx(int16_t *dst, int16_t *src,
                                    int32_t de_qval);
@@ -97,7 +94,4 @@ void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
                                        int bidir, int edges, int step,
                                        int mask_mv0, int mask_mv1, int field);
 
-void ff_h264_add_pixels4_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
-
-void ff_h264_add_pixels8_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
 #endif  // #ifndef AVCODEC_LOONGARCH_H264DSP_LASX_H
diff --git a/libavcodec/loongarch/h264idct_lasx.c b/libavcodec/loongarch/h264idct_lasx.c
index 88dc56ef2c..46bd3b74d5 100644
--- a/libavcodec/loongarch/h264idct_lasx.c
+++ b/libavcodec/loongarch/h264idct_lasx.c
@@ -91,8 +91,10 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
 
     src[0] += 32;
-    DUP4_ARG2(__lasx_xvld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
-    DUP4_ARG2(__lasx_xvld, src, 64, src, 80, src, 96, src, 112, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvld, src, 0, src, 16, src, 32, src, 48,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 64, src, 80, src, 96, src, 112,
+              src4, src5, src6, src7);
     __lasx_xvst(zero, src, 0);
     __lasx_xvst(zero, src, 32);
     __lasx_xvst(zero, src, 64);
@@ -141,8 +143,10 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     LASX_TRANSPOSE8x8_H(res0, res1, res2, res3, res4, res5, res6, res7,
                         res0, res1, res2, res3, res4, res5, res6, res7);
 
-    DUP4_ARG1(__lasx_vext2xv_w_h, res0, res1, res2, res3, tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG1(__lasx_vext2xv_w_h, res4, res5, res6, res7, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG1(__lasx_vext2xv_w_h, res0, res1, res2, res3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_vext2xv_w_h, res4, res5, res6, res7,
+              tmp4, tmp5, tmp6, tmp7);
     vec0 = __lasx_xvadd_w(tmp0, tmp4);
     vec1 = __lasx_xvsub_w(tmp0, tmp4);
 
@@ -188,10 +192,12 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     LASX_BUTTERFLY_4_W(tmp0, tmp2, tmp5, tmp7, res0, res1, res6, res7);
     LASX_BUTTERFLY_4_W(tmp4, tmp6, tmp1, tmp3, res2, res3, res4, res5);
 
-    DUP4_ARG2(__lasx_xvsrai_w, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1, res2, res3);
-    DUP4_ARG2(__lasx_xvsrai_w, res4, 6, res5, 6, res6, 6, res7, 6, res4, res5, res6, res7);
-    DUP4_ARG2(__lasx_xvpickev_h, res1, res0, res3, res2, res5, res4, res7, res6,
+    DUP4_ARG2(__lasx_xvsrai_w, res0, 6, res1, 6, res2, 6, res3, 6,
               res0, res1, res2, res3);
+    DUP4_ARG2(__lasx_xvsrai_w, res4, 6, res5, 6, res6, 6, res7, 6,
+              res4, res5, res6, res7);
+    DUP4_ARG2(__lasx_xvpickev_h, res1, res0, res3, res2, res5, res4, res7,
+              res6, res0, res1, res2, res3);
     DUP4_ARG2(__lasx_xvpermi_d, res0, 0xd8, res1, 0xd8, res2, 0xd8, res3, 0xd8,
               res0, res1, res2, res3);
 
@@ -205,13 +211,14 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
               dst0, dst1, dst2, dst3);
     DUP4_ARG2(__lasx_xvilvl_b, zero, dst4, zero, dst5, zero, dst6, zero, dst7,
               dst4, dst5, dst6, dst7);
-    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5, dst4, 0x20,
-              dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5,
+              dst4, 0x20, dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
     res0 = __lasx_xvadd_h(res0, dst0);
     res1 = __lasx_xvadd_h(res1, dst1);
     res2 = __lasx_xvadd_h(res2, dst2);
     res3 = __lasx_xvadd_h(res3, dst3);
-    DUP4_ARG1(__lasx_xvclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
+    DUP4_ARG1(__lasx_xvclip255_h, res0, res1, res2, res3, res0, res1,
+              res2, res3);
     DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, res0, res1);
     __lasx_xvstelm_d(res0, dst, 0, 0);
     __lasx_xvstelm_d(res0, dst + dst_stride, 0, 2);
@@ -270,15 +277,18 @@ void ff_h264_idct8_dc_addblk_lasx(uint8_t *dst, int16_t *src,
     DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
               dst, dst_stride_3x, dst4, dst5, dst6, dst7);
     dst -= dst_stride_4x;
-    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst0, dst1, dst2, dst3, dst0, dst1, dst2, dst3);
-    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst4, dst5, dst6, dst7, dst4, dst5, dst6, dst7);
-    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5, dst4, 0x20,
-              dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst0, dst1, dst2, dst3,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst4, dst5, dst6, dst7,
+              dst4, dst5, dst6, dst7);
+    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5,
+              dst4, 0x20, dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
     dst0 = __lasx_xvadd_h(dst0, dc);
     dst1 = __lasx_xvadd_h(dst1, dc);
     dst2 = __lasx_xvadd_h(dst2, dc);
     dst3 = __lasx_xvadd_h(dst3, dc);
-    DUP4_ARG1(__lasx_xvclip255_h, dst0, dst1, dst2, dst3, dst0, dst1, dst2, dst3);
+    DUP4_ARG1(__lasx_xvclip255_h, dst0, dst1, dst2, dst3,
+              dst0, dst1, dst2, dst3);
     DUP2_ARG2(__lasx_xvpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
     __lasx_xvstelm_d(dst0, dst, 0, 0);
     __lasx_xvstelm_d(dst0, dst + dst_stride, 0, 2);
@@ -446,15 +456,19 @@ void ff_h264_deq_idct_luma_dc_lasx(int16_t *dst, int16_t *src,
     __m256i vres0, vres1, vres2, vres3;
     __m256i de_q_vec = __lasx_xvreplgr2vr_w(de_qval);
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src, 8, src, 16, src, 24, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src, 8, src, 16, src, 24,
+              src0, src1, src2, src3);
     LASX_TRANSPOSE4x4_H(src0, src1, src2, src3, tmp0, tmp1, tmp2, tmp3);
     LASX_BUTTERFLY_4_H(tmp0, tmp2, tmp3, tmp1, vec0, vec3, vec2, vec1);
     LASX_BUTTERFLY_4_H(vec0, vec1, vec2, vec3, hres0, hres3, hres2, hres1);
-    LASX_TRANSPOSE4x4_H(hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3);
+    LASX_TRANSPOSE4x4_H(hres0, hres1, hres2, hres3,
+                        hres0, hres1, hres2, hres3);
     LASX_BUTTERFLY_4_H(hres0, hres1, hres3, hres2, vec0, vec3, vec2, vec1);
     LASX_BUTTERFLY_4_H(vec0, vec1, vec2, vec3, vres0, vres1, vres2, vres3);
-    DUP4_ARG1(__lasx_vext2xv_w_h, vres0, vres1, vres2, vres3, vres0, vres1, vres2, vres3);
-    DUP2_ARG3(__lasx_xvpermi_q, vres1, vres0, 0x20, vres3, vres2, 0x20, vres0, vres1);
+    DUP4_ARG1(__lasx_vext2xv_w_h, vres0, vres1, vres2, vres3,
+              vres0, vres1, vres2, vres3);
+    DUP2_ARG3(__lasx_xvpermi_q, vres1, vres0, 0x20, vres3, vres2, 0x20,
+              vres0, vres1);
 
     vres0 = __lasx_xvmul_w(vres0, de_q_vec);
     vres1 = __lasx_xvmul_w(vres1, de_q_vec);
diff --git a/libavcodec/loongarch/h264qpel_init_loongarch.c b/libavcodec/loongarch/h264qpel_init_loongarch.c
index b98b2dd793..969c9c376c 100644
--- a/libavcodec/loongarch/h264qpel_init_loongarch.c
+++ b/libavcodec/loongarch/h264qpel_init_loongarch.c
@@ -20,6 +20,7 @@
  */
 
 #include "h264qpel_lasx.h"
+#include "libavutil/attributes.h"
 #include "libavutil/loongarch/cpu.h"
 #include "libavcodec/h264qpel.h"
 
@@ -34,6 +35,7 @@ av_cold void ff_h264qpel_init_loongarch(H264QpelContext *c, int bit_depth)
             c->put_h264_qpel_pixels_tab[0][3]  = ff_put_h264_qpel16_mc30_lasx;
             c->put_h264_qpel_pixels_tab[0][4]  = ff_put_h264_qpel16_mc01_lasx;
             c->put_h264_qpel_pixels_tab[0][5]  = ff_put_h264_qpel16_mc11_lasx;
+
             c->put_h264_qpel_pixels_tab[0][6]  = ff_put_h264_qpel16_mc21_lasx;
             c->put_h264_qpel_pixels_tab[0][7]  = ff_put_h264_qpel16_mc31_lasx;
             c->put_h264_qpel_pixels_tab[0][8]  = ff_put_h264_qpel16_mc02_lasx;
diff --git a/libavcodec/loongarch/h264qpel_lasx.c b/libavcodec/loongarch/h264qpel_lasx.c
index 833cb07027..1c142e510e 100644
--- a/libavcodec/loongarch/h264qpel_lasx.c
+++ b/libavcodec/loongarch/h264qpel_lasx.c
@@ -62,17 +62,17 @@ static const uint8_t luma_mask_arr[16 * 6] __attribute__((aligned(0x40))) = {
 } )
 
 static av_always_inline
-void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
-                                             const uint8_t *src_y,
-                                             uint8_t *dst, int32_t stride)
+void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(uint8_t *src_x,
+                                             uint8_t *src_y,
+                                             uint8_t *dst, ptrdiff_t stride)
 {
     const int16_t filt_const0 = 0xfb01;
     const int16_t filt_const1 = 0x1414;
     const int16_t filt_const2 = 0x1fb;
     uint32_t loop_cnt;
-    int32_t stride_2x = stride << 1;
-    int32_t stride_3x = stride_2x + stride;
-    int32_t stride_4x = stride << 2;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tmp0, tmp1;
     __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
     __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
@@ -91,24 +91,26 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
     mask0 = __lasx_xvld(luma_mask_arr, 0);
     DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
     src_vt0 = __lasx_xvld(src_y, 0);
-    DUP4_ARG2(__lasx_xvld, src_y + stride, 0, src_y + stride_2x, 0, src_y + stride_3x, 0,
-              src_y + stride_4x, 0, src_vt1, src_vt2, src_vt3, src_vt4);
-    src_y += stride_4x + stride;
+    DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x, src_y, stride_3x,
+              src_y, stride_4x, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x;
 
     src_vt0 = __lasx_xvxori_b(src_vt0, 128);
-    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128, src_vt4, 128,
-              src_vt1, src_vt2, src_vt3, src_vt4);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128,
+              src_vt4, 128, src_vt1, src_vt2, src_vt3, src_vt4);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lasx_xvld, src_x, 0, src_x + stride, 0, src_x + stride_2x, 0,
-                  src_x + stride_3x, 0, src_hz0, src_hz1, src_hz2, src_hz3);
+        src_hz0 = __lasx_xvld(src_x, 0);
+        DUP2_ARG2(__lasx_xvldx, src_x, stride, src_x, stride_2x,
+                  src_hz1, src_hz2);
+        src_hz3 = __lasx_xvldx(src_x, stride_3x);
         src_x  += stride_4x;
         src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
         src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
         src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
         src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
-        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128, src_hz3,
-                  128, src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128,
+                  src_hz3, 128, src_hz0, src_hz1, src_hz2, src_hz3);
 
         hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
         hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
@@ -117,27 +119,30 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
         hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
         hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
 
-        DUP4_ARG2(__lasx_xvld, src_y, 0, src_y + stride, 0, src_y + stride_2x, 0,
-                  src_y + stride_3x, 0, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x,
+                  src_y, stride_3x, src_y, stride_4x,
+                  src_vt5, src_vt6, src_vt7, src_vt8);
         src_y += stride_4x;
 
-        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128, src_vt8,
-                  128, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128,
+                  src_vt8, 128, src_vt5, src_vt6, src_vt7, src_vt8);
 
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5, 0x02,
-                  src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02, src_vt0, src_vt1,
-                  src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5,
+                  0x02, src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
         src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
-        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                  src_vt87_h, src_vt3, src_hz0, src_hz1, src_hz2, src_hz3);
-        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                  src_vt87_h, src_vt3, src_vt0, src_vt1, src_vt2, src_vt3);
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1, 0x02,
-                  src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02, src_vt10_h, src_vt21_h,
-                  src_vt32_h, src_vt43_h);
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1, 0x13,
-                  src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13, src_vt54_h, src_vt65_h,
-                  src_vt76_h, src_vt87_h);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1,
+                  0x02, src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02,
+                  src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1,
+                  0x13, src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13,
+                  src_vt54_h, src_vt65_h, src_vt76_h, src_vt87_h);
         vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0,
                                  filt1, filt2);
         vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0,
@@ -149,14 +154,17 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
         vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
         vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
 
-        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
-        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out1, out3);
         tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
         tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
 
         DUP2_ARG2(__lasx_xvxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-        DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
-                  out0, out1, out2, out3);
+        out0 = __lasx_xvld(dst, 0);
+        DUP2_ARG2(__lasx_xvldx, dst, stride, dst, stride_2x, out1, out2);
+        out3 = __lasx_xvldx(dst, stride_3x);
         out0 = __lasx_xvpermi_q(out0, out2, 0x02);
         out1 = __lasx_xvpermi_q(out1, out3, 0x02);
         out2 = __lasx_xvilvl_d(out1, out0);
@@ -186,16 +194,16 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
 }
 
 static av_always_inline void
-avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *dst,
-                           int32_t stride)
+avc_luma_hv_qrt_16x16_lasx(uint8_t *src_x, uint8_t *src_y,
+                           uint8_t *dst, ptrdiff_t stride)
 {
     const int16_t filt_const0 = 0xfb01;
     const int16_t filt_const1 = 0x1414;
     const int16_t filt_const2 = 0x1fb;
     uint32_t loop_cnt;
-    int32_t stride_2x = stride << 1;
-    int32_t stride_3x = stride_2x + stride;
-    int32_t stride_4x = stride << 2;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tmp0, tmp1;
     __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
     __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
@@ -214,24 +222,26 @@ avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *
     mask0 = __lasx_xvld(luma_mask_arr, 0);
     DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
     src_vt0 = __lasx_xvld(src_y, 0);
-    DUP4_ARG2(__lasx_xvld, src_y + stride, 0, src_y + stride_2x, 0, src_y + stride_3x,
-              0, src_y + stride_4x, 0, src_vt1, src_vt2, src_vt3, src_vt4);
-    src_y += stride_4x + stride;
+    DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x, src_y, stride_3x,
+              src_y, stride_4x, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x;
 
     src_vt0 = __lasx_xvxori_b(src_vt0, 128);
-    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128, src_vt4, 128,
-              src_vt1, src_vt2, src_vt3, src_vt4);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128,
+              src_vt4, 128, src_vt1, src_vt2, src_vt3, src_vt4);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lasx_xvld, src_x, 0, src_x + stride, 0, src_x + stride_2x, 0,
-                  src_x + stride_3x, 0, src_hz0, src_hz1, src_hz2, src_hz3);
+        src_hz0 = __lasx_xvld(src_x, 0);
+        DUP2_ARG2(__lasx_xvldx, src_x, stride, src_x, stride_2x,
+                  src_hz1, src_hz2);
+        src_hz3 = __lasx_xvldx(src_x, stride_3x);
         src_x  += stride_4x;
         src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
         src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
         src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
         src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
-        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128, src_hz3,
-                  128, src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128,
+                  src_hz3, 128, src_hz0, src_hz1, src_hz2, src_hz3);
 
         hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
         hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
@@ -240,36 +250,45 @@ avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *
         hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
         hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
 
-        DUP4_ARG2(__lasx_xvld, src_y, 0, src_y + stride, 0, src_y + stride_2x, 0,
-                  src_y + stride_3x, 0, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x,
+                  src_y, stride_3x, src_y, stride_4x,
+                  src_vt5, src_vt6, src_vt7, src_vt8);
         src_y += stride_4x;
 
-        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128, src_vt8,
-                  128, src_vt5, src_vt6, src_vt7, src_vt8);
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5, 0x02,
-                  src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02, src_vt0, src_vt1,
-                  src_vt2, src_vt3);
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128,
+                  src_vt8, 128, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5,
+                  0x02, src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
         src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
-        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                  src_vt87_h, src_vt3, src_hz0, src_hz1, src_hz2, src_hz3);
-        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                  src_vt87_h, src_vt3, src_vt0, src_vt1, src_vt2, src_vt3);
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1, 0x02,
-                  src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02, src_vt10_h, src_vt21_h,
-                  src_vt32_h, src_vt43_h);
-        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1, 0x13,
-                  src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13, src_vt54_h, src_vt65_h,
-                  src_vt76_h, src_vt87_h);
-
-        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0, filt1, filt2);
-        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0, filt1, filt2);
-        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h, filt0, filt1, filt2);
-        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h, filt0, filt1, filt2);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1,
+                  src_hz1, 0x02, src_vt2, src_hz2, 0x02, src_vt3, src_hz3,
+                  0x02, src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1,
+                  src_hz1, 0x13, src_vt2, src_hz2, 0x13, src_vt3, src_hz3,
+                  0x13, src_vt54_h, src_vt65_h, src_vt76_h, src_vt87_h);
+
+        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h,
+                                 filt0, filt1, filt2);
+        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h,
+                                 filt0, filt1, filt2);
+        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h,
+                                 filt0, filt1, filt2);
+        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h,
+                                 filt0, filt1, filt2);
         vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
         vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
 
-        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
-        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out1, out3);
         tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
         tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
 
@@ -298,45 +317,39 @@ static av_always_inline void
 put_pixels8_8_inline_asm(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
 {
     uint64_t tmp[8];
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        "ld.d       %[tmp0],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp1],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp2],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp3],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp4],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp5],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp6],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp7],    %[src],    0x0         \n\t"
-
-        "st.d       %[tmp0],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp1],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp2],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp3],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp4],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp5],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp6],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp7],    %[dst],    0x0         \n\t"
-        : [tmp0]"=&r"(tmp[0]),        [tmp1]"=&r"(tmp[1]),
-          [tmp2]"=&r"(tmp[2]),        [tmp3]"=&r"(tmp[3]),
-          [tmp4]"=&r"(tmp[4]),        [tmp5]"=&r"(tmp[5]),
-          [tmp6]"=&r"(tmp[6]),        [tmp7]"=&r"(tmp[7]),
-          [dst]"+&r"(dst),            [src]"+&r"(src)
-        : [stride]"r"(stride)
-        : "memory"
+    "slli.d     %[stride_2],     %[stride],   1           \n\t"
+    "add.d      %[stride_3],     %[stride_2], %[stride]   \n\t"
+    "slli.d     %[stride_4],     %[stride_2], 1           \n\t"
+    "ld.d       %[tmp0],         %[src],      0x0         \n\t"
+    "ldx.d      %[tmp1],         %[src],      %[stride]   \n\t"
+    "ldx.d      %[tmp2],         %[src],      %[stride_2] \n\t"
+    "ldx.d      %[tmp3],         %[src],      %[stride_3] \n\t"
+    "add.d      %[src],          %[src],      %[stride_4] \n\t"
+    "ld.d       %[tmp4],         %[src],      0x0         \n\t"
+    "ldx.d      %[tmp5],         %[src],      %[stride]   \n\t"
+    "ldx.d      %[tmp6],         %[src],      %[stride_2] \n\t"
+    "ldx.d      %[tmp7],         %[src],      %[stride_3] \n\t"
+
+    "st.d       %[tmp0],         %[dst],      0x0         \n\t"
+    "stx.d      %[tmp1],         %[dst],      %[stride]   \n\t"
+    "stx.d      %[tmp2],         %[dst],      %[stride_2] \n\t"
+    "stx.d      %[tmp3],         %[dst],      %[stride_3] \n\t"
+    "add.d      %[dst],          %[dst],      %[stride_4] \n\t"
+    "st.d       %[tmp4],         %[dst],      0x0         \n\t"
+    "stx.d      %[tmp5],         %[dst],      %[stride]   \n\t"
+    "stx.d      %[tmp6],         %[dst],      %[stride_2] \n\t"
+    "stx.d      %[tmp7],         %[dst],      %[stride_3] \n\t"
+    : [tmp0]"=&r"(tmp[0]),        [tmp1]"=&r"(tmp[1]),
+      [tmp2]"=&r"(tmp[2]),        [tmp3]"=&r"(tmp[3]),
+      [tmp4]"=&r"(tmp[4]),        [tmp5]"=&r"(tmp[5]),
+      [tmp6]"=&r"(tmp[6]),        [tmp7]"=&r"(tmp[7]),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),
+      [dst]"+&r"(dst),            [src]"+&r"(src)
+    : [stride]"r"(stride)
+    : "memory"
     );
 }
 
@@ -347,67 +360,61 @@ static av_always_inline void
 avg_pixels8_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
 {
     uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr1,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr2,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr3,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr4,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr5,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr6,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr7,    %[src],  0          \n\t"
-
-        "vld     $vr8,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr9,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr10,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr11,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr12,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr13,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr14,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr15,   %[tmp],  0          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0       \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1       \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2       \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3       \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4       \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5       \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6       \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7       \n\t"
-
-        "vstelm.d  $vr0,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr1,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr2,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr3,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr4,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr5,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr6,  %[dst],  0,  0      \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vstelm.d  $vr7,  %[dst],  0,  0      \n\t"
-        : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src)
-        : [stride]"r"(stride)
-        : "memory"
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[stride],   1           \n\t"
+    "add.d      %[stride_3],     %[stride_2], %[stride]   \n\t"
+    "slli.d     %[stride_4],     %[stride_2], 1           \n\t"
+    "vld        $vr0,            %[src],      0           \n\t"
+    "vldx       $vr1,            %[src],      %[stride]   \n\t"
+    "vldx       $vr2,            %[src],      %[stride_2] \n\t"
+    "vldx       $vr3,            %[src],      %[stride_3] \n\t"
+    "add.d      %[src],          %[src],      %[stride_4] \n\t"
+    "vld        $vr4,            %[src],      0           \n\t"
+    "vldx       $vr5,            %[src],      %[stride]   \n\t"
+    "vldx       $vr6,            %[src],      %[stride_2] \n\t"
+    "vldx       $vr7,            %[src],      %[stride_3] \n\t"
+
+    "vld        $vr8,            %[tmp],      0           \n\t"
+    "vldx       $vr9,            %[tmp],      %[stride]   \n\t"
+    "vldx       $vr10,           %[tmp],      %[stride_2] \n\t"
+    "vldx       $vr11,           %[tmp],      %[stride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],      %[stride_4] \n\t"
+    "vld        $vr12,           %[tmp],      0           \n\t"
+    "vldx       $vr13,           %[tmp],      %[stride]   \n\t"
+    "vldx       $vr14,           %[tmp],      %[stride_2] \n\t"
+    "vldx       $vr15,           %[tmp],      %[stride_3] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,        $vr0        \n\t"
+    "vavgr.bu    $vr1,           $vr9,        $vr1        \n\t"
+    "vavgr.bu    $vr2,           $vr10,       $vr2        \n\t"
+    "vavgr.bu    $vr3,           $vr11,       $vr3        \n\t"
+    "vavgr.bu    $vr4,           $vr12,       $vr4        \n\t"
+    "vavgr.bu    $vr5,           $vr13,       $vr5        \n\t"
+    "vavgr.bu    $vr6,           $vr14,       $vr6        \n\t"
+    "vavgr.bu    $vr7,           $vr15,       $vr7        \n\t"
+
+    "vstelm.d    $vr0,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr1,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr2,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr3,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr4,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr5,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr6,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr7,           %[dst],      0,  0       \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
     );
 }
 
@@ -418,60 +425,60 @@ static av_always_inline void
 put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
                      ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],   0             \n\t"
-
-        "vld     $vr8,    %[half],  0x00          \n\t"
-        "vld     $vr9,    %[half],  0x08          \n\t"
-        "vld     $vr10,   %[half],  0x10          \n\t"
-        "vld     $vr11,   %[half],  0x18          \n\t"
-        "vld     $vr12,   %[half],  0x20          \n\t"
-        "vld     $vr13,   %[half],  0x28          \n\t"
-        "vld     $vr14,   %[half],  0x30          \n\t"
-        "vld     $vr15,   %[half],  0x38          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,     $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,     $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,    $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,    $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,    $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,    $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,    $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,    $vr7          \n\t"
-
-        "vstelm.d  $vr0,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr1,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr2,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr3,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr4,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr5,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr6,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr7,  %[dst],   0,  0         \n\t"
-        : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src)
-        : [srcStride]"r"(srcStride), [dstStride]"r"(dstStride)
-        : "memory"
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x08         \n\t"
+    "vld        $vr10,           %[half],        0x10         \n\t"
+    "vld        $vr11,           %[half],        0x18         \n\t"
+    "vld        $vr12,           %[half],        0x20         \n\t"
+    "vld        $vr13,           %[half],        0x28         \n\t"
+    "vld        $vr14,           %[half],        0x30         \n\t"
+    "vld        $vr15,           %[half],        0x38         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vstelm.d   $vr0,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr1,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr2,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr3,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr4,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr5,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr6,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr7,            %[dst],         0,  0        \n\t"
+    : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [srcStride]"r"(srcStride), [dstStride]"r"(dstStride)
+    : "memory"
     );
 }
 
@@ -483,85 +490,82 @@ avg_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
                      ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
     uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],   0             \n\t"
-        "add.d   %[src],  %[src],   %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],   0             \n\t"
-
-        "vld     $vr8,    %[half],  0x00          \n\t"
-        "vld     $vr9,    %[half],  0x08          \n\t"
-        "vld     $vr10,   %[half],  0x10          \n\t"
-        "vld     $vr11,   %[half],  0x18          \n\t"
-        "vld     $vr12,   %[half],  0x20          \n\t"
-        "vld     $vr13,   %[half],  0x28          \n\t"
-        "vld     $vr14,   %[half],  0x30          \n\t"
-        "vld     $vr15,   %[half],  0x38          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,     $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,     $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,    $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,    $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,    $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,    $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,    $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,    $vr7          \n\t"
-
-        "vld     $vr8,    %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr9,    %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr10,   %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr11,   %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr12,   %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr13,   %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr14,   %[tmp],   0             \n\t"
-        "add.d   %[tmp],  %[tmp],   %[dstStride]  \n\t"
-        "vld     $vr15,   %[tmp],   0             \n\t"
-
-        "vavgr.bu $vr0,   $vr8,     $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,     $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,    $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,    $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,    $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,    $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,    $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,    $vr7          \n\t"
-
-        "vstelm.d  $vr0,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr1,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr2,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr3,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr4,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr5,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr6,  %[dst],   0,  0         \n\t"
-        "add.d   %[dst],  %[dst],   %[dstStride]  \n\t"
-        "vstelm.d  $vr7,  %[dst],   0,  0         \n\t"
-        : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half), [src]"+&r"(src)
-        : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
-        : "memory"
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x08         \n\t"
+    "vld        $vr10,           %[half],        0x10         \n\t"
+    "vld        $vr11,           %[half],        0x18         \n\t"
+    "vld        $vr12,           %[half],        0x20         \n\t"
+    "vld        $vr13,           %[half],        0x28         \n\t"
+    "vld        $vr14,           %[half],        0x30         \n\t"
+    "vld        $vr15,           %[half],        0x38         \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "slli.d     %[stride_2],     %[dstStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[dstStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vstelm.d    $vr0,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr1,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr2,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr3,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr4,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr5,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr6,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr7,           %[dst],         0,  0        \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half),
+      [src]"+&r"(src), [stride_2]"=&r"(stride_2),
+      [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
     );
 }
 
@@ -569,75 +573,57 @@ avg_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
 static av_always_inline void
 put_pixels16_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
 {
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        "vld     $vr0,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr1,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr2,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr3,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr4,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr5,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr6,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr7,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-
-        "vst     $vr0,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr1,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr2,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr3,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr4,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr5,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr6,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr7,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-
-        "vld     $vr0,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr1,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr2,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr3,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr4,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr5,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr6,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr7,    %[src],  0          \n\t"
-
-        "vst     $vr0,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr1,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr2,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr3,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr4,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr5,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr6,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr7,    %[dst],  0          \n\t"
-        : [dst]"+&r"(dst),            [src]"+&r"(src)
-        : [stride]"r"(stride)
-        : "memory"
+    "slli.d     %[stride_2],     %[stride],      1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[stride]    \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    : [dst]"+&r"(dst),            [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
     );
 }
 
@@ -648,128 +634,98 @@ static av_always_inline void
 avg_pixels16_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
 {
     uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr1,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr2,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr3,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr4,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr5,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr6,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr7,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-
-        "vld     $vr8,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr9,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr10,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr11,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr12,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr13,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr14,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr15,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0       \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1       \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2       \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3       \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4       \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5       \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6       \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7       \n\t"
-
-        "vst     $vr0,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr1,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr2,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr3,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr4,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr5,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr6,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr7,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-
-        /* h8~h15 */
-        "vld     $vr0,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr1,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr2,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr3,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr4,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr5,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr6,    %[src],  0          \n\t"
-        "add.d   %[src],  %[src],  %[stride]  \n\t"
-        "vld     $vr7,    %[src],  0          \n\t"
-
-        "vld     $vr8,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr9,    %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr10,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr11,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr12,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr13,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr14,   %[tmp],  0          \n\t"
-        "add.d   %[tmp],  %[tmp],  %[stride]  \n\t"
-        "vld     $vr15,   %[tmp],  0          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0       \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1       \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2       \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3       \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4       \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5       \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6       \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7       \n\t"
-
-        "vst     $vr0,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr1,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr2,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr3,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr4,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr5,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr6,    %[dst],  0          \n\t"
-        "add.d   %[dst],  %[dst],  %[stride]  \n\t"
-        "vst     $vr7,    %[dst],  0          \n\t"
-        : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src)
-        : [stride]"r"(stride)
-        : "memory"
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[stride],      1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[stride]    \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[stride]    \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[stride]    \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+
+    /* h8~h15 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[stride]    \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[stride]    \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
     );
 }
 
@@ -780,113 +736,100 @@ static av_always_inline void
 put_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
                       ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
+    ptrdiff_t stride_2, stride_3, stride_4;
+    ptrdiff_t dstride_2, dstride_3, dstride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-
-        "vld     $vr8,    %[half], 0x00          \n\t"
-        "vld     $vr9,    %[half], 0x10          \n\t"
-        "vld     $vr10,   %[half], 0x20          \n\t"
-        "vld     $vr11,   %[half], 0x30          \n\t"
-        "vld     $vr12,   %[half], 0x40          \n\t"
-        "vld     $vr13,   %[half], 0x50          \n\t"
-        "vld     $vr14,   %[half], 0x60          \n\t"
-        "vld     $vr15,   %[half], 0x70          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vst     $vr0,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr1,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr2,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr3,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr4,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr5,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr6,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr7,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-
-        /* h8~h15 */
-        "vld     $vr0,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],  0             \n\t"
-
-        "vld     $vr8,    %[half], 0x80          \n\t"
-        "vld     $vr9,    %[half], 0x90          \n\t"
-        "vld     $vr10,   %[half], 0xa0          \n\t"
-        "vld     $vr11,   %[half], 0xb0          \n\t"
-        "vld     $vr12,   %[half], 0xc0          \n\t"
-        "vld     $vr13,   %[half], 0xd0          \n\t"
-        "vld     $vr14,   %[half], 0xe0          \n\t"
-        "vld     $vr15,   %[half], 0xf0          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vst     $vr0,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr1,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr2,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr3,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr4,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr5,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr6,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr7,    %[dst],  0             \n\t"
-        : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src)
-        : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
-        : "memory"
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "slli.d     %[dstride_2],    %[dstStride],   1            \n\t"
+    "add.d      %[dstride_3],    %[dstride_2],   %[dstStride] \n\t"
+    "slli.d     %[dstride_4],    %[dstride_2],   1            \n\t"
+    /* h0~h7 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x10         \n\t"
+    "vld        $vr10,           %[half],        0x20         \n\t"
+    "vld        $vr11,           %[half],        0x30         \n\t"
+    "vld        $vr12,           %[half],        0x40         \n\t"
+    "vld        $vr13,           %[half],        0x50         \n\t"
+    "vld        $vr14,           %[half],        0x60         \n\t"
+    "vld        $vr15,           %[half],        0x70         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+
+    /* h8~h15 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x80         \n\t"
+    "vld        $vr9,            %[half],        0x90         \n\t"
+    "vld        $vr10,           %[half],        0xa0         \n\t"
+    "vld        $vr11,           %[half],        0xb0         \n\t"
+    "vld        $vr12,           %[half],        0xc0         \n\t"
+    "vld        $vr13,           %[half],        0xd0         \n\t"
+    "vld        $vr14,           %[half],        0xe0         \n\t"
+    "vld        $vr15,           %[half],        0xf0         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),  [dstride_2]"=&r"(dstride_2),
+      [dstride_3]"=&r"(dstride_3), [dstride_4]"=&r"(dstride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
     );
 }
 
@@ -898,164 +841,139 @@ avg_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
                       ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
     uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
+    ptrdiff_t dstride_2, dstride_3, dstride_4;
     __asm__ volatile (
-        /* h0~h7 */
-        "vld     $vr0,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-
-        "vld     $vr8,    %[half], 0x00          \n\t"
-        "vld     $vr9,    %[half], 0x10          \n\t"
-        "vld     $vr10,   %[half], 0x20          \n\t"
-        "vld     $vr11,   %[half], 0x30          \n\t"
-        "vld     $vr12,   %[half], 0x40          \n\t"
-        "vld     $vr13,   %[half], 0x50          \n\t"
-        "vld     $vr14,   %[half], 0x60          \n\t"
-        "vld     $vr15,   %[half], 0x70          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vld     $vr8,    %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr9,    %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr10,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr11,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr12,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr13,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr14,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr15,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vst     $vr0,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr1,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr2,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr3,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr4,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr5,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr6,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr7,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-
-        /* h8~h15 */
-        "vld     $vr0,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr1,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr2,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr3,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr4,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr5,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr6,    %[src],  0             \n\t"
-        "add.d   %[src],  %[src],  %[srcStride]  \n\t"
-        "vld     $vr7,    %[src],  0             \n\t"
-
-        "vld     $vr8,    %[half], 0x80          \n\t"
-        "vld     $vr9,    %[half], 0x90          \n\t"
-        "vld     $vr10,   %[half], 0xa0          \n\t"
-        "vld     $vr11,   %[half], 0xb0          \n\t"
-        "vld     $vr12,   %[half], 0xc0          \n\t"
-        "vld     $vr13,   %[half], 0xd0          \n\t"
-        "vld     $vr14,   %[half], 0xe0          \n\t"
-        "vld     $vr15,   %[half], 0xf0          \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vld     $vr8,    %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr9,    %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr10,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr11,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr12,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr13,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr14,   %[tmp],  0             \n\t"
-        "add.d   %[tmp],  %[tmp],  %[dstStride]  \n\t"
-        "vld     $vr15,   %[tmp],  0             \n\t"
-
-        "vavgr.bu $vr0,   $vr8,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr9,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr10,   $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr11,   $vr3          \n\t"
-        "vavgr.bu $vr4,   $vr12,   $vr4          \n\t"
-        "vavgr.bu $vr5,   $vr13,   $vr5          \n\t"
-        "vavgr.bu $vr6,   $vr14,   $vr6          \n\t"
-        "vavgr.bu $vr7,   $vr15,   $vr7          \n\t"
-
-        "vst     $vr0,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr1,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr2,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr3,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr4,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr5,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr6,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr7,    %[dst],  0             \n\t"
-        : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half), [src]"+&r"(src)
-        : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
-        : "memory"
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "slli.d     %[dstride_2],    %[dstStride],   1            \n\t"
+    "add.d      %[dstride_3],    %[dstride_2],   %[dstStride] \n\t"
+    "slli.d     %[dstride_4],    %[dstride_2],   1            \n\t"
+    /* h0~h7 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x10         \n\t"
+    "vld        $vr10,           %[half],        0x20         \n\t"
+    "vld        $vr11,           %[half],        0x30         \n\t"
+    "vld        $vr12,           %[half],        0x40         \n\t"
+    "vld        $vr13,           %[half],        0x50         \n\t"
+    "vld        $vr14,           %[half],        0x60         \n\t"
+    "vld        $vr15,           %[half],        0x70         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr11,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr15,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+
+    /* h8~h15    */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x80         \n\t"
+    "vld        $vr9,            %[half],        0x90         \n\t"
+    "vld        $vr10,           %[half],        0xa0         \n\t"
+    "vld        $vr11,           %[half],        0xb0         \n\t"
+    "vld        $vr12,           %[half],        0xc0         \n\t"
+    "vld        $vr13,           %[half],        0xd0         \n\t"
+    "vld        $vr14,           %[half],        0xe0         \n\t"
+    "vld        $vr15,           %[half],        0xf0         \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr11,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr15,           %[tmp],         %[dstride_3] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),  [dstride_2]"=&r"(dstride_2),
+      [dstride_3]"=&r"(dstride_3), [dstride_4]"=&r"(dstride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
     );
 }
 
@@ -1086,21 +1004,15 @@ put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     int dstStride_2x = dstStride << 1;
     __m256i src00, src01, src02, src03, src04, src05, src10;
     __m256i out0, out1, out2, out3;
-    __m256i zero = {0};
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i h_16 = {16};
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
     __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
     __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
     __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
     __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
     __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
 
-    zero = __lasx_xvreplve0_h(zero);
-    h_20 = __lasx_xvreplve0_h(h_20);
-    h_5  = __lasx_xvreplve0_h(h_5);
-    h_16 = __lasx_xvreplve0_h(h_16);
-
     QPEL8_H_LOWPASS(out0)
     QPEL8_H_LOWPASS(out1)
     QPEL8_H_LOWPASS(out2)
@@ -1138,7 +1050,7 @@ put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
 }
 
 static av_always_inline void
-put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
+put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, uint8_t *src, int dstStride,
                               int srcStride)
 {
     int srcStride_2x = srcStride << 1;
@@ -1148,26 +1060,20 @@ put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     __m256i src00, src01, src02, src03, src04, src05, src06;
     __m256i src07, src08, src09, src10, src11, src12;
     __m256i tmp00, tmp01, tmp02, tmp03, tmp04, tmp05;
-    __m256i zero = {0};
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i h_16 = {16};
-
-    zero = __lasx_xvreplve0_h(zero);
-    h_20 = __lasx_xvreplve0_h(h_20);
-    h_5  = __lasx_xvreplve0_h(h_5);
-    h_16 = __lasx_xvreplve0_h(h_16);
-
-    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0, src00, src01);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
-              src + srcStride_3x, 0, src02, src03, src04, src05);
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0,
+              src00, src01);
+    src02 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src03, src04, src05, src06);
     src += srcStride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
-              src + srcStride_3x, 0, src06, src07, src08, src09);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src07, src08, src09, src10);
     src += srcStride_4x;
-    DUP2_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src10, src11);
-    src += srcStride_2x;
-    src12 = __lasx_xvld(src, 0);
+    DUP2_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src11, src12);
 
     QPEL8_V_LOWPASS(src00, src01, src02, src03, src04, src05, src06,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
@@ -1191,7 +1097,7 @@ put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
 }
 
 static av_always_inline void
-avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
+avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, uint8_t *src, int dstStride,
                               int srcStride)
 {
     int srcStride_2x = srcStride << 1;
@@ -1201,34 +1107,32 @@ avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     int srcStride_3x = srcStride_2x + srcStride;
     int dstStride_3x = dstStride_2x + dstStride;
     __m256i src00, src01, src02, src03, src04, src05, src06;
-    __m256i src07, src08, src09, src10, src11, src12;
-    __m256i tmp00, tmp01, tmp02, tmp03, tmp04, tmp05, tmp06, tmp07, tmp08, tmp09;
-    __m256i zero = {0};
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i h_16 = {16};
-
-    zero = __lasx_xvreplve0_h(zero);
-    h_20 = __lasx_xvreplve0_h(h_20);
-    h_5  = __lasx_xvreplve0_h(h_5);
-    h_16 = __lasx_xvreplve0_h(h_16);
-
-    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0, src00, src01);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
-              src + srcStride_3x, 0, src02, src03, src04, src05);
+    __m256i src07, src08, src09, src10, src11, src12, tmp00;
+    __m256i tmp01, tmp02, tmp03, tmp04, tmp05, tmp06, tmp07, tmp08, tmp09;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+
+
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0,
+              src00, src01);
+    src02 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src03, src04, src05, src06);
     src += srcStride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
-              src + srcStride_3x, 0, src06, src07, src08, src09);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src07, src08, src09, src10);
     src += srcStride_4x;
-    DUP2_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src10, src11);
-    src += srcStride_2x;
-    src12 = __lasx_xvld(src, 0);
+    DUP2_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src11, src12);
 
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              tmp06, tmp07, tmp02, tmp03);
+    tmp06 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x,
+              dst, dstStride_3x, dst, dstStride_4x,
+              tmp07, tmp02, tmp03, tmp04);
     dst += dstStride_4x;
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              tmp04, tmp05, tmp00, tmp01);
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x,
+              tmp05, tmp00);
+    tmp01 = __lasx_xvldx(dst, dstStride_3x);
     dst -= dstStride_4x;
 
     tmp06 = __lasx_xvpermi_q(tmp06, tmp07, 0x02);
@@ -1241,19 +1145,19 @@ avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     tmp06 = __lasx_xvavgr_bu(tmp06, tmp02);
     __lasx_xvstelm_d(tmp06, dst, 0, 0);
     __lasx_xvstelm_d(tmp06, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src02, src03, src04, src05, src06, src07, src08,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp07 = __lasx_xvavgr_bu(tmp07, tmp02);
     __lasx_xvstelm_d(tmp07, dst, 0, 0);
     __lasx_xvstelm_d(tmp07, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src04, src05, src06, src07, src08, src09, src10,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp08 = __lasx_xvavgr_bu(tmp08, tmp02);
     __lasx_xvstelm_d(tmp08, dst, 0, 0);
     __lasx_xvstelm_d(tmp08, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src06, src07, src08, src09, src10, src11, src12,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp09 = __lasx_xvavgr_bu(tmp09, tmp02);
@@ -1313,10 +1217,10 @@ put_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     __m256i src00, src01, src02, src03, src04, src05, src10;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
     __m256i tmp7, tmp8, tmp9, tmp10, tmp11, tmp12;
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i w_20 = {20};
-    __m256i w_5  = {5};
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i w_20 = __lasx_xvldi(0x814);
+    __m256i w_5  = __lasx_xvldi(0x805);
     __m256i w_512 = {512};
     __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
     __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
@@ -1324,10 +1228,6 @@ put_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
     __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
 
-    h_20 = __lasx_xvreplve0_h(w_20);
-    h_5  = __lasx_xvreplve0_h(w_5);
-    w_20 = __lasx_xvreplve0_w(w_20);
-    w_5  = __lasx_xvreplve0_w(w_5);
     w_512 = __lasx_xvreplve0_w(w_512);
 
     src -= srcStride << 1;
@@ -1354,16 +1254,20 @@ put_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
                        src02, src03, src04, src05, tmp6)
     __lasx_xvstelm_d(tmp0, dst, 0, 0);
-    __lasx_xvstelm_d(tmp0, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp0, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp2, dst, 0, 0);
-    __lasx_xvstelm_d(tmp2, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp4, dst, 0, 0);
-    __lasx_xvstelm_d(tmp4, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp6, dst, 0, 0);
-    __lasx_xvstelm_d(tmp6, dst + dstStride, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 2);
 }
 
 static av_always_inline void
@@ -1376,30 +1280,25 @@ avg_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     __m256i src00, src01, src02, src03, src04, src05, src10;
     __m256i dst00, dst01, dst0, dst1, dst2, dst3;
     __m256i out0, out1, out2, out3;
-    __m256i zero = {0};
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i h_16 = {16};
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
     __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
     __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
     __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
     __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
     __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
 
-    zero = __lasx_xvreplve0_h(zero);
-    h_20 = __lasx_xvreplve0_h(h_20);
-    h_5  = __lasx_xvreplve0_h(h_5);
-    h_16 = __lasx_xvreplve0_h(h_16);
-
     QPEL8_H_LOWPASS(out0)
     QPEL8_H_LOWPASS(out1)
     QPEL8_H_LOWPASS(out2)
     QPEL8_H_LOWPASS(out3)
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              src00, src01, src02, src03);
+    src00 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, dst,
+              dstStride_3x, dst, dstStride_4x, src01, src02, src03, src04);
     dst += dstStride_4x;
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              src04, src05, dst00, dst01);
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, src05, dst00);
+    dst01 = __lasx_xvldx(dst, dstStride_3x);
     dst -= dstStride_4x;
     dst0 = __lasx_xvpermi_q(src00, src01, 0x02);
     dst1 = __lasx_xvpermi_q(src02, src03, 0x02);
@@ -1427,10 +1326,10 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     __m256i src00, src01, src02, src03, src04, src05, src10;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
     __m256i tmp7, tmp8, tmp9, tmp10, tmp11, tmp12;
-    __m256i h_20 = {20};
-    __m256i h_5  = {5};
-    __m256i w_20 = {20};
-    __m256i w_5  = {5};
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i w_20 = __lasx_xvldi(0x814);
+    __m256i w_5  = __lasx_xvldi(0x805);
     __m256i w_512 = {512};
     __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
     __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
@@ -1441,10 +1340,6 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     ptrdiff_t dstStride_4x = dstStride << 2;
     ptrdiff_t dstStride_3x = dstStride_2x + dstStride;
 
-    h_20 = __lasx_xvreplve0_h(w_20);
-    h_5  = __lasx_xvreplve0_h(w_5);
-    w_20 = __lasx_xvreplve0_w(w_20);
-    w_5  = __lasx_xvreplve0_w(w_5);
     w_512 = __lasx_xvreplve0_w(w_512);
 
     src -= srcStride << 1;
@@ -1471,11 +1366,12 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
                        src02, src03, src04, src05, tmp6)
 
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              src00, src01, src02, src03);
+    src00 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, dst,
+              dstStride_3x, dst, dstStride_4x, src01, src02, src03, src04);
     dst += dstStride_4x;
-    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
-              src04, src05, tmp8, tmp9);
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, src05, tmp8);
+    tmp9 = __lasx_xvldx(dst, dstStride_3x);
     dst -= dstStride_4x;
     tmp1 = __lasx_xvpermi_q(src00, src01, 0x02);
     tmp3 = __lasx_xvpermi_q(src02, src03, 0x02);
@@ -1486,16 +1382,20 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     tmp4 = __lasx_xvavgr_bu(tmp4, tmp5);
     tmp6 = __lasx_xvavgr_bu(tmp6, tmp7);
     __lasx_xvstelm_d(tmp0, dst, 0, 0);
-    __lasx_xvstelm_d(tmp0, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp0, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp2, dst, 0, 0);
-    __lasx_xvstelm_d(tmp2, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp4, dst, 0, 0);
-    __lasx_xvstelm_d(tmp4, dst + dstStride, 0, 2);
-    dst += dstStride << 1;
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 2);
+    dst += dstStride;
     __lasx_xvstelm_d(tmp6, dst, 0, 0);
-    __lasx_xvstelm_d(tmp6, dst + dstStride, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 2);
 }
 
 static av_always_inline void
@@ -1525,27 +1425,27 @@ avg_h264_qpel16_h_lowpass_lasx(uint8_t *dst, const uint8_t *src,
 static void put_h264_qpel16_v_lowpass_lasx(uint8_t *dst, const uint8_t *src,
                                            int dstStride, int srcStride)
 {
-    put_h264_qpel8_v_lowpass_lasx(dst, src, dstStride, srcStride);
-    put_h264_qpel8_v_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
     src += 8*srcStride;
     dst += 8*dstStride;
-    put_h264_qpel8_v_lowpass_lasx(dst, src, dstStride, srcStride);
-    put_h264_qpel8_v_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
 }
 
 static void avg_h264_qpel16_v_lowpass_lasx(uint8_t *dst, const uint8_t *src,
                                            int dstStride, int srcStride)
 {
-    avg_h264_qpel8_v_lowpass_lasx(dst, src, dstStride, srcStride);
-    avg_h264_qpel8_v_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
     src += 8*srcStride;
     dst += 8*dstStride;
-    avg_h264_qpel8_v_lowpass_lasx(dst, src, dstStride, srcStride);
-    avg_h264_qpel8_v_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
 }
 
 static void put_h264_qpel16_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
-                                            ptrdiff_t dstStride, ptrdiff_t srcStride)
+                                     ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
     put_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
     put_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
@@ -1556,7 +1456,7 @@ static void put_h264_qpel16_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
 }
 
 static void avg_h264_qpel16_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
-                                            ptrdiff_t dstStride, ptrdiff_t srcStride)
+                                     ptrdiff_t dstStride, ptrdiff_t srcStride)
 {
     avg_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
     avg_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
@@ -1604,7 +1504,7 @@ void ff_put_h264_qpel8_mc01_lasx(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
 
-    put_h264_qpel8_v_lowpass_lasx(half, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(half, (uint8_t*)src, 8, stride);
     put_pixels8_l2_8_lsx(dst, src, half, stride, stride);
 }
 
@@ -1615,7 +1515,7 @@ void ff_put_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1638,14 +1538,14 @@ void ff_put_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
 void ff_put_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
                                  ptrdiff_t stride)
 {
-    put_h264_qpel8_v_lowpass_lasx(dst, src, stride, stride);
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, stride, stride);
 }
 
 void ff_put_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
@@ -1656,7 +1556,7 @@ void ff_put_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t *const halfH  = temp + 64;
 
     put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
 }
 
@@ -1674,7 +1574,7 @@ void ff_put_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t *const halfH  = temp + 64;
 
     put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfH, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src + 1, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
 }
 
@@ -1683,7 +1583,7 @@ void ff_put_h264_qpel8_mc03_lasx(uint8_t *dst, const uint8_t *src,
 {
     uint8_t half[64];
 
-    put_h264_qpel8_v_lowpass_lasx(half, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(half, (uint8_t*)src, 8, stride);
     put_pixels8_l2_8_lsx(dst, src + stride, half, stride, stride);
 }
 
@@ -1694,7 +1594,7 @@ void ff_put_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1717,7 +1617,7 @@ void ff_put_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
     put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1760,7 +1660,7 @@ void ff_avg_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1783,14 +1683,14 @@ void ff_avg_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
 void ff_avg_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
                                  ptrdiff_t stride)
 {
-    avg_h264_qpel8_v_lowpass_lasx(dst, src, stride, stride);
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, stride, stride);
 }
 
 void ff_avg_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
@@ -1801,7 +1701,7 @@ void ff_avg_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t *const halfH  = temp + 64;
 
     put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
 }
 
@@ -1819,7 +1719,7 @@ void ff_avg_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t *const halfH  = temp + 64;
 
     put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfH, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src + 1, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
 }
 
@@ -1830,7 +1730,7 @@ void ff_avg_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1853,7 +1753,7 @@ void ff_avg_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
     uint8_t halfV[64];
 
     put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
-    put_h264_qpel8_v_lowpass_lasx(halfV, src + 1, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
     avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
 }
 
@@ -1901,7 +1801,8 @@ void ff_put_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
 void ff_put_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_16x16_lasx(src - 2, src - (stride * 2), dst, stride);
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src - 2, (uint8_t*)src - (stride * 2),
+                               dst, stride);
 }
 
 void ff_put_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
@@ -1919,7 +1820,8 @@ void ff_put_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
 void ff_put_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_16x16_lasx(src - 2, src - (stride * 2) + 1, dst, stride);
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src - 2, (uint8_t*)src - (stride * 2) + 1,
+                               dst, stride);
 }
 
 void ff_put_h264_qpel16_mc02_lasx(uint8_t *dst, const uint8_t *src,
@@ -1970,8 +1872,8 @@ void ff_put_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
 void ff_put_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_16x16_lasx(src + stride - 2, src - (stride * 2), dst,
-                               stride);
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src + stride - 2, (uint8_t*)src - (stride * 2),
+                               dst, stride);
 }
 
 void ff_put_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
@@ -1989,8 +1891,8 @@ void ff_put_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
 void ff_put_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_16x16_lasx(src + stride - 2, src - (stride * 2) + 1, dst,
-                               stride);
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src + stride - 2,
+                               (uint8_t*)src - (stride * 2) + 1, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc00_lasx(uint8_t *dst, const uint8_t *src,
@@ -2037,9 +1939,9 @@ void ff_avg_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
 void ff_avg_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_and_aver_dst_16x16_lasx(src - 2,
-                                            src - (stride * 2),
-                                            dst, stride);
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src - 2,
+                                           (uint8_t*)src - (stride * 2),
+                                           dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
@@ -2057,9 +1959,8 @@ void ff_avg_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
 void ff_avg_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_and_aver_dst_16x16_lasx(src - 2,
-                                            src - (stride * 2) +
-                                            sizeof(uint8_t),
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src - 2,
+                                            (uint8_t*)src - (stride * 2) + 1,
                                             dst, stride);
 }
 
@@ -2111,8 +2012,8 @@ void ff_avg_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
 void ff_avg_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_and_aver_dst_16x16_lasx(src + stride - 2,
-                                            src - (stride * 2),
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src + stride - 2,
+                                            (uint8_t*)src - (stride * 2),
                                             dst, stride);
 }
 
@@ -2131,8 +2032,7 @@ void ff_avg_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
 void ff_avg_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
                                   ptrdiff_t stride)
 {
-    avc_luma_hv_qrt_and_aver_dst_16x16_lasx(src + stride - 2,
-                                            src - (stride * 2) +
-                                            sizeof(uint8_t),
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src + stride - 2,
+                                            (uint8_t*)src - (stride * 2) + 1,
                                             dst, stride);
 }
diff --git a/libavcodec/loongarch/hevc_idct_lsx.c b/libavcodec/loongarch/hevc_idct_lsx.c
index c73193d1f9..2193b27546 100644
--- a/libavcodec/loongarch/hevc_idct_lsx.c
+++ b/libavcodec/loongarch/hevc_idct_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -326,7 +327,7 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     res3 = __lsx_vsat_w(res3, 15);                            \
 }
 
-static void hevc_idct_4x4_lsx(int16_t *coeffs)
+void ff_hevc_idct_4x4_lsx(int16_t *coeffs, int col_limit)
 {
     __m128i in0, in1;
     __m128i in_r0, in_l0, in_r1, in_l1;
@@ -356,7 +357,7 @@ static void hevc_idct_4x4_lsx(int16_t *coeffs)
     __lsx_vst(in1, coeffs, 16);
 }
 
-static void hevc_idct_8x8_lsx(int16_t *coeffs)
+void ff_hevc_idct_8x8_lsx(int16_t *coeffs, int col_limit)
 {
     const int16_t *filter = &gt8x8_cnst[0];
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
@@ -372,17 +373,17 @@ static void hevc_idct_8x8_lsx(int16_t *coeffs)
     LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
                        in0, in1, in2, in3, in4, in5, in6, in7);
 
-    __lsx_vst(in0, coeffs,0);
-    __lsx_vst(in1, coeffs,16);
-    __lsx_vst(in2, coeffs,32);
-    __lsx_vst(in3, coeffs,48);
-    __lsx_vst(in4, coeffs,64);
-    __lsx_vst(in5, coeffs,80);
-    __lsx_vst(in6, coeffs,96);
-    __lsx_vst(in7, coeffs,112);
+    __lsx_vst(in0, coeffs, 0);
+    __lsx_vst(in1, coeffs, 16);
+    __lsx_vst(in2, coeffs, 32);
+    __lsx_vst(in3, coeffs, 48);
+    __lsx_vst(in4, coeffs, 64);
+    __lsx_vst(in5, coeffs, 80);
+    __lsx_vst(in6, coeffs, 96);
+    __lsx_vst(in7, coeffs, 112);
 }
 
-static void hevc_idct_16x16_lsx(int16_t *coeffs)
+void ff_hevc_idct_16x16_lsx(int16_t *coeffs, int col_limit)
 {
     int16_t i, j, k;
     int16_t buf[256];
@@ -474,12 +475,15 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
 
     src = coeffs + 8;
     DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in0, in1, in2, in3);
-    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in4, in5, in6, in7);
     LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
                        vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     src = coeffs + 128;
-    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in8, in9, in10, in11);
-    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in12, in13, in14, in15);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+              in8, in9, in10, in11);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in12, in13, in14, in15);
 
     __lsx_vst(vec0, src, 0);
     __lsx_vst(vec1, src, 32);
@@ -502,8 +506,10 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __lsx_vst(vec7, src, 224);
 
     src = coeffs + 136;
-    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in0, in1, in2, in3);
-    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in4, in5, in6, in7);
     LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
                        vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     __lsx_vst(vec0, src, 0);
@@ -516,22 +522,22 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __lsx_vst(vec7, src, 224);
 }
 
-static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
+static void hevc_idct_8x32_column_lsx(int16_t *coeffs, int32_t buf_pitch,
                                       uint8_t round)
 {
     uint8_t i;
-    uint16_t buf_pitch_2x  = (uint16_t)buf_pitch << 1;
-    uint16_t buf_pitch_4x  = (uint16_t)buf_pitch << 2;
-    uint16_t buf_pitch_8x  = (uint16_t)buf_pitch << 3;
-    uint16_t buf_pitch_16x = (uint16_t)buf_pitch << 4;
-    uint16_t buf_pitch_32x = (uint16_t)buf_pitch << 5;
+    int32_t buf_pitch_2  = buf_pitch << 1;
+    int32_t buf_pitch_4  = buf_pitch << 2;
+    int32_t buf_pitch_8  = buf_pitch << 3;
+    int32_t buf_pitch_16 = buf_pitch << 4;
+
     const int16_t *filter_ptr0 = &gt32x32_cnst0[0];
     const int16_t *filter_ptr1 = &gt32x32_cnst1[0];
     const int16_t *filter_ptr2 = &gt32x32_cnst2[0];
     const int16_t *filter_ptr3 = &gt8x8_cnst[0];
     int16_t *src0 = (coeffs + buf_pitch);
-    int16_t *src1 = (coeffs + 2 * buf_pitch);
-    int16_t *src2 = (coeffs + 4 * buf_pitch);
+    int16_t *src1 = (coeffs + buf_pitch_2);
+    int16_t *src2 = (coeffs + buf_pitch_4);
     int16_t *src3 = (coeffs);
     int32_t tmp_buf[8 * 32 + 15];
     int32_t *tmp_buf_ptr = tmp_buf + 15;
@@ -546,88 +552,91 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
 
     /* process coeff 4, 12, 20, 28 */
     in0 = __lsx_vld(src2, 0);
-    // TODO: Use vldx once gcc fixed.
-    //in1 = __lsx_vldx(src2, buf_pitch_16x);
-    //in2 = __lsx_vldx(src2, buf_pitch_32x);
-    //in3 = __lsx_vldx(src2, buf_pitch_32x + buf_pitch_16x);
-    in1 = __lsx_vld(src2 + buf_pitch_8x, 0);
-    in2 = __lsx_vld(src2 + buf_pitch_16x, 0);
-    in3 = __lsx_vld(src2 + buf_pitch_16x + buf_pitch_8x, 0);
+    in1 = __lsx_vld(src2 + buf_pitch_8, 0);
+    in2 = __lsx_vld(src2 + buf_pitch_16, 0);
+    in3 = __lsx_vld(src2 + buf_pitch_16 + buf_pitch_8, 0);
     in4 = __lsx_vld(src3, 0);
-    // TODO: Use vldx once gcc fixed.
-    //in5 = __lsx_vldx(src3, buf_pitch_16x);
-    //in6 = __lsx_vldx(src3, buf_pitch_32x);
-    //in7 = __lsx_vldx(src3, buf_pitch_32x + buf_pitch_16x);
-    in5 = __lsx_vld(src3 + buf_pitch_8x, 0);
-    in6 = __lsx_vld(src3 + buf_pitch_16x, 0);
-    in7 = __lsx_vld(src3 + buf_pitch_16x + buf_pitch_8x, 0);
+    in5 = __lsx_vld(src3 + buf_pitch_8, 0);
+    in6 = __lsx_vld(src3 + buf_pitch_16, 0);
+    in7 = __lsx_vld(src3 + buf_pitch_16 + buf_pitch_8, 0);
     DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in6, in4, in7, in5,
               src0_r, src1_r, src2_r, src3_r);
     DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in6, in4, in7, in5,
               src0_l, src1_l, src2_l, src3_l);
 
-    /* loop for all columns of constants */
-    for (i = 0; i < 2; i++) {
-        /* processing single column of constants */
-        filter0 = __lsx_vldrepl_w(filter_ptr2, 0);
-        filter1 = __lsx_vldrepl_w(filter_ptr2, 4);
-        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
-        __lsx_vst(sum0_r, (tmp_buf_ptr + 2 * i * 8), 0);
-        __lsx_vst(sum0_l, (tmp_buf_ptr + 2 * i * 8), 16);
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 0);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 4);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 0);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 16);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 8);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 12);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 32);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 48);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 16);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 20);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 64);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 80);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 24);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 28);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 96);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 112);
 
-        /* processing single column of constants */
-        filter0 = __lsx_vldrepl_w(filter_ptr2, 8);
-        filter1 = __lsx_vldrepl_w(filter_ptr2, 12);
-        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
-        __lsx_vst(sum0_r, (tmp_buf_ptr + (2 * i + 1) * 8), 0);
-        __lsx_vst(sum0_l, (tmp_buf_ptr + (2 * i + 1) * 8), 16);
+    /* process coeff 0, 8, 16, 24 */
+    filter0 = __lsx_vldrepl_w(filter_ptr3, 0);
+    filter1 = __lsx_vldrepl_w(filter_ptr3, 4);
 
-        filter_ptr2 += 8;
-    }
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
+              src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
+    sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
+    sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
+    sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
+    sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
 
-    /* process coeff 0, 8, 16, 24 */
-    /* loop for all columns of constants */
-    for (i = 0; i < 2; i++) {
-        /* processing first column of filter constants */
-        filter0 = __lsx_vldrepl_w(filter_ptr3, 0);
-        filter1 = __lsx_vldrepl_w(filter_ptr3, 4);
-
-        DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
-                  src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
-        sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
-        sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
-        sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
-        sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum0_r, sum0_l, 0, 7);
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum1_r, sum1_l, 3, 4);
 
-        HEVC_EVEN16_CALC(tmp_buf_ptr, sum0_r, sum0_l, i, (7 - i));
-        HEVC_EVEN16_CALC(tmp_buf_ptr, sum1_r, sum1_l, (3 - i), (4 + i));
+    filter0 = __lsx_vldrepl_w(filter_ptr3, 16);
+    filter1 = __lsx_vldrepl_w(filter_ptr3, 20);
 
-        filter_ptr3 += 8;
-    }
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
+              src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
+    sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
+    sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
+    sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
+    sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
+
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum0_r, sum0_l, 1, 6);
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum1_r, sum1_l, 2, 5);
 
     /* process coeff 2 6 10 14 18 22 26 30 */
     in0 = __lsx_vld(src1, 0);
-    // TODO: Use vldx once gcc fixed.
-    //in1 = __lsx_vldx(src1, buf_pitch_8x);
-    //in2 = __lsx_vldx(src1, buf_pitch_16x);
-    //in3 = __lsx_vldx(src1, buf_pitch_16x + buf_pitch_8x);
-    //in4 = __lsx_vldx(src1, buf_pitch_32x);
-    //in5 = __lsx_vldx(src1, buf_pitch_32x + buf_pitch_8x);
-    //in6 = __lsx_vldx(src1, buf_pitch_32x + buf_pitch_16x);
-    //in7 = __lsx_vldx(src1, buf_pitch_32x + buf_pitch_16x + buf_pitch_8x);
-    in1 = __lsx_vld(src1 + buf_pitch_4x, 0);
-    in2 = __lsx_vld(src1 + buf_pitch_8x, 0);
-    in3 = __lsx_vld(src1 + buf_pitch_8x + buf_pitch_4x, 0);
-    in4 = __lsx_vld(src1 + buf_pitch_16x, 0);
-    in5 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_4x, 0);
-    in6 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_8x, 0);
-    in7 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_8x + buf_pitch_4x, 0);
+    in1 = __lsx_vld(src1 + buf_pitch_4, 0);
+    in2 = __lsx_vld(src1 + buf_pitch_8, 0);
+    in3 = __lsx_vld(src1 + buf_pitch_8 + buf_pitch_4, 0);
+    in4 = __lsx_vld(src1 + buf_pitch_16, 0);
+    in5 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_4, 0);
+    in6 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_8, 0);
+    in7 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_8 + buf_pitch_4, 0);
+
     DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
               src0_r, src1_r, src2_r, src3_r);
     DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
@@ -667,42 +676,28 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
 
     /* process coeff 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 */
     in0 = __lsx_vld(src0, 0);
-    // TODO: Use vldx once gcc fixed.
-    //in1 = __lsx_vldx(src0, buf_pitch_4x);
-    //in2 = __lsx_vldx(src0, buf_pitch_8x);
-    //in3 = __lsx_vldx(src0, buf_pitch_8x + buf_pitch_4x);
-    //in4 = __lsx_vldx(src0, buf_pitch_16x);
-    //in5 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_4x);
-    //in6 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_8x);
-    //in7 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_8x + buf_pitch_4x);
-    in1 = __lsx_vld(src0 + buf_pitch_2x, 0);
-    in2 = __lsx_vld(src0 + buf_pitch_4x, 0);
-    in3 = __lsx_vld(src0 + buf_pitch_4x + buf_pitch_2x, 0);
-    in4 = __lsx_vld(src0 + buf_pitch_8x, 0);
-    in5 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_2x, 0);
-    in6 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x, 0);
-    in7 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x + buf_pitch_2x, 0);
+    in1 = __lsx_vld(src0 + buf_pitch_2, 0);
+    in2 = __lsx_vld(src0 + buf_pitch_4, 0);
+    in3 = __lsx_vld(src0 + buf_pitch_4 + buf_pitch_2, 0);
+    in4 = __lsx_vld(src0 + buf_pitch_8, 0);
+    in5 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_2, 0);
+    in6 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4, 0);
+    in7 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2, 0);
+
     src0 += 16 * buf_pitch;
     DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
               src0_r, src1_r, src2_r, src3_r);
     DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
               src0_l, src1_l, src2_l, src3_l);
     in0 = __lsx_vld(src0, 0);
-    // TODO: Use vldx once gcc fixed.
-    //in1 = __lsx_vldx(src0, buf_pitch_4x);
-    //in2 = __lsx_vldx(src0, buf_pitch_8x);
-    //in3 = __lsx_vldx(src0, buf_pitch_8x + buf_pitch_4x);
-    //in4 = __lsx_vldx(src0, buf_pitch_16x);
-    //in5 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_4x);
-    //in6 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_8x);
-    //in7 = __lsx_vldx(src0, buf_pitch_16x + buf_pitch_8x + buf_pitch_4x);
-    in1 = __lsx_vld(src0 + buf_pitch_2x, 0);
-    in2 = __lsx_vld(src0 + buf_pitch_4x, 0);
-    in3 = __lsx_vld(src0 + buf_pitch_4x + buf_pitch_2x, 0);
-    in4 = __lsx_vld(src0 + buf_pitch_8x, 0);
-    in5 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_2x, 0);
-    in6 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x, 0);
-    in7 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x + buf_pitch_2x, 0);
+    in1 = __lsx_vld(src0 + buf_pitch_2, 0);
+    in2 = __lsx_vld(src0 + buf_pitch_4, 0);
+    in3 = __lsx_vld(src0 + buf_pitch_4 + buf_pitch_2, 0);
+    in4 = __lsx_vld(src0 + buf_pitch_8, 0);
+    in5 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_2, 0);
+    in6 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4, 0);
+    in7 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2, 0);
+
     DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
               src4_r, src5_r, src6_r, src7_r);
     DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
@@ -813,14 +808,14 @@ static void hevc_idct_transpose_8x32_to_32x8(int16_t *tmp_buf, int16_t *coeffs)
     }
 }
 
-static void hevc_idct_32x32_lsx(int16_t *coeffs)
+void ff_hevc_idct_32x32_lsx(int16_t *coeffs, int col_limit)
 {
     uint8_t row_cnt, col_cnt;
     int16_t *src = coeffs;
     int16_t tmp_buf[8 * 32 + 31];
     int16_t *tmp_buf_ptr = tmp_buf + 31;
     uint8_t round;
-    uint8_t buf_pitch;
+    int32_t buf_pitch;
 
     /* Align pointer to 64 byte boundary */
     tmp_buf_ptr = (int16_t *)(((uintptr_t) tmp_buf_ptr) & ~(uintptr_t) 63);
@@ -845,23 +840,3 @@ static void hevc_idct_32x32_lsx(int16_t *coeffs)
         hevc_idct_transpose_8x32_to_32x8(tmp_buf_ptr, src);
     }
 }
-
-void ff_hevc_idct_4x4_lsx(int16_t *coeffs, int col_limit)
-{
-    hevc_idct_4x4_lsx(coeffs);
-}
-
-void ff_hevc_idct_8x8_lsx(int16_t *coeffs, int col_limit)
-{
-    hevc_idct_8x8_lsx(coeffs);
-}
-
-void ff_hevc_idct_16x16_lsx(int16_t *coeffs, int col_limit)
-{
-    hevc_idct_16x16_lsx(coeffs);
-}
-
-void ff_hevc_idct_32x32_lsx(int16_t *coeffs, int col_limit)
-{
-    hevc_idct_32x32_lsx(coeffs);
-}
diff --git a/libavcodec/loongarch/hevc_lpf_sao_lsx.c b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
index 556e45e1ff..fc10e8eda8 100644
--- a/libavcodec/loongarch/hevc_lpf_sao_lsx.c
+++ b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -22,26 +23,26 @@
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 
-static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
-                                         int32_t beta, int32_t *tc,
-                                         uint8_t *p_is_pcm, uint8_t *q_is_pcm)
+void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm)
 {
-    uint8_t *p3 = src - (stride << 2);
-    uint8_t *p2 = src - ((stride << 1) + stride);
-    uint8_t *p1 = src - (stride << 1);
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
+    uint8_t *p3 = src - stride_4x;
+    uint8_t *p2 = src - stride_3x;
+    uint8_t *p1 = src - stride_2x;
     uint8_t *p0 = src - stride;
     uint8_t *q0 = src;
     uint8_t *q1 = src + stride;
-    uint8_t *q2 = src + (stride << 1);
-    uint8_t *q3 = src + (stride << 1) + stride;
+    uint8_t *q2 = src + stride_2x;
+    uint8_t *q3 = src + stride_3x;
     uint8_t flag0, flag1;
     int32_t dp00, dq00, dp30, dq30, d00, d30, d0030, d0434;
     int32_t dp04, dq04, dp34, dq34, d04, d34;
     int32_t tc0, p_is_pcm0, q_is_pcm0, beta30, beta20, tc250;
     int32_t tc4, p_is_pcm4, q_is_pcm4, tc254, tmp;
-    const int32_t stride_2x = (stride << 1);
-    const int32_t stride_4x = (stride << 2);
-    const int32_t stride_3x = stride_2x + stride;
 
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
     __m128i cmp0, cmp1, cmp2, cmp3, p_is_pcm_vec, q_is_pcm_vec;
@@ -80,7 +81,8 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
 
     if ((!p_is_pcm0 || !p_is_pcm4 || !q_is_pcm0 || !q_is_pcm4) &&
         (!d0030 || !d0434)) {
-        DUP4_ARG2(__lsx_vld, p3, 0, p2, 0, p1, 0, p0, 0, p3_src, p2_src, p1_src, p0_src);
+        DUP4_ARG2(__lsx_vld, p3, 0, p2, 0, p1, 0, p0, 0,
+                  p3_src, p2_src, p1_src, p0_src);
         DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
@@ -95,19 +97,22 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
         DUP2_ARG1(__lsx_vreplgr2vr_h, tc0, tc4, cmp0, cmp1);
         DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
                   p0_src, p3_src, p2_src, p1_src, p0_src);
-        DUP4_ARG2(__lsx_vld, q0, 0, q1, 0, q2, 0, q3, 0, q0_src, q1_src, q2_src, q3_src);
-        flag0 = abs(p3[0] - p0[0]) + abs(q3[0] - q0[0]) < beta30 && abs(p0[0] - q0[0])
-                < tc250;
+        DUP4_ARG2(__lsx_vld, q0, 0, q1, 0, q2, 0, q3, 0,
+                  q0_src, q1_src, q2_src, q3_src);
+        flag0 = abs(p3[0] - p0[0]) + abs(q3[0] - q0[0]) < beta30 &&
+                abs(p0[0] - q0[0]) < tc250;
         flag0 = flag0 && (abs(p3[3] - p0[3]) + abs(q3[3] - q0[3]) < beta30 &&
-                abs(p0[3] - q0[3]) < tc250 && (d00 << 1) < beta20 && (d30 << 1) < beta20);
+                abs(p0[3] - q0[3]) < tc250 && (d00 << 1) < beta20 &&
+                (d30 << 1) < beta20);
         tc_pos = __lsx_vpackev_d(cmp1, cmp0);
-        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero, q3_src,
-                  q0_src, q1_src, q2_src, q3_src);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src,
+                  zero, q3_src, q0_src, q1_src, q2_src, q3_src);
 
-        flag1 = abs(p3[4] - p0[4]) + abs(q3[4] - q0[4]) < beta30 && abs(p0[4] - q0[4])
-                < tc254;
+        flag1 = abs(p3[4] - p0[4]) + abs(q3[4] - q0[4]) < beta30 &&
+                abs(p0[4] - q0[4]) < tc254;
         flag1 = flag1 && (abs(p3[7] - p0[7]) + abs(q3[7] - q0[7]) < beta30 &&
-                abs(p0[7] - q0[7]) < tc254 && (d04 << 1) < beta20 && (d34 << 1) < beta20);
+                abs(p0[7] - q0[7]) < tc254 && (d04 << 1) < beta20 &&
+                (d34 << 1) < beta20);
         DUP2_ARG1(__lsx_vreplgr2vr_w, flag0, flag1, cmp0, cmp1);
         cmp2 = __lsx_vpackev_w(cmp1, cmp0);
         cmp2 = __lsx_vseqi_w(cmp2, 0);
@@ -118,7 +123,8 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
@@ -134,19 +140,21 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             dst1 = __lsx_vadd_h(temp2, p1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src,
+                      temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p0_src);
             temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                      p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1,
+                      p1_src, p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
@@ -162,15 +170,16 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             dst4 = __lsx_vadd_h(temp2, q1_src);
 
             temp0 = __lsx_vslli_h(temp0, 1);
-            DUP2_ARG2(__lsx_vadd_h, temp0, p1_src, temp1, q2_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp0, p1_src, temp1, q2_src,
+                      temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q0_src);
             temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                      q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4,
+                      q1_src, q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
 
             /* pack results to 8 bit */
@@ -178,11 +187,13 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             dst2 = __lsx_vpickev_b(dst5, dst4);
 
             /* pack src to 8 bit */
-            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src,
+                      dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, q1_src);
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3,
+                      dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
 
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -195,12 +206,14 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
         } else if (flag0 == flag1) { /* weak only */
             /* weak filter */
             tc_neg = __lsx_vneg_h(tc_pos);
-            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                      diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
-            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
             abs_delta0 = __lsx_vadda_h(delta0, zero);
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
@@ -208,8 +221,8 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
             temp2 = __lsx_vadd_h(delta0, p0_src);
             temp2 = __lsx_vclip255_h(temp2);
-            temp0 = __lsx_vbitsel_v(temp2, p0_src, __lsx_vnor_v(p_is_pcm_vec,
-                                    p_is_pcm_vec));
+            temp0 = __lsx_vbitsel_v(temp2, p0_src,
+                                    __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec));
             temp2 = __lsx_vsub_h(q0_src, delta0);
             temp2 = __lsx_vclip255_h(temp2);
             temp2 = __lsx_vbitsel_v(temp2, q0_src, __lsx_vnor_v(q_is_pcm_vec,
@@ -218,40 +231,47 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
                       q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
 
             tmp = (beta + (beta >> 1)) >> 3;
-            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp, cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
+                      cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             cmp0 = __lsx_vseqi_d(cmp0, 0);
             p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, cmp0);
 
-            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp, cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
+                      cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             cmp0 = __lsx_vseqi_d(cmp0, 0);
             q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, cmp0);
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
             DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2,
+                      tc_neg, tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
                       delta1, delta2);
-            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
             DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
-            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                      q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
-                      abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src, abs_delta0,
-                      dst1, dst2, dst3, dst4);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0,
+                      p0_src,  abs_delta0, temp2, q0_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, dst1, dst2, dst3, dst4);
             /* pack results to 8 bit */
             DUP2_ARG2(__lsx_vpickev_b, dst2, dst1, dst4, dst3, dst0, dst1);
             /* pack src to 8 bit */
-            DUP2_ARG2(__lsx_vpickev_b, p0_src, p1_src, q1_src, q0_src, dst2, dst3);
+            DUP2_ARG2(__lsx_vpickev_b, p0_src, p1_src, q1_src, q0_src,
+                      dst2, dst3);
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst2, cmp3, dst1, dst3, cmp3, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst2, cmp3, dst1, dst3, cmp3,
+                      dst0, dst1);
 
             p2 += stride;
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -265,7 +285,8 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
@@ -288,12 +309,13 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                      p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1,
+                      p1_src, p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1,  q2_src, temp1, temp0, temp1, temp1);
@@ -316,8 +338,8 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                      q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4,
+                      q1_src, q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
 
             /* pack strong results to 8 bit */
@@ -329,12 +351,14 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                      diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
-            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
             abs_delta0 = __lsx_vadda_h(delta0, zero);
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
@@ -349,47 +373,56 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
 
             tmp = (beta + (beta >> 1)) >> 3;
-            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp, cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
+                      cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
-            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp, cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
+                      cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
 
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
             DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
                       delta1, delta2);
-            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
             DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
-            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                      q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
-                      abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src,
-                      abs_delta0, delta1, delta2, temp0, temp2);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, temp0, p0_src, abs_delta0, temp2,
+                      q0_src, abs_delta0, delta1, delta2, temp0, temp2);
             /* weak filter ends */
 
             /* pack weak results to 8 bit */
-            DUP2_ARG2(__lsx_vpickev_b, delta1, p2_src, temp2, temp0, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, delta1, p2_src, temp2, temp0,
+                      dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, delta2);
 
             /* select between weak or strong */
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp2, dst1, dst4, cmp2, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp2, dst1, dst4, cmp2,
+                      dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp2);
 
             /* pack src to 8 bit */
-            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src,
+                      dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, q1_src);
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3,
+                      dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
 
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -402,13 +435,13 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
     }
 }
 
-static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
-                                         int32_t beta, int32_t *tc,
-                                         uint8_t *p_is_pcm, uint8_t *q_is_pcm)
+void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm)
 {
-    const int32_t stride_2x = (stride << 1);
-    const int32_t stride_4x = (stride << 2);
-    const int32_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
     uint8_t *p3 = src;
     uint8_t *p2 = src + stride_3x;
     uint8_t *p1 = src + stride_4x;
@@ -510,8 +543,8 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             /* strong filter */
             tc_neg = __lsx_vneg_h(tc_pos);
             /* p part */
-            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
-
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
@@ -539,7 +572,8 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
             DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
@@ -571,12 +605,14 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                      diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
-            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
             abs_delta0 = __lsx_vadda_h(delta0, zero);
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
@@ -605,27 +641,31 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
             DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
                       delta1, delta2);
-            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
             DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
-            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                      q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
-                      abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src,
-                      abs_delta0, dst0, dst1, dst2, dst3);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0,
+                      p0_src, abs_delta0, temp2, q0_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, dst0, dst1, dst2, dst3);
             /* weak filter ends */
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            DUP4_ARG3(__lsx_vbitsel_v, dst0, p1_src, cmp3, dst1, p0_src, cmp3, dst2,
-                      q0_src, cmp3, dst3, q1_src, cmp3, dst0, dst1, dst2, dst3);
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p1_src, cmp3, dst1, p0_src,
+                      cmp3, dst2, q0_src, cmp3, dst3, q1_src, cmp3,
+                      dst0, dst1, dst2, dst3);
             DUP2_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst0, dst1);
 
             /* transpose */
@@ -650,7 +690,8 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
 
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
@@ -711,13 +752,15 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                      diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
 
-            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),  __lsx_vslli_h(tc_pos, 1));
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                    __lsx_vslli_h(tc_pos, 1));
             abs_delta0 = __lsx_vadda_h(delta0, zero);
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
@@ -742,38 +785,44 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
             DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
                       delta1, delta2);
-            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
             DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
-            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                      q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
-                      abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src, abs_delta0,
-                      delta1, delta2, temp0, temp2);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, temp0, p0_src, abs_delta0, temp2,
+                      q0_src, abs_delta0, delta1, delta2, temp0, temp2);
             /* weak filter ends*/
 
             /* select between weak or strong */
-            DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp2, dst1, delta1, cmp2, dst2,
-                      temp0, cmp2, dst3, temp2, cmp2, dst0, dst1, dst2, dst3);
-            DUP2_ARG3(__lsx_vbitsel_v, dst4, delta2, cmp2, dst5, q2_src, cmp2, dst4, dst5);
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp2, dst1, delta1,
+                      cmp2, dst2, temp0, cmp2, dst3, temp2, cmp2,
+                      dst0, dst1, dst2, dst3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst4, delta2, cmp2, dst5, q2_src, cmp2,
+                      dst4, dst5);
         }
 
         cmp3 = __lsx_vnor_v(cmp3, cmp3);
         DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp3, dst1, p1_src, cmp3, dst2,
                   p0_src, cmp3, dst3, q0_src, cmp3, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vbitsel_v, dst4, q1_src, cmp3, dst5, q2_src, cmp3, dst4, dst5);
+        DUP2_ARG3(__lsx_vbitsel_v, dst4, q1_src, cmp3, dst5, q2_src, cmp3,
+                  dst4, dst5);
 
         /* pack results to 8 bit */
-        DUP4_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst4, dst4, dst5, dst5,
-                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst4, dst4, dst5,
+                  dst5, dst0, dst1, dst2, dst3);
 
         /* transpose */
         DUP2_ARG2(__lsx_vilvl_b, dst1, dst0, dst3, dst2, dst4, dst6);
@@ -811,9 +860,9 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
     }
 }
 
-static void hevc_loopfilter_chroma_hor_lsx(uint8_t *src, int32_t stride,
-                                           int32_t *tc, uint8_t *p_is_pcm,
-                                           uint8_t *q_is_pcm)
+void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm)
 {
     uint8_t *p1_ptr = src - (stride << 1);
     uint8_t *p0_ptr = src - stride;
@@ -837,8 +886,10 @@ static void hevc_loopfilter_chroma_hor_lsx(uint8_t *src, int32_t stride,
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
 
-        DUP4_ARG2(__lsx_vld, p1_ptr, 0, p0_ptr, 0, q0_ptr, 0, q1_ptr, 0, p1, p0, q0, q1);
-        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vld, p1_ptr, 0, p0_ptr, 0, q0_ptr, 0, q1_ptr, 0,
+                  p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1,
+                  p1, p0, q0, q1);
         DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
         temp0 = __lsx_vslli_h(temp0, 2);
         temp0 = __lsx_vadd_h(temp0, temp1);
@@ -855,20 +906,21 @@ static void hevc_loopfilter_chroma_hor_lsx(uint8_t *src, int32_t stride,
         temp1 = __lsx_vbitsel_v(temp1, q0, q_is_pcm_vec);
 
         tc_pos = __lsx_vslei_d(tc_pos, 0);
-        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos, temp0, temp1);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
+                  temp0, temp1);
         temp0 = __lsx_vpickev_b(temp1, temp0);
         __lsx_vstelm_d(temp0, p0_ptr, 0, 0);
         __lsx_vstelm_d(temp0, p0_ptr + stride, 0, 1);
     }
 }
 
-static void hevc_loopfilter_chroma_ver_lsx(uint8_t *src, int32_t stride,
-                                           int32_t *tc, uint8_t *p_is_pcm,
-                                           uint8_t *q_is_pcm)
+void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm)
 {
-    const int32_t stride_2x = (stride << 1);
-    const int32_t stride_4x = (stride << 2);
-    const int32_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m128i cmp0, cmp1, p_is_pcm_vec, q_is_pcm_vec;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i p1, p0, q0, q1;
@@ -897,7 +949,8 @@ static void hevc_loopfilter_chroma_ver_lsx(uint8_t *src, int32_t stride,
         src -= stride_4x;
         LSX_TRANSPOSE8x4_B(src0, src1, src2, src3, src4, src5, src6, src7,
                            p1, p0, q0, q1);
-        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1,
+                  p1, p0, q0, q1);
 
         DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
         temp0 = __lsx_vslli_h(temp0, 2);
@@ -908,13 +961,14 @@ static void hevc_loopfilter_chroma_ver_lsx(uint8_t *src, int32_t stride,
         temp0 = __lsx_vadd_h(p0, delta);
         temp1 = __lsx_vsub_h(q0, delta);
         DUP2_ARG1(__lsx_vclip255_h, temp0, temp1, temp0, temp1);
-        DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec, q_is_pcm_vec,
-                  p_is_pcm_vec, q_is_pcm_vec);
-        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, p_is_pcm_vec, temp1, q0, q_is_pcm_vec,
-                  temp0, temp1);
+        DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
+                  q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, p_is_pcm_vec, temp1, q0,
+                  q_is_pcm_vec, temp0, temp1);
 
         tc_pos = __lsx_vslei_d(tc_pos, 0);
-        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos, temp0, temp1);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
+                  temp0, temp1);
         temp0 = __lsx_vpackev_b(temp1, temp0);
 
         src += 1;
@@ -961,24 +1015,25 @@ static void hevc_sao_edge_filter_0degree_4width_lsx(uint8_t *dst,
         src0 = __lsx_vshuf_b(zero, src_minus10, shuf1);
         src_plus10 = __lsx_vshuf_b(zero, src_minus10, shuf2);
 
-        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+        diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
 
         offset = __lsx_vadd_b(diff_minus10, diff_minus11);
         offset = __lsx_vaddi_bu(offset, 2);
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
+                  src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset,
+                  sao_offset, sao_offset, offset, offset, offset);
         src0 = __lsx_vxori_b(src0, 128);
         dst0 = __lsx_vsadd_b(src0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1041,32 +1096,33 @@ static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11, shuf1,
-                  src0, src1);
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-                  src_plus10, src_plus11);
-        DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
-                  src_minus10, src_plus10);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros,
+                  src_minus11, shuf1, src0, src1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros,
+                  src_minus11, shuf2, src_plus10, src_plus11);
+        DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11,
+                  src_plus10, src_minus10, src_plus10);
         src0 = __lsx_vpickev_d(src1, src0);
 
-        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+        diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
 
         offset = __lsx_vadd_b(diff_minus10, diff_minus11);
         offset = __lsx_vaddi_bu(offset, 2);
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
+                  src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
         src0 = __lsx_vxori_b(src0, 128);
         dst0 = __lsx_vsadd_b(src0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1076,12 +1132,12 @@ static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
         dst += dst_stride_2x;
     }
 
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11, shuf1,
-              src0, src1);
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-              src_plus10, src_plus11);
-    DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
-              src_minus10, src_plus10);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11,
+              shuf1, src0, src1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_plus10, src_plus11);
+    DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11,
+              src_plus10, src_minus10, src_plus10);
     src0 =  __lsx_vpickev_d(src1, src0);
 
     DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
@@ -1097,8 +1153,8 @@ static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
 
     offset = __lsx_vadd_b(diff_minus10, diff_minus11);
     offset = __lsx_vaddi_bu(offset, 2);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     src0 = __lsx_vxori_b(src0, 128);
     dst0 = __lsx_vsadd_b(src0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1144,78 +1200,91 @@ static void hevc_sao_edge_filter_0degree_16multiple_lsx(uint8_t *dst,
 
     for (; height; height -= 4) {
         src_minus1 = src - 1;
-        DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
-                  src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
-                  src_minus10, src_minus11, src_minus12, src_minus13);
+        src_minus10 = __lsx_vld(src_minus1, 0);
+        DUP2_ARG2(__lsx_vldx, src_minus1, src_stride, src_minus1,
+                  src_stride_2x, src_minus11, src_minus12);
+        src_minus13 = __lsx_vldx(src_minus1, src_stride_3x);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus1 += 16;
             dst_ptr = dst + v_cnt;
-            DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
-                      src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
-                      src10, src11, src12, src13);
-            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf1, src11, src_minus11,
-                      shuf1, src12, src_minus12, shuf1, src13, src_minus13, shuf1,
-                      src_zero0, src_zero1, src_zero2, src_zero3);
-            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf2, src11, src_minus11,
-                      shuf2, src12, src_minus12, shuf2, src13, src_minus13, shuf2,
-                      src_plus10, src_plus11, src_plus12, src_plus13);
-
-            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
+            src10 = __lsx_vld(src_minus1, 0);
+            DUP2_ARG2(__lsx_vldx, src_minus1, src_stride, src_minus1,
+                      src_stride_2x, src11, src12);
+            src13 = __lsx_vldx(src_minus1, src_stride_3x);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf1, src11,
+                      src_minus11, shuf1, src12, src_minus12, shuf1, src13,
+                      src_minus13, shuf1, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf2, src11,
+                      src_minus11, shuf2, src12, src_minus12, shuf2, src13,
+                      src_minus13, shuf2, src_plus10, src_plus11,
+                      src_plus12, src_plus13);
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
                       diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
                       diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
 
-            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11, diff_plus11,
-                      diff_minus12, diff_plus12, diff_minus13, diff_plus13, offset_mask0,
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
                       offset_mask1, offset_mask2, offset_mask3);
-            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
                       offset_mask3);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128,
+                      src_zero2, 128, src_zero3, 128, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
             DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                      dst0, dst1, dst2, dst3);
-            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                      dst0, dst1, dst2, dst3);
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
 
             src_minus10 = src10;
             src_minus11 = src11;
@@ -1264,23 +1333,23 @@ static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
                   src_minus11, src_zero1);
         DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                 diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
 
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
@@ -1289,15 +1358,17 @@ static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
         __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
         dst += dst_stride_2x;
     }
 
-    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-              src_minus11, src10, src10, src_minus10, src_zero0, src_minus11, src_zero1);
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+              src11,  src_minus11, src10, src10, src_minus10, src_zero0,
+              src_minus11, src_zero1);
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -1311,11 +1382,12 @@ static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
 
     DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
               diff_minus11, offset_mask0, offset_mask1);
-    DUP2_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask0, offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+              offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1345,32 +1417,32 @@ static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
 
     /* load in advance */
     DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
-        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-                  src_minus11, src10, src10, src_minus10, src_zero0, src_minus11,
-                  src_zero1);
+        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+                  src11, src_minus11, src10, src10, src_minus10, src_zero0,
+                  src_minus11, src_zero1);
         DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
 
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
@@ -1379,15 +1451,17 @@ static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src10, src11);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
         dst += dst_stride_2x;
     }
 
-    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-              src_minus11, src10, src10, src_minus10, src_zero0, src_minus11, src_zero1);
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+              src11, src_minus11, src10, src10, src_minus10, src_zero0,
+              src_minus11, src_zero1);
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -1399,13 +1473,14 @@ static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
     DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
               const1, cmp_minus11, diff_minus10, diff_minus11);
 
-    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11, diff_minus11,
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
               offset_mask0, offset_mask1);
-    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0, offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 =  __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1449,73 +1524,84 @@ static void hevc_sao_edge_filter_90degree_16multiple_lsx(uint8_t *dst,
         src = src_orig + v_cnt;
         dst = dst_orig + v_cnt;
 
-        DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0,
+                  src_minus10, src_minus11);
 
         for (h_cnt = (height >> 2); h_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src + src_stride_4x, 0,
+            DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                      src, src_stride_3x, src, src_stride_4x,
                       src10, src11, src12, src13);
-
-            DUP4_ARG2(__lsx_vseq_b, src_minus11, src_minus10, src_minus11, src10,
-                      src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
-                      cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vseq_b, src11, src10, src11, src12, src12, src11, src12,
-                      src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
-            DUP4_ARG2(__lsx_vsle_bu, src_minus11, src_minus10, src_minus11, src10,
-                      src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
-                      cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vsle_bu, src11, src10, src11, src12, src12, src11, src12,
-                      src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
+            DUP4_ARG2(__lsx_vseq_b, src_minus11, src_minus10, src_minus11,
+                      src10, src10, src_minus11, src10, src11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src11, src10, src11, src12, src12, src11,
+                      src12, src13, cmp_minus12, cmp_plus12,
+                      cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_minus11, src_minus10, src_minus11,
+                      src10, src10, src_minus11, src10, src11, cmp_minus10,
                       cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src11, src10, src11, src12, src12, src11,
+                      src12, src13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
                       diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
                       diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
 
             DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
                       diff_plus11, diff_minus12, diff_plus12, diff_minus13,
                       diff_plus13, offset_mask0, offset_mask1, offset_mask2,
                       offset_mask3);
-            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0,\
+                      offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
                       offset_mask3);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
 
             src_minus10 = src12;
             DUP4_ARG2(__lsx_vxori_b, src_minus11, 128, src10, 128, src11, 128,
                       src12, 128, src_minus11, src10, src11, src12);
-            DUP4_ARG2(__lsx_vsadd_b, src_minus11, offset_mask0, src10, offset_mask1,
-                      src11, offset_mask2, src12, offset_mask3, dst0, dst1, dst2, dst3);
-            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                      dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vsadd_b, src_minus11, offset_mask0, src10,
+                      offset_mask1, src11, offset_mask2, src12,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
             src_minus11 = src13;
 
             __lsx_vst(dst0, dst, 0);
-            __lsx_vst(dst1, dst + dst_stride, 0);
-            __lsx_vst(dst2, dst + dst_stride_2x, 0);
-            __lsx_vst(dst3, dst + dst_stride_3x, 0);
+            __lsx_vstx(dst1, dst, dst_stride);
+            __lsx_vstx(dst2, dst, dst_stride_2x);
+            __lsx_vstx(dst3, dst, dst_stride_3x);
             src += src_stride_4x;
             dst += dst_stride_4x;
         }
@@ -1547,41 +1633,42 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
               src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
         DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
                   src_plus0, src_plus1);
 
-        DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
-                  src_minus10, src_minus11);
-        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
-                  src_zero0, src_zero1);
-        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1,
+                  src_minus11, src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1,
+                  src_zero1, src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+             diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1590,7 +1677,7 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
                   src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
@@ -1600,13 +1687,13 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
 
     DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
               src_zero0, src_zero1);
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus0,
-              src_plus1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+              src_plus0, src_plus1);
 
     DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
               src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-              src_zero1);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -1622,10 +1709,10 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
               diff_minus11, offset_mask0, offset_mask1);
     DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
               offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1661,40 +1748,40 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
     /* load in advance */
     DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
               src_minus11);
-    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
               src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
         DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
                   src_plus10, src_plus11);
 
-        DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
-                  src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11,
+                  src_minus11, src_minus10, src_minus11);
         DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
                   src_zero0, src_zero1);
         DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+               diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1703,7 +1790,7 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
                   src10, src11)
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
@@ -1712,17 +1799,17 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
 
     DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
               src_zero0, src_zero1);
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus10,
-              src_plus11);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+              src_plus10, src_plus11);
     DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
               src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-              src_zero1);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
 
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
-    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+              cmp_minus11, diff_minus10, diff_minus11);
     DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -1734,10 +1821,10 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
               diff_minus11, offset_mask0, offset_mask1);
     DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
               offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1746,7 +1833,7 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
     src_minus11 = src11;
 
     /* load in advance */
-    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
               src10, src11);
 
     __lsx_vstelm_d(dst0, dst, 0, 0);
@@ -1783,8 +1870,8 @@ static void hevc_sao_edge_filter_45degree_16multiple_lsx(uint8_t *dst,
     __m128i offset_mask0, offset_mask1, offset_mask2, offset_mask3;
     __m128i src10, src_minus10, dst0, src11, src_minus11, dst1;
     __m128i src12, src_minus12, dst2, src13, src_minus13, dst3;
-    __m128i src_zero0, src_plus10, src_zero1, src_plus11, src_zero2, src_plus12;
-    __m128i src_zero3, sao_offset;
+    __m128i src_zero0, src_plus10, src_zero1, src_plus11, src_zero2;
+    __m128i src_zero3, sao_offset, src_plus12;
 
     sao_offset = __lsx_vld(sao_offset_val, 0);
     sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
@@ -1792,81 +1879,98 @@ static void hevc_sao_edge_filter_45degree_16multiple_lsx(uint8_t *dst,
     for (; height; height -= 4) {
         src_orig = src - 1;
         dst_orig = dst;
-        DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
-                  src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
-                  src_minus11, src_minus12, src_minus13, src_minus14);
+        src_minus11 = __lsx_vld(src_orig, 0);
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src_minus12, src_minus13);
+        src_minus14 = __lsx_vldx(src_orig, src_stride_3x);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus10 = __lsx_vld(src_orig - src_stride, 0);
-            DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
-                      src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
-                      src10, src11, src12, src13);
-            src_plus13 = __lsx_vld(src + v_cnt + src_stride_4x, 1);
             src_orig += 16;
+            src10 = __lsx_vld(src_orig, 0);
+            DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig,
+                      src_stride_2x, src11, src12);
+            src13 = __lsx_vldx(src_orig, src_stride_3x);
+            src_plus13 = __lsx_vld(src + v_cnt + src_stride_4x, 1);
 
-            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_minus12,
-                      shuf1, src12, src_minus13, shuf1, src13, src_minus14, shuf1,
-                      src_zero0, src_zero1, src_zero2, src_zero3);
-            DUP2_ARG3(__lsx_vshuf_b, src11, src_minus12, shuf2, src12, src_minus13,
-                      shuf2, src_plus10, src_plus11);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11,
+                      src_minus12, shuf1, src12, src_minus13, shuf1,
+                      src13, src_minus14, shuf1, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_minus12, shuf2, src12,
+                      src_minus13, shuf2, src_plus10, src_plus11);
             src_plus12 = __lsx_vshuf_b(src13, src_minus14, shuf2);
 
-            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10,
+                      cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12,
+                      cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
                       diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
                       diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
 
-            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11, diff_plus11,
-                      diff_minus12, diff_plus12, diff_minus13, diff_plus13, offset_mask0,
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
                       offset_mask1, offset_mask2, offset_mask3);
-            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
                       offset_mask3);
 
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2,
+                      128, src_zero3, 128, src_zero0, src_zero1, src_zero2,
+                      src_zero3);
             DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                      dst0, dst1, dst2, dst3);
-            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                      dst0, dst1, dst2, dst3);
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
 
             src_minus11 = src10;
             src_minus12 = src11;
@@ -1874,9 +1978,9 @@ static void hevc_sao_edge_filter_45degree_16multiple_lsx(uint8_t *dst,
             src_minus14 = src13;
 
             __lsx_vst(dst0, dst_orig, 0);
-            __lsx_vst(dst1, dst_orig + dst_stride, 0);
-            __lsx_vst(dst2, dst_orig + dst_stride_2x, 0);
-            __lsx_vst(dst3, dst_orig + dst_stride_3x, 0);
+            __lsx_vstx(dst1, dst_orig, dst_stride);
+            __lsx_vstx(dst2, dst_orig, dst_stride_2x);
+            __lsx_vstx(dst3, dst_orig, dst_stride_3x);
             dst_orig += 16;
         }
         src += src_stride_4x;
@@ -1910,41 +2014,42 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
               src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
         DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
                   shuf2, src_minus10, src_minus11);
 
-        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                  src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+                  src_minus10, src_minus11);
         DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
                   src_zero0, src_zero1);
         DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+               diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1953,7 +2058,7 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
                   src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
@@ -1963,17 +2068,17 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
 
     DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
               src_zero0, src_zero1);
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-              src_minus10, src_minus11);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_minus10, src_minus11);
 
-    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-              src_minus11);
-    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-              src_zero1);
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
-    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+              cmp_minus11, diff_minus10, diff_minus11);
     DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -1985,10 +2090,10 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
               diff_minus11, offset_mask0, offset_mask1);
     DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
               offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -2024,41 +2129,42 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
-    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
               src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
         DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
                   shuf2, src_minus10, src_minus11);
 
-        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                  src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+                  src_minus10, src_minus11);
         DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
                   src_zero0, src_zero1);
         DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
                   cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+              diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
 
         DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
                   diff_minus11, offset_mask0, offset_mask1);
-        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -2067,7 +2173,7 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
                   src10, src11);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
@@ -2077,13 +2183,13 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
 
     DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
               src_zero0, src_zero1);
-    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-              src_minus10, src_minus11);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_minus10, src_minus11);
 
-    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-              src_minus11);
-    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-              src_zero1);
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
     DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
               cmp_minus10, cmp_minus11);
     DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
@@ -2099,10 +2205,10 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
               diff_minus11, offset_mask0, offset_mask1);
     DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
               offset_mask1);
-    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-              offset, dst0);
-    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-              offset, offset, offset);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -2148,82 +2254,96 @@ static void hevc_sao_edge_filter_135degree_16multiple_lsx(uint8_t *dst,
         src_orig = src - 1;
         dst_orig = dst;
 
-        DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
-                  src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
-                  src_minus11, src_plus10, src_plus11, src_plus12);
+        src_minus11 = __lsx_vld(src_orig, 0);
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src_plus10, src_plus11);
+        src_plus12 = __lsx_vldx(src_orig, src_stride_3x);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus10 = __lsx_vld(src_orig - src_stride, 2);
-            DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
-                      src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
-                      src10, src11, src12, src13);
-            src_plus13 = __lsx_vld(src_orig + src_stride_4x, 0);
+            src_plus13 = __lsx_vldx(src_orig, src_stride_4x);
             src_orig += 16;
-
-            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_plus10,
-                      shuf1, src12, src_plus11, shuf1, src13, src_plus12, shuf1,
-                      src_zero0, src_zero1, src_zero2, src_zero3);
+            src10 = __lsx_vld(src_orig, 0);
+            DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                      src11, src12);
+            src13 =__lsx_vldx(src_orig, src_stride_3x);
+
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11,
+                      src_plus10,  shuf1, src12, src_plus11, shuf1, src13,
+                      src_plus12, shuf1, src_zero0, src_zero1, src_zero2,
+                      src_zero3);
             src_minus11 = __lsx_vshuf_b(src10, src_minus11, shuf2);
-            DUP2_ARG3(__lsx_vshuf_b, src11, src_plus10, shuf2, src12, src_plus11,
-                      shuf2, src_minus12, src_minus13);
-
-            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                      cmp_plus10, cmp_minus11, cmp_plus11);
-            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_plus10, shuf2, src12,
+                      src_plus11, shuf2, src_minus12, src_minus13);
+
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10,  src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
                       diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                      diff_plus10, diff_minus11, diff_plus11);
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
             DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
                       diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                      diff_plus12, diff_minus13, diff_plus13);
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
 
             DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
                       diff_plus11, diff_minus12, diff_plus12, diff_minus13,
                       diff_plus13, offset_mask0, offset_mask1, offset_mask2,
                       offset_mask3);
-            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
                       offset_mask3);
 
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128,
+                      src_zero2, 128, src_zero3, 128, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
             DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                      dst0, dst1, dst2, dst3);
-            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                      dst0, dst1, dst2, dst3);
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
 
             src_minus11 = src10;
             src_plus10 = src11;
@@ -2231,9 +2351,9 @@ static void hevc_sao_edge_filter_135degree_16multiple_lsx(uint8_t *dst,
             src_plus12 = src13;
 
             __lsx_vst(dst0, dst_orig, 0);
-            __lsx_vst(dst1, dst_orig + dst_stride, 0);
-            __lsx_vst(dst2, dst_orig + dst_stride_2x, 0);
-            __lsx_vst(dst3, dst_orig + dst_stride_3x, 0);
+            __lsx_vstx(dst1, dst_orig, dst_stride);
+            __lsx_vstx(dst2, dst_orig, dst_stride_2x);
+            __lsx_vstx(dst3, dst_orig, dst_stride_3x);
             dst_orig += 16;
         }
 
@@ -2242,44 +2362,12 @@ static void hevc_sao_edge_filter_135degree_16multiple_lsx(uint8_t *dst,
     }
 }
 
-void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src,
-                                      ptrdiff_t src_stride,
-                                      int32_t beta, int32_t *tc,
-                                      uint8_t *no_p, uint8_t *no_q)
-{
-    hevc_loopfilter_luma_hor_lsx(src, src_stride, beta, tc, no_p, no_q);
-}
-
-void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src,
-                                      ptrdiff_t src_stride,
-                                      int32_t beta, int32_t *tc,
-                                      uint8_t *no_p, uint8_t *no_q)
-{
-    hevc_loopfilter_luma_ver_lsx(src, src_stride, beta, tc, no_p, no_q);
-}
-
-void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src,
-                                        ptrdiff_t src_stride,
-                                        int32_t *tc, uint8_t *no_p,
-                                        uint8_t *no_q)
-{
-    hevc_loopfilter_chroma_hor_lsx(src, src_stride, tc, no_p, no_q);
-}
-
-void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src,
-                                        ptrdiff_t src_stride,
-                                        int32_t *tc, uint8_t *no_p,
-                                        uint8_t *no_q)
-{
-    hevc_loopfilter_chroma_ver_lsx(src, src_stride, tc, no_p, no_q);
-}
-
 void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                    ptrdiff_t stride_dst,
                                    int16_t *sao_offset_val,
                                    int eo, int width, int height)
 {
-    ptrdiff_t stride_src = (2 * MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE) / sizeof(uint8_t);
+    ptrdiff_t stride_src = (2 * MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE);
 
     switch (eo) {
     case 0:
@@ -2289,8 +2377,8 @@ void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                                         sao_offset_val,
                                                         width - (width & 0x0F),
                                                         height);
-            dst += width - (width & 0x0F);
-            src += width - (width & 0x0F);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
             width &= 0x0F;
         }
 
@@ -2317,8 +2405,8 @@ void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                                          sao_offset_val,
                                                          width - (width & 0x0F),
                                                          height);
-            dst += width - (width & 0x0F);
-            src += width - (width & 0x0F);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
             width &= 0x0F;
         }
 
@@ -2345,8 +2433,8 @@ void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                                          sao_offset_val,
                                                          width - (width & 0x0F),
                                                          height);
-            dst += width - (width & 0x0F);
-            src += width - (width & 0x0F);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
             width &= 0x0F;
         }
 
@@ -2373,8 +2461,8 @@ void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                                           sao_offset_val,
                                                           width - (width & 0x0F),
                                                           height);
-            dst += width - (width & 0x0F);
-            src += width - (width & 0x0F);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
             width &= 0x0F;
         }
 
diff --git a/libavcodec/loongarch/hevc_macros_lsx.h b/libavcodec/loongarch/hevc_macros_lsx.h
deleted file mode 100644
index 809efc289c..0000000000
--- a/libavcodec/loongarch/hevc_macros_lsx.h
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
- * Contributed by Lu Wang <wanglu@loongson.cn>
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- */
-
-#ifndef AVCODEC_LOONGARCH_HEVC_MACROS_LSX_H
-#define AVCODEC_LOONGARCH_HEVC_MACROS_LSX_H
-
-static inline __m128i __lsx_hevc_filt_8tap_h(__m128i in0, __m128i in1, __m128i in2,
-                                             __m128i in3, __m128i filt0, __m128i filt1,
-                                             __m128i filt2, __m128i filt3)
-{
-    __m128i out_m;
-
-    out_m = __lsx_vdp2_h_b(in0, filt0);
-    out_m = __lsx_vdp2add_h_b(out_m, in1, filt1);
-    out_m = __lsx_vdp2add_h_b(out_m, in2, filt2);
-    out_m = __lsx_vdp2add_h_b(out_m, in3, filt3);
-    return out_m;
-}
-
-static inline __m128i __lsx_hevc_filt_8tap_w(__m128i in0, __m128i in1, __m128i in2,
-                                             __m128i in3, __m128i filt0, __m128i filt1,
-                                             __m128i filt2, __m128i filt3)
-{
-    __m128i out_m;
-
-    out_m = __lsx_vdp2_w_h(in0, filt0);
-    out_m = __lsx_vdp2add_w_h(out_m, in1, filt1);
-    out_m = __lsx_vdp2add_w_h(out_m, in2, filt2);
-    out_m = __lsx_vdp2add_w_h(out_m, in3, filt3);
-    return out_m;
-}
-
-static inline __m128i __lsx_hevc_filt_4tap_h(__m128i in0, __m128i in1, __m128i filt0,
-                                             __m128i filt1)
-{
-    __m128i out_m;
-
-    out_m = __lsx_vdp2_h_b(in0, filt0);
-    out_m = __lsx_vdp2add_h_b(out_m, in1, filt1);
-    return out_m;
-}
-
-static inline __m128i __lsx_hevc_filt_4tap_w(__m128i in0, __m128i in1, __m128i filt0,
-                                             __m128i filt1)
-{
-    __m128i out_m;
-
-    out_m = __lsx_vdp2_w_h(in0, filt0);
-    out_m = __lsx_vdp2add_w_h(out_m, in1, filt1);
-    return out_m;
-}
-
-#endif  /* AVCODEC_LOONGARCH_HEVC_MACROS_LSX_H */
diff --git a/libavcodec/loongarch/hevc_mc_bi_lsx.c b/libavcodec/loongarch/hevc_mc_bi_lsx.c
index 00ee34fff8..9092fdccb2 100644
--- a/libavcodec/loongarch/hevc_mc_bi_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_bi_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -21,7 +22,6 @@
 
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
-#include "hevc_macros_lsx.h"
 
 static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     /* 8 width cases */
@@ -29,200 +29,159 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20
 };
 
-static av_always_inline
-void __lsx_hevc_bi_rnd_clip2(__m128i in0, __m128i in1, __m128i vec0, __m128i vec1,
-                             __m128i *out0, __m128i *out1)
+static av_always_inline __m128i
+hevc_bi_rnd_clip(__m128i in0, __m128i vec0, __m128i in1, __m128i vec1)
 {
-    *out0 = __lsx_vsadd_h(vec0, in0);
-    *out1 = __lsx_vsadd_h(vec1, in1);
-    *out0 = __lsx_vsrari_h(*out0, 7);
-    *out1 = __lsx_vsrari_h(*out1, 7);
-    *out0 = __lsx_vclip255_h(*out0);
-    *out1 = __lsx_vclip255_h(*out1);
-}
+    __m128i out;
 
-static av_always_inline
-void __lsx_hevc_bi_rnd_clip4(__m128i in0, __m128i in1, __m128i in2, __m128i in3,
-                             __m128i vec0, __m128i vec1, __m128i vec2,
-                             __m128i vec3, __m128i *out0, __m128i *out1,
-                             __m128i *out2, __m128i *out3)
-{
-    __lsx_hevc_bi_rnd_clip2(in0, in1, vec0, vec1, out0, out1);
-    __lsx_hevc_bi_rnd_clip2(in2, in3, vec2, vec3, out2, out3);
+    vec0 = __lsx_vsadd_h(in0, vec0);
+    vec1 = __lsx_vsadd_h(in1, vec1);
+    out  = __lsx_vssrarni_bu_h(vec1, vec0, 7);
+    return out;
 }
 
-static void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr,
-                                int32_t src_stride,
-                                int16_t *src1_ptr,
-                                int32_t src2_stride,
-                                uint8_t *dst,
-                                int32_t dst_stride,
-                                int32_t height)
+/* hevc_bi_copy: dst = av_clip_uint8((src0 << 6 + src1) >> 7) */
+static
+void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt, tp0, tp1, tp2, tp3;
-    uint64_t tpd0, tpd1, tpd2, tpd3;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = (height & 0x07) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_2x = (src2_stride << 1);
+    int32_t src2_stride_4x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride;
     __m128i src0, src1;
-    __m128i zero = {0};
+    __m128i zero = __lsx_vldi(0);
     __m128i in0, in1, in2, in3;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i reg0, reg1, reg2, reg3;
     __m128i dst0, dst1, dst2, dst3;
 
-    if (2 == height) {
-        tp0 = *(uint32_t *)src0_ptr;
-        tp1 = *(uint32_t *)(src0_ptr + src_stride);
-        DUP2_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, src0);
-        tpd0 = *(uint64_t *)src1_ptr;
-        tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
-        DUP2_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in0, in0);
-
-        dst0 = __lsx_vilvl_b(zero, src0);
-        dst0 = __lsx_vslli_h(dst0, 6);
-        dst0 = __lsx_vadd_h(dst0, in0);
-        dst0 = __lsx_vmaxi_h(dst0, 0);
-        dst0 = __lsx_vssrlrni_bu_h(dst0, dst0, 7);
-        __lsx_vstelm_w(dst0, dst, 0, 0);
-        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
-    } else if (4 == height) {
-        tp0 = *(uint32_t *)src0_ptr;
-        tp1 = *(uint32_t *)(src0_ptr + src_stride);
-        tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
-        tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2, src0,
-                  tp3, 3, src0, src0, src0, src0);
-        tpd0 = *(uint64_t *)src1_ptr;
-        tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
-        tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
-        tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0, in1,
-                  tpd3, 1, in0, in0, in1, in1);
-        dst0 = __lsx_vilvl_b(zero, src0);
-        dst1 = __lsx_vilvh_b(zero, src0);
-        DUP2_ARG2(__lsx_vslli_h, dst0, 6, dst1, 6, dst0, dst1);
-        __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
-        dst0 = __lsx_vpickev_b(dst1, dst0);
+    for (;loop_cnt--;) {
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_w(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_w(src0_ptr + src_stride_3x, 0);
+        src0_ptr += src_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+        src0 = __lsx_vilvl_d(tmp1, tmp0);
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_w(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_w(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+        src1 = __lsx_vilvl_d(tmp1, tmp0);
+        src0_ptr += src_stride_4x;
+
+        tmp0 = __lsx_vldrepl_d(src1_ptr, 0);
+        tmp1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        tmp2 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+        tmp3 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+        src1_ptr += src2_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, in0, in1);
+        tmp0 = __lsx_vldrepl_d(src1_ptr, 0);
+        tmp1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        tmp2 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+        tmp3 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+        src1_ptr += src2_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, in2, in3);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
+        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
         __lsx_vstelm_w(dst0, dst, 0, 0);
         __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
         __lsx_vstelm_w(dst0, dst + dst_stride_2x, 0, 2);
         __lsx_vstelm_w(dst0, dst + dst_stride_3x, 0, 3);
-    } else if (0 == (height & 0x07)) {
-        for (loop_cnt = (height >> 3); loop_cnt--;) {
-            tp0 = *(uint32_t *)src0_ptr;
-            tp1 = *(uint32_t *)(src0_ptr + src_stride);
-            tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
-            tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2,
-                      src0, tp3, 3, src0, src0, src0, src0);
-            src0_ptr += src_stride_4x;
-            tp0 = *(uint32_t *)src0_ptr;
-            tp1 = *(uint32_t *)(src0_ptr + src_stride);
-            tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
-            tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_w, src1, tp0, 0, src1, tp1, 1, src1, tp2, 2,
-                      src1, tp3, 3, src1, src1, src1, src1);
-            src0_ptr += src_stride_4x;
-            tpd0 = *(uint64_t *)src1_ptr;
-            tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
-            tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
-            tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0,
-                      in1, tpd3, 1, in0, in0, in1, in1);
-            src1_ptr += src2_stride_4x;
-            tpd0 = *(uint64_t *)src1_ptr;
-            tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
-            tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
-            tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_d, in2, tpd0, 0, in2, tpd1, 1, in3, tpd2, 0,
-                      in3, tpd3, 1, in2, in2, in3, in3);
-            src1_ptr += src2_stride_4x;
-            DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-            DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-            DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
-
-            __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                    &dst0, &dst1, &dst2, &dst3);
-            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
-            __lsx_vstelm_w(dst0, dst, 0, 0);
-            __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
-            __lsx_vstelm_w(dst0, dst + dst_stride_2x, 0, 2);
-            __lsx_vstelm_w(dst0, dst + dst_stride_3x, 0, 3);
-            dst += dst_stride_4x;
-            __lsx_vstelm_w(dst1, dst, 0, 0);
-            __lsx_vstelm_w(dst1, dst + dst_stride, 0, 1);
-            __lsx_vstelm_w(dst1, dst + dst_stride_2x, 0, 2);
-            __lsx_vstelm_w(dst1, dst + dst_stride_3x, 0, 3);
-            dst += dst_stride_4x;
-        }
+        dst += dst_stride_4x;
+        __lsx_vstelm_w(dst1, dst, 0, 0);
+        __lsx_vstelm_w(dst1, dst + dst_stride, 0, 1);
+        __lsx_vstelm_w(dst1, dst + dst_stride_2x, 0, 2);
+        __lsx_vstelm_w(dst1, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+    }
+    for(;res--;) {
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src1_ptr, 0);
+        reg3 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        src0 = __lsx_vilvl_w(reg1, reg0);
+        in0  = __lsx_vilvl_d(reg3, reg2);
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst0 = __lsx_vsadd_h(dst0, in0);
+        dst0 = __lsx_vssrarni_bu_h(dst0, dst0, 7);
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
+        src0_ptr += src_stride_2x;
+        src1_ptr += src2_stride_2x;
+        dst += dst_stride_2x;
     }
 }
 
-static void hevc_bi_copy_6w_lsx(uint8_t *src0_ptr,
-                                int32_t src_stride,
-                                int16_t *src1_ptr,
-                                int32_t src2_stride,
-                                uint8_t *dst,
-                                int32_t dst_stride,
-                                int32_t height)
+static
+void hevc_bi_copy_6w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt;
-    uint64_t tp0, tp1, tp2, tp3;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t loop_cnt;
+    int32_t res = (height & 0x07) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
     __m128i out0, out1, out2, out3;
-    __m128i zero = {0};
+    __m128i zero = __lsx_vldi(0);
     __m128i src0, src1, src2, src3;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i reg0, reg1, reg2, reg3;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-        tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                  tp3, 1, src0, src0, src1, src1);
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src0, src1);
         src0_ptr += src_stride_4x;
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-        tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0, src3,
-                  tp3, 1, src2, src2, src3, src3);
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src2, src3);
         src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        src1_ptr += src2_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in4, in5, in6, in7);
-        src1_ptr += src2_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in5, in6);
+        in7 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
         DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   dst0, dst2, dst4, dst6);
         DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
                   dst1, dst3, dst5, dst7);
         DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
                   dst5, dst7);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        out3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
         __lsx_vstelm_w(out0, dst, 0, 0);
         __lsx_vstelm_w(out0, dst + dst_stride, 0, 2);
         __lsx_vstelm_h(out0, dst, 4, 2);
@@ -242,196 +201,158 @@ static void hevc_bi_copy_6w_lsx(uint8_t *src0_ptr,
         __lsx_vstelm_h(out3, dst + dst_stride_3x, 4, 6);
         dst += dst_stride_4x;
     }
+    for (;res--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        src0 = __lsx_vilvl_d(reg1, reg0);
+        src0_ptr += src_stride_2x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        in1 = __lsx_vldx(src1_ptr, src2_stride_x);
+        src1_ptr += src2_stride_x;
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst1 = __lsx_vilvh_b(zero, src0);
+        dst1 = __lsx_vslli_h(dst1, 6);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        __lsx_vstelm_h(out0, dst, 4, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        __lsx_vstelm_h(out0, dst, 4, 6);
+        dst += dst_stride;
+    }
 }
 
-static void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr,
-                                int32_t src_stride,
-                                int16_t *src1_ptr,
-                                int32_t src2_stride,
-                                uint8_t *dst,
-                                int32_t dst_stride,
-                                int32_t height)
+static
+void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint64_t tp0, tp1, tp2, tp3;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = (height & 7) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
     __m128i out0, out1, out2, out3;
     __m128i src0, src1, src2, src3;
-    __m128i zero = {0};
+    __m128i zero = __lsx_vldi(0);
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i reg0, reg1, reg2, reg3;
 
-    if (2 == height) {
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        DUP2_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src0, src0);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
-        dst1 = __lsx_vilvh_b(zero, src0);
-        dst1 = __lsx_vslli_h(dst1, 6);
-        __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
-        out0 = __lsx_vpickev_b(dst1, dst0);
-        __lsx_vstelm_d(out0, dst, 0, 0);
-        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
-    } else if (4 == height) {
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-        tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                  tp3, 1, src0, src0, src1, src1);
-        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0, in0,
-                  in1, in2, in3);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src0, src1);
+        src0_ptr += src_stride_4x;
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src2, src3);
+        src0_ptr += src_stride_4x;
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1,
+                  dst3, dst5, dst7);
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in5, in6);
+        in7 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        out3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
         __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
-    } else if (6 == height) {
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-        tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                  tp3, 1, src0, src0, src1, src1);
-        src0_ptr += src_stride_4x;
-        tp0 = *(uint64_t *)src0_ptr;
-        tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        DUP2_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src2, src2);
-
-        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
-        dst4 = __lsx_vsllwil_hu_bu(src2, 6);
-        dst5 = __lsx_vilvh_b(zero, src2);
-        dst5 = __lsx_vslli_h(dst5, 6);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        src1_ptr += src2_stride_4x;
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        out2 = __lsx_vpickev_b(dst5, dst4);
+        dst += dst_stride_4x;
+        __lsx_vstelm_d(out2, dst, 0, 0);
+        __lsx_vstelm_d(out2, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out3, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        src0 = __lsx_vilvl_d(reg1, reg0);
+        in0  = __lsx_vld(src1_ptr, 0);
+        in1  = __lsx_vldx(src1_ptr, src2_stride_x);
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst1 = __lsx_vilvh_b(zero, src0);
+        dst1 = __lsx_vslli_h(dst1, 6);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
-        __lsx_vstelm_d(out2, dst + dst_stride_4x, 0, 0);
-        __lsx_vstelm_d(out2, dst + dst_stride_4x + dst_stride, 0, 1);
-    } else if (0 == (height & 0x07)) {
-        uint32_t loop_cnt;
-
-        for (loop_cnt = (height >> 3); loop_cnt--;) {
-            tp0 = *(uint64_t *)src0_ptr;
-            tp1 = *(uint64_t *)(src0_ptr + src_stride);
-            tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-            tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0,
-                      src1, tp3, 1, src0, src0, src1, src1);
-            src0_ptr += src_stride_4x;
-            tp0 = *(uint64_t *)src0_ptr;
-            tp1 = *(uint64_t *)(src0_ptr + src_stride);
-            tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
-            tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-            DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0,
-                      src3, tp3, 1, src2, src2, src3, src3);
-            src0_ptr += src_stride_4x;
-            DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6, dst0,
-                      dst2, dst4, dst6);
-            DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst1, dst3, dst5, dst7);
-            DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1,
-                      dst3, dst5, dst7);
-            DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
-            src1_ptr += src2_stride_4x;
-            DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in4, in5, in6, in7);
-            src1_ptr += src2_stride_4x;
-            __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2,
-                                    dst3, &dst0, &dst1, &dst2, &dst3);
-            __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6,
-                                    dst7, &dst4, &dst5, &dst6, &dst7);
-            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-            DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
-            __lsx_vstelm_d(out0, dst, 0, 0);
-            __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
-            __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
-            __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
-            dst += dst_stride_4x;
-            __lsx_vstelm_d(out2, dst, 0, 0);
-            __lsx_vstelm_d(out2, dst + dst_stride, 0, 1);
-            __lsx_vstelm_d(out3, dst + dst_stride_2x, 0, 0);
-            __lsx_vstelm_d(out3, dst + dst_stride_3x, 0, 1);
-            dst += dst_stride_4x;
-        }
+        src0_ptr += src_stride_2x;
+        src1_ptr += src2_stride_x;
+        dst += dst_stride_2x;
     }
 }
 
-static void hevc_bi_copy_12w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_12w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t* _src1 = src1_ptr + 8;
     __m128i out0, out1, out2;
     __m128i src0, src1, src2, src3;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                  src0, src1, src2, src3);
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
         src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                  in4, in5, in6, in7);
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
 
-        src1_ptr += src2_stride_4x;
         DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
         DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   dst0, dst1, dst2, dst3)
         DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
         DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst4, dst5)
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        out2 = __lsx_vpickev_b(dst5, dst4);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -444,24 +365,22 @@ static void hevc_bi_copy_12w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t *_src1 = src1_ptr + 8;
     __m128i out0, out1, out2, out3;
     __m128i src0, src1, src2, src3;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
@@ -469,30 +388,32 @@ static void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr,
     __m128i zero = {0};
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                  src0, src1, src2, src3);
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
         src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                  in4, in5, in6, in7);
-        src1_ptr += src2_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
         DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   dst0_r, dst1_r, dst2_r, dst3_r)
         DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
                   dst0_l, dst1_l, dst2_l, dst3_l);
-        DUP4_ARG2(__lsx_vslli_h, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
-                  dst1_l, dst2_l, dst3_l);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in4, in5, dst0_r, dst1_r, dst0_l,
-                                dst1_l, &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-        __lsx_hevc_bi_rnd_clip4(in2, in3, in6, in7, dst2_r, dst3_r, dst2_l,
-                                dst3_l, &dst2_r, &dst3_r, &dst2_l, &dst3_l);
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, out0, out1);
-        DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, out2, out3);
+        DUP4_ARG2(__lsx_vslli_h, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6,
+                  dst0_l, dst1_l, dst2_l, dst3_l);
+
+        out0 = hevc_bi_rnd_clip(in0, dst0_r, in4, dst0_l);
+        out1 = hevc_bi_rnd_clip(in1, dst1_r, in5, dst1_l);
+        out2 = hevc_bi_rnd_clip(in2, dst2_r, in6, dst2_l);
+        out3 = hevc_bi_rnd_clip(in3, dst3_r, in7, dst3_l);
         __lsx_vst(out0, dst, 0);
         __lsx_vstx(out1, dst, dst_stride);
         __lsx_vstx(out2, dst, dst_stride_2x);
@@ -501,245 +422,54 @@ static void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_bi_copy_24w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
-    __m128i out0, out1, out2, out3, out4, out5;
-    __m128i zero = {0};
-    __m128i in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11;
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10, dst11;
-
-    for (loop_cnt = 8; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                  in0, in1, in4, in5);
-        DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
-                  src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
-                  in2, in3, in6, in7);
-        DUP4_ARG2(__lsx_vsllwil_hu_bu, in0, 6, in1, 6, in2, 6, in3, 6,
-                  dst0, dst2, dst4, dst5)
-
-        DUP4_ARG2(__lsx_vilvh_b, zero, in0, zero, in1, zero, in4, zero, in5,
-                  dst1, dst3, dst7, dst9);
-        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst7, 6, dst9, 6,
-                  dst1, dst3, dst7, dst9);
-        DUP4_ARG2(__lsx_vsllwil_hu_bu, in4, 6, in5, 6, in6, 6, in7, 6,
-                  dst6, dst8, dst10, dst11)
-
-        src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                  in4, in5, in6, in7);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32,
-                  src1_ptr + src2_stride_2x, 32, src1_ptr + src2_stride_3x, 32,
-                  in8, in9, in10, in11);
-        src1_ptr += src2_stride_4x;
-
-        __lsx_hevc_bi_rnd_clip4(in0, in4, in1, in5, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in8, in9, in2, in6, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-        __lsx_hevc_bi_rnd_clip4(in3, in7, in10, in11, dst8, dst9, dst10, dst11,
-                                &dst8, &dst9, &dst10, &dst11);
-        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                  out0, out1, out2, out3);
-        DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
-        __lsx_vst(out0, dst, 0);
-        __lsx_vstelm_d(out2, dst, 16, 0);
-        __lsx_vstx(out1, dst, dst_stride);
-        __lsx_vstelm_d(out2, dst + dst_stride, 16, 1);
-        __lsx_vstx(out3, dst, dst_stride_2x);
-        __lsx_vstelm_d(out5, dst + dst_stride_2x, 16, 0);
-        __lsx_vstx(out4, dst, dst_stride_3x);
-        __lsx_vstelm_d(out5, dst + dst_stride_3x, 16, 1);
-        dst += dst_stride_4x;
-    }
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_8w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
 }
 
-static void hevc_bi_copy_32w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i out0, out1, out2, out3;
-    __m128i src0, src1, src2, src3;
-    __m128i zero = {0};
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
-
-    for (loop_cnt = (height >> 1); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        src0_ptr += src_stride;
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src3);
-        src0_ptr += src_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                  in0, in1, in2, in3);
-        src1_ptr += src2_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                  in4, in5, in6, in7);
-        src1_ptr += src2_stride;
-
-        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                  dst0, dst2, dst4, dst6)
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  dst1, dst3, dst5, dst7);
-        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                  dst5, dst7);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
-        __lsx_vst(out0, dst, 0);
-        __lsx_vst(out1, dst, 16);
-        dst += dst_stride;
-        __lsx_vst(out2, dst, 0);
-        __lsx_vst(out3, dst, 16);
-        dst += dst_stride;
-    }
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
 }
 
-static void hevc_bi_copy_48w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i out0, out1, out2, out3, out4, out5;
-    __m128i src0, src1, src2, src3, src4, src5;
-    __m128i zero = {0};
-    __m128i in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11;
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10, dst11;
-
-    for (loop_cnt = (height >> 1); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        src2 = __lsx_vld(src0_ptr, 32);
-        src0_ptr += src_stride;
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src3, src4);
-        src5 = __lsx_vld(src0_ptr, 32);
-        src0_ptr += src_stride;
-
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                  in0, in1, in2, in3);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
-        src1_ptr += src2_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                  in6, in7, in8, in9);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in10, in11);
-        src1_ptr += src2_stride;
-
-        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                  dst0, dst2, dst4, dst6);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  dst1, dst3, dst5, dst7);
-        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                  dst5, dst7);
-        DUP2_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, dst8, dst10);
-        DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, dst9, dst11);
-        DUP2_ARG2(__lsx_vslli_h, dst9, 6, dst11, 6, dst9, dst11);
-
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-        __lsx_hevc_bi_rnd_clip4(in8, in9, in10, in11, dst8, dst9, dst10, dst11,
-                                &dst8, &dst9, &dst10, &dst11);
-        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                  out0, out1, out2, out3);
-        DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
-        __lsx_vst(out0, dst, 0);
-        __lsx_vst(out1, dst, 16);
-        __lsx_vst(out2, dst, 32);
-        dst += dst_stride;
-        __lsx_vst(out3, dst, 0);
-        __lsx_vst(out4, dst, 16);
-        __lsx_vst(out5, dst, 32);
-        dst += dst_stride;
-    }
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_32w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
 }
 
-static void hevc_bi_copy_64w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 int32_t height)
+static
+void hevc_bi_copy_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i out0, out1, out2, out3;
-    __m128i src0, src1, src2, src3;
-    __m128i zero = {0};
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
-
-    for (loop_cnt = height; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0_ptr, 32, src0_ptr, 48,
-                  src0, src1, src2, src3);
-        src0_ptr += src_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                  in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr, 112,
-                  in4, in5, in6, in7);
-        src1_ptr += src2_stride;
-
-        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                  dst0, dst2, dst4, dst6);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  dst1, dst3, dst5, dst7);
-        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                  dst5, dst7);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
-
-        __lsx_vst(out0, dst, 0);
-        __lsx_vst(out1, dst, 16);
-        __lsx_vst(out2, dst, 32);
-        __lsx_vst(out3, dst, 48);
-        dst += dst_stride;
-    }
+    hevc_bi_copy_32w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_32w_lsx(src0_ptr + 32, src_stride, src1_ptr + 32, src2_stride,
+                         dst + 32, dst_stride, height);
 }
 
-static void hevc_hz_bi_8t_16w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr,  int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
     const int32_t dst_stride_2x = (dst_stride << 1);
@@ -749,13 +479,11 @@ static void hevc_hz_bi_8t_16w_lsx(uint8_t *src0_ptr,
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3;
     __m128i in0, in1, in2, in3;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src0_ptr -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
@@ -769,43 +497,44 @@ static void hevc_hz_bi_8t_16w_lsx(uint8_t *src0_ptr,
         src1_ptr += src2_stride;
         DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
         src1_ptr += src2_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3, &dst0, &dst1,
-                                &dst2, &dst3);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vstx(dst1, dst, dst_stride);
         dst += dst_stride_2x;
     }
 }
 
-static void hevc_hz_bi_8t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
     __m128i src0, src1, tmp0, tmp1;
@@ -814,16 +543,14 @@ static void hevc_hz_bi_8t_24w_lsx(uint8_t *src0_ptr,
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2;
     __m128i in0, in1, in2;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src0_ptr -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
 
@@ -833,26 +560,24 @@ static void hevc_hz_bi_8t_24w_lsx(uint8_t *src0_ptr,
         DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
         in2 = __lsx_vld(src1_ptr, 32);
         src1_ptr += src2_stride;
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                  mask0, src0, src0, mask1, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, dst0, vec3, filt1, dst0, dst1, dst2, dst0);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask5, src1, src1, mask1, src0, src0,
-                  mask2, src1, src0, mask6, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst1, vec0, filt1, dst2, vec1, filt1, dst0, vec2,
-                  filt2, dst1, vec3, filt2, dst1, dst2, dst0, dst1);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1,
+                  src1, mask0, src0, src0, mask1, vec0, vec1, vec2, vec3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec2, filt0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask5, src1, src1, mask1, src0,
+                  src0, mask2, src1, src0, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec0, filt1, dst2, vec1, filt1,
+                  dst0, vec2, filt2, dst1, vec3, filt2, dst1, dst2, dst0, dst1);
         DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask2, src0, src0, mask3, src1, src0,
                   mask7, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst2, vec0, filt2, dst0, vec1, filt3, dst1,
-                  vec2, filt3, dst2, vec3, filt3, dst2, dst0, dst1, dst2);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec0, filt2, dst0, vec1, filt3,
+                  dst1, vec2, filt3, dst2, vec3, filt3, dst2, dst0, dst1, dst2);
 
-        __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
-        tmp0 = __lsx_vpickev_b(dst1, dst0);
+        tmp0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
         dst2 = __lsx_vsadd_h(dst2, in2);
-        dst2 = __lsx_vmaxi_h(dst2, 0);
-        tmp1 = __lsx_vssrlrni_bu_h(dst2, dst2, 7);
+        tmp1 = __lsx_vssrarni_bu_h(dst2, dst2, 7);
 
         __lsx_vst(tmp0, dst, 0);
         __lsx_vstelm_d(tmp1, dst, 16, 0);
@@ -860,273 +585,54 @@ static void hevc_hz_bi_8t_24w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_hz_bi_8t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i src0, src1, src2, tmp0, tmp1;
-    __m128i filt0, filt1, filt2, filt3;
-    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
-    __m128i vec0, vec1, vec2, vec3;
-    __m128i dst0, dst1, dst2, dst3;
-    __m128i in0, in1, in2, in3;
-    __m128i const_vec;
-    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-
-    src0_ptr -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
-
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
-    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
-    mask7 = __lsx_vaddi_bu(mask0, 14);
-
-    for (loop_cnt = height; loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        src2 = __lsx_vld(src0_ptr, 24);
-        src0_ptr += src_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                  48, in0, in1, in2, in3);
-        src1_ptr += src2_stride;
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2, vec2,
-                  filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2, vec2,
-                  filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
-        __lsx_vst(tmp0, dst, 0);
-        __lsx_vst(tmp1, dst, 16);
-        dst += dst_stride;
-    }
+    hevc_hz_8t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
 }
 
-static void hevc_hz_bi_8t_48w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i src0, src1, src2, src3;
-    __m128i tmp0, tmp1, tmp2;
-    __m128i filt0, filt1, filt2, filt3;
-    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
-    __m128i vec0, vec1, vec2, vec3;
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
-    __m128i in0, in1, in2, in3, in4, in5;
-    __m128i const_vec;
-    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-
-    src0_ptr -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
-
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
-    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
-    mask7 = __lsx_vaddi_bu(mask0, 14);
-
-    for (loop_cnt = 64; loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 40, src2, src3);
-        src0_ptr += src_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                  48, in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
-
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                  mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                  mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                  mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                  mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
-        __lsx_hevc_bi_rnd_clip2(in2, in3, dst2, dst3, &dst2, &dst3);
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
-        __lsx_vst(tmp0, dst, 0);
-        __lsx_vst(tmp1, dst, 16);
-
-        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
-        src1_ptr += src2_stride;
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, src2, src2,
-                  mask1, src3, src3, mask1, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  dst4, vec2, filt1, dst5, vec3, filt1, dst4, dst5, dst4, dst5);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, src2, src2,
-                  mask3, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst4, vec0, filt2, dst5, vec1, filt2, dst4,
-                  vec2, filt3, dst5, vec3, filt3, dst4, dst5, dst4, dst5);
-
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
-        tmp2 = __lsx_vpickev_b(dst5, dst4);
-        __lsx_vst(tmp2, dst, 32);
-        dst += dst_stride;
-    }
+    hevc_hz_8t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_32w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
 }
 
-static void hevc_hz_bi_8t_64w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    uint32_t loop_cnt;
-    __m128i src0, src1, src2, src3, src4, src5, tmp0, tmp1;
-    __m128i filt0, filt1, filt2, filt3;
-    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
-    __m128i vec0, vec1, vec2, vec3;
-    __m128i dst0, dst1, dst2, dst3;
-    __m128i in0, in1, in2, in3;
-    __m128i const_vec;
-    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-
-    src0_ptr -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
-
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
-    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
-    mask7 = __lsx_vaddi_bu(mask0, 14);
-
-    for (loop_cnt = height; loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        src2 = __lsx_vld(src0_ptr, 24);
-        DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 48, src3, src4);
-        src5 = __lsx_vld(src0_ptr, 56);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                  48, in0, in1, in2, in3);
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
-        __lsx_vst(tmp0, dst, 0);
-        __lsx_vst(tmp1, dst, 16);
-
-        src0 = src3;
-        src1 = src4;
-        src2 = src5;
-
-        DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr,
-                  112, in0, in1, in2, in3);
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
-        __lsx_vst(tmp0, dst, 32);
-        __lsx_vst(tmp1, dst, 48);
-        src1_ptr += src2_stride;
-        src0_ptr += src_stride;
-        dst += dst_stride;
-    }
+    hevc_hz_8t_32w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_32w_lsx(src0_ptr + 32, src_stride, src1_ptr + 32, src2_stride,
+                       dst + 32, dst_stride, filter, height);
 }
 
 static av_always_inline
-void hevc_vt_bi_8t_8w_lsx(uint8_t *src0_ptr,
-                          int32_t src_stride,
-                          int16_t *src1_ptr,
-                          int32_t src2_stride,
-                          uint8_t *dst,
-                          int32_t dst_stride,
-                          const int8_t *filter,
-                          int32_t height)
+void hevc_vt_8t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                       int32_t src2_stride, uint8_t *dst, int32_t dst_stride,\
+                       const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
     __m128i src0, src1, src2, src3, src4, src5;
     __m128i src6, src7, src8, src9, src10;
     __m128i in0, in1, in2, in3;
@@ -1134,59 +640,53 @@ void hevc_vt_bi_8t_8w_lsx(uint8_t *src0_ptr,
     __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
     __m128i dst0_r, dst1_r, dst2_r, dst3_r;
     __m128i filt0, filt1, filt2, filt3;
-    __m128i const_vec;
 
     src0_ptr -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src0_ptr, src_stride_3x);
     src0_ptr += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src0_ptr + src_stride_2x, 0);
+    src4 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src5, src6);
     src0_ptr += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 =  __lsx_vxori_b(src6, 128);
     DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
               src10_r, src32_r, src54_r, src21_r);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                  src7, src8, src9, src10);
+        src7 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src8, src9);
+        src10 = __lsx_vldx(src0_ptr, src_stride_3x);
         src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        src1_ptr += src2_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10);
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr, src2_stride_2x,
+                  in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
         DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
                   src76_r, src87_r, src98_r, src109_r);
 
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                  dst0_r, dst0_r, dst0_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                  dst1_r, dst1_r, dst1_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                  dst2_r, dst2_r, dst2_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                  dst3_r, dst3_r, dst3_r);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst2_r, dst3_r,
-                                &dst0_r, &dst1_r, &dst2_r, &dst3_r);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                  filt0, src43_r, filt0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r, src43_r,
+                  filt1, dst2_r, src54_r, filt1, dst3_r, src65_r, filt1,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src54_r, filt2, dst1_r, src65_r,
+                  filt2, dst2_r, src76_r, filt2, dst3_r, src87_r, filt2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src76_r, filt3, dst1_r, src87_r,
+                  filt3, dst2_r, src98_r, filt3, dst3_r, src109_r, filt3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in1, dst1_r);
+        dst1_r = hevc_bi_rnd_clip(in2, dst2_r, in3, dst3_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(dst1_r, dst + dst_stride_2x, 0, 0);
@@ -1205,26 +705,21 @@ void hevc_vt_bi_8t_8w_lsx(uint8_t *src0_ptr,
 }
 
 static av_always_inline
-void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
-                                    int32_t src_stride,
-                                    int16_t *src1_ptr,
-                                    int32_t src2_stride,
-                                    uint8_t *dst,
-                                    int32_t dst_stride,
-                                    const int8_t *filter,
-                                    int32_t height,
-                                    int32_t width)
+void hevc_vt_8t_16multx2mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                 int16_t *src1_ptr, int32_t src2_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height,
+                                 int32_t width)
 {
     uint8_t *src0_ptr_tmp;
     int16_t *src1_ptr_tmp;
     uint8_t *dst_tmp;
     uint32_t loop_cnt;
     uint32_t cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m128i in0, in1, in2, in3;
     __m128i src10_r, src32_r, src54_r, src76_r;
@@ -1234,10 +729,8 @@ void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
     __m128i src21_l, src43_l, src65_l, src87_l;
     __m128i dst0_l, dst1_l;
     __m128i filt0, filt1, filt2, filt3;
-    __m128i const_vec;
 
     src0_ptr -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filt0, filt1, filt2, filt3);
@@ -1247,17 +740,15 @@ void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
         src1_ptr_tmp = src1_ptr;
         dst_tmp = dst;
 
-        DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                  src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
-                  0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
         src0_ptr_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src5, src6);
         src0_ptr_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
         DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
                   src10_r, src32_r, src54_r, src21_r);
@@ -1267,39 +758,33 @@ void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
         DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 1); loop_cnt--;) {
-            DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                      src7, src8);
+            src7 = __lsx_vld(src0_ptr_tmp, 0);
+            src8 = __lsx_vldx(src0_ptr_tmp, src_stride);
             src0_ptr_tmp += src_stride_2x;
-            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                      in0, in1);
-            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 16, src1_ptr_tmp + src2_stride,
-                      16, in2, in3);
-            src1_ptr_tmp += src2_stride_2x;
-            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp, 16, in0, in2);
+            src1_ptr_tmp += src2_stride;
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp, 16, in1, in3);
+            src1_ptr_tmp += src2_stride;
 
             DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
             DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                      dst0_r, dst0_r, dst0_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                      dst1_r, dst1_r, dst1_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3, dst0_l,
-                      dst0_l, dst0_l, dst0_l);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3, dst1_l,
-                      dst1_l, dst1_l, dst1_l);
-
-            __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                    &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-
-            DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src10_l,
+                      filt0, src21_l, filt0, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r,
+                      src43_r, filt1, dst0_l, src32_l, filt1, dst1_l, src43_l,
+                      filt1, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src54_r, filt2, dst1_r,
+                      src65_r, filt2, dst0_l, src54_l, filt2, dst1_l, src65_l,
+                      filt2, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src76_r, filt3, dst1_r,
+                      src87_r, filt3, dst0_l, src76_l, filt3, dst1_l, src87_l,
+                      filt3, dst0_r, dst1_r, dst0_l, dst1_l);
+            dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+            dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
-            __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
+            __lsx_vstx(dst1_r, dst_tmp, dst_stride);
             dst_tmp += dst_stride_2x;
 
             src10_r = src32_r;
@@ -1323,93 +808,68 @@ void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_vt_bi_8t_16w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    hevc_vt_bi_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                   dst, dst_stride, filter, height, 16);
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 16);
 }
 
-static void hevc_vt_bi_8t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    hevc_vt_bi_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                   dst, dst_stride, filter, height, 16);
-    hevc_vt_bi_8t_8w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
-                         dst + 16, dst_stride, filter, height);
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 16);
+    hevc_vt_8t_8w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                      dst + 16, dst_stride, filter, height);
 }
 
-static void hevc_vt_bi_8t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    hevc_vt_bi_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                   dst, dst_stride, filter, height, 32);
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 32);
 }
 
-static void hevc_vt_bi_8t_48w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    hevc_vt_bi_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                   dst, dst_stride, filter, height, 48);
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 48);
 }
 
-static void hevc_vt_bi_8t_64w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    hevc_vt_bi_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                   dst, dst_stride, filter, height, 64);
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 64);
 }
 
 static av_always_inline
-void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
-                                   int32_t src_stride,
-                                   int16_t *src1_ptr,
-                                   int32_t src2_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height,
-                                   int32_t width)
+void hevc_hv_8t_8multx1mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                int16_t *src1_ptr, int32_t src2_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter_x, const int8_t *filter_y,
+                                int32_t height, int32_t width)
 {
     uint32_t loop_cnt;
     uint32_t cnt;
     uint8_t *src0_ptr_tmp;
     int16_t *src1_ptr_tmp;
     uint8_t *dst_tmp;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i out;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0, tmp;
@@ -1417,7 +877,6 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
     __m128i filt_h0, filt_h1, filt_h2, filt_h3;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     __m128i mask1, mask2, mask3;
-    __m128i const_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
@@ -1426,10 +885,9 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
     __m128i dst10_l, dst32_l, dst54_l, dst76_l;
 
     src0_ptr -= src_stride_3x + 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-              filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x,
+              6, filt0, filt1, filt2, filt3);
     filt_h3 = __lsx_vld(filter_y, 0);
     filt_h3 = __lsx_vsllwil_h_b(filt_h3, 0);
 
@@ -1444,52 +902,50 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
         dst_tmp = dst;
         src1_ptr_tmp = src1_ptr;
 
-        DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                  src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x, 0,
-                  src0, src1, src2, src3);
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
         src0_ptr_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src5, src6);
         src0_ptr_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-        dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                      filt3);
-        dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                      filt3);
-        dst2 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                      filt3);
-        dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1,
-                                      filt2, filt3);
-
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-        dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                      filt3);
-        dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                      filt3);
-        dst6 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                      filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
 
         for (loop_cnt = height; loop_cnt--;) {
             src7 = __lsx_vld(src0_ptr_tmp, 0);
-            src7 = __lsx_vxori_b(src7, 128);
             src0_ptr_tmp += src_stride;
 
             in0 = __lsx_vld(src1_ptr_tmp, 0);
@@ -1497,22 +953,27 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
 
             DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
                       src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-            dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
-                                          filt2, filt3);
-            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_r, dst32_r, dst54_r, dst76_r);
-            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_l, dst32_l, dst54_l, dst76_l);
-
-            dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_l, dst32_l, dst54_l, dst76_l);
+
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
             dst0_r = __lsx_vsrli_w(dst0_r, 6);
             dst0_l = __lsx_vsrli_w(dst0_l, 6);
 
             tmp = __lsx_vpickev_h(dst0_l, dst0_r);
-            DUP2_ARG2(__lsx_vsadd_h, tmp, in0, tmp, const_vec, tmp, tmp);
+            tmp = __lsx_vsadd_h(tmp, in0);
             tmp = __lsx_vmaxi_h(tmp, 0);
             out = __lsx_vssrlrni_bu_h(tmp, tmp, 7);
             __lsx_vstelm_d(out, dst_tmp, 0, 0);
@@ -1533,117 +994,80 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_hv_bi_8t_8w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 const int8_t *filter_x,
-                                 const int8_t *filter_y,
-                                 int32_t height)
+static void hevc_hv_8t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 8);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 8);
 }
 
-static void hevc_hv_bi_8t_16w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 16);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 16);
 }
 
-static void hevc_hv_bi_8t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 24);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 24);
 }
 
-static void hevc_hv_bi_8t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 32);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 32);
 }
 
-static void hevc_hv_bi_8t_48w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 48);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 48);
 }
 
-static void hevc_hv_bi_8t_64w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 64);
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 64);
 }
 
-static void hevc_hz_bi_4t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     int16_t *src1_ptr_tmp;
     uint8_t *dst_tmp;
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_x = src2_stride << 1;
+    int32_t src2_stride_2x = src2_stride << 2;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
 
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
@@ -1652,10 +1076,8 @@ static void hevc_hz_bi_4t_24w_lsx(uint8_t *src0_ptr,
     __m128i mask1, mask2, mask3;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    __m128i const_vec;
 
     src0_ptr -= 1;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
@@ -1665,73 +1087,68 @@ static void hevc_hz_bi_4t_24w_lsx(uint8_t *src0_ptr,
     src1_ptr_tmp = src1_ptr + 16;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                  src0, src2, src4, src6);
-        DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
-                  src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
-                  src1, src3, src5, src7);
-        src0_ptr += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in2, in4, in6);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                  in1, in3, in5, in7);
-        src1_ptr += src2_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                  src5, src6, src7);
-
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src2, src2,
-                  mask0, src3, src2, mask2, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src2, src2,
-                  mask1, src3, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src4, mask2, src6, src6,
-                  mask0, src7, src6, mask2, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst4, dst5, dst6, dst7);
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src4, mask3, src6, src6,
-                  mask1, src7, src6, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst4, vec0, filt1, dst5, vec1, filt1, dst6,
-                  vec2, filt1, dst7, vec3, filt1, dst4, dst5, dst6, dst7);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
-                                &dst4, &dst5, &dst6, &dst7);
-
-        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                  dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src3);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src4, src5);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src6, src7);
+        src0_ptr += src_stride;
+
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in4, in5);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in6, in7);
+        src1_ptr += src2_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src2,
+                  src2, mask0, src3, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src2,
+                  src2, mask1, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src4, mask2, src6,
+                  src6, mask0, src7, src6, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst4, dst5, dst6, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src4, mask3, src6,
+                  src6, mask1, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec0, filt1, dst5, vec1, filt1,
+                  dst6, vec2, filt1, dst7, vec3, filt1, dst4, dst5, dst6, dst7);
+
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        dst2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        dst3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
         __lsx_vst(dst0, dst, 0);
         __lsx_vstx(dst1, dst, dst_stride);
         __lsx_vstx(dst2, dst, dst_stride_2x);
         __lsx_vstx(dst3, dst, dst_stride_3x);
         dst += dst_stride_4x;
 
-        DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                  src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        src1_ptr_tmp += src2_stride_4x;
-
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src3, src3, mask0, src5, src5,
-                  mask0, src7, src7, mask0, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask1, src3, src3, mask1, src5, src5,
-                  mask1, src7, src7, mask1, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+        in0 = __lsx_vld(src1_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr_tmp, src2_stride_x, src1_ptr_tmp,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr_tmp, src2_stride_3x);
+        src1_ptr_tmp += src2_stride_2x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src3, src3, mask0, src5,
+                  src5, mask0, src7, src7, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask1, src3, src3, mask1, src5,
+                  src5, mask1, src7, src7, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
         __lsx_vstelm_d(dst0, dst_tmp, 0, 0);
         __lsx_vstelm_d(dst0, dst_tmp + dst_stride, 0, 1);
         __lsx_vstelm_d(dst1, dst_tmp + dst_stride_2x, 0, 0);
@@ -1740,14 +1157,10 @@ static void hevc_hz_bi_4t_24w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_hz_bi_4t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_hz_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
     __m128i src0, src1, src2;
@@ -1757,11 +1170,9 @@ static void hevc_hz_bi_4t_32w_lsx(uint8_t *src0_ptr,
     __m128i mask1, mask2, mask3;
     __m128i dst0, dst1, dst2, dst3;
     __m128i vec0, vec1, vec2, vec3;
-    __m128i const_vec;
 
     src0_ptr -= 1;
 
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
@@ -1771,47 +1182,41 @@ static void hevc_hz_bi_4t_32w_lsx(uint8_t *src0_ptr,
         DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src2 = __lsx_vld(src0_ptr, 24);
         src0_ptr += src_stride;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                      48, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32,
+                  src1_ptr, 48, in0, in1, in2, in3);
         src1_ptr += src2_stride;
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src1, src1,
-                  mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src1, src1,
-                  mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2, vec2,
-                  filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
-                                &dst0, &dst1, &dst2, &dst3);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src1,
+                  src1, mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src1,
+                  src1, mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         dst += dst_stride;
     }
 }
 
-static void hevc_vt_bi_4t_12w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_4t_12w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t *_src1 = src1_ptr + 8;
     __m128i src0, src1, src2, src3, src4, src5, src6;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
     __m128i src10_r, src32_r, src21_r, src43_r, src54_r, src65_r;
@@ -1819,36 +1224,35 @@ static void hevc_vt_bi_4t_12w_lsx(uint8_t *src0_ptr,
     __m128i src10_l, src32_l, src54_l, src21_l, src43_l, src65_l;
     __m128i src2110, src4332, src6554;
     __m128i dst0_l, dst1_l, filt0, filt1;
-    __m128i const_vec;
 
     src0_ptr -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
     src0_ptr += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
     src2110 = __lsx_vilvl_d(src21_l, src10_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
-        src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src6);
-        src0_ptr += src_stride_2x;
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                  in4, in5, in6, in7);
-        src1_ptr += src2_stride_4x;
+        src3 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
         DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
-        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                  src4, src5, src6);
 
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
@@ -1857,21 +1261,18 @@ static void hevc_vt_bi_4t_12w_lsx(uint8_t *src0_ptr,
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src54_l, src65_l);
         src6554 = __lsx_vilvl_d(src65_l, src54_l);
 
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, const_vec, src21_r, filt0, dst1_r,  src43_r, filt1, dst0_r,
-                  dst0_r, dst1_r, dst1_r );
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
-                  filt1, const_vec, src32_r, filt0, dst2_r, src54_r, filt1, dst0_l,
-                  dst0_l, dst2_r, dst2_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                  filt1, const_vec, src4332, filt0, dst1_l, src6554, filt1, dst3_r,
-                  dst3_r, dst1_l, dst1_l);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst2_r, dst3_r,
-                                &dst0_r, &dst1_r, &dst2_r, &dst3_r);
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst0_l, dst1_l, &dst0_l, &dst1_l);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
-        dst0_l = __lsx_vpickev_b(dst1_l, dst0_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src2110,
+                  filt0, src32_r, filt0, dst0_r, dst1_r, dst0_l, dst2_r);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src43_r, filt0, src4332, filt0,
+                  dst3_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r,
+                  src43_r, filt1, dst0_l, src4332, filt1, dst2_r, src54_r,
+                  filt1, dst0_r, dst1_r, dst0_l, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst1_l,
+                  src6554, filt1, dst3_r, dst1_l);
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in1, dst1_r);
+        dst1_r = hevc_bi_rnd_clip(in2, dst2_r, in3, dst3_r);
+        dst0_l = hevc_bi_rnd_clip(in4, dst0_l, in5, dst1_l);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(dst1_r, dst + dst_stride_2x, 0, 0);
@@ -1889,19 +1290,14 @@ static void hevc_vt_bi_4t_12w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_vt_bi_4t_16w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_4t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
     const int32_t src_stride_2x = (src_stride << 1);
     const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src2_stride_2x = (src2_stride << 1);
     const int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i src0, src1, src2, src3, src4, src5;
     __m128i in0, in1, in2, in3;
@@ -1909,87 +1305,70 @@ static void hevc_vt_bi_4t_16w_lsx(uint8_t *src0_ptr,
     __m128i src10_l, src32_l, src21_l, src43_l;
     __m128i dst0_r, dst1_r, dst0_l, dst1_l;
     __m128i filt0, filt1;
-    __m128i const_vec;
 
     src0_ptr -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
     src0_ptr += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
+        src3 = __lsx_vld(src0_ptr, 0);
+        src4 = __lsx_vldx(src0_ptr, src_stride);
         src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        src1_ptr += src2_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        src1_ptr += src2_stride;
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                  filt1, dst1_l, dst1_l);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src10_l,
+                  filt0, src21_l, filt0, dst0_r, dst1_r, dst0_l, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r, src43_r,
+                  filt1, dst0_l, src32_l, filt1, dst1_l, src43_l, filt1,
+                  dst0_r, dst1_r, dst0_l, dst1_l);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
-        __lsx_vst(dst1_r, dst + dst_stride, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride);
         dst += dst_stride_2x;
 
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
+        src5 = __lsx_vld(src0_ptr, 0);
+        src2 = __lsx_vldx(src0_ptr, src_stride);
         src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        src1_ptr += src2_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        src1_ptr += src2_stride;
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                  filt1, dst1_l, dst1_l);
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
         dst += dst_stride_2x;
     }
 }
 
-static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_2x = dst_stride << 1;
     __m128i src0, src1, src2, src3, src4, src5;
     __m128i src6, src7, src8, src9, src10, src11;
     __m128i in0, in1, in2, in3, in4, in5;
@@ -1999,60 +1378,53 @@ static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
     __m128i dst0_r, dst1_r, dst2_r, dst3_r;
     __m128i dst0_l, dst1_l;
     __m128i filt0, filt1;
-    __m128i const_vec;
 
     src0_ptr -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     /* 16width */
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src6);
+    src0_ptr += src_stride;
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src1, src7);
+    src0_ptr += src_stride;
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src8);
+    src0_ptr += src_stride;
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
     /* 8width */
-    DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src0_ptr + src_stride_2x, 16);
-    src0_ptr += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
     DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         /* 16width */
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src1_ptr, 0,
-                  src1_ptr + src2_stride, 0, src3, src4, in0, in1);
-        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, src1_ptr,
-                  32, src1_ptr + src2_stride, 32, in2, in3, in4, in5);
-        src1_ptr += src2_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src3, src9);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src4, src10);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        in4 = __lsx_vld(src1_ptr, 32);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        in5 = __lsx_vld(src1_ptr, 32);
+        src1_ptr += src2_stride;
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
         /* 8width */
-        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
-        src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
         DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
         /* 16width */
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, const_vec, src10_l, filt0, dst0_l, src32_l, filt1, dst0_r,
-                  dst0_r, dst0_l, dst0_l);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, const_vec, src21_l, filt0, dst1_l, src43_l, filt1, dst1_r,
-                  dst1_r, dst1_l, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1,  dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l, filt1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
         /* 8width */
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                  filt1, const_vec, src87_r, filt0, dst3_r, src109_r, filt1, dst2_r,
-                  dst2_r, dst3_r, dst3_r);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst3_r,
+                  src109_r, filt1, dst2_r, dst3_r);
         /* 16width */
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst2_r, dst3_r, &dst2_r, &dst3_r);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
-        dst2_r = __lsx_vpickev_b(dst3_r, dst2_r);
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        dst2_r = hevc_bi_rnd_clip(in4, dst2_r, in5, dst3_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
         __lsx_vstelm_d(dst2_r, dst, 16, 0);
@@ -2060,37 +1432,35 @@ static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
         dst += dst_stride_2x;
 
         /* 16width */
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
-        src1_ptr += src2_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src1_ptr, 0, src1_ptr, 16, src1_ptr,
+                  32, src5, in0, in2, in4);
+        src1_ptr += src2_stride;
+        DUP4_ARG2(__lsx_vld, src0_ptr, 16,  src1_ptr, 0, src1_ptr, 16, src1_ptr,
+                  32, src11, in1, in3, in5);
+        src1_ptr += src2_stride;
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0,  src0_ptr, 16, src2, src8);
+        src0_ptr += src_stride;
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
         /* 8width */
-        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src11, src8);
-        src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
         DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
         /* 16width */
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                  filt1, const_vec, src32_l, filt0, dst0_l, src10_l, filt1, dst0_r,
-                  dst0_r, dst0_l, dst0_l);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                  filt1, const_vec, src43_l, filt0, dst1_l, src21_l, filt1, dst1_r,
-                  dst1_r, dst1_l, dst1_l);
-        /* 8width */
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                  filt1, const_vec, src109_r, filt0, dst3_r, src87_r, filt1, dst2_r,
-                  dst2_r, dst3_r, dst3_r);
-
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-        __lsx_hevc_bi_rnd_clip2(in4, in5, dst2_r, dst3_r, &dst2_r, &dst3_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
 
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
-        dst2_r = __lsx_vpickev_b(dst3_r, dst2_r);
+        /* 8width */
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b,  dst2_r, src76_r, filt1, dst3_r,
+                  src87_r, filt1, dst2_r, dst3_r);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        dst2_r = hevc_bi_rnd_clip(in4, dst2_r, in5, dst3_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
         __lsx_vstelm_d(dst2_r, dst, 16, 0);
@@ -2099,152 +1469,42 @@ static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_vt_bi_4t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter,
-                                  int32_t height)
+static void hevc_vt_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
 {
-    uint32_t loop_cnt;
-    uint8_t *dst_tmp = dst + 16;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    __m128i src0, src1, src2, src3, src4, src6, src7, src8, src9, src10;
-    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
-    __m128i src10_r, src32_r, src76_r, src98_r;
-    __m128i src21_r, src43_r, src87_r, src109_r;
-    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
-    __m128i src10_l, src32_l, src76_l, src98_l;
-    __m128i src21_l, src43_l, src87_l, src109_l;
-    __m128i dst0_l, dst1_l, dst2_l, dst3_l;
-    __m128i filt0, filt1;
-    __m128i const_vec;
-
-    src0_ptr -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-
-    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
-
-    /* 16width */
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
-    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
-
-    /* next 16width */
-    DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src0_ptr + src_stride_2x, 16);
-    src0_ptr += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
-    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
-    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
-
-    for (loop_cnt = (height >> 1); loop_cnt--;) {
-        /* 16width */
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
-        DUP2_ARG2(__lsx_vld, src1_ptr, 48, src1_ptr + src2_stride, 48, in6, in7);
-        src1_ptr += src2_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
-        /* 16width */
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                  filt1, dst1_l, dst1_l);
-        /* 16width */
-        __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
-                                &dst0_r, &dst1_r, &dst0_l, &dst1_l);
-
-        src10_r = src32_r;
-        src21_r = src43_r;
-        src10_l = src32_l;
-        src21_l = src43_l;
-        src2 = src4;
-
-        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
-        __lsx_vst(dst0_r, dst, 0);
-        __lsx_vst(dst1_r, dst + dst_stride, 0);
-        dst += dst_stride_2x;
-
-        /* next 16width */
-        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
-        src0_ptr += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
-        /* next 16width */
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                  filt1, dst2_r, dst2_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
-                  filt1, dst2_l, dst2_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                  filt1, dst3_r, dst3_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
-                  filt1, dst3_l, dst3_l);
-        /* next 16width */
-        __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst2_r, dst3_r, dst2_l, dst3_l,
-                                &dst2_r, &dst3_r, &dst2_l, &dst3_l);
-
-        DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
-        __lsx_vst(dst2_r, dst_tmp, 0);
-        __lsx_vstx(dst3_r, dst_tmp, dst_stride);
-        dst_tmp += dst_stride_2x;
-
-        src76_r = src98_r;
-        src87_r = src109_r;
-        src76_l = src98_l;
-        src87_l = src109_l;
-        src8 = src10;
-    }
+    hevc_vt_4t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_vt_4t_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
 }
 
-static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 const int8_t *filter_x,
-                                 const int8_t *filter_y,
-                                 int32_t height)
+static void hevc_hv_4t_6w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
 {
-    uint32_t tpw0, tpw1, tpw2, tpw3;
-    uint64_t tp0, tp1;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_2x = (src2_stride << 1);
+    int32_t src2_stride_4x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride;
     __m128i out0, out1;
     __m128i src0, src1, src2, src3, src4, src5, src6;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, mask1;
-    __m128i filt0, filt1, filt_h0, filt_h1, const_vec;
+    __m128i filt0, filt1, filt_h0, filt_h1;
     __m128i dsth0, dsth1, dsth2, dsth3, dsth4, dsth5;
     __m128i dsth6, dsth7, dsth8, dsth9, dsth10;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
     __m128i dst4_r, dst5_r, dst6_r, dst7_r;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8;
-    __m128i zero ={0};
+    __m128i reg0, reg1, reg2, reg3;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src0_ptr -= (src_stride + 1);
@@ -2255,110 +1515,114 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
     DUP2_ARG2(__lsx_vreplvei_w, filt_h1, 0, filt_h1, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
     src0_ptr += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
 
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
     DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
-    dsth0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dsth1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dsth2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dsth0, dsth1);
+    dsth2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dsth0, vec1, filt1, dsth1, vec3, filt1,
+              dsth0, dsth1);
+    dsth2 = __lsx_vdp2add_h_bu_b(dsth2, vec5, filt1);
 
     DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, tmp0, tmp2);
     DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, tmp1, tmp3);
 
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src3, src4, src5, src6);
-    DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
-              src5, src6);
+    src3 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src4, src5);
+    src6 = __lsx_vldx(src0_ptr, src_stride_3x);
+    src0_ptr += src_stride_4x;
     DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
     DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
     DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
-    dsth3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dsth4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dsth5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dsth6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dsth3, dsth4, dsth5, dsth6);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth3, vec1, filt1, dsth4, vec3, filt1, dsth5,
+              vec5, filt1, dsth6, vec7, filt1, dsth3, dsth4, dsth5, dsth6);
 
-    src0_ptr += src_stride_4x;
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src3, src4, src5, src6);
-    DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
-              src5, src6);
+    src3 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src4, src5);
+    src6 = __lsx_vldx(src0_ptr, src_stride_3x);
 
     DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
     DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
     DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
-    dsth7 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dsth8 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dsth9 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dsth10 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dsth7, dsth8, dsth9, dsth10);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth7, vec1, filt1, dsth8, vec3, filt1, dsth9,
+              vec5, filt1, dsth10, vec7, filt1, dsth7, dsth8, dsth9, dsth10);
 
     DUP2_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, tmp4, tmp6);
     DUP2_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, tmp5, tmp7);
     DUP2_ARG2(__lsx_vilvl_h, dsth5, dsth4, dsth6, dsth5, dsth0, dsth2);
     DUP2_ARG2(__lsx_vilvh_h, dsth5, dsth4, dsth6, dsth5, dsth1, dsth3);
-    dst0_r = __lsx_hevc_filt_4tap_w(tmp0, tmp4, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(tmp2, tmp6, filt_h0, filt_h1);
-    dst2_r = __lsx_hevc_filt_4tap_w(tmp4, dsth0, filt_h0, filt_h1);
-    dst3_r = __lsx_hevc_filt_4tap_w(tmp6, dsth2, filt_h0, filt_h1);
+    DUP4_ARG2(__lsx_vdp2_w_h, tmp0, filt_h0, tmp2, filt_h0, tmp4, filt_h0,
+              tmp6, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, tmp4, filt_h1, dst1_r, tmp6,
+              filt_h1, dst2_r, dsth0, filt_h1, dst3_r, dsth2, filt_h1,
+              dst0_r, dst1_r, dst2_r, dst3_r);
     DUP2_ARG2(__lsx_vpickev_d, tmp3, tmp1, tmp7, tmp5, tmp0, tmp8);
-    dst0_l = __lsx_hevc_filt_4tap_w(tmp0, tmp8, filt_h0, filt_h1);
+    dst0_l = __lsx_vdp2_w_h(tmp0, filt_h0);
+    dst0_l = __lsx_vdp2add_w_h(dst0_l, tmp8, filt_h1);
 
     DUP2_ARG2(__lsx_vilvl_h, dsth7, dsth6, dsth8, dsth7, tmp0, tmp2);
     DUP2_ARG2(__lsx_vilvh_h, dsth7, dsth6, dsth8, dsth7, tmp1, tmp3);
     DUP2_ARG2(__lsx_vilvl_h, dsth9, dsth8, dsth10, dsth9, tmp4, tmp6);
     DUP2_ARG2(__lsx_vilvh_h, dsth9, dsth8, dsth10, dsth9, tmp5, tmp7);
-    dst4_r = __lsx_hevc_filt_4tap_w(dsth0, tmp0, filt_h0, filt_h1);
-    dst5_r = __lsx_hevc_filt_4tap_w(dsth2, tmp2, filt_h0, filt_h1);
-    dst6_r = __lsx_hevc_filt_4tap_w(tmp0, tmp4, filt_h0, filt_h1);
-    dst7_r = __lsx_hevc_filt_4tap_w(tmp2, tmp6, filt_h0, filt_h1);
+    DUP4_ARG2(__lsx_vdp2_w_h, dsth0, filt_h0, dsth2, filt_h0, tmp0, filt_h0,
+              tmp2, filt_h0, dst4_r, dst5_r, dst6_r, dst7_r);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, tmp0, filt_h1, dst5_r, tmp2,
+              filt_h1, dst6_r, tmp4, filt_h1, dst7_r, tmp6, filt_h1,
+              dst4_r, dst5_r, dst6_r, dst7_r);
     DUP2_ARG2(__lsx_vpickev_d, dsth3, dsth1, tmp3, tmp1, tmp0, tmp1);
     tmp2 = __lsx_vpickev_d(tmp7, tmp5);
 
-    dst1_l = __lsx_hevc_filt_4tap_w(tmp8, tmp0, filt_h0, filt_h1);
-    dst2_l = __lsx_hevc_filt_4tap_w(tmp0, tmp1, filt_h0, filt_h1);
-    dst3_l = __lsx_hevc_filt_4tap_w(tmp1, tmp2, filt_h0, filt_h1);
-
-    DUP4_ARG2(__lsx_vsrai_d, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6, dst0_r,
-              dst1_r, dst2_r, dst3_r);
-    DUP4_ARG2(__lsx_vsrai_d, dst4_r, 6, dst5_r, 6, dst6_r, 6, dst7_r, 6, dst4_r,
-              dst5_r, dst6_r, dst7_r);
-    DUP4_ARG2(__lsx_vsrai_d, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
-              dst1_l, dst2_l, dst3_l);
+    DUP2_ARG2(__lsx_vdp2_w_h, tmp8, filt_h0, tmp0, filt_h0, dst1_l, dst2_l);
+    dst3_l = __lsx_vdp2_w_h(tmp1, filt_h0);
+    DUP2_ARG3(__lsx_vdp2add_w_h, dst1_l, tmp0, filt_h1, dst2_l, tmp1, filt_h1,
+              dst1_l, dst2_l);
+    dst3_l = __lsx_vdp2add_w_h(dst3_l, tmp2, filt_h1);
+
+    DUP4_ARG2(__lsx_vsrai_d, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+              dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst4_r, 6, dst5_r, 6, dst6_r, 6, dst7_r, 6,
+              dst4_r, dst5_r, dst6_r, dst7_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6,
+              dst0_l, dst1_l, dst2_l, dst3_l);
     DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
     DUP2_ARG2(__lsx_vpickev_h, dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
     DUP2_ARG2(__lsx_vpickev_h, dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
 
-    tp0 = *(uint64_t *)src1_ptr;
-    tp1 = *(uint64_t *)(src1_ptr + src2_stride);
-    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth0, tp1, 1, dsth0, dsth0);
-    tp0 = *(uint64_t *)(src1_ptr + src2_stride_2x);
-    tp1 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth1, tp1, 1, dsth1, dsth1);
+    reg0 = __lsx_vldrepl_d(src1_ptr, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+    dsth0 = __lsx_vilvl_d(reg1, reg0);
+    reg0 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+    dsth1 = __lsx_vilvl_d(reg1, reg0);
     src1_ptr += src2_stride_4x;
-    tp0 = *(uint64_t *)src1_ptr;
-    tp1 = *(uint64_t *)(src1_ptr + src2_stride);
-    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth2, tp1, 1, dsth2, dsth2);
-    tp0 = *(uint64_t *)(src1_ptr + src2_stride_2x);
-    tp1 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth3, tp1, 1, dsth3, dsth3);
-
-    DUP4_ARG2(__lsx_vsadd_h, dsth0, const_vec, dsth1, const_vec, dsth2, const_vec,
-              dsth3, const_vec, dsth0, dsth1, dsth2, dsth3);
-    DUP4_ARG2(__lsx_vsadd_h, dsth0, tmp0, dsth1, tmp1, dsth2, tmp2, dsth3, tmp3,
+    reg0 = __lsx_vldrepl_d(src1_ptr, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+    dsth2 = __lsx_vilvl_d(reg1, reg0);
+    reg0 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+    dsth3 = __lsx_vilvl_d(reg1, reg0);
+
+    DUP4_ARG2(__lsx_vsadd_h, dsth0, tmp0, dsth1, tmp1, dsth2, tmp2, dsth3,
+              tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
               tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
 
     __lsx_vstelm_w(out0, dst, 0, 0);
@@ -2373,20 +1637,21 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
     dst -= dst_stride_4x;
 
     src1_ptr -= src2_stride_4x;
-    tpw0 = *(uint32_t *)(src1_ptr + 4);
-    tpw1 = *(uint32_t *)(src1_ptr + 4 + src2_stride);
-    tpw2 = *(uint32_t *)(src1_ptr + 4 + src2_stride_2x);
-    tpw3 = *(uint32_t *)(src1_ptr + 4 + src2_stride_3x);
-    DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth4, tpw1, 1, dsth4, tpw2, 2,
-              dsth4, tpw3, 3, dsth4, dsth4, dsth4, dsth4);
+
+    reg0 = __lsx_vldrepl_w(src1_ptr, 8);
+    reg1 = __lsx_vldrepl_w(src1_ptr + src2_stride, 8);
+    reg2 = __lsx_vldrepl_w(src1_ptr + src2_stride_2x, 8);
+    reg3 = __lsx_vldrepl_w(src1_ptr + src2_stride_3x, 8);
+    DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+    dsth4 = __lsx_vilvl_d(tmp1, tmp0);
     src1_ptr += src2_stride_4x;
-    tpw0 = *(uint32_t *)(src1_ptr + 4);
-    tpw1 = *(uint32_t *)(src1_ptr + 4 + src2_stride);
-    tpw2 = *(uint32_t *)(src1_ptr + 4 + src2_stride_2x);
-    tpw3 = *(uint32_t *)(src1_ptr + 4 + src2_stride_3x);
-    DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth5, tpw1, 1, dsth5, tpw2, 2,
-              dsth5, tpw3, 3, dsth5, dsth5, dsth5, dsth5);
-    DUP2_ARG2(__lsx_vsadd_h, dsth4, const_vec, dsth5, const_vec, dsth4, dsth5);
+
+    reg0 = __lsx_vldrepl_w(src1_ptr, 8);
+    reg1 = __lsx_vldrepl_w(src1_ptr + src2_stride, 8);
+    reg2 = __lsx_vldrepl_w(src1_ptr + src2_stride_2x, 8);
+    reg3 = __lsx_vldrepl_w(src1_ptr + src2_stride_3x, 8);
+    DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+    dsth5 = __lsx_vilvl_d(tmp1, tmp0);
     DUP2_ARG2(__lsx_vsadd_h, dsth4, tmp4, dsth5, tmp5, tmp4, tmp5);
     DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 7, tmp4, tmp5);
     out0 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
@@ -2403,26 +1668,20 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
 }
 
 static av_always_inline
-void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
-                           int32_t src_stride,
-                           int16_t *src1_ptr,
-                           int32_t src2_stride,
-                           uint8_t *dst,
-                           int32_t dst_stride,
-                           const int8_t *filter_x,
-                           const int8_t *filter_y)
+void hevc_hv_4t_8x2_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                        int32_t src2_stride, uint8_t *dst, int32_t dst_stride,
+                        const int8_t *filter_x, const int8_t *filter_y)
 {
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
     __m128i out;
     __m128i src0, src1, src2, src3, src4;
     __m128i filt0, filt1;
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i mask1, filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
     __m128i dst0, dst1, dst2, dst3, dst4;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l;
@@ -2439,17 +1698,13 @@ void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
-    src4 = __lsx_vld(src0_ptr + src_stride_4x, 0);
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    src4 = __lsx_vxori_b(src4, 128);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP4_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src0_ptr, src_stride_3x, src0_ptr, src_stride_4x,
+              src1, src2, src3, src4);
 
     DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-    DUP2_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in0, in1);
 
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
@@ -2457,22 +1712,24 @@ void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
     DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
     DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
 
-    dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-    dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
 
     DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
     DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
     DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-              dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
     DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
     DUP2_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, tmp0, tmp1);
     DUP2_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp0, tmp1);
@@ -2482,29 +1739,26 @@ void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
 }
 
 static av_always_inline
-void hevc_hv_bi_4t_8multx4_lsx(uint8_t *src0_ptr,
-                               int32_t src_stride,
-                               int16_t *src1_ptr,
-                               int32_t src2_stride,
-                               uint8_t *dst,
-                               int32_t dst_stride,
-                               const int8_t *filter_x,
-                               const int8_t *filter_y,
-                               int32_t width8mult)
+void hevc_hv_4t_8multx4_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                            int16_t *src1_ptr, int32_t src2_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            const int8_t *filter_x, const int8_t *filter_y,
+                            int32_t width8mult)
 {
     uint32_t cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
 
     __m128i out0, out1;
     __m128i src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, const_vec;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, tmp0, tmp1, tmp2, tmp3;
     __m128i in0, in1, in2, in3;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
@@ -2520,73 +1774,80 @@ void hevc_hv_bi_4t_8multx4_lsx(uint8_t *src0_ptr,
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width8mult; cnt--;) {
-        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0,
-                  src1, src2, src3);
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
         src0_ptr += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src0_ptr + src_stride_2x, 0);
+        src4 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src5, src6);
         src0_ptr += (8 - src_stride_4x);
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
-
-        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                  in0, in1, in2, in3);
+
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
         src1_ptr += 8;
-        DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec,
-                  in3, const_vec, in0, in1, in2, in3);
 
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
 
-        dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
-        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
 
-        dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-        dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
 
         DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
         DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
         DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
         DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
-        dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-        dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-        dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-        dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
 
         DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
                   dst0_r, dst0_l, dst1_r, dst1_l);
         DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
                   dst2_r, dst2_l, dst3_r, dst3_l);
-        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
-                  dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                  dst2_r, dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
         DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                   tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1,
-                  tmp2, tmp3);
+        DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
+                  tmp0, tmp1, tmp2, tmp3);
         DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
@@ -2597,24 +1858,19 @@ void hevc_hv_bi_4t_8multx4_lsx(uint8_t *src0_ptr,
 }
 
 static av_always_inline
-void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
-                           int32_t src_stride,
-                           int16_t *src1_ptr,
-                           int32_t src2_stride,
-                           uint8_t *dst,
-                           int32_t dst_stride,
-                           const int8_t *filter_x,
-                           const int8_t *filter_y)
+void hevc_hv_4t_8x6_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                        int32_t src2_stride, uint8_t *dst, int32_t dst_stride,
+                        const int8_t *filter_x, const int8_t *filter_y)
 {
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
 
     __m128i out0, out1, out2;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
@@ -2622,8 +1878,7 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     __m128i filt0, filt1;
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i mask1, filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
     __m128i vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
@@ -2643,29 +1898,24 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src0_ptr, src_stride_3x);
     src0_ptr += src_stride_4x;
-    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
-              0, src0_ptr + src_stride_3x, 0, src4, src5, src6, src7);
-    src8 = __lsx_vld(src0_ptr + src_stride_4x, 0);
-
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4, src5,
-              src6, src7)
-    src8 = __lsx_vxori_b(src8, 128);
-
-    DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-              src1_ptr + src2_stride_2x, 0,  src1_ptr + src2_stride_3x, 0, in0, in1,
-              in2, in3);
-    src1_ptr += src2_stride_4x;
-    DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
-    DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec, in3,
-              const_vec, in0, in1, in2, in3);
-    DUP2_ARG2(__lsx_vsadd_h, in4, const_vec, in5, const_vec, in4, in5);
+    src4 = __lsx_vld(src0_ptr, 0);
+    DUP4_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src0_ptr, src_stride_3x, src0_ptr, src_stride_4x,
+              src5, src6, src7, src8);
+
+    in0 = __lsx_vld(src1_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr, src2_stride_2x,
+              in1, in2);
+    in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+    src1_ptr += src2_stride_2x;
+    in4 = __lsx_vld(src1_ptr, 0);
+    in5 = __lsx_vldx(src1_ptr, src2_stride_x);
 
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
@@ -2677,15 +1927,16 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     DUP2_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, vec14, vec15);
     DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
 
-    dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-    dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
-    dst5 = __lsx_hevc_filt_4tap_h(vec10, vec11, filt0, filt1);
-    dst6 = __lsx_hevc_filt_4tap_h(vec12, vec13, filt0, filt1);
-    dst7 = __lsx_hevc_filt_4tap_h(vec14, vec15, filt0, filt1);
-    dst8 = __lsx_hevc_filt_4tap_h(vec16, vec17, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec10, filt0, vec12, filt0, vec14, filt0,
+              vec16, filt0, dst5, dst6, dst7, dst8);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec11, filt1, dst6, vec13, filt1,
+              dst7, vec15, filt1, dst8, vec17, filt1, dst5, dst6, dst7, dst8);
 
     DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
               dst10_r, dst21_r, dst32_r, dst43_r);
@@ -2696,32 +1947,36 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
               dst54_l, dst65_l, dst76_l, dst87_l);
 
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-    dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-    dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-    dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-    dst4_r = __lsx_hevc_filt_4tap_w(dst54_r, dst76_r, filt_h0, filt_h1);
-    dst4_l = __lsx_hevc_filt_4tap_w(dst54_l, dst76_l, filt_h0, filt_h1);
-    dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
-    dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
-
-    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-              dst0_l, dst1_r, dst1_l);
-    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
-              dst2_l, dst3_r, dst3_l);
-    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
-              dst4_l, dst5_r, dst5_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
+
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6,
+              dst4_r, dst4_l, dst5_r, dst5_l);
     DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
               dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, tmp4, tmp5);
-    DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1,
-              tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+              tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lsx_vsadd_h, in4, tmp4, in5, tmp5, tmp4, tmp5);
-    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
+              tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 0, tmp4, tmp5);
     DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
     out2 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
@@ -2735,16 +1990,11 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
 }
 
 static av_always_inline
-void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
-                                   int32_t src_stride,
-                                   int16_t *src1_ptr,
-                                   int32_t src2_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height,
-                                   int32_t width)
+void hevc_hv_4t_8multx4mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                int16_t *src1_ptr, int32_t src2_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter_x, const int8_t *filter_y,
+                                int32_t height, int32_t width)
 {
     uint32_t loop_cnt, cnt;
     uint8_t *src0_ptr_tmp;
@@ -2754,19 +2004,18 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
     const int32_t dst_stride_2x = (dst_stride << 1);
     const int32_t src_stride_4x = (src_stride << 2);
     const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src2_stride_2x = (src2_stride << 1);
-    const int32_t src2_stride_4x = (src2_stride << 2);
+    const int32_t src2_stride_x = (src2_stride << 1);
+    const int32_t src2_stride_2x = (src2_stride << 2);
     const int32_t src_stride_3x = src_stride_2x + src_stride;
     const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    const int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    const int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
     __m128i out0, out1;
     __m128i src0, src1, src2, src3, src4, src5, src6;
     __m128i in0, in1, in2, in3;
     __m128i filt0, filt1;
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i mask1, filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
@@ -2785,68 +2034,71 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width >> 3; cnt--;) {
         src0_ptr_tmp = src0_ptr;
         dst_tmp = dst;
         src1_ptr_tmp = src1_ptr;
 
-        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src0, src1);
-        src2 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
         src0_ptr_tmp += src_stride_3x;
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
 
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
 
-        dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = height >> 2; loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                      src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
-                      0, src3, src4, src5, src6);
+            src3 = __lsx_vld(src0_ptr_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                      src_stride_2x, src4, src5);
+            src6 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
             src0_ptr_tmp += src_stride_4x;
-            DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                      src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x,
-                      0, in0, in1, in2, in3);
-            src1_ptr_tmp += src2_stride_4x;
-            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                      src3, src4, src5, src6);
-
-            DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2,
-                      const_vec, in3, const_vec, in0, in1, in2, in3);
+            in0 = __lsx_vld(src1_ptr_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src1_ptr_tmp, src2_stride_x, src1_ptr_tmp,
+                      src2_stride_2x, in1, in2);
+            in3 = __lsx_vldx(src1_ptr_tmp, src2_stride_3x);
+            src1_ptr_tmp += src2_stride_2x;
 
             DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
                       src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
             DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
                       src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
 
-            dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-            dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-            dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-            dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1,
+                      dst3, dst4, dst5, dst6);
 
             DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
             DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
             DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
             DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
-            dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-            dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-            dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-            dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-            dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-            dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-            dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-            dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
 
             DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
                       dst0_r, dst0_l, dst1_r, dst1_l);
@@ -2878,81 +2130,60 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
     }
 }
 
-static void hevc_hv_bi_4t_8w_lsx(uint8_t *src0_ptr,
-                                 int32_t src_stride,
-                                 int16_t *src1_ptr,
-                                 int32_t src2_stride,
-                                 uint8_t *dst,
-                                 int32_t dst_stride,
-                                 const int8_t *filter_x,
-                                 const int8_t *filter_y,
-                                 int32_t height)
+static void hevc_hv_4t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
 {
     if (2 == height) {
-        hevc_hv_bi_4t_8x2_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                              dst, dst_stride, filter_x, filter_y);
+        hevc_hv_4t_8x2_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                           dst, dst_stride, filter_x, filter_y);
     } else if (4 == height) {
-        hevc_hv_bi_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y, 1);
+        hevc_hv_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, 1);
     } else if (6 == height) {
-        hevc_hv_bi_4t_8x6_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                              dst, dst_stride, filter_x, filter_y);
+        hevc_hv_4t_8x6_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                           dst, dst_stride, filter_x, filter_y);
     } else {
-        hevc_hv_bi_4t_8multx4mult_lsx(src0_ptr, src_stride,
-                                      src1_ptr, src2_stride,
-                                      dst, dst_stride,
-                                      filter_x, filter_y, height, 8);
+        hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter_x, filter_y, height, 8);
     }
 }
 
-static void hevc_hv_bi_4t_16w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_4t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
     if (4 == height) {
-        hevc_hv_bi_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y, 2);
+        hevc_hv_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, 2);
     } else {
-        hevc_hv_bi_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr,
-                                      src2_stride, dst, dst_stride, filter_x,
-                                      filter_y, height, 16);
+        hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter_x, filter_y, height, 16);
     }
 }
 
-static void hevc_hv_bi_4t_24w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 24);
+    hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                            dst, dst_stride, filter_x, filter_y, height, 24);
 }
 
-static void hevc_hv_bi_4t_32w_lsx(uint8_t *src0_ptr,
-                                  int32_t src_stride,
-                                  int16_t *src1_ptr,
-                                  int32_t src2_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
 {
-    hevc_hv_bi_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
-                                  dst, dst_stride, filter_x, filter_y,
-                                  height, 32);
+    hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                            dst, dst_stride, filter_x, filter_y, height, 32);
 }
 
 #define BI_MC_COPY(WIDTH)                                                 \
@@ -2995,9 +2226,9 @@ void ff_hevc_put_hevc_bi_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,          \
 {                                                                            \
     const int8_t *filter = ff_hevc_##PEL##_filters[FILT_DIR - 1];            \
                                                                              \
-    hevc_##DIR1##_bi_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,     \
-                                             MAX_PB_SIZE, dst, dst_stride,   \
-                                             filter, height);                \
+    hevc_##DIR1##_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,        \
+                                          MAX_PB_SIZE, dst, dst_stride,      \
+                                          filter, height);                   \
 }
 
 BI_MC(qpel, h, 16, 8, hz, mx);
@@ -3037,9 +2268,9 @@ void ff_hevc_put_hevc_bi_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,          \
     const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];             \
     const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];             \
                                                                           \
-    hevc_hv_bi_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,        \
-                                       MAX_PB_SIZE, dst, dst_stride,      \
-                                       filter_x, filter_y, height);       \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,           \
+                                    MAX_PB_SIZE, dst, dst_stride,         \
+                                    filter_x, filter_y, height);          \
 }
 
 BI_MC_HV(qpel, 8, 8);
diff --git a/libavcodec/loongarch/hevc_mc_uni_lsx.c b/libavcodec/loongarch/hevc_mc_uni_lsx.c
index 9976502f18..a15c86268f 100644
--- a/libavcodec/loongarch/hevc_mc_uni_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_uni_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -21,7 +22,6 @@
 
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
-#include "hevc_macros_lsx.h"
 
 static const uint8_t ff_hevc_mask_arr[16 * 3] __attribute__((aligned(0x40))) = {
     /* 8 width cases */
@@ -38,7 +38,7 @@ void common_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
                           const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
-    __m128i mask0, mask1, mask2, mask3, out;
+    __m128i mask0, mask1, mask2, mask3, out1, out2;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i filt0, filt1, filt2, filt3;
@@ -48,73 +48,78 @@ void common_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
     src -= 3;
 
     /* rearranging filter */
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = height; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src, 8, src, 16, src, 24, src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vld, src, 32, src, 40, src, 48, src, 56, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 8, src, 16, src, 24,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 32, src, 40, src, 48, src, 56,
+                  src4, src5, src6, src7);
         src += src_stride;
 
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                  src5, src6, src7);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        DUP4_ARG2(__lsx_vdp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3, filt0,
-                  res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
-                  vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec6, vec7);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
-                  vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec6, vec7);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
-                  vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
-
-        out = __lsx_vssrarni_b_h(res1, res0, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        out = __lsx_vssrarni_b_h(res3, res2, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 16);
-
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src5, mask0, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src7, src7, mask0, vec2, vec3);
-        DUP4_ARG2(__lsx_vdp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3,
-                  filt0, res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask2, src5, src5, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask2, src7, src7, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
-                  vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src5, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask1, src7, src7, mask1, vec6, vec7);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
-                  vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask3, src5, src5, mask3, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask3, src7, src7, mask3, vec6, vec7);
-        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
-                  vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
-
-        DUP4_ARG2(__lsx_vsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1,
-                  res2, res3);
-        DUP4_ARG2(__lsx_vsat_h, res0, 7, res1, 7, res2, 7, res3, 7, res0, res1,
-                  res2, res3);
-        out = __lsx_vpickev_b(res1, res0);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 32);
-        out = __lsx_vpickev_b(res3, res2);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 48);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec0, filt2, res1, vec1, filt2,
+                  res2, vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt1, res1, vec5, filt1,
+                  res2, vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt3, res1, vec5, filt3,
+                  res2, vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, res1, res0, 6, res3, res2, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vst(out2, dst, 16);
+
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src5, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src7, src7, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask2, src5, src5, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask2, src7, src7, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec0, filt2, res1, vec1, filt2,
+                  res2, vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask1, src7, src7, mask1,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt1, res1, vec5, filt1,
+                  res2, vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask3, src5, src5, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask3, src7, src7, mask3,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt3, res1, vec5, filt3,
+                  res2, vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, res1, res0, 6, res3, res2, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 32);
+        __lsx_vst(out2, dst, 48);
         dst += dst_stride;
     }
 }
@@ -125,12 +130,12 @@ void common_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
                          const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_4x = (src_stride << 2);
-    const int32_t dst_stride_4x = (dst_stride << 2);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
-    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
 
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i src10_r, src32_r, src54_r, src76_r, src98_r, src21_r, src43_r;
@@ -139,48 +144,42 @@ void common_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
     __m128i out0_r, out1_r, out2_r, out3_r;
 
     src -= src_stride_3x;
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
-    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, src10_r,
-              src32_r, src54_r, src21_r);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
 
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  src76_r, src87_r, src98_r, src109_r);
-        DUP4_ARG2(__lsx_vdp2_h_b, src10_r, filt0, src21_r, filt0, src32_r, filt0,
-                  src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src32_r, filt1, out1_r, src43_r, filt1,
-                  out2_r, src54_r, filt1, out3_r, src65_r, filt1, out0_r, out1_r,
-                  out2_r, out3_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src54_r, filt2, out1_r, src65_r, filt2,
-                  out2_r, src76_r, filt2, out3_r, src87_r, filt2, out0_r, out1_r,
-                  out2_r, out3_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src76_r, filt3, out1_r, src87_r, filt3,
-                  out2_r, src98_r, filt3, out3_r, src109_r, filt3, out0_r, out1_r,
-                  out2_r, out3_r);
-
-        tmp0 = __lsx_vssrarni_b_h(out1_r, out0_r, 6);
-        tmp0 = __lsx_vxori_b(tmp0, 128);
-        tmp1 = __lsx_vssrarni_b_h(out3_r, out2_r, 6);
-        tmp1 = __lsx_vxori_b(tmp1, 128);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                  filt0, src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out1_r,
+                  src43_r, filt1, out2_r, src54_r, filt1, out3_r, src65_r,
+                  filt1, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src54_r, filt2, out1_r,
+                  src65_r, filt2, out2_r, src76_r, filt2, out3_r, src87_r,
+                  filt2, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src76_r, filt3, out1_r,
+                  src87_r, filt3, out2_r, src98_r, filt3, out3_r, src109_r,
+                  filt3, out0_r, out1_r, out2_r, out3_r);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
+                  tmp0, tmp1)
         __lsx_vstelm_d(tmp0, dst, 0, 0);
         __lsx_vstelm_d(tmp0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(tmp1, dst + dst_stride_2x, 0, 0);
@@ -198,10 +197,9 @@ void common_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
 }
 
 static av_always_inline
-void common_vt_8t_16w_mult_lsx(uint8_t *src, int32_t src_stride,
-                               uint8_t *dst, int32_t dst_stride,
-                               const int8_t *filter, int32_t height,
-                               int32_t width)
+void common_vt_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                          int32_t dst_stride, const int8_t *filter,
+                          int32_t height, int32_t width)
 {
     uint8_t *src_tmp;
     uint8_t *dst_tmp;
@@ -229,16 +227,14 @@ void common_vt_8t_16w_mult_lsx(uint8_t *src, int32_t src_stride,
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
-                  src1, src2, src3);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
         src_tmp += src_stride_3x;
         DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
                   src10_r, src32_r, src54_r, src21_r);
@@ -248,35 +244,39 @@ void common_vt_8t_16w_mult_lsx(uint8_t *src, int32_t src_stride,
         DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src7,
-                      src8, src9, src10);
-            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride_3x);
             src_tmp += src_stride_4x;
             DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
                       src9, src76_r, src87_r, src98_r, src109_r);
             DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
                       src9, src76_l, src87_l, src98_l, src109_l);
-            out0_r = __lsx_hevc_filt_8tap_h(src10_r, src32_r, src54_r, src76_r,
-                                            filt0, filt1, filt2, filt3);
-            out1_r = __lsx_hevc_filt_8tap_h(src21_r, src43_r, src65_r, src87_r,
-                                            filt0, filt1, filt2, filt3);
-            out2_r = __lsx_hevc_filt_8tap_h(src32_r, src54_r, src76_r, src98_r,
-                                            filt0, filt1, filt2, filt3);
-            out3_r = __lsx_hevc_filt_8tap_h(src43_r, src65_r, src87_r, src109_r,
-                                            filt0, filt1, filt2, filt3);
-            out0_l = __lsx_hevc_filt_8tap_h(src10_l, src32_l, src54_l, src76_l,
-                                            filt0, filt1, filt2, filt3);
-            out1_l = __lsx_hevc_filt_8tap_h(src21_l, src43_l, src65_l, src87_l,
-                                            filt0, filt1, filt2, filt3);
-            out2_l = __lsx_hevc_filt_8tap_h(src32_l, src54_l, src76_l, src98_l,
-                                            filt0, filt1, filt2, filt3);
-            out3_l = __lsx_hevc_filt_8tap_h(src43_l, src65_l, src87_l, src109_l,
-                                            filt0, filt1, filt2, filt3);
-            DUP4_ARG3(__lsx_vssrarni_b_h, out0_l, out0_r, 6, out1_l, out1_r, 6,
-                      out2_l, out2_r, 6, out3_l, out3_r, 6, tmp0, tmp1, tmp2, tmp3);
-            DUP4_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp2, 128, tmp3, 128,
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                      filt0, src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out1_r,
+                      src43_r, filt1, out2_r, src54_r, filt1, out3_r, src65_r,
+                      filt1, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src54_r, filt2, out1_r,
+                      src65_r, filt2, out2_r, src76_r, filt2, out3_r, src87_r,
+                      filt2, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src76_r, filt3, out1_r,
+                      src87_r, filt3, out2_r, src98_r, filt3, out3_r, src109_r,
+                      filt3, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_l, filt0, src21_l, filt0, src32_l,
+                      filt0, src43_l, filt0, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src32_l, filt1, out1_l,
+                      src43_l, filt1, out2_l, src54_l, filt1, out3_l, src65_l,
+                      filt1, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src54_l, filt2, out1_l,
+                      src65_l, filt2, out2_l, src76_l, filt2, out3_l, src87_l,
+                      filt2, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src76_l, filt3, out1_l,
+                      src87_l, filt3, out2_l, src98_l, filt3, out3_l, src109_l,
+                      filt3, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out1_l, out1_r,
+                      6, out2_l, out2_r, 6, out3_l, out3_r, 6,
                       tmp0, tmp1, tmp2, tmp3);
             __lsx_vst(tmp0, dst_tmp, 0);
             __lsx_vstx(tmp1, dst_tmp, dst_stride);
@@ -308,8 +308,7 @@ static void common_vt_8t_24w_lsx(uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter, int32_t height)
 {
-    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height,
-                              16);
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 16);
     common_vt_8t_8w_lsx(src + 16, src_stride, dst + 16, dst_stride, filter,
                         height);
 }
@@ -318,34 +317,27 @@ static void common_vt_8t_32w_lsx(uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter, int32_t height)
 {
-    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height,
-                              32);
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 32);
 }
 
 static void common_vt_8t_48w_lsx(uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter, int32_t height)
 {
-    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height,
-                              48);
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 48);
 }
 
 static void common_vt_8t_64w_lsx(uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter, int32_t height)
 {
-    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height,
-                              64);
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 64);
 }
 
 static av_always_inline
-void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
-                                    int32_t src_stride,
-                                    uint8_t *dst,
-                                    int32_t dst_stride,
-                                    const int8_t *filter_x,
-                                    const int8_t *filter_y,
-                                    int32_t height, int32_t width)
+void hevc_hv_8t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t width)
 {
     uint32_t loop_cnt, cnt;
     uint8_t *src_tmp;
@@ -372,8 +364,8 @@ void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= (src_stride_3x + 3);
-    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-              filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
@@ -387,170 +379,169 @@ void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
-                  src1, src2, src3);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
         src_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-        dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
-        dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2, filt3);
-        dst2 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2, filt3);
-        dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1, filt2, filt3);
-
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-        dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
-        dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2, filt3);
-        dst6 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_r, dst32_r, dst54_r, dst21_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_l, dst32_l, dst54_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
 
         for (loop_cnt = height >> 1; loop_cnt--;) {
-            DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
-            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
+            src7 = __lsx_vld(src_tmp, 0);
+            src8 = __lsx_vldx(src_tmp, src_stride);
             src_tmp += src_stride_2x;
 
-            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                      dst10_r, dst32_r, dst54_r, dst21_r);
-            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                      dst10_l, dst32_l, dst54_l, dst21_l);
-            DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
-            DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
-
             DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
                       src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-            dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
-
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
             dst76_r = __lsx_vilvl_h(dst7, dst6);
             dst76_l = __lsx_vilvh_h(dst7, dst6);
-            dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
-                                            filt_h1, filt_h2, filt_h3);
-            dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l, filt_h0,
-                                            filt_h1, filt_h2, filt_h3);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
             DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
 
             DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
                       src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
-            dst8 =  __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
-                                           filt2, filt3);
+            dst8 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst8, vec1, filt1, dst8, vec2,
+                      filt2, dst8, dst8);
+            dst8 = __lsx_vdp2add_h_bu_b(dst8, vec3, filt3);
 
             dst87_r = __lsx_vilvl_h(dst8, dst7);
             dst87_l = __lsx_vilvh_h(dst8, dst7);
-            dst1_r = __lsx_hevc_filt_8tap_w(dst21_r, dst43_r, dst65_r, dst87_r,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst1_l = __lsx_hevc_filt_8tap_w(dst21_l, dst43_l, dst65_l, dst87_l,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst21_r, filt_h0, dst21_l, filt_h0,
+                      dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst1_r, dst65_r, filt_h2, dst1_l,
+                      dst65_l, filt_h2, dst1_r, dst1_l, dst1_r, dst1_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst1_r, dst87_r, filt_h3, dst1_l,
+                      dst87_l, filt_h3, dst1_r, dst1_l);
             DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
-            DUP2_ARG3(__lsx_vssrarni_b_h, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+            DUP4_ARG2(__lsx_vsrari_w, dst0_r, 6, dst0_l, 6,dst1_r, 6, dst1_l,
+                      6, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG1(__lsx_vclip255_w, dst0_l, dst0_r, dst1_l, dst1_r,
+                      dst0_l, dst0_r, dst1_l, dst1_r);
+            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
                       dst0, dst1);
             out = __lsx_vpickev_b(dst1, dst0);
-            out = __lsx_vxori_b(out, 128);
             __lsx_vstelm_d(out, dst_tmp, 0, 0);
             __lsx_vstelm_d(out, dst_tmp + dst_stride, 0, 1);
             dst_tmp += dst_stride_2x;
 
-            dst0 = dst2;
-            dst1 = dst3;
-            dst2 = dst4;
-            dst3 = dst5;
-            dst4 = dst6;
-            dst5 = dst7;
+            dst10_r = dst32_r;
+            dst32_r = dst54_r;
+            dst54_r = dst76_r;
+            dst10_l = dst32_l;
+            dst32_l = dst54_l;
+            dst54_l = dst76_l;
+            dst21_r = dst43_r;
+            dst43_r = dst65_r;
+            dst65_r = dst87_r;
+            dst21_l = dst43_l;
+            dst43_l = dst65_l;
+            dst65_l = dst87_l;
             dst6 = dst8;
         }
-
         src += 8;
         dst += 8;
     }
 }
 
-static void hevc_hv_uni_8t_8w_lsx(uint8_t *src,
-                                  int32_t src_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static void hevc_hv_8t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                              int32_t dst_stride, const int8_t *filter_x,
+                              const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 8);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 8);
 }
 
-static void hevc_hv_uni_8t_16w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 16);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                      filter_x, filter_y, height, 16);
 }
 
-static void hevc_hv_uni_8t_24w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_8t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 24);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 24);
 }
 
-static void hevc_hv_uni_8t_32w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_8t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 32);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 32);
 }
 
-static void hevc_hv_uni_8t_48w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_8t_48w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 48);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 48);
 }
 
-static void hevc_hv_uni_8t_64w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_8t_64w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 64);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 64);
 }
 
 static av_always_inline
@@ -559,109 +550,102 @@ void common_vt_4t_24w_lsx(uint8_t *src, int32_t src_stride,
                           const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
 
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i src11, filt0, filt1;
     __m128i src10_r, src32_r, src76_r, src98_r, src21_r, src43_r, src87_r;
     __m128i src109_r, src10_l, src32_l, src21_l, src43_l;
-    __m128i out, out0_r, out1_r, out2_r, out3_r, out0_l, out1_l;
+    __m128i out0_r, out1_r, out2_r, out3_r, out0_l, out1_l;
+    __m128i out1, out2, out3, out4;
 
     src -= src_stride;
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    _src = src + 16;
 
     /* 16 width */
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     /* 8 width */
-    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src + src_stride_2x, 16);
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
     src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
+    _src += src_stride_3x;
     DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = 8; loop_cnt--;) {
         /* 16 width */
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
         /* 8 width */
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
 
         /* 16 width */
-        out0_r = __lsx_hevc_filt_4tap_h(src10_r, src32_r, filt0, filt1);
-        out0_l = __lsx_hevc_filt_4tap_h(src10_l, src32_l, filt0, filt1);
-        out1_r = __lsx_hevc_filt_4tap_h(src21_r, src43_r, filt0, filt1);
-        out1_l = __lsx_hevc_filt_4tap_h(src21_l, src43_l, filt0, filt1);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out0_l, src32_l,
+                  filt1, out1_r, src43_r, filt1, out1_l, src43_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
 
         /* 8 width */
-        out2_r = __lsx_hevc_filt_4tap_h(src76_r, src98_r, filt0, filt1);
-        out3_r = __lsx_hevc_filt_4tap_h(src87_r, src109_r, filt0, filt1);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  out2_r, out3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src98_r, filt1, out3_r,
+                  src109_r, filt1, out2_r, out3_r);
 
         /* 16 + 8 width */
-        out = __lsx_vssrarni_b_h(out0_l, out0_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        DUP2_ARG3(__lsx_vssrarni_b_h, out2_r, out2_r, 6, out3_r, out3_r, 6,
-                  out2_r, out3_r);
-        DUP2_ARG2(__lsx_vxori_b, out2_r, 128, out3_r, 128, out2_r, out3_r);
-        __lsx_vstelm_d(out2_r, dst, 16, 0);
+        DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out2_r, out2_r, 6,
+                out3_r, out3_r, 6, out1_l, out1_r, 6, out1, out2, out3, out4);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstelm_d(out2, dst, 16, 0);
         dst += dst_stride;
-        out = __lsx_vssrarni_b_h(out1_l, out1_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        __lsx_vstelm_d(out3_r, dst, 16, 0);
+        __lsx_vst(out4, dst, 0);
+        __lsx_vstelm_d(out3, dst, 16, 0);
         dst += dst_stride;
 
         /* 16 width */
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
         /* 8 width */
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
         src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
 
         /* 16 width */
-        out0_r = __lsx_hevc_filt_4tap_h(src32_r, src10_r, filt0, filt1);
-        out0_l = __lsx_hevc_filt_4tap_h(src32_l, src10_l, filt0, filt1);
-        out1_r = __lsx_hevc_filt_4tap_h(src43_r, src21_r, filt0, filt1);
-        out1_l = __lsx_hevc_filt_4tap_h(src43_l, src21_l, filt0, filt1);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src10_r, filt1, out0_l, src10_l,
+                  filt1, out1_r, src21_r, filt1, out1_l, src21_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
 
         /* 8 width */
-        out2_r = __lsx_hevc_filt_4tap_h(src98_r, src76_r, filt0, filt1);
-        out3_r = __lsx_hevc_filt_4tap_h(src109_r, src87_r, filt0, filt1);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  out2_r, out3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src76_r, filt1, out3_r,
+                  src87_r, filt1, out2_r, out3_r);
 
         /* 16 + 8 width */
-        out = __lsx_vssrarni_b_h(out0_l, out0_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        out = __lsx_vssrarni_b_h(out2_r, out2_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vstelm_d(out, dst, 16, 0);
-        dst += dst_stride;
+        DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out2_r, out2_r, 6,
+                  out1_l, out1_r, 6, out3_r, out3_r, 6, out1, out2, out3, out4);
 
-        out = __lsx_vssrarni_b_h(out1_l, out1_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        out = __lsx_vssrarni_b_h(out3_r, out3_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vstelm_d(out, dst, 16, 0);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstelm_d(out2, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(out3, dst, 0);
+        __lsx_vstelm_d(out4, dst, 16, 0);
         dst += dst_stride;
     }
 }
@@ -672,9 +656,10 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
                           const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
-    const int32_t src_stride_2x = (src_stride << 1);
-    const int32_t dst_stride_2x = (dst_stride << 1);
-    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
 
     __m128i src0, src1, src2, src3, src4, src6, src7, src8, src9, src10;
     __m128i src10_r, src32_r, src76_r, src98_r;
@@ -683,50 +668,46 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
     __m128i src10_l, src32_l, src76_l, src98_l;
     __m128i src21_l, src43_l, src87_l, src109_l;
     __m128i filt0, filt1;
-    __m128i out;
+    __m128i out1, out2;
 
     src -= src_stride;
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    _src = src + 16;
 
     /* 16 width */
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
 
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     /* next 16 width */
-    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src + src_stride_2x, 16);
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
     src += src_stride_3x;
+    _src += src_stride_3x;
 
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
     DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
     DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         /* 16 width */
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
         /* 16 width */
-        out0_r = __lsx_hevc_filt_4tap_h(src10_r, src32_r, filt0, filt1);
-        out0_l = __lsx_hevc_filt_4tap_h(src10_l, src32_l, filt0, filt1);
-        out1_r = __lsx_hevc_filt_4tap_h(src21_r, src43_r, filt0, filt1);
-        out1_l = __lsx_hevc_filt_4tap_h(src21_l, src43_l, filt0, filt1);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out0_l, src32_l,
+                  filt1, out1_r, src43_r, filt1, out1_l, src43_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
 
-        /* 16 width */
-        out = __lsx_vssrarni_b_h(out0_l, out0_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 0);
-        out = __lsx_vssrarni_b_h(out1_l, out1_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vstx(out, dst, dst_stride);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out1_l, out1_r, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstx(out2, dst, dst_stride);
 
         src10_r = src32_r;
         src21_r = src43_r;
@@ -735,25 +716,23 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
         src2 = src4;
 
         /* next 16 width */
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
         DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
 
         /* next 16 width */
-        out2_r = __lsx_hevc_filt_4tap_h(src76_r, src98_r, filt0, filt1);
-        out2_l = __lsx_hevc_filt_4tap_h(src76_l, src98_l, filt0, filt1);
-        out3_r = __lsx_hevc_filt_4tap_h(src87_r, src109_r, filt0, filt1);
-        out3_l = __lsx_hevc_filt_4tap_h(src87_l, src109_l, filt0, filt1);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src76_l, filt0, src87_r,
+                  filt0, src87_l, filt0, out2_r, out2_l, out3_r, out3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src98_r, filt1, out2_l, src98_l,
+                  filt1, out3_r, src109_r, filt1, out3_l, src109_l, filt1,
+                  out2_r, out2_l, out3_r, out3_l);
 
         /* next 16 width */
-        out = __lsx_vssrarni_b_h(out2_l, out2_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst, 16);
-        out = __lsx_vssrarni_b_h(out3_l, out3_r, 6);
-        out = __lsx_vxori_b(out, 128);
-        __lsx_vst(out, dst + dst_stride, 16);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out2_l, out2_r, 6, out3_l, out3_r, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 16);
+        __lsx_vst(out2, dst + dst_stride, 16);
 
         dst += dst_stride_2x;
 
@@ -766,12 +745,9 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
 }
 
 static av_always_inline
-void hevc_hv_uni_4t_8x2_lsx(uint8_t *src,
-                            int32_t src_stride,
-                            uint8_t *dst,
-                            int32_t dst_stride,
-                            const int8_t *filter_x,
-                            const int8_t *filter_y)
+void hevc_hv_4t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y)
 {
     const int32_t src_stride_2x = (src_stride << 1);
     const int32_t src_stride_4x = (src_stride << 2);
@@ -797,12 +773,9 @@ void hevc_hv_uni_4t_8x2_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
-    src4 = __lsx_vld(src + src_stride_4x, 0);
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3);
-    src4 = __lsx_vxori_b(src4, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
 
     DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
               mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
@@ -810,34 +783,32 @@ void hevc_hv_uni_4t_8x2_lsx(uint8_t *src,
               mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
     DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
 
-    dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-    dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
     DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
               dst10_r, dst21_r, dst32_r, dst43_r);
     DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
               dst10_l, dst21_l, dst32_l, dst43_l);
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    DUP2_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, out0_r, out1_r);
-    out = __lsx_vssrarni_b_h(out1_r, out0_r, 6);
-    out = __lsx_vxori_b(out, 128);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+              out0_r, out1_r);
+    out = __lsx_vssrarni_bu_h(out1_r, out0_r, 6);
     __lsx_vstelm_d(out, dst, 0, 0);
     __lsx_vstelm_d(out, dst + dst_stride, 0, 1);
 }
 
 static av_always_inline
-void hevc_hv_uni_4t_8multx4_lsx(uint8_t *src,
-                                int32_t src_stride,
-                                uint8_t *dst,
-                                int32_t dst_stride,
-                                const int8_t *filter_x,
-                                const int8_t *filter_y,
-                                int32_t width8mult)
+void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                            int32_t dst_stride, const int8_t *filter_x,
+                            const int8_t *filter_y, int32_t width8mult)
 {
     uint32_t cnt;
     const int32_t src_stride_2x = (src_stride << 1);
@@ -866,55 +837,62 @@ void hevc_hv_uni_4t_8multx4_lsx(uint8_t *src,
     mask1 = __lsx_vaddi_bu(mask0, 2);
 
     for (cnt = width8mult; cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src + src_stride_2x, 0);
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
         src += (8 - src_stride_4x);
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-
-        dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
-        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-
-        dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-        dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-
-        DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                  dst32_r, dst43_r, dst54_r, dst65_r);
-        DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                  dst32_l, dst43_l, dst54_l, dst65_l);
-
-        dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-        dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-        dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-        dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-
-        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
-                  dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
-        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
+
+        DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6,
+                  dst5, dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6,
+                  dst5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+                  dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -924,12 +902,9 @@ void hevc_hv_uni_4t_8multx4_lsx(uint8_t *src,
 }
 
 static av_always_inline
-void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
-                            int32_t src_stride,
-                            uint8_t *dst,
-                            int32_t dst_stride,
-                            const int8_t *filter_x,
-                            const int8_t *filter_y)
+void hevc_hv_4t_8x6_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y)
 {
     const int32_t src_stride_2x = (src_stride << 1);
     const int32_t dst_stride_2x = (dst_stride << 1);
@@ -962,17 +937,13 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
-    src4 = __lsx_vld(src + src_stride_4x, 0);
-    src += (src_stride_4x + src_stride);
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src5, src6, src7, src8);
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3)
-    src4 = __lsx_vxori_b(src4, 128);
-    DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5, src6,
-              src7, src8);
+
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+    src += src_stride_4x;
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,src,
+              src_stride_3x, src, src_stride_4x, src5, src6, src7, src8);
 
     DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
               mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
@@ -984,15 +955,16 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
               mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
     DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
 
-    dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-    dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-    dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
-    dst5 = __lsx_hevc_filt_4tap_h(vec10, vec11, filt0, filt1);
-    dst6 = __lsx_hevc_filt_4tap_h(vec12, vec13, filt0, filt1);
-    dst7 = __lsx_hevc_filt_4tap_h(vec14, vec15, filt0, filt1);
-    dst8 = __lsx_hevc_filt_4tap_h(vec16, vec17, filt0, filt1);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec8, filt0, vec10, filt0, vec12, filt0, vec14,
+              filt0, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2_h_bu_b(vec16, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec9, filt1, dst5, vec11, filt1, dst6,
+              vec13, filt1, dst7, vec15, filt1, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2add_h_bu_b(dst8, vec17, filt1);
 
     DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
               dst10_r, dst21_r, dst32_r, dst43_r);
@@ -1003,26 +975,29 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
     DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
               dst54_l, dst65_l, dst76_l, dst87_l);
 
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-    dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-    dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-    dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-    dst4_r = __lsx_hevc_filt_4tap_w(dst54_r, dst76_r, filt_h0, filt_h1);
-    dst4_l = __lsx_hevc_filt_4tap_w(dst54_l, dst76_l, filt_h0, filt_h1);
-    dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
-    dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
 
     DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
               dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r, out2_r, out3_r);
-    DUP2_ARG3(__lsx_vsrani_h_w, dst4_l, dst4_r, 6, dst5_l, dst5_r, 6, out4_r, out5_r);
-    DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6, out0, out1);
-    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1)
-    out2 = __lsx_vssrarni_b_h(out5_r, out4_r, 6);
-    out2 = __lsx_vxori_b(out2, 128);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst4_l, dst4_r, 6, dst5_l, dst5_r, 6,
+              out4_r, out5_r);
+    DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
+              out0, out1);
+    out2 = __lsx_vssrarni_bu_h(out5_r, out4_r, 6);
 
     __lsx_vstelm_d(out0, dst, 0, 0);
     __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
@@ -1034,14 +1009,10 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
 }
 
 static av_always_inline
-void hevc_hv_uni_4t_8multx4mult_lsx(uint8_t *src,
-                                    int32_t src_stride,
-                                    uint8_t *dst,
-                                    int32_t dst_stride,
-                                    const int8_t *filter_x,
-                                    const int8_t *filter_y,
-                                    int32_t height,
-                                    int32_t width8mult)
+void hevc_hv_4t_8multx4mult_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                                int32_t dst_stride, const int8_t *filter_x,
+                                const int8_t *filter_y, int32_t height,
+                                int32_t width8mult)
 {
     uint32_t loop_cnt, cnt;
     uint8_t *src_tmp;
@@ -1079,63 +1050,66 @@ void hevc_hv_uni_4t_8multx4mult_lsx(uint8_t *src,
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
-        src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
         src_tmp += src_stride_3x;
 
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
 
-        dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src3, src4, src5, src6);
+            src3 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src4, src5);
+            src6 = __lsx_vldx(src_tmp, src_stride_3x);
             src_tmp += src_stride_4x;
 
-            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                      src3, src4, src5, src6);
-
             DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
                       src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
             DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
                       src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
 
-            dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-            dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-            dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-            dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-
-            DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                      dst32_r, dst43_r, dst54_r, dst65_r);
-            DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                      dst32_l, dst43_l, dst54_l, dst65_l);
-
-            dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-            dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-            dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-            dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-            dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-            dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-            dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-            dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1,
+                      dst3, dst4, dst5, dst6);
+
+            DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4,
+                      dst6, dst5, dst32_r, dst43_r, dst54_r, dst65_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4,
+                      dst6, dst5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
 
             DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
                       dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r,
                       out2_r, out3_r);
-            DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
-                      out0, out1);
-            DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+            DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r,
+                      6, out0, out1);
             __lsx_vstelm_d(out0, dst_tmp, 0, 0);
             __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
             __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
@@ -1148,43 +1122,33 @@ void hevc_hv_uni_4t_8multx4mult_lsx(uint8_t *src,
             dst21_l = dst65_l;
             dst2 = dst6;
         }
-
         src += 8;
         dst += 8;
     }
 }
 
-static void hevc_hv_uni_4t_8w_lsx(uint8_t *src,
-                                  int32_t src_stride,
-                                  uint8_t *dst,
-                                  int32_t dst_stride,
-                                  const int8_t *filter_x,
-                                  const int8_t *filter_y,
-                                  int32_t height)
+static
+void hevc_hv_4t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                       int32_t dst_stride, const int8_t *filter_x,
+                       const int8_t *filter_y, int32_t height)
 {
     if (2 == height) {
-        hevc_hv_uni_4t_8x2_lsx(src, src_stride, dst, dst_stride,
-                               filter_x, filter_y);
+        hevc_hv_4t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x, filter_y);
     } else if (4 == height) {
-        hevc_hv_uni_4t_8multx4_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, 1);
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, 1);
     } else if (6 == height) {
-        hevc_hv_uni_4t_8x6_lsx(src, src_stride, dst, dst_stride,
-                               filter_x, filter_y);
+        hevc_hv_4t_8x6_lsx(src, src_stride, dst, dst_stride, filter_x, filter_y);
     } else if (0 == (height & 0x03)) {
-        hevc_hv_uni_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
-                                       filter_x, filter_y, height, 1);
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 1);
     }
 }
 
 static av_always_inline
-void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
-                            int32_t src_stride,
-                            uint8_t *dst,
-                            int32_t dst_stride,
-                            const int8_t *filter_x,
-                            const int8_t *filter_y,
-                            int32_t height)
+void hevc_hv_4t_12w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height)
 {
     uint32_t loop_cnt;
     uint8_t *src_tmp, *dst_tmp;
@@ -1220,60 +1184,61 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     src_tmp = src;
     dst_tmp = dst;
 
-    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
     src_tmp += src_stride_3x;
 
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
-
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
     DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
-    dsth0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dsth1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-    dsth2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dsth0, dsth1);
+    dsth2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dsth0, vec1, filt1, dsth1, vec3, filt1,
+              dsth0, dsth1);
+    dsth2 = __lsx_vdp2add_h_bu_b(dsth2, vec5, filt1);
 
     DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, dst10_l, dst21_l);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                  src3, src4, src5, src6);
+        src3 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                  src3, src4, src5, src6);
-
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4, src4,
-                  mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6, src6,
-                  mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
-
-        dsth3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dsth4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dsth5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-        dsth6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
-
-        DUP4_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
-                  dsth5, dst32_r, dst43_r, dst54_r, dst65_r);
-        DUP4_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
-                  dsth5, dst32_l, dst43_l, dst54_l, dst65_l);
-
-        dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-        dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-        dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-        dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                  src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                  src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dsth3, dsth4, dsth5, dsth6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth3, vec1, filt1, dsth4,
+                  vec3, filt1, dsth5, vec5, filt1, dsth6, vec7, filt1,
+                  dsth3, dsth4, dsth5, dsth6);
+
+        DUP4_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4,
+                  dsth6, dsth5, dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4,
+                  dsth6, dsth5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
 
         DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
                   dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
-        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
 
         __lsx_vstelm_d(out0, dst_tmp, 0, 0);
         __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
@@ -1294,41 +1259,39 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
     mask3 = __lsx_vaddi_bu(mask2, 2);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
     DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
 
-    dst10 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-    dst21 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst10, dst21);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, vec1, filt1, dst21, vec3, filt1,
+              dst10, dst21);
 
     dst10_r = __lsx_vilvl_h(dst21, dst10);
     dst21_r = __lsx_vilvh_h(dst21, dst10);
     dst22 = __lsx_vreplvei_d(dst21, 1);
 
     for (loop_cnt = 2; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src3, src4, src5, src6);
+        src3 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src4, src5);
+        src6 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                  src4, src5, src6)
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10)
-        DUP4_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, src8, src4,
-                  mask2, src8, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, src10, src6,
-                  mask2, src10, src6, mask3, vec4, vec5, vec6, vec7);
-
-        dst73 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
-        dst84 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
-        dst95 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
-        dst106 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, src8,
+                  src4, mask2, src8, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, src10,
+                  src6, mask2, src10, src6, mask3, vec4, vec5, vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst73, dst84, dst95, dst106);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst73, vec1, filt1, dst84, vec3,
+                  filt1, dst95, vec5, filt1, dst106, vec7, filt1,
+                  dst73, dst84, dst95, dst106);
 
         dst32_r = __lsx_vilvl_h(dst73, dst22);
         DUP2_ARG2(__lsx_vilvl_h, dst84, dst73, dst95, dst84, dst43_r, dst54_r);
@@ -1338,19 +1301,20 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
         dst22 = __lsx_vreplvei_d(dst73, 1);
         dst76_r = __lsx_vilvl_h(dst22, dst106);
 
-        dst0 = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst1 = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst2 = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst3 = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst4 = __lsx_hevc_filt_4tap_w(dst54_r, dst76_r, filt_h0, filt_h1);
-        dst5 = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
-        dst6 = __lsx_hevc_filt_4tap_w(dst76_r, dst98_r, filt_h0, filt_h1);
-        dst7 = __lsx_hevc_filt_4tap_w(dst87_r, dst109_r, filt_h0, filt_h1);
-
-        DUP4_ARG3(__lsx_vsrani_h_w, dst1, dst0, 6, dst3, dst2, 6, dst5, dst4, 6,
-                  dst7, dst6, 6, tmp0, tmp1, tmp2, tmp3);
-        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst65_r, filt_h0, dst76_r,
+                  filt_h0, dst87_r, filt_h0, dst4, dst5, dst6, dst7);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0, dst32_r, filt_h1, dst1, dst43_r,
+                  filt_h1, dst2, dst54_r, filt_h1, dst3, dst65_r, filt_h1,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst4, dst76_r, filt_h1, dst5, dst87_r,
+                  filt_h1, dst6, dst98_r, filt_h1, dst7, dst109_r, filt_h1,
+                  dst4, dst5, dst6, dst7);
+
+        DUP4_ARG3(__lsx_vsrani_h_w, dst1, dst0, 6, dst3, dst2, 6, dst5, dst4,
+                  6, dst7, dst6, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
 
         __lsx_vstelm_w(out0, dst, 0, 0);
         __lsx_vstelm_w(out0, dst + dst_stride, 0, 1);
@@ -1369,45 +1333,33 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     }
 }
 
-static void hevc_hv_uni_4t_16w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_4t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
     if (4 == height) {
-        hevc_hv_uni_4t_8multx4_lsx(src, src_stride, dst, dst_stride, filter_x,
-                                   filter_y, 2);
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride, filter_x,
+                               filter_y, 2);
     } else {
-        hevc_hv_uni_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
-                                       filter_x, filter_y, height, 2);
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 2);
     }
 }
 
-static void hevc_hv_uni_4t_24w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_4t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 3);
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 3);
 }
 
-static void hevc_hv_uni_4t_32w_lsx(uint8_t *src,
-                                   int32_t src_stride,
-                                   uint8_t *dst,
-                                   int32_t dst_stride,
-                                   const int8_t *filter_x,
-                                   const int8_t *filter_y,
-                                   int32_t height)
+static void hevc_hv_4t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
 {
-    hevc_hv_uni_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
-                                   filter_x, filter_y, height, 4);
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 4);
 }
 
 #define UNI_MC(PEL, DIR, WIDTH, TAP, DIR1, FILT_DIR)                           \
@@ -1451,8 +1403,8 @@ void ff_hevc_put_hevc_uni_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,          \
     const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];              \
     const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];              \
                                                                            \
-    hevc_hv_uni_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride,  \
-                                        filter_x, filter_y, height);       \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride,  \
+                                    filter_x, filter_y, height);       \
 }
 
 UNI_MC_HV(qpel, 8, 8);
diff --git a/libavcodec/loongarch/hevc_mc_uniw_lsx.c b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
index 541d814438..118f5b820e 100644
--- a/libavcodec/loongarch/hevc_mc_uniw_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -21,7 +22,6 @@
 
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
-#include "hevc_macros_lsx.h"
 
 static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     /* 8 width cases */
@@ -31,17 +31,10 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
 };
 
 static av_always_inline
-void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
-                                       int32_t src_stride,
-                                       uint8_t *dst,
-                                       int32_t dst_stride,
-                                       const int8_t *filter_x,
-                                       const int8_t *filter_y,
-                                       int32_t height,
-                                       int32_t weight,
-                                       int32_t offset,
-                                       int32_t rnd_val,
-                                       int32_t width)
+void hevc_hv_8t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val, int32_t width)
 {
     uint32_t loop_cnt, cnt;
     uint8_t *src_tmp;
@@ -64,22 +57,16 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
     __m128i dst10_l, dst32_l, dst54_l, dst76_l;
     __m128i dst21_r, dst43_r, dst65_r, dst87_r;
     __m128i dst21_l, dst43_l, dst65_l, dst87_l;
-    __m128i weight_vec, offset_vec, rnd_vec, const_128, denom_vec;
+    __m128i weight_vec, offset_vec, rnd_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= (src_stride_3x + 3);
     weight_vec = __lsx_vreplgr2vr_w(weight);
     offset_vec = __lsx_vreplgr2vr_w(offset);
     rnd_vec = __lsx_vreplgr2vr_w(rnd_val);
-    denom_vec = __lsx_vsubi_wu(rnd_vec, 6);
 
-    const_128 = __lsx_vldi(0x880);
-    const_128 = __lsx_vmul_w(weight_vec, const_128);
-    denom_vec = __lsx_vsrar_w(const_128, denom_vec);
-    offset_vec = __lsx_vadd_w(denom_vec, offset_vec);
-
-    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-              filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
     DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
@@ -91,93 +78,108 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
-                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
         src_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-        dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                      filt3);
-        dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                      filt3);
-        dst2 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                      filt3);
-        dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1,
-                                      filt2, filt3);
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-        dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                      filt3);
-        dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                      filt3);
-        dst6 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                      filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
 
-        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                  dst10_r, dst32_r, dst54_r, dst21_r);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_r, dst32_r, dst54_r, dst21_r);
         DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
-        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                  dst10_l, dst32_l, dst54_l, dst21_l);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_l, dst32_l, dst54_l, dst21_l);
         DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
 
         for (loop_cnt = height >> 1; loop_cnt--;) {
-            DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
+            src7 = __lsx_vld(src_tmp, 0);
+            src8 = __lsx_vldx(src_tmp, src_stride);
             src_tmp += src_stride_2x;
-            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
             DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
                       src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-            dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
-                                          filt2, filt3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
             dst76_r = __lsx_vilvl_h(dst7, dst6);
             dst76_l = __lsx_vilvh_h(dst7, dst6);
-            dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
             DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
 
             /* row 8 */
             DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
                       src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
-            dst8 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
-                                          filt2, filt3);
+            dst8 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst8, vec1, filt1, dst8, vec2,
+                      filt2, dst8, dst8);
+            dst8 = __lsx_vdp2add_h_bu_b(dst8, vec3, filt3);
 
             dst87_r = __lsx_vilvl_h(dst8, dst7);
             dst87_l = __lsx_vilvh_h(dst8, dst7);
-            dst1_r = __lsx_hevc_filt_8tap_w(dst21_r, dst43_r, dst65_r, dst87_r,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst1_l = __lsx_hevc_filt_8tap_w(dst21_l, dst43_l, dst65_l, dst87_l,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst21_r, filt_h0, dst21_l, filt_h0,
+                      dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst1_r, dst65_r, filt_h2, dst1_l,
+                      dst65_l, filt_h2, dst1_r, dst1_l, dst1_r, dst1_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst1_r, dst87_r, filt_h3, dst1_l,
+                      dst87_l, filt_h3, dst1_r, dst1_l);
             DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
 
-            DUP2_ARG2(__lsx_vmul_w, dst0_r, weight_vec, dst0_l, weight_vec, dst0_r, dst0_l);
-            DUP2_ARG2(__lsx_vmul_w, dst1_r, weight_vec, dst1_l, weight_vec, dst1_r, dst1_l);
+            DUP2_ARG2(__lsx_vmul_w, dst0_r, weight_vec, dst0_l, weight_vec,
+                      dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vmul_w, dst1_r, weight_vec, dst1_l, weight_vec,
+                      dst1_r, dst1_l);
             DUP4_ARG2(__lsx_vsrar_w, dst0_r, rnd_vec, dst1_r, rnd_vec, dst0_l,
-                      rnd_vec, dst1_l, rnd_vec, dst0_r, dst1_r, dst0_l, dst1_l);
+                     rnd_vec, dst1_l, rnd_vec, dst0_r, dst1_r, dst0_l, dst1_l);
 
-            DUP2_ARG2(__lsx_vadd_w, dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
-            DUP2_ARG2(__lsx_vadd_w, dst1_r, offset_vec, dst1_l, offset_vec, dst1_r, dst1_l);
+            DUP2_ARG2(__lsx_vadd_w, dst0_r, offset_vec, dst0_l, offset_vec,
+                      dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vadd_w, dst1_r, offset_vec, dst1_l, offset_vec,
+                      dst1_r, dst1_l);
             DUP4_ARG1(__lsx_vclip255_w, dst0_r, dst1_r, dst0_l, dst1_l, dst0_r,
                       dst1_r, dst0_l, dst1_l);
-            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
+                      dst0_r, dst1_r);
             dst0_r = __lsx_vpickev_b(dst1_r, dst0_r);
 
             __lsx_vstelm_d(dst0_r, dst_tmp, 0, 0);
@@ -204,123 +206,86 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
     }
 }
 
-static void hevc_hv_uniwgt_8t_8w_lsx(uint8_t *src,
-                                     int32_t src_stride,
-                                     uint8_t *dst,
-                                     int32_t dst_stride,
-                                     const int8_t *filter_x,
-                                     const int8_t *filter_y,
-                                     int32_t height,
-                                     int32_t weight,
-                                     int32_t offset,
-                                     int32_t rnd_val)
+static
+void hevc_hv_8t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                       int32_t dst_stride, const int8_t *filter_x,
+                       const int8_t *filter_y, int32_t height, int32_t weight,
+                       int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 8);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 8);
 }
 
-static void hevc_hv_uniwgt_8t_16w_lsx(uint8_t *src,
-                                      int32_t src_stride,
-                                      uint8_t *dst,
-                                      int32_t dst_stride,
-                                      const int8_t *filter_x,
-                                      const int8_t *filter_y,
-                                      int32_t height,
-                                      int32_t weight,
-                                      int32_t offset,
-                                      int32_t rnd_val)
+static
+void hevc_hv_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 16);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 16);
 }
 
-static void hevc_hv_uniwgt_8t_24w_lsx(uint8_t *src,
-                                      int32_t src_stride,
-                                      uint8_t *dst,
-                                      int32_t dst_stride,
-                                      const int8_t *filter_x,
-                                      const int8_t *filter_y,
-                                      int32_t height,
-                                      int32_t weight,
-                                      int32_t offset,
-                                      int32_t rnd_val)
+static
+void hevc_hv_8t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 24);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 24);
 }
 
-static void hevc_hv_uniwgt_8t_32w_lsx(uint8_t *src,
-                                      int32_t src_stride,
-                                      uint8_t *dst,
-                                      int32_t dst_stride,
-                                      const int8_t *filter_x,
-                                      const int8_t *filter_y,
-                                      int32_t height,
-                                      int32_t weight,
-                                      int32_t offset,
-                                      int32_t rnd_val)
+static
+void hevc_hv_8t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 32);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 32);
 }
 
-static void hevc_hv_uniwgt_8t_48w_lsx(uint8_t *src,
-                                      int32_t src_stride,
-                                      uint8_t *dst,
-                                      int32_t dst_stride,
-                                      const int8_t *filter_x,
-                                      const int8_t *filter_y,
-                                      int32_t height,
-                                      int32_t weight,
-                                      int32_t offset,
-                                      int32_t rnd_val)
+static
+void hevc_hv_8t_48w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 48);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 48);
 }
 
-static void hevc_hv_uniwgt_8t_64w_lsx(uint8_t *src,
-                                      int32_t src_stride,
-                                      uint8_t *dst,
-                                      int32_t dst_stride,
-                                      const int8_t *filter_x,
-                                      const int8_t *filter_y,
-                                      int32_t height,
-                                      int32_t weight,
-                                      int32_t offset,
-                                      int32_t rnd_val)
+static
+void hevc_hv_8t_64w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
 {
-    hevc_hv_uniwgt_8t_8multx2mult_lsx(src, src_stride, dst, dst_stride,
-                                      filter_x, filter_y, height, weight,
-                                      offset, rnd_val, 64);
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 64);
 }
 
 
-#define UNI_W_MC_HV(PEL, WIDTH, TAP)                                          \
-void ff_hevc_put_hevc_uni_w_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,           \
-                                                      ptrdiff_t dst_stride,   \
-                                                      uint8_t *src,           \
-                                                      ptrdiff_t src_stride,   \
-                                                      int height,             \
-                                                      int denom,              \
-                                                      int weight,             \
-                                                      int offset,             \
-                                                      intptr_t mx,            \
-                                                      intptr_t my,            \
-                                                      int width)              \
-{                                                                             \
-    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];                 \
-    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];                 \
-    int shift = denom + 14 - 8;                                               \
-                                                                              \
-    hevc_hv_uniwgt_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride,  \
-                                           filter_x, filter_y,  height,       \
-                                           weight, offset, shift);            \
+#define UNI_W_MC_HV(PEL, WIDTH, TAP)                                           \
+void ff_hevc_put_hevc_uni_w_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,            \
+                                                      ptrdiff_t dst_stride,    \
+                                                      uint8_t *src,            \
+                                                      ptrdiff_t src_stride,    \
+                                                      int height,              \
+                                                      int denom,               \
+                                                      int weight,              \
+                                                      int offset,              \
+                                                      intptr_t mx,             \
+                                                      intptr_t my,             \
+                                                      int width)               \
+{                                                                              \
+    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];                  \
+    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];                  \
+    int shift = denom + 14 - 8;                                                \
+                                                                               \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride, filter_x,\
+                                    filter_y,  height, weight, offset, shift); \
 }
 
 UNI_W_MC_HV(qpel, 8, 8);
diff --git a/libavcodec/loongarch/hevcdsp_init_loongarch.c b/libavcodec/loongarch/hevcdsp_init_loongarch.c
index 94e650d997..22739c6f5b 100644
--- a/libavcodec/loongarch/hevcdsp_init_loongarch.c
+++ b/libavcodec/loongarch/hevcdsp_init_loongarch.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -20,7 +21,7 @@
  */
 
 #include "libavutil/loongarch/cpu.h"
-#include "libavcodec/loongarch/hevcdsp_lsx.h"
+#include "hevcdsp_lsx.h"
 
 void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth)
 {
@@ -85,36 +86,6 @@ void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth)
             c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_lsx;
             c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_lsx;
 
-            c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_lsx;
-
-            c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_lsx;
-            c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_lsx;
-            c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_lsx;
-            c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_lsx;
-
-            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_lsx;
-            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_lsx;
-            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_lsx;
-            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_lsx;
-            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_lsx;
-            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_lsx;
-
-            c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_lsx;
-            c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_lsx;
-
-            c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_lsx;
-            c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_lsx;
-            c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_lsx;
-            c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_lsx;
-            c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_lsx;
-
-            c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_lsx;
-            c->put_hevc_qpel_uni_w[5][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv16_8_lsx;
-            c->put_hevc_qpel_uni_w[6][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv24_8_lsx;
-            c->put_hevc_qpel_uni_w[7][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv32_8_lsx;
-            c->put_hevc_qpel_uni_w[8][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv48_8_lsx;
-            c->put_hevc_qpel_uni_w[9][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv64_8_lsx;
-
             c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_lsx;
             c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_lsx;
             c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_lsx;
@@ -166,6 +137,36 @@ void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth)
             c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_lsx;
             c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_lsx;
 
+            c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_lsx;
+
+            c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_lsx;
+            c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_lsx;
+            c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_lsx;
+            c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_lsx;
+
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_lsx;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_lsx;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_lsx;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_lsx;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_lsx;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_lsx;
+
+            c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_lsx;
+            c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_lsx;
+
+            c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_lsx;
+            c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_lsx;
+            c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_lsx;
+            c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_lsx;
+            c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_lsx;
+
+            c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_lsx;
+            c->put_hevc_qpel_uni_w[5][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv16_8_lsx;
+            c->put_hevc_qpel_uni_w[6][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv24_8_lsx;
+            c->put_hevc_qpel_uni_w[7][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv32_8_lsx;
+            c->put_hevc_qpel_uni_w[8][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv48_8_lsx;
+            c->put_hevc_qpel_uni_w[9][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv64_8_lsx;
+
             c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_lsx;
 
             c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_lsx;
diff --git a/libavcodec/loongarch/hevcdsp_lsx.c b/libavcodec/loongarch/hevcdsp_lsx.c
index e4779c77a6..a520f02bd1 100644
--- a/libavcodec/loongarch/hevcdsp_lsx.c
+++ b/libavcodec/loongarch/hevcdsp_lsx.c
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -21,7 +22,6 @@
 
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
-#include "hevc_macros_lsx.h"
 
 static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     /* 8 width cases */
@@ -30,70 +30,56 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20
 };
 
+/* hevc_copy: dst = src << 6 */
 static void hevc_copy_4w_lsx(uint8_t *src, int32_t src_stride,
                              int16_t *dst, int32_t dst_stride,
                              int32_t height)
 {
-    __m128i zero = {0};
     int32_t src_stride_2x = (src_stride << 1);
     int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
     int32_t dst_stride_4x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
 
-    if (2 == height) {
-        __m128i src0, src1;
-        __m128i in0;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3;
+    for (; loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src5, src4, src7, src6,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0, in1, in2, in3);
 
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-        src0 = __lsx_vilvl_w(src1, src0);
-        in0 = __lsx_vilvl_b(zero, src0);
-        in0 = __lsx_vslli_h(in0, 6);
-        __lsx_vstelm_d(in0, dst, 0, 0);
-        __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
-    } else if (4 == height) {
-        __m128i src0, src1, src2, src3;
-        __m128i in0, in1;
-
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
         __lsx_vstelm_d(in0, dst, 0, 0);
         __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(in1, dst + dst_stride_2x, 0, 0);
         __lsx_vstelm_d(in1, dst + dst_stride_3x, 0, 1);
-    } else if (0 == (height & 0x07)) {
-        __m128i src0, src1, src2, src3, src4, src5, src6, src7;
-        __m128i in0, in1, in2, in3;
-        uint32_t loop_cnt;
-        for (loop_cnt = (height >> 3); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-            src += src_stride_4x;
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
-            src += src_stride_4x;
-
-            DUP4_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src5, src4, src7, src6,
-                      src0, src1, src2, src3);
-            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0, in1, in2, in3);
-            DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-
-            __lsx_vstelm_d(in0, dst, 0, 0);
-            __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
-            __lsx_vstelm_d(in1, dst + dst_stride_2x, 0, 0);
-            __lsx_vstelm_d(in1, dst + dst_stride_3x, 0, 1);
-            dst += dst_stride_4x;
-            __lsx_vstelm_d(in2, dst, 0, 0);
-            __lsx_vstelm_d(in2, dst + dst_stride, 0, 1);
-            __lsx_vstelm_d(in3, dst + dst_stride_2x, 0, 0);
-            __lsx_vstelm_d(in3, dst + dst_stride_3x, 0, 1);
-            dst += dst_stride_4x;
-        }
+        dst += dst_stride_4x;
+        __lsx_vstelm_d(in2, dst, 0, 0);
+        __lsx_vstelm_d(in2, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(in3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(in3, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        src += src_stride;
+        dst += dst_stride;
     }
 }
 
@@ -101,67 +87,61 @@ static void hevc_copy_6w_lsx(uint8_t *src, int32_t src_stride,
                              int16_t *dst, int32_t dst_stride,
                              int32_t height)
 {
-    uint32_t loop_cnt;
-    uint64_t out0_m, out1_m, out2_m, out3_m;
-    uint64_t out4_m, out5_m, out6_m, out7_m;
-    uint32_t out8_m, out9_m, out10_m, out11_m;
-    uint32_t out12_m, out13_m, out14_m, out15_m;
+    int32_t loop_cnt = (height >> 3);
+    int32_t res = height & 0x07;
     int32_t src_stride_2x = (src_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    __m128i zero = {0};
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
                   in4, in5, in6, in7);
-        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
-
-        DUP4_ARG2(__lsx_vpickve2gr_du, in0, 0, in1, 0, in2, 0, in3, 0, out0_m,
-                  out1_m, out2_m, out3_m);
-        DUP4_ARG2(__lsx_vpickve2gr_du, in4, 0, in5, 0, in6, 0, in7, 0, out4_m,
-                  out5_m, out6_m, out7_m);
 
-        DUP4_ARG2(__lsx_vpickve2gr_wu, in0, 2, in1, 2, in2, 2, in3, 2, out8_m,
-                  out9_m, out10_m, out11_m);
-        DUP4_ARG2(__lsx_vpickve2gr_wu, in4, 2, in5, 2, in6, 2, in7, 2, out12_m,
-                  out13_m, out14_m, out15_m);
-
-        *(uint64_t *)dst = out0_m;
-        *(uint32_t *)(dst + 4) = out8_m;
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        __lsx_vstelm_w(in0, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 0, 0);
+        __lsx_vstelm_w(in1, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out1_m;
-        *(uint32_t *)(dst + 4) = out9_m;
+        __lsx_vstelm_d(in2, dst, 0, 0);
+        __lsx_vstelm_w(in2, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out2_m;
-        *(uint32_t *)(dst + 4) = out10_m;
+        __lsx_vstelm_d(in3, dst, 0, 0);
+        __lsx_vstelm_w(in3, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out3_m;
-        *(uint32_t *)(dst + 4) = out11_m;
+        __lsx_vstelm_d(in4, dst, 0, 0);
+        __lsx_vstelm_w(in4, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out4_m;
-        *(uint32_t *)(dst + 4) = out12_m;
+        __lsx_vstelm_d(in5, dst, 0, 0);
+        __lsx_vstelm_w(in5, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out5_m;
-        *(uint32_t *)(dst + 4) = out13_m;
+        __lsx_vstelm_d(in6, dst, 0, 0);
+        __lsx_vstelm_w(in6, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out6_m;
-        *(uint32_t *)(dst + 4) = out14_m;
+        __lsx_vstelm_d(in7, dst, 0, 0);
+        __lsx_vstelm_w(in7, dst, 8, 2);
         dst += dst_stride;
-        *(uint64_t *)dst = out7_m;
-        *(uint32_t *)(dst + 4) = out15_m;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
+        src += src_stride;
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        __lsx_vstelm_w(in0, dst, 8, 2);
         dst += dst_stride;
     }
 }
@@ -170,85 +150,50 @@ static void hevc_copy_8w_lsx(uint8_t *src, int32_t src_stride,
                              int16_t *dst, int32_t dst_stride,
                              int32_t height)
 {
-    __m128i zero = {0};
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride_x << 1);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-
-    if (2 == height) {
-        __m128i src0, src1;
-        __m128i in0, in1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
-        __lsx_vst(in0, dst, 0);
-        __lsx_vst(in1, dst + dst_stride, 0);
-    } else if (4 == height) {
-        __m128i src0, src1, src2, src3;
-        __m128i in0, in1, in2, in3;
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
 
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   in0, in1, in2, in3);
-        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in4, in5, in6, in7);
         __lsx_vst(in0, dst, 0);
-        __lsx_vst(in1, dst + dst_stride, 0);
-        __lsx_vst(in2, dst + dst_stride_2x, 0);
-        __lsx_vst(in3, dst + dst_stride_3x, 0);
-    } else if (6 == height) {
-        __m128i src0, src1, src2, src3, src4, src5;
-        __m128i in0, in1, in2, in3, in4, in5;
-
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        src += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3, in0,
-                  in1, in2, in3);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4, in5);
-        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-        DUP2_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in4, in5);
+        __lsx_vstx(in1, dst, dst_stride_x);
+        __lsx_vstx(in2, dst, dst_stride_2x);
+        __lsx_vstx(in3, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+        __lsx_vst(in4, dst, 0);
+        __lsx_vstx(in5, dst, dst_stride_x);
+        __lsx_vstx(in6, dst, dst_stride_2x);
+        __lsx_vstx(in7, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
         __lsx_vst(in0, dst, 0);
-        __lsx_vst(in1, dst + dst_stride, 0);
-        __lsx_vst(in2, dst + dst_stride_2x, 0);
-        __lsx_vst(in3, dst + dst_stride_3x, 0);
-        __lsx_vst(in4, dst + dst_stride_4x, 0);
-        __lsx_vst(in5, dst + dst_stride_4x + dst_stride, 0);
-    } else if (0 == (height & 0x07)) {
-        uint32_t loop_cnt;
-        __m128i src0, src1, src2, src3, src4, src5, src6, src7;
-        __m128i in0, in1, in2, in3, in4, in5, in6, in7;
-
-        for (loop_cnt = (height >> 3); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-            src += src_stride_4x;
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
-            src += src_stride_4x;
-
-            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0, in1, in2, in3);
-            DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in4, in5, in6, in7);
-            DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-            DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
-            __lsx_vst(in0, dst, 0);
-            __lsx_vst(in1, dst + dst_stride, 0);
-            __lsx_vst(in2, dst + dst_stride_2x, 0);
-            __lsx_vst(in3, dst + dst_stride_3x, 0);
-            dst += dst_stride_4x;
-            __lsx_vst(in4, dst, 0);
-            __lsx_vst(in5, dst + dst_stride, 0);
-            __lsx_vst(in6, dst + dst_stride_2x, 0);
-            __lsx_vst(in7, dst + dst_stride_3x, 0);
-            dst += dst_stride_4x;
-        }
+        src += src_stride;
+        dst += dst_stride;
     }
 }
 
@@ -257,57 +202,70 @@ static void hevc_copy_12w_lsx(uint8_t *src, int32_t src_stride,
                               int32_t height)
 {
     uint32_t loop_cnt;
+    uint32_t res = height & 0x07;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride_x << 1);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    __m128i zero = {0};
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    __m128i zero = __lsx_vldi(0);
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0, in1, in0_r, in1_r, in2_r, in3_r;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
         DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, in0, in1);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
         __lsx_vstelm_d(in0, dst, 16, 0);
-        __lsx_vstelm_d(in0, dst + dst_stride, 16, 1);
-        __lsx_vstelm_d(in1, dst + dst_stride_2x, 16, 0);
-        __lsx_vstelm_d(in1, dst + dst_stride_3x, 16, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(in0, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 1);
+        dst += dst_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
         DUP2_ARG2(__lsx_vilvh_w, src5, src4, src7, src6, src0, src1);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, in0, in1);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
         __lsx_vstelm_d(in0, dst, 16, 0);
-        __lsx_vstelm_d(in0, dst + dst_stride, 16, 1);
-        __lsx_vstelm_d(in1, dst + dst_stride_2x, 16, 0);
-        __lsx_vstelm_d(in1, dst + dst_stride_3x, 16, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(in0, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 1);
+        dst += dst_stride;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0  = __lsx_vsllwil_hu_bu(src0, 6);
+        src1 = __lsx_vilvh_b(zero, src0);
+        in1  = __lsx_vslli_h(src1, 6);
+        __lsx_vst(in0, dst, 0);
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        src += src_stride;
+        dst += dst_stride;
     }
 }
 
@@ -315,153 +273,84 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
                               int16_t *dst, int32_t dst_stride,
                               int32_t height)
 {
-    __m128i zero = {0};
+    __m128i zero = __lsx_vldi(0);
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-
-    if (4 == height) {
-        __m128i src0, src1, src2, src3;
-        __m128i in0_r, in1_r, in2_r, in3_r;
-        __m128i in0_l, in1_l, in2_l, in3_l;
-
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
+    int16_t* dst1 = dst + 8;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                  in1_l, in2_l, in3_l);
-        __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-        __lsx_vst(in0_l, dst, 16);
-        __lsx_vst(in1_l, dst + dst_stride, 16);
-        __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-        __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-   } else if (12 == height) {
-        __m128i src0, src1, src2, src3, src4, src5, src6, src7;
-        __m128i src8, src9, src10, src11;
-        __m128i in0_r, in1_r, in2_r, in3_r;
-        __m128i in0_l, in1_l, in2_l, in3_l;
-
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src8, src9, src10, src11);
-
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-        __lsx_vst(in0_l, dst, 16);
-        __lsx_vst(in1_l, dst + dst_stride, 16);
-        __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-        __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-        dst += dst_stride_4x;
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
+        dst1 += dst_stride_2x;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-        __lsx_vst(in0_l, dst, 16);
-        __lsx_vst(in1_l, dst + dst_stride, 16);
-        __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-        __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-        dst += dst_stride_4x;
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
+        dst1 += dst_stride_2x;
+    }
+    if (res) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src8, zero, src9, zero, src10, zero, src11,
-                  in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src8, zero, src9, zero, src10, zero, src11,
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-        __lsx_vst(in0_l, dst, 16);
-        __lsx_vst(in1_l, dst + dst_stride, 16);
-        __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-        __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-    } else if (0 == (height & 0x07)) {
-        uint32_t loop_cnt;
-        __m128i src0, src1, src2, src3, src4, src5, src6, src7;
-        __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
-
-        for (loop_cnt = (height >> 3); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src0, src1, src2, src3);
-            src += src_stride_4x;
-            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src4, src5, src6, src7);
-            src += src_stride_4x;
-            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-            DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-            DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-            DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
-            __lsx_vst(in0_r, dst, 0);
-            __lsx_vst(in1_r, dst + dst_stride, 0);
-            __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-            __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-            __lsx_vst(in0_l, dst, 16);
-            __lsx_vst(in1_l, dst + dst_stride, 16);
-            __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-            __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-            dst += dst_stride_4x;
-
-            DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-            DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_l, in1_l, in2_l, in3_l);
-            DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-            DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
-            __lsx_vst(in0_r, dst, 0);
-            __lsx_vst(in1_r, dst + dst_stride, 0);
-            __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-            __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-            __lsx_vst(in0_l, dst, 16);
-            __lsx_vst(in1_l, dst + dst_stride, 16);
-            __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-            __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-            dst += dst_stride_4x;
-        }
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        dst += 8;
+        __lsx_vst(in0_l, dst, 0);
+        __lsx_vstx(in1_l, dst, dst_stride_x);
+        __lsx_vstx(in2_l, dst, dst_stride_2x);
+        __lsx_vstx(in3_l, dst, dst_stride_3x);
     }
 }
 
@@ -471,46 +360,51 @@ static void hevc_copy_24w_lsx(uint8_t *src, int32_t src_stride,
 {
     uint32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    __m128i zero = {0};
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    uint8_t *_src = src + 16;
+    int16_t *dst1 = dst;
+    __m128i zero = __lsx_vldi(0);
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
-                  16, src + src_stride_3x, 16, src4, src5, src6, src7);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        src4 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
-        __lsx_vst(in1_r, dst + dst_stride, 0);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 0);
-        __lsx_vst(in0_l, dst, 16);
-        __lsx_vst(in1_l, dst + dst_stride, 16);
-        __lsx_vst(in2_l, dst + dst_stride_2x, 16);
-        __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        dst1 = dst + 8;
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
-        __lsx_vst(in0_r, dst, 32);
-        __lsx_vst(in1_r, dst + dst_stride, 32);
-        __lsx_vst(in2_r, dst + dst_stride_2x, 32);
-        __lsx_vst(in3_r, dst + dst_stride_3x, 32);
-        dst += dst_stride_4x;
+        dst1 = dst1 + 8;
+        __lsx_vst(in0_r, dst1, 0);
+        __lsx_vstx(in1_r, dst1, dst_stride_x);
+        __lsx_vstx(in2_r, dst1, dst_stride_2x);
+        __lsx_vstx(in3_r, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
     }
 }
 
@@ -522,25 +416,28 @@ static void hevc_copy_32w_lsx(uint8_t *src, int32_t src_stride,
     int32_t src_stride_2x = (src_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src = src + 16;
     __m128i zero = {0};
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src2, src4, src6);
-        DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
-                  16, src + src_stride_3x, 16, src1, src3, src5, src7);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src2, src4);
+        src6 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        src1 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src3, src5);
+        src7 = __lsx_vldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -552,12 +449,10 @@ static void hevc_copy_32w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst, 48);
         dst += dst_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
@@ -598,18 +493,15 @@ static void hevc_copy_48w_lsx(uint8_t *src, int32_t src_stride,
         src11 = __lsx_vld(src, 32);
         src += src_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                  in0_l, in1_l, in2_l, in3_l);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4_r, in5_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
         DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, in4_l, in5_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
-                  in5_r, in4_l, in5_l);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vslli_h, in4_l, 6, in5_l, 6, in4_l, in5_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -625,18 +517,15 @@ static void hevc_copy_48w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in5_l, dst, 80);
         dst += dst_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src6, zero, src7, zero, src8, zero, src9,
-                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vilvh_b, zero, src6, zero, src7, zero, src8, zero, src9,
                   in0_l, in1_l, in2_l, in3_l);
-        DUP2_ARG2(__lsx_vilvl_b, zero, src10, zero, src11, in4_r, in5_r);
         DUP2_ARG2(__lsx_vilvh_b, zero, src10, zero, src11, in4_l, in5_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src6, 6, src7, 6, src8, 6, src9, 6,
+                  in0_r, in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
-                  in5_r, in4_l, in5_l);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src10, 6, src11, 6, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vslli_h, in4_l, 6, in5_l, 6, in4_l, in5_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -665,19 +554,19 @@ static void hevc_copy_64w_lsx(uint8_t *src, int32_t src_stride,
 
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
         src += src_stride;
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
         src += src_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
+                  in0_l, in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -688,12 +577,10 @@ static void hevc_copy_64w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst, 112);
         dst += dst_stride;
 
-        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero,
+                  src7, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
                   in0_r, in1_r, in2_r, in3_r);
-        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                  in0_l, in1_l, in2_l, in3_l);
-        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                  in1_r, in2_r, in3_r);
         DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                   in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
@@ -712,7 +599,8 @@ static void hevc_hz_8t_4w_lsx(uint8_t *src, int32_t src_stride,
                               int16_t *dst, int32_t dst_stride,
                               const int8_t *filter, int32_t height)
 {
-    uint32_t loop_cnt;
+    uint32_t loop_cnt = height >> 3;
+    uint32_t res = (height & 0x7) >> 1;
     int32_t src_stride_2x = (src_stride << 1);
     int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
@@ -724,44 +612,48 @@ static void hevc_hz_8t_4w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask1, mask2, mask3;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 16);
 
     src -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
-    for (loop_cnt = (height >> 3); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    for (;loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128,
-                  src4, src5, src6, src7);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1, src0,
-                  mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask0, src3, src2, mask1, src3, src2,
-                  mask2, src3, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src4, mask0, src5, src4, mask1, src5, src4,
-                  mask2, src5, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
-        DUP4_ARG3(__lsx_vshuf_b, src7, src6, mask0, src7, src6, mask1, src7, src6,
-                  mask2, src7, src6, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1,
+                  src0, mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask0, src3, src2, mask1, src3,
+                  src2, mask2, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src4, mask0, src5, src4, mask1, src5,
+                  src4, mask2, src5, src4, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src6, mask0, src7, src6, mask1, src7,
+                  src6, mask2, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
@@ -774,6 +666,20 @@ static void hevc_hz_8t_4w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vstelm_d(dst3, dst + dst_stride_3x, 0, 1);
         dst += dst_stride_4x;
     }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        src1 = __lsx_vldx(src, src_stride);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1,
+                  src0, mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        src += src_stride_2x;
+        dst += dst_stride_2x;
+    }
 }
 
 static void hevc_hz_8t_8w_lsx(uint8_t *src, int32_t src_stride,
@@ -782,56 +688,61 @@ static void hevc_hz_8t_8w_lsx(uint8_t *src, int32_t src_stride,
 {
     uint32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
     __m128i src0, src1, src2, src3;
     __m128i filt0, filt1, filt2, filt3;
     __m128i mask1, mask2, mask3;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
 
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
 
         __lsx_vst(dst0, dst, 0);
-        __lsx_vst(dst1, dst + dst_stride, 0);
-        __lsx_vst(dst2, dst + dst_stride_2x, 0);
-        __lsx_vst(dst3, dst + dst_stride_3x, 0);
-        dst += dst_stride_4x;
+        __lsx_vstx(dst1, dst, dst_stride_x);
+        __lsx_vstx(dst2, dst, dst_stride_2x);
+        __lsx_vstx(dst3, dst, dst_stride_3x);
+        dst += dst_stride_2x;
     }
 }
 
@@ -841,21 +752,18 @@ static void hevc_hz_8t_12w_lsx(uint8_t *src, int32_t src_stride,
 {
     uint32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    uint8_t *_src;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5;
     __m128i filt0, filt1, filt2, filt3, dst0, dst1, dst2, dst3, dst4, dst5;
-    __m128i const_vec;
 
     src -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    _src = src + 8;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
@@ -865,55 +773,69 @@ static void hevc_hz_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     mask7 = __lsx_vaddi_bu(mask4, 6);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src + src_stride_2x, 8,
-                  src + src_stride_3x, 8, src4, src5, src6, src7);
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src4 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(_src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                  src5, src6, src7);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask4, src7, src6, mask4, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                  dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+        _src += src_stride_4x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask4, src7, src6, mask4,
+                  vec4, vec5);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask5, src7, src6, mask5,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask6, src7, src6, mask6,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask7, src7, src6, mask7,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
                   dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask5, src7, src6, mask5, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask6, src7, src6, mask6, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask7, src7, src6, mask7, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
 
         __lsx_vst(dst0, dst, 0);
-        __lsx_vst(dst1, dst + dst_stride, 0);
-        __lsx_vst(dst2, dst + dst_stride_2x, 0);
-        __lsx_vst(dst3, dst + dst_stride_3x, 0);
-
         __lsx_vstelm_d(dst4, dst, 16, 0);
-        __lsx_vstelm_d(dst4, dst + dst_stride, 16, 1);
-        __lsx_vstelm_d(dst5, dst + dst_stride_2x, 16, 0);
-        __lsx_vstelm_d(dst5, dst + dst_stride_3x, 16, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vst(dst1, dst, 0);
+        __lsx_vstelm_d(dst4, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vst(dst2, dst, 0);
+        __lsx_vstelm_d(dst5, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst3, dst, 0);
+        __lsx_vstelm_d(dst5, dst, 16, 1);
+        dst += dst_stride;
     }
-
 }
 
 static void hevc_hz_8t_16w_lsx(uint8_t *src, int32_t src_stride,
@@ -921,18 +843,14 @@ static void hevc_hz_8t_16w_lsx(uint8_t *src, int32_t src_stride,
                                const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
-    int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     __m128i src0, src1, src2, src3;
     __m128i filt0, filt1, filt2, filt3;
     __m128i mask1, mask2, mask3;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3;
-    __m128i const_vec;
     __m128i mask0;
 
     src -= 3;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filt0, filt1, filt2, filt3);
@@ -941,35 +859,42 @@ static void hevc_hz_8t_16w_lsx(uint8_t *src, int32_t src_stride,
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
-        DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
-        src += src_stride_2x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                  dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src0, src1);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
+        src += src_stride;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
 
         __lsx_vst(dst0, dst, 0);
-        __lsx_vst(dst2, dst + dst_stride, 0);
         __lsx_vst(dst1, dst, 16);
-        __lsx_vst(dst3, dst + dst_stride, 16);
-        dst += dst_stride_2x;
+        dst += dst_stride;
+        __lsx_vst(dst2, dst, 0);
+        __lsx_vst(dst3, dst, 16);
+        dst += dst_stride;
     }
 }
 
@@ -983,52 +908,54 @@ static void hevc_hz_8t_24w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src += src_stride;
         DUP2_ARG2(__lsx_vld, src, 0, src, 16, src2, src3);
         src += src_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
 
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                  mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src3, mask0, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1,
+                  src1, mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src3, mask0,
+                  vec4, vec5);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1,
+                  src1, mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask5, src3, src3, mask1,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1,
+                  src1, mask2, src2, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask6, src3, src3, mask2,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1,
+                  src1, mask3, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask7, src3, src3, mask3,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
                   dst4, dst5);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                  mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask5, src3, src3, mask1, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                  mask2, src2, src2, mask2, vec0, vec1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask6, src3, src3, mask2, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                  mask3, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask7, src3, src3, mask3, vec4, vec5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
@@ -1051,43 +978,47 @@ static void hevc_hz_8t_32w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2,  filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2,  filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8,
+              mask1, mask2, mask3, mask4);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
         DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 24);
         src += src_stride;
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-
-        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
-                  mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1,
+                  src0, mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
         DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
                   mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
         DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
                   mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
@@ -1107,58 +1038,59 @@ static void hevc_hz_8t_48w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
         DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 32);
         src3 = __lsx_vld(src, 40);
         src += src_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
 
         DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
                   mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                  dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                  mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                  mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                  mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1,
+                  src1, mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1,
+                  src1, mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1,
+                  src1, mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         __lsx_vst(dst2, dst, 32);
         __lsx_vst(dst3, dst, 48);
 
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec4, vec5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec4, vec5);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
                   dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec4, vec5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec4, vec5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
         __lsx_vst(dst4, dst, 64);
         __lsx_vst(dst5, dst, 80);
         dst += dst_stride;
@@ -1175,73 +1107,85 @@ static void hevc_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    __m128i const_vec;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-              mask3, mask4);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6)
     mask7 = __lsx_vaddi_bu(mask0, 14);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16,  src, 32, src, 48, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16,  src, 32, src, 48,
+                  src0, src1, src2, src3);
         src4 = __lsx_vld(src, 56);
         src += src_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        src4 = __lsx_vxori_b(src4, 128);
-
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
         __lsx_vst(dst0, dst, 0);
 
-        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
-                  mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1,
+                  src0, mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
         __lsx_vst(dst1, dst, 16);
 
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
         __lsx_vst(dst2, dst, 32);
 
-        DUP4_ARG3(__lsx_vshuf_b, src2, src1, mask4, src2, src1, mask5, src2, src1,
-                  mask6, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src1, mask4, src2, src1, mask5, src2,
+                  src1, mask6, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
         __lsx_vst(dst3, dst, 48);
 
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
-                  vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+                  dst4, dst4);
+        dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
         __lsx_vst(dst4, dst, 64);
 
-        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src2, mask5, src3, src2,
-                  mask6, src3, src2, mask7, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst5, vec1, filt1, dst5,
-                  vec2, filt2, dst5, vec3, filt3, dst5, dst5, dst5, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src2, mask5, src3,
+                  src2, mask6, src3, src2, mask7, vec0, vec1, vec2, vec3);
+        dst5 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec1, filt1, dst5, vec2, filt2,
+                  dst5, dst5);
+        dst5 = __lsx_vdp2add_h_bu_b(dst5, vec3, filt3);
         __lsx_vst(dst5, dst, 80);
 
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst6, vec1, filt1, dst6,
-                  vec2, filt2, dst6, vec3, filt3, dst6, dst6, dst6, dst6);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        dst6 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec1, filt1, dst6, vec2, filt2,
+                  dst6, dst6);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec3, filt3);
         __lsx_vst(dst6, dst, 96);
 
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1, dst7,
-                  vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2, filt2,
+                  dst7, dst7);
+        dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
         __lsx_vst(dst7, dst, 112);
         dst += dst_stride;
     }
@@ -1252,12 +1196,10 @@ static void hevc_vt_8t_4w_lsx(uint8_t *src, int32_t src_stride,
                               const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
+    int32_t res = (height & 0x07) >> 1;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
-    int32_t src_stride_3x = (src_stride << 1) + src_stride;
-    int32_t dst_stride_3x = (dst_stride << 1) + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m128i src9, src10, src11, src12, src13, src14;
     __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
@@ -1267,34 +1209,35 @@ static void hevc_vt_8t_4w_lsx(uint8_t *src, int32_t src_stride,
     __m128i src12111110, src14131312;
     __m128i dst10, dst32, dst54, dst76;
     __m128i filt0, filt1, filt2, filt3;
-    __m128i const_vec;
 
     src -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
     DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
               src10_r, src32_r, src54_r, src21_r);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-    DUP2_ARG2(__lsx_vilvl_d, src21_r, src10_r, src43_r, src32_r, src2110, src4332);
+    DUP2_ARG2(__lsx_vilvl_d, src21_r, src10_r, src43_r, src32_r,
+              src2110, src4332);
     src6554 = __lsx_vilvl_d(src65_r, src54_r);
-    DUP2_ARG2(__lsx_vxori_b, src2110, 128, src4332, 128, src2110, src4332);
-    src6554 = __lsx_vxori_b(src6554, 128);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,  src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src11, src12, src13, src14);
+        src11 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src12, src13);
+        src14 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
 
         DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
@@ -1302,40 +1245,70 @@ static void hevc_vt_8t_4w_lsx(uint8_t *src, int32_t src_stride,
         DUP4_ARG2(__lsx_vilvl_b, src11, src10, src12, src11, src13, src12, src14,
                   src13, src1110_r, src1211_r, src1312_r, src1413_r);
         DUP4_ARG2(__lsx_vilvl_d, src87_r, src76_r, src109_r, src98_r, src1211_r,
-                  src1110_r, src1413_r, src1312_r, src8776, src10998, src12111110,
-                  src14131312);
-        DUP4_ARG2(__lsx_vxori_b, src8776, 128, src10998, 128, src12111110, 128,
-                  src14131312, 128, src8776, src10998, src12111110, src14131312);
-
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst10, src4332, filt1,
-                  dst10, src6554, filt2, dst10, src8776, filt3, dst10, dst10, dst10,
-                  dst10);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src4332, filt0, dst32, src6554, filt1,
-                  dst32, src8776, filt2, dst32, src10998, filt3, dst32, dst32,
-                  dst32, dst32);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src6554, filt0, dst54, src8776, filt1,
-                  dst54, src10998, filt2, dst54, src12111110, filt3, dst54, dst54,
-                  dst54, dst54);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src8776, filt0, dst76, src10998,
-                  filt1, dst76, src12111110, filt2, dst76, src14131312, filt3, dst76,
-                  dst76, dst76, dst76);
+                  src1110_r, src1413_r, src1312_r, src8776, src10998,
+                  src12111110, src14131312);
+
+        dst10 = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, src4332, filt1, dst10, src6554,
+                  filt2, dst10, dst10);
+        dst10 = __lsx_vdp2add_h_bu_b(dst10, src8776, filt3);
+        dst32 = __lsx_vdp2_h_bu_b(src4332, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst32, src6554, filt1, dst32, src8776,
+                  filt2, dst32, dst32);
+        dst32 = __lsx_vdp2add_h_bu_b(dst32, src10998, filt3);
+        dst54 = __lsx_vdp2_h_bu_b(src6554, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst54, src8776, filt1,
+                  dst54, src10998, filt2, dst54, dst54);
+        dst54 = __lsx_vdp2add_h_bu_b(dst54, src12111110, filt3);
+        dst76 = __lsx_vdp2_h_bu_b(src8776, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst76, src10998, filt1, dst76,
+                  src12111110, filt2, dst76, dst76);
+        dst76 = __lsx_vdp2add_h_bu_b(dst76, src14131312, filt3);
 
         __lsx_vstelm_d(dst10, dst, 0, 0);
-        __lsx_vstelm_d(dst10, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(dst32, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(dst32, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(dst10, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst32, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst32, dst, 0, 1);
+        dst += dst_stride;
         __lsx_vstelm_d(dst54, dst, 0, 0);
-        __lsx_vstelm_d(dst54, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(dst76, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(dst76, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(dst54, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst76, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst76, dst, 0, 1);
+        dst += dst_stride;
 
         src2110 = src10998;
         src4332 = src12111110;
         src6554 = src14131312;
         src6 = src14;
     }
+    for (;res--;) {
+        src7 = __lsx_vld(src, 0);
+        src8 = __lsx_vldx(src, src_stride);
+        DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+        src += src_stride_2x;
+        src8776 = __lsx_vilvl_d(src87_r, src76_r);
+
+        dst10 = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, src4332, filt1, dst10, src6554,
+                  filt2, dst10, dst10);
+        dst10 = __lsx_vdp2add_h_bu_b(dst10, src8776, filt3);
+
+        __lsx_vstelm_d(dst10, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst10, dst, 0, 1);
+        dst += dst_stride;
+
+        src2110 = src4332;
+        src4332 = src6554;
+        src6554 = src8776;
+        src6 = src8;
+    }
 }
 
 static void hevc_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
@@ -1344,65 +1317,63 @@ static void hevc_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
 {
     int32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
-    int32_t src_stride_3x = (src_stride << 1) + src_stride;
-    int32_t dst_stride_3x = (dst_stride << 1) + dst_stride;
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
     __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
     __m128i dst0_r, dst1_r, dst2_r, dst3_r;
-    __m128i const_vec;
     __m128i filt0, filt1, filt2, filt3;
 
     src -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filt0, filt1, filt2, filt3);
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
     DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
               src10_r, src32_r, src54_r, src21_r);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                  src7, src8, src9, src10);
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  src76_r, src87_r, src98_r, src109_r);
-
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                  dst0_r, dst0_r, dst0_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                  dst1_r, dst1_r, dst1_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                  dst2_r, dst2_r, dst2_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                  dst3_r, dst3_r, dst3_r);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+
+        dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                  src54_r, filt2, dst0_r, dst0_r);
+        dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+        dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                  src65_r, filt2, dst1_r, dst1_r);
+        dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+        dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                  src76_r, filt2, dst2_r, dst2_r);
+        dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+        dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                  src87_r, filt2, dst3_r, dst3_r);
+        dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
 
         __lsx_vst(dst0_r, dst, 0);
-        __lsx_vst(dst1_r, dst + dst_stride, 0);
-        __lsx_vst(dst2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(dst3_r, dst + dst_stride_3x, 0);
-        dst += dst_stride_4x;
+        __lsx_vstx(dst1_r, dst, dst_stride_x);
+        __lsx_vstx(dst2_r, dst, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst, dst_stride_3x);
+        dst += dst_stride_2x;
 
         src10_r = src54_r;
         src32_r = src76_r;
@@ -1420,11 +1391,8 @@ static void hevc_vt_8t_12w_lsx(uint8_t *src, int32_t src_stride,
 {
     int32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
-    int32_t src_stride_3x = (src_stride << 1) + src_stride;
-    int32_t dst_stride_3x = (dst_stride << 1) + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
     __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
@@ -1433,73 +1401,78 @@ static void hevc_vt_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     __m128i src21_l, src43_l, src65_l, src87_l, src109_l;
     __m128i src2110, src4332, src6554, src8776, src10998;
     __m128i dst0_l, dst1_l;
-    __m128i const_vec;
     __m128i filt0, filt1, filt2, filt3;
 
     src -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
     DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
               src10_r, src32_r, src54_r, src21_r);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
     DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
               src10_l, src32_l, src54_l, src21_l);
     DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
-    DUP2_ARG2(__lsx_vilvl_d, src21_l, src10_l, src43_l, src32_l, src2110, src4332);
+    DUP2_ARG2(__lsx_vilvl_d, src21_l, src10_l, src43_l, src32_l,
+              src2110, src4332);
     src6554 = __lsx_vilvl_d(src65_l, src54_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src+ src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10);
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  src76_r, src87_r, src98_r, src109_r);
-        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  src76_l, src87_l, src98_l, src109_l);
-        DUP2_ARG2(__lsx_vilvl_d, src87_l, src76_l, src109_l, src98_l, src8776, src10998);
-
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                  dst0_r, dst0_r, dst0_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                  dst1_r, dst1_r, dst1_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                  dst2_r, dst2_r, dst2_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                  dst3_r, dst3_r, dst3_r);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
-                  filt1, dst0_l, src6554, filt2, dst0_l, src8776, filt3, dst0_l,
-                  dst0_l, dst0_l, dst0_l);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src4332, filt0, dst1_l, src6554,
-                  filt1, dst1_l, src8776, filt2, dst1_l, src10998, filt3, dst1_l,
-                  dst1_l, dst1_l, dst1_l);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_l, src87_l, src98_l, src109_l);
+        DUP2_ARG2(__lsx_vilvl_d, src87_l, src76_l, src109_l, src98_l,
+                  src8776, src10998);
+
+        dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                  src54_r, filt2, dst0_r, dst0_r);
+        dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+        dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                  src65_r, filt2, dst1_r, dst1_r);
+        dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+        dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                  src76_r, filt2, dst2_r, dst2_r);
+        dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+        dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                  src87_r, filt2, dst3_r, dst3_r);
+        dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
+        dst0_l = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_l, src4332, filt1, dst0_l,
+                  src6554, filt2, dst0_l, dst0_l);
+        dst0_l = __lsx_vdp2add_h_bu_b(dst0_l, src8776, filt3);
+        dst1_l = __lsx_vdp2_h_bu_b(src4332, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_l, src6554, filt1, dst1_l,
+                  src8776, filt2, dst1_l, dst1_l);
+        dst1_l = __lsx_vdp2add_h_bu_b(dst1_l, src10998, filt3);
 
         __lsx_vst(dst0_r, dst, 0);
-        __lsx_vst(dst1_r, dst + dst_stride, 0);
-        __lsx_vst(dst2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(dst3_r, dst + dst_stride_3x, 0);
         __lsx_vstelm_d(dst0_l, dst, 16, 0);
-        __lsx_vstelm_d(dst0_l, dst + dst_stride, 16, 1);
-        __lsx_vstelm_d(dst1_l, dst + dst_stride_2x, 16, 0);
-        __lsx_vstelm_d(dst1_l, dst + dst_stride_3x, 16, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vstelm_d(dst0_l, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vst(dst2_r, dst, 0);
+        __lsx_vstelm_d(dst1_l, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst3_r, dst, 0);
+        __lsx_vstelm_d(dst1_l, dst, 16, 1);
+        dst += dst_stride;
 
         src10_r = src54_r;
         src32_r = src76_r;
@@ -1526,11 +1499,8 @@ static void hevc_vt_8t_16multx4mult_lsx(uint8_t *src,
     int16_t *dst_tmp;
     int32_t loop_cnt, cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
-    int32_t src_stride_3x = (src_stride << 1) + src_stride;
-    int32_t dst_stride_3x = (dst_stride << 1) + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
     __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
@@ -1538,29 +1508,26 @@ static void hevc_vt_8t_16multx4mult_lsx(uint8_t *src,
     __m128i src10_l, src32_l, src54_l, src76_l, src98_l;
     __m128i src21_l, src43_l, src65_l, src87_l, src109_l;
     __m128i dst0_l, dst1_l, dst2_l, dst3_l;
-    __m128i const_vec;
     __m128i filt0, filt1, filt2, filt3;
 
     src -= src_stride_3x;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     for (cnt = width >> 4; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
-                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
         src_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
         DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
                   src10_r, src32_r, src54_r, src21_r);
         DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
@@ -1569,51 +1536,61 @@ static void hevc_vt_8t_16multx4mult_lsx(uint8_t *src,
         DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src7, src8, src9, src10);
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride_3x);
             src_tmp += src_stride_4x;
-            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_l, src87_l, src98_l, src109_l);
-
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3,
-                      dst0_r, dst0_r, dst0_r, dst0_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3,
-                      dst1_r, dst1_r, dst1_r, dst1_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                      filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3,
-                      dst2_r, dst2_r, dst2_r, dst2_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                      filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3,
-                      dst3_r, dst3_r, dst3_r, dst3_r);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3,
-                      dst0_l, dst0_l, dst0_l, dst0_l);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3,
-                      dst1_l, dst1_l, dst1_l, dst1_l);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst2_l, src54_l,
-                      filt1, dst2_l, src76_l, filt2, dst2_l, src98_l, filt3,
-                      dst2_l, dst2_l, dst2_l, dst2_l);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst3_l, src65_l,
-                      filt1, dst3_l, src87_l, filt2, dst3_l, src109_l, filt3,
-                      dst3_l, dst3_l, dst3_l, dst3_l);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src76_r, src87_r, src98_r, src109_r);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src76_l, src87_l, src98_l, src109_l);
+
+            dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                      src54_r, filt2, dst0_r, dst0_r);
+            dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+            dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                      src65_r, filt2, dst1_r, dst1_r);
+            dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+            dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                      src76_r, filt2, dst2_r, dst2_r);
+            dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+            dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                      src87_r, filt2, dst3_r, dst3_r);
+            dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
+            dst0_l = __lsx_vdp2_h_bu_b(src10_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_l, src32_l, filt1, dst0_l,
+                      src54_l, filt2, dst0_l, dst0_l);
+            dst0_l = __lsx_vdp2add_h_bu_b(dst0_l, src76_l, filt3);
+            dst1_l = __lsx_vdp2_h_bu_b(src21_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_l, src43_l, filt1, dst1_l,
+                      src65_l, filt2, dst1_l, dst1_l);
+            dst1_l = __lsx_vdp2add_h_bu_b(dst1_l, src87_l, filt3);
+            dst2_l = __lsx_vdp2_h_bu_b(src32_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_l, src54_l, filt1, dst2_l,
+                      src76_l, filt2, dst2_l, dst2_l);
+            dst2_l = __lsx_vdp2add_h_bu_b(dst2_l, src98_l, filt3);
+            dst3_l = __lsx_vdp2_h_bu_b(src43_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_l, src65_l, filt1, dst3_l,
+                      src87_l, filt2, dst3_l, dst3_l);
+            dst3_l = __lsx_vdp2add_h_bu_b(dst3_l, src109_l, filt3);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
-            __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
-            __lsx_vst(dst2_r, dst_tmp + dst_stride_2x, 0);
-            __lsx_vst(dst3_r, dst_tmp + dst_stride_3x, 0);
             __lsx_vst(dst0_l, dst_tmp, 16);
-            __lsx_vst(dst1_l, dst_tmp + dst_stride, 16);
-            __lsx_vst(dst2_l, dst_tmp + dst_stride_2x, 16);
-            __lsx_vst(dst3_l, dst_tmp + dst_stride_3x, 16);
-            dst_tmp += dst_stride_4x;
+            dst_tmp += dst_stride;
+            __lsx_vst(dst1_r, dst_tmp, 0);
+            __lsx_vst(dst1_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+            __lsx_vst(dst2_r, dst_tmp, 0);
+            __lsx_vst(dst2_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+            __lsx_vst(dst3_r, dst_tmp, 0);
+            __lsx_vst(dst3_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
 
             src10_r = src54_r;
             src32_r = src76_r;
@@ -1683,16 +1660,13 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
 {
     uint32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i filt0, filt1, filt2, filt3;
     __m128i filt_h0, filt_h1, filt_h2, filt_h3;
     __m128i mask1, mask2, mask3;
-    __m128i filter_vec, const_vec;
+    __m128i filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     __m128i dst30, dst41, dst52, dst63, dst66, dst97, dst108;
@@ -1713,18 +1687,14 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
               filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
 
     DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask0, src3, src0, mask1, src3, src0,
               mask2, src3, src0, mask3, vec0, vec1, vec2, vec3);
@@ -1734,14 +1704,22 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
               mask2, src5, src2, mask3, vec8, vec9, vec10, vec11);
     DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask0, src6, src3, mask1, src6, src3,
               mask2, src6, src3, mask3, vec12, vec13, vec14, vec15);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
-              vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
-              vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
-              vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
-              vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
+    dst30 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst30, vec1, filt1, dst30, vec2, filt2,
+              dst30, dst30);
+    dst30 = __lsx_vdp2add_h_bu_b(dst30, vec3, filt3);
+    dst41 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst41, vec5, filt1, dst41, vec6, filt2,
+              dst41, dst41);
+    dst41 = __lsx_vdp2add_h_bu_b(dst41, vec7, filt3);
+    dst52 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst52, vec9, filt1, dst52, vec10, filt2,
+              dst52, dst52);
+    dst52 = __lsx_vdp2add_h_bu_b(dst52, vec11, filt3);
+    dst63 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst63, vec13, filt1, dst63, vec14, filt2,
+              dst63, dst63);
+    dst63 = __lsx_vdp2add_h_bu_b(dst63, vec15, filt3);
 
     DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
@@ -1750,43 +1728,52 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
     dst66 = __lsx_vreplvei_d(dst63, 1);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                  src7, src8, src9, src10);
 
         DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask0, src9, src7, mask1, src9, src7,
                   mask2, src9, src7, mask3, vec0, vec1, vec2, vec3);
         DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask0, src10, src8, mask1, src10, src8,
                   mask2, src10, src8, mask3, vec4, vec5, vec6, vec7);
 
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
-                  dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97, dst97);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1,
-                  dst108, vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108, dst108);
+        dst97 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst97, vec1, filt1, dst97, vec2, filt2,
+                  dst97, dst97);
+        dst97 = __lsx_vdp2add_h_bu_b(dst97, vec3, filt3);
+        dst108 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst108, vec5, filt1, dst108, vec6,
+                  filt2, dst108, dst108);
+        dst108 = __lsx_vdp2add_h_bu_b(dst108, vec7, filt3);
 
         DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
         dst109_r = __lsx_vilvh_h(dst108, dst97);
         dst66 = __lsx_vreplvei_d(dst97, 1);
         dst98_r = __lsx_vilvl_h(dst66, dst108);
 
-        dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
-                                        filt_h0, filt_h1, filt_h2, filt_h3);
-        dst1_r = __lsx_hevc_filt_8tap_w(dst21_r, dst43_r, dst65_r, dst87_r,
-                                        filt_h0, filt_h1, filt_h2, filt_h3);
-        dst2_r = __lsx_hevc_filt_8tap_w(dst32_r, dst54_r, dst76_r, dst98_r,
-                                        filt_h0, filt_h1, filt_h2, filt_h3);
-        dst3_r = __lsx_hevc_filt_8tap_w(dst43_r, dst65_r, dst87_r, dst109_r,
-                                        filt_h0, filt_h1, filt_h2, filt_h3);
-        DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst1_r, dst43_r,
+                  filt_h1, dst2_r, dst54_r, filt_h1, dst3_r, dst65_r, filt_h1,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst54_r, filt_h2, dst1_r, dst65_r,
+                  filt_h2, dst2_r, dst76_r, filt_h2, dst3_r, dst87_r, filt_h2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst1_r, dst87_r,
+                  filt_h3, dst2_r, dst98_r, filt_h3, dst3_r, dst109_r, filt_h3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
                   dst0_r, dst1_r, dst2_r, dst3_r);
         DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
-        __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(dst2_r, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(dst2_r, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0_r, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 1);
+        dst += dst_stride;
 
         dst10_r = dst54_r;
         dst32_r = dst76_r;
@@ -1817,7 +1804,7 @@ static void hevc_hv_8t_8multx1mult_lsx(uint8_t *src,
     __m128i filt0, filt1, filt2, filt3;
     __m128i filt_h0, filt_h1, filt_h2, filt_h3;
     __m128i mask1, mask2, mask3;
-    __m128i filter_vec, const_vec;
+    __m128i filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
@@ -1838,75 +1825,91 @@ static void hevc_hv_8t_8multx1mult_lsx(uint8_t *src,
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width >> 3; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
-                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
         src_tmp += src_stride_3x;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
-                  vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
-                  vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1,
-                  dst3, vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        dst1 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec5, filt1, dst1, vec6, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec7, filt3);
+        dst2 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec9, filt1, dst2, vec10, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec11, filt3);
+        dst3 = __lsx_vdp2_h_bu_b(vec12, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec13, filt1, dst3, vec14, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec15, filt3);
 
         /* row 4 row 5 row 6 */
-        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, filt1, vec1, dst4,
-                  vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
-                  vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
-                  vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+                  dst4, dst4);
+        dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
+        dst5 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec5, filt1, dst5, vec6, filt2,
+                  dst5, dst5);
+        dst5 = __lsx_vdp2add_h_bu_b(dst5, vec7, filt3);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec9, filt1, dst6, vec10, filt2,
+                  dst6, dst6);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
 
         for (loop_cnt = height; loop_cnt--;) {
             src7 = __lsx_vld(src_tmp, 0);
-            src7 = __lsx_vxori_b(src7, 128);
             src_tmp += src_stride;
 
             DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
                       src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1,
-                      dst7, vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
-
-            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_r, dst32_r, dst54_r, dst76_r);
-            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_l, dst32_l, dst54_l, dst76_l);
-
-            dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l,
-                                            filt_h0, filt_h1, filt_h2, filt_h3);
-            dst0_r = __lsx_vsrli_w(dst0_r, 6);
-            dst0_l = __lsx_vsrli_w(dst0_l, 6);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_l, dst32_l, dst54_l, dst76_l);
+
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
+            dst0_r = __lsx_vsrai_w(dst0_r, 6);
+            dst0_l = __lsx_vsrai_w(dst0_l, 6);
 
             dst0_r = __lsx_vpickev_h(dst0_l, dst0_r);
             __lsx_vst(dst0_r, dst_tmp, 0);
@@ -1943,17 +1946,14 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     uint8_t *src_tmp;
     int16_t *dst_tmp;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     __m128i filt0, filt1, filt2, filt3, filt_h0, filt_h1, filt_h2, filt_h3;
-    __m128i filter_vec, const_vec;
+    __m128i filter_vec;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     __m128i dst30, dst41, dst52, dst63, dst66, dst97, dst108;
     __m128i dst10_r, dst32_r, dst54_r, dst76_r, dst98_r, dst21_r, dst43_r;
@@ -1973,22 +1973,19 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     src_tmp = src;
     dst_tmp = dst;
 
-    DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-              src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-              src0, src1, src2, src3);
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src_tmp, src_stride_3x);
     src_tmp += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
+    src4 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src5, src6);
     src_tmp += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
 
     /* row 0 row 1 row 2 row 3 */
     DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
@@ -1999,14 +1996,22 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
               mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
     DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
               mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-              vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
-              vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
-              vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1, dst3,
-              vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
+    dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+              dst0, dst0);
+    dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+    dst1 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec5, filt1, dst1, vec6, filt2,
+              dst1, dst1);
+    dst1 = __lsx_vdp2add_h_bu_b(dst1, vec7, filt3);
+    dst2 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec9, filt1, dst2, vec10, filt2,
+              dst2, dst2);
+    dst2 = __lsx_vdp2add_h_bu_b(dst2, vec11, filt3);
+    dst3 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec13, filt1, dst3, vec14, filt2,
+              dst3, dst3);
+    dst3 = __lsx_vdp2add_h_bu_b(dst3, vec15, filt3);
 
     /* row 4 row 5 row 6 */
     DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
@@ -2015,33 +2020,42 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
               mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
     DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
               mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
-              vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
-              vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
-    dst6 = const_vec;
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
-              vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
+    dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+              dst4, dst4);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
+    dst5 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec5, filt1, dst5, vec6, filt2,
+              dst5, dst5);
+    dst5 = __lsx_vdp2add_h_bu_b(dst5, vec7, filt3);
+    dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec9, filt1, dst6, vec10, filt2,
+              dst6, dst6);
+    dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
 
     for (loop_cnt = height; loop_cnt--;) {
         src7 = __lsx_vld(src_tmp, 0);
-        src7 = __lsx_vxori_b(src7, 128);
         src_tmp += src_stride;
 
-        DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7, src7,
-                  mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1, dst7,
-                  vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                  src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+        dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2, filt2,
+                  dst7, dst7);
+        dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
         DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                   dst10_r, dst32_r, dst54_r, dst76_r);
         DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                   dst10_l, dst32_l, dst54_l, dst76_l);
-        dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        dst0_r = __lsx_vsrli_w(dst0_r, 6);
-        dst0_l = __lsx_vsrli_w(dst0_l, 6);
+        DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                  dst0_r, dst0_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst0_r, dst54_r, filt_h2, dst0_l, dst54_l, filt_h2,
+                  dst0_r, dst0_l, dst0_r, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l, dst76_l,
+                  filt_h3, dst0_r, dst0_l)
+        dst0_r = __lsx_vsrai_w(dst0_r, 6);
+        dst0_l = __lsx_vsrai_w(dst0_l, 6);
 
         dst0_r = __lsx_vpickev_h(dst0_l, dst0_r);
         __lsx_vst(dst0_r, dst_tmp, 0);
@@ -2062,16 +2076,13 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask4, 6);
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
     src += src_stride_4x;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-    src6 = __lsx_vld(src + src_stride_2x, 0);
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
     src += src_stride_3x;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3)
-    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-    src6 = __lsx_vxori_b(src6, 128);
 
     DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask4, src3, src0, mask5, src3, src0,
               mask6, src3, src0, mask7, vec0, vec1, vec2, vec3);
@@ -2081,14 +2092,22 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
               mask6, src5, src2, mask7, vec8, vec9, vec10, vec11);
     DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask4, src6, src3, mask5, src6, src3,
               mask6, src6, src3, mask7, vec12, vec13, vec14, vec15);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
-              vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
-              vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
-              vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
-              vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
+    dst30 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst30, vec1, filt1, dst30, vec2, filt2,
+              dst30, dst30);
+    dst30 = __lsx_vdp2add_h_bu_b(dst30, vec3, filt3);
+    dst41 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst41, vec5, filt1, dst41, vec6, filt2,
+              dst41, dst41);
+    dst41 = __lsx_vdp2add_h_bu_b(dst41, vec7, filt3);
+    dst52 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst52, vec9, filt1, dst52, vec10, filt2,
+              dst52, dst52);
+    dst52 = __lsx_vdp2add_h_bu_b(dst52, vec11, filt3);
+    dst63 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst63, vec13, filt1, dst63, vec14, filt2,
+              dst63, dst63);
+    dst63 = __lsx_vdp2add_h_bu_b(dst63, vec15, filt3);
 
     DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
@@ -2098,42 +2117,52 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     dst66 = __lsx_vreplvei_d(dst63, 1);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10);
 
-        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask4, src9, src7, mask5, src9, src7,
-                  mask6, src9, src7, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask4, src9, src7, mask5, src9,
+                  src7, mask6, src9, src7, mask7, vec0, vec1, vec2, vec3);
         DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask4, src10, src8, mask5, src10,
                   src8, mask6, src10, src8, mask7, vec4, vec5, vec6, vec7);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
-                  dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97, dst97);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1, dst108,
-                  vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108, dst108);
+        dst97 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst97, vec1, filt1, dst97, vec2, filt2,
+                  dst97, dst97);
+        dst97 = __lsx_vdp2add_h_bu_b(dst97, vec3, filt3);
+        dst108 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst108, vec5, filt1, dst108, vec6,
+                  filt2, dst108, dst108);
+        dst108 = __lsx_vdp2add_h_bu_b(dst108, vec7, filt3);
 
         DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
         dst109_r = __lsx_vilvh_h(dst108, dst97);
         dst66 = __lsx_vreplvei_d(dst97, 1);
         dst98_r = __lsx_vilvl_h(dst66, dst108);
 
-        dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        dst1_r = __lsx_hevc_filt_8tap_w(dst21_r, dst43_r, dst65_r, dst87_r, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        dst2_r = __lsx_hevc_filt_8tap_w(dst32_r, dst54_r, dst76_r, dst98_r, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        dst3_r = __lsx_hevc_filt_8tap_w(dst43_r, dst65_r, dst87_r, dst109_r, filt_h0,
-                                        filt_h1, filt_h2, filt_h3);
-        DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst1_r, dst43_r,
+                  filt_h1, dst2_r, dst54_r, filt_h1, dst3_r, dst65_r, filt_h1,
                   dst0_r, dst1_r, dst2_r, dst3_r);
-        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst54_r, filt_h2, dst1_r, dst65_r,
+                  filt_h2, dst2_r, dst76_r, filt_h2, dst3_r, dst87_r, filt_h2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst1_r, dst87_r,
+                  filt_h3, dst2_r, dst98_r, filt_h3, dst3_r, dst109_r, filt_h3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r,
+                  dst0_r, dst2_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
-        __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(dst2_r, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(dst2_r, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0_r, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 1);
+        dst += dst_stride;
 
         dst10_r = dst54_r;
         dst32_r = dst76_r;
@@ -2204,11 +2233,9 @@ static void hevc_hz_4t_32w_lsx(uint8_t *src,
     __m128i mask1, mask2, mask3;
     __m128i dst0, dst1, dst2, dst3;
     __m128i vec0, vec1, vec2, vec3;
-    __m128i const_vec;
 
     src -= 1;
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 10);
@@ -2218,18 +2245,18 @@ static void hevc_hz_4t_32w_lsx(uint8_t *src,
         src2 = __lsx_vld(src, 24);
         src += src_stride;
 
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
-
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                  dst3);
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         __lsx_vst(dst2, dst, 32);
@@ -2253,34 +2280,27 @@ static void hevc_vt_4t_16w_lsx(uint8_t *src,
     __m128i src10_l, src32_l, src21_l, src43_l;
     __m128i dst0_r, dst1_r, dst0_l, dst1_l;
     __m128i filt0, filt1;
-    __m128i const_vec;
 
     src -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        src3 = __lsx_vld(src, 0);
+        src4 = __lsx_vldx(src, src_stride);
         src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                  filt1, dst1_l, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
         dst += dst_stride;
@@ -2288,19 +2308,16 @@ static void hevc_vt_4t_16w_lsx(uint8_t *src,
         __lsx_vst(dst1_l, dst, 16);
         dst += dst_stride;
 
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
+        src5 = __lsx_vld(src, 0);
+        src2 = __lsx_vldx(src, src_stride);
         src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                  filt1, dst1_l, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
         dst += dst_stride;
@@ -2320,6 +2337,7 @@ static void hevc_vt_4t_24w_lsx(uint8_t *src,
     int32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
 
     __m128i src0, src1, src2, src3, src4, src5;
     __m128i src6, src7, src8, src9, src10, src11;
@@ -2329,49 +2347,40 @@ static void hevc_vt_4t_24w_lsx(uint8_t *src,
     __m128i src10_l, src32_l, src21_l, src43_l;
     __m128i dst0_l, dst1_l;
     __m128i filt0, filt1;
-    __m128i const_vec;
 
     src -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
+    _src = src + 16;
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
-    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src + src_stride_2x, 16);
-    src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src  += src_stride_3x;
+    _src += src_stride_3x;
     DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
-        src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
         DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                  filt1, dst1_l, dst1_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                  filt1, dst2_r, dst2_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                  filt1, dst3_r, dst3_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst3_r,
+                  src109_r, filt1, dst2_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2382,28 +2391,24 @@ static void hevc_vt_4t_24w_lsx(uint8_t *src,
         __lsx_vst(dst3_r, dst, 32);
         dst += dst_stride;
 
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
-        src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
         DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                  filt1, dst1_l, dst1_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                  filt1, dst2_r, dst2_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
-                  filt1, dst3_r, dst3_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l, src10_l,
+                  filt1, dst1_r, src21_r, filt1, dst1_l, src21_l, filt1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src76_r, filt1, dst3_r, src87_r,
+                  filt1, dst2_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2426,6 +2431,7 @@ static void hevc_vt_4t_32w_lsx(uint8_t *src,
     int32_t loop_cnt;
     int32_t src_stride_2x = (src_stride << 1);
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
 
     __m128i src0, src1, src2, src3, src4, src5;
     __m128i src6, src7, src8, src9, src10, src11;
@@ -2436,55 +2442,44 @@ static void hevc_vt_4t_32w_lsx(uint8_t *src,
     __m128i src21_l, src43_l, src87_l, src109_l;
     __m128i dst0_l, dst1_l, dst2_l, dst3_l;
     __m128i filt0, filt1;
-    __m128i const_vec;
 
     src -= src_stride;
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
+    _src = src + 16;
     DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1)
-    src2 = __lsx_vxori_b(src2, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
     DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
-    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
-    src8 = __lsx_vld(src + src_stride_2x, 16);
-    src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
-    src8 = __lsx_vxori_b(src8, 128);
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src  += src_stride_3x;
+    _src += src_stride_3x;
     DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
     DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
         DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
-        src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
         DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
         DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l,src43_l,
-                  filt1, dst1_l, dst1_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                  filt1, dst2_r, dst2_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
-                  filt1, dst2_l, dst2_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                  filt1, dst3_r, dst3_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
-                  filt1, dst3_l, dst3_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l,src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src76_l, filt0, src87_r,
+                  filt0, src87_l, filt0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst2_l, src98_l,
+                  filt1, dst3_r, src109_r, filt1, dst3_l, src109_l, filt1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2497,33 +2492,27 @@ static void hevc_vt_4t_32w_lsx(uint8_t *src,
         __lsx_vst(dst3_l, dst, 48);
         dst += dst_stride;
 
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
         DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
         DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
-        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
-        src += src_stride_2x;
-        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
         DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
         DUP2_ARG2(__lsx_vilvh_b, src11, src10, src8, src11, src76_l, src87_l);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                  filt1, dst0_r, dst0_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                  filt1, dst0_l, dst0_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                  filt1, dst1_r, dst1_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                  filt1, dst1_l, dst1_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                  filt1, dst2_r, dst2_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_l, filt0, dst2_l, src76_l,
-                  filt1, dst2_l, dst2_l);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
-                  filt1, dst3_r, dst3_r);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_l, filt0, dst3_l, src87_l,
-                  filt1, dst3_l, dst3_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src98_l, filt0, src109_r,
+                  filt0, src109_l, filt0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src76_r, filt1, dst2_l, src76_l,
+                  filt1, dst3_r, src87_r, filt1, dst3_l, src87_l, filt1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2554,7 +2543,7 @@ static void hevc_hv_4t_8x2_lsx(uint8_t *src,
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
     __m128i dst0, dst1, dst2, dst3, dst4;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l;
@@ -2569,14 +2558,11 @@ static void hevc_hv_4t_8x2_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
-    src4 = __lsx_vld(src + src_stride_4x, 0);
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3);
-    src4 = __lsx_vxori_b(src4, 128);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src4 = __lsx_vldx(src, src_stride_4x);
 
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
@@ -2584,28 +2570,25 @@ static void hevc_hv_4t_8x2_lsx(uint8_t *src,
     DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
     DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
 
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-              dst0, dst0);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
-              dst1, dst1);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-              dst2, dst2);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst3, vec7, filt1,
-              dst3, dst3);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
-              dst4, dst4);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b,  vec0, filt0, vec2, filt0, vec4, filt0,
+              vec6, filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst2, vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
 
     DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
     DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
     DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
 
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-              dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
     DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
     __lsx_vst(dst0_r, dst, 0);
     __lsx_vst(dst1_r, dst + dst_stride, 0);
@@ -2618,14 +2601,15 @@ static void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride,
 {
     int32_t cnt;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
 
     __m128i src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, const_vec;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6;
     __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
     __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
@@ -2640,64 +2624,72 @@ static void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width8mult; cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+        src0 = __lsx_vld(src, 0);
+        DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+                  src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
         src += src_stride_4x;
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-        src6 = __lsx_vld(src + src_stride_2x, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
         src += (8 - src_stride_4x);
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3)
-        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
-        src6 = __lsx_vxori_b(src6, 128);
 
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
 
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
-        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1, dst4, dst4);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5, dst5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1, dst6, dst6);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
         DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
         DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
         DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
         DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
-        dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-        dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
 
-        dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-        dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
         DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
                   dst0_r, dst0_l, dst1_r, dst1_l);
         DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
                   dst2_r, dst2_l, dst3_r, dst3_l);
-        DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
-        DUP2_ARG2(__lsx_vpickev_h, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
+                  dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst2_l, dst2_r, dst3_l, dst3_r,
+                  dst2_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
-        __lsx_vst(dst1_r, dst + dst_stride, 0);
-        __lsx_vst(dst2_r, dst + dst_stride_2x, 0);
-        __lsx_vst(dst3_r, dst + dst_stride_3x, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride_x);
+        __lsx_vstx(dst2_r, dst, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst, dst_stride_3x);
         dst += 8;
     }
 }
@@ -2717,8 +2709,7 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     __m128i filt0, filt1;
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i mask1, filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
     __m128i vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
@@ -2737,20 +2728,13 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src0, src1, src2, src3);
-    src4 = __lsx_vld(src + src_stride_4x, 0);
-    src += (src_stride_4x + src_stride);
-    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-              src + src_stride_3x, 0, src5, src6, src7, src8);
 
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-              src1, src2, src3);
-    src4 = __lsx_vxori_b(src4, 128);
-    DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
-              src6, src7, src8);
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+    src += src_stride_4x;
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src5, src6, src7, src8);
 
     DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
               mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
@@ -2760,17 +2744,19 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
               mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
     DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
               mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
-    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
-
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-              const_vec, vec2, filt0, dst1, vec3, filt1, dst0, dst0, dst1, dst1);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-              const_vec, vec6, filt0, dst3, vec7, filt1, dst2, dst2, dst3, dst3);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
-              const_vec, vec10, filt0, dst5, vec11, filt1, dst4, dst4, dst5, dst5);
-    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst6, vec13, filt1,
-              const_vec, vec14, filt0, dst7, vec15, filt1, dst6, dst6, dst7, dst7);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec16, filt0, dst8, vec17, filt1, dst8, dst8);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1,
+              vec16, vec17);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst2, vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b,  vec8, filt0, vec10, filt0, vec12, filt0,
+              vec14, filt0, dst4, dst5, dst6, dst7);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec9, filt1, dst5, vec11, filt1, dst6,
+              vec13, filt1, dst7, vec15, filt1, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2_h_bu_b(vec16, filt0);
+    dst8 = __lsx_vdp2add_h_bu_b(dst8, vec17, filt1);
 
     DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
               dst10_r, dst21_r, dst32_r, dst43_r);
@@ -2781,19 +2767,21 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
               dst54_l, dst65_l, dst76_l, dst87_l);
 
-    dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-    dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-    dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-    dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-    dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-    dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-    dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-    dst4_r = __lsx_hevc_filt_4tap_w(dst54_r, dst76_r, filt_h0, filt_h1);
-    dst4_l = __lsx_hevc_filt_4tap_w(dst54_l, dst76_l, filt_h0, filt_h1);
-    dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
-    dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
-
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
     DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
               dst0_l, dst1_r, dst1_l);
     DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
@@ -2806,13 +2794,13 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, dst4_r, dst5_r);
 
     __lsx_vst(dst0_r, dst, 0);
-    __lsx_vst(dst1_r, dst + dst_stride, 0);
+    __lsx_vstx(dst1_r, dst, dst_stride_2x);
     dst += dst_stride_2x;
     __lsx_vst(dst2_r, dst, 0);
-    __lsx_vst(dst3_r, dst + dst_stride, 0);
+    __lsx_vstx(dst3_r, dst, dst_stride_2x);
     dst += dst_stride_2x;
     __lsx_vst(dst4_r, dst, 0);
-    __lsx_vst(dst5_r, dst + dst_stride, 0);
+    __lsx_vstx(dst5_r, dst, dst_stride_2x);
 }
 
 static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
@@ -2828,18 +2816,17 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
     uint8_t *src_tmp;
     int16_t *dst_tmp;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
 
     __m128i src0, src1, src2, src3, src4, src5, src6;
     __m128i filt0, filt1;
     __m128i filt_h0, filt_h1;
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    __m128i mask1;
-    __m128i filter_vec, const_vec;
+    __m128i mask1, filter_vec;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6;
     __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
@@ -2854,65 +2841,69 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
     DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width8mult; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
-        src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
         src_tmp += src_stride_3x;
 
-        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-        src2 = __lsx_vxori_b(src2, 128);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
 
-        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
         DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
         DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = height >> 2; loop_cnt--;) {
-            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src3, src4, src5, src6);
+            src3 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src4, src5);
+            src6 = __lsx_vldx(src_tmp, src_stride_3x);
             src_tmp += src_stride_4x;
-            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                      src3, src4, src5, src6);
-
-            DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-            DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-            DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-            DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-
-            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1,
-                      dst3, dst3);
-            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1,
-                      dst4, dst4);
-            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1,
-                      dst5, dst5);
-            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1,
-                      dst6, dst6);
+
+            DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                      vec0, vec1);
+            DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                      vec2, vec3);
+            DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                      vec4, vec5);
+            DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                      vec6, vec7);
+
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1, dst3,
+                      dst4, dst5, dst6);
 
             DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
             DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
             DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
             DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
-            dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-            dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-            dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-            dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-            dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-            dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-            dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-            dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
 
             DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
                       dst0_r, dst0_l, dst1_r, dst1_l);
@@ -2923,10 +2914,10 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
                       dst2_r, dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
-            __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
-            __lsx_vst(dst2_r, dst_tmp + dst_stride_2x, 0);
-            __lsx_vst(dst3_r, dst_tmp + dst_stride_3x, 0);
-            dst_tmp += dst_stride_4x;
+            __lsx_vstx(dst1_r, dst_tmp, dst_stride_x);
+            __lsx_vstx(dst2_r, dst_tmp, dst_stride_2x);
+            __lsx_vstx(dst3_r, dst_tmp, dst_stride_3x);
+            dst_tmp += dst_stride_2x;
 
             dst10_r = dst54_r;
             dst10_l = dst54_l;
@@ -2975,17 +2966,17 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
     uint8_t *src_tmp;
     int16_t *dst_tmp;
     int32_t src_stride_2x = (src_stride << 1);
-    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
     int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
     int32_t src_stride_3x = src_stride_2x + src_stride;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
 
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i mask0, mask1, mask2, mask3;
-    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, const_vec;
-    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst10, dst21, dst22, dst73;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, dst0;
+    __m128i dst1, dst2, dst3, dst4, dst5, dst6, dst10, dst21, dst22, dst73;
     __m128i dst84, dst95, dst106, dst76_r, dst98_r, dst87_r, dst109_r;
     __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
     __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
@@ -3001,61 +2992,65 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     src_tmp = src;
     dst_tmp = dst;
 
-    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
     src_tmp += src_stride_3x;
 
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
-
     DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
     DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+    dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst0, dst1);
+    dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
 
     DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
     DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src3,
-                  src4, src5, src6);
+        src3 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src_tmp, src_stride_3x);
         src_tmp += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                  src4, src5, src6);
 
-        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3, dst3);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1, dst4, dst4);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5, dst5);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1, dst6, dst6);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                  filt1, dst5, vec5, filt1, dst6, vec7, filt1, dst3,
+                  dst4, dst5, dst6);
 
         DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
         DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
         DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
         DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
-        dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
-        dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-        dst2_r = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
-        dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
         DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
                   dst0_r, dst0_l, dst1_r, dst1_l);
         DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
@@ -3063,10 +3058,10 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
         DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
                   dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
         __lsx_vst(dst0_r, dst_tmp, 0);
-        __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
-        __lsx_vst(dst2_r, dst_tmp + dst_stride_2x, 0);
-        __lsx_vst(dst3_r, dst_tmp + dst_stride_3x, 0);
-        dst_tmp += dst_stride_4x;
+        __lsx_vstx(dst1_r, dst_tmp, dst_stride_x);
+        __lsx_vstx(dst2_r, dst_tmp, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst_tmp, dst_stride_3x);
+        dst_tmp += dst_stride_2x;
 
         dst10_r = dst54_r;
         dst10_l = dst54_l;
@@ -3081,43 +3076,41 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
     mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
     mask3 = __lsx_vaddi_bu(mask2, 2);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src2 = __lsx_vld(src + src_stride_2x, 0);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
     src += src_stride_3x;
-    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-    src2 = __lsx_vxori_b(src2, 128);
     DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
     DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst10, vec1, filt1, dst10, dst10);
-    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst21, vec3, filt1, dst21, dst21);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst10, dst21);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, vec1, filt1, dst21, vec3, filt1,
+              dst10, dst21);
     dst10_r = __lsx_vilvl_h(dst21, dst10);
     dst21_r = __lsx_vilvh_h(dst21, dst10);
     dst22 = __lsx_vreplvei_d(dst21, 1);
 
     for (loop_cnt = 2; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src3, src4, src5, src6);
+        src3 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src4, src5);
+        src6 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
         src += src_stride_4x;
-        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                  src4, src5, src6);
-        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                  src8, src9, src10);
-        DUP2_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, vec0, vec1);
-        DUP2_ARG3(__lsx_vshuf_b, src8, src4, mask2, src8, src4, mask3, vec2, vec3);
-        DUP2_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, vec4, vec5);
-        DUP2_ARG3(__lsx_vshuf_b, src10, src6, mask2, src10, src6, mask3, vec6, vec7);
-
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst73, vec1, filt1,
-                  dst73, dst73);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst84, vec3, filt1,
-                  dst84, dst84);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst95, vec5, filt1,
-                  dst95, dst95);
-        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst106, vec7, filt1,
-                  dst106, dst106);
+        DUP2_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src8, src4, mask2, src8, src4, mask3,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src10, src6, mask2, src10, src6, mask3,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst73, dst84, dst95, dst106);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst73, vec1, filt1, dst84, vec3,
+                  filt1, dst95, vec5, filt1, dst106, vec7, filt1, dst73,
+                  dst84, dst95, dst106);
 
         DUP2_ARG2(__lsx_vilvl_h, dst73, dst22, dst84, dst73, dst32_r, dst43_r);
         DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
@@ -3126,30 +3119,39 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
         dst22 = __lsx_vreplvei_d(dst73, 1);
         dst76_r = __lsx_vilvl_h(dst22, dst106);
 
-        tmp0 = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
-        tmp1 = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
-        tmp2 = __lsx_hevc_filt_4tap_w(dst32_r, dst54_r, filt_h0, filt_h1);
-        tmp3 = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
-        tmp4 = __lsx_hevc_filt_4tap_w(dst54_r, dst76_r, filt_h0, filt_h1);
-        tmp5 = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
-        tmp6 = __lsx_hevc_filt_4tap_w(dst76_r, dst98_r, filt_h0, filt_h1);
-        tmp7 = __lsx_hevc_filt_4tap_w(dst87_r, dst109_r, filt_h0, filt_h1);
-
-        DUP4_ARG2(__lsx_vsrai_w, tmp0, 6, tmp1, 6, tmp2, 6, tmp3, 6, tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG2(__lsx_vsrai_w, tmp4, 6, tmp5, 6, tmp6, 6, tmp7, 6, tmp4, tmp5, tmp6, tmp7);
-        DUP4_ARG2(__lsx_vpickev_h, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst65_r, filt_h0, dst76_r,
+                  filt_h0, dst87_r, filt_h0, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vdp2add_w_h, tmp0, dst32_r, filt_h1, tmp1, dst43_r,
+                  filt_h1, tmp2, dst54_r, filt_h1, tmp3, dst65_r, filt_h1,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_w_h, tmp4, dst76_r, filt_h1, tmp5, dst87_r,
+                  filt_h1, tmp6, dst98_r, filt_h1, tmp7, dst109_r, filt_h1,
+                  tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsrai_w, tmp0, 6, tmp1, 6, tmp2, 6, tmp3, 6,
                   tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsrai_w, tmp4, 6, tmp5, 6, tmp6, 6, tmp7, 6,
+                  tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vpickev_h, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4,
+                  tmp7, tmp6, tmp0, tmp1, tmp2, tmp3);
 
         __lsx_vstelm_d(tmp0, dst, 0, 0);
-        __lsx_vstelm_d(tmp0, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(tmp1, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(tmp1, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
         __lsx_vstelm_d(tmp2, dst, 0, 0);
-        __lsx_vstelm_d(tmp2, dst + dst_stride, 0, 1);
-        __lsx_vstelm_d(tmp3, dst + dst_stride_2x, 0, 0);
-        __lsx_vstelm_d(tmp3, dst + dst_stride_3x, 0, 1);
-        dst += dst_stride_4x;
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp2, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp3, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp3, dst, 0, 1);
+        dst += dst_stride;
 
         dst10_r = dst98_r;
         dst21_r = dst109_r;
diff --git a/libavcodec/loongarch/hevcdsp_lsx.h b/libavcodec/loongarch/hevcdsp_lsx.h
index 609cfa081b..0c517af887 100644
--- a/libavcodec/loongarch/hevcdsp_lsx.h
+++ b/libavcodec/loongarch/hevcdsp_lsx.h
@@ -1,6 +1,7 @@
 /*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
  * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
  *
  * This file is part of FFmpeg.
  *
@@ -84,6 +85,63 @@ MC(epel, hv, 32);
 
 #undef MC
 
+#define BI_MC(PEL, DIR, WIDTH)                                               \
+void ff_hevc_put_hevc_bi_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                      ptrdiff_t dst_stride,  \
+                                                      uint8_t *src,          \
+                                                      ptrdiff_t src_stride,  \
+                                                      int16_t *src_16bit,    \
+                                                      int height,            \
+                                                      intptr_t mx,           \
+                                                      intptr_t my,           \
+                                                      int width)
+
+BI_MC(pel, pixels, 4);
+BI_MC(pel, pixels, 6);
+BI_MC(pel, pixels, 8);
+BI_MC(pel, pixels, 12);
+BI_MC(pel, pixels, 16);
+BI_MC(pel, pixels, 24);
+BI_MC(pel, pixels, 32);
+BI_MC(pel, pixels, 48);
+BI_MC(pel, pixels, 64);
+
+BI_MC(qpel, h, 16);
+BI_MC(qpel, h, 24);
+BI_MC(qpel, h, 32);
+BI_MC(qpel, h, 48);
+BI_MC(qpel, h, 64);
+
+BI_MC(qpel, v, 8);
+BI_MC(qpel, v, 16);
+BI_MC(qpel, v, 24);
+BI_MC(qpel, v, 32);
+BI_MC(qpel, v, 48);
+BI_MC(qpel, v, 64);
+
+BI_MC(qpel, hv, 8);
+BI_MC(qpel, hv, 16);
+BI_MC(qpel, hv, 24);
+BI_MC(qpel, hv, 32);
+BI_MC(qpel, hv, 48);
+BI_MC(qpel, hv, 64);
+
+BI_MC(epel, h, 24);
+BI_MC(epel, h, 32);
+
+BI_MC(epel, v, 12);
+BI_MC(epel, v, 16);
+BI_MC(epel, v, 24);
+BI_MC(epel, v, 32);
+
+BI_MC(epel, hv, 6);
+BI_MC(epel, hv, 8);
+BI_MC(epel, hv, 16);
+BI_MC(epel, hv, 24);
+BI_MC(epel, hv, 32);
+
+#undef BI_MC
+
 #define UNI_MC(PEL, DIR, WIDTH)                                              \
 void ff_hevc_put_hevc_uni_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,         \
                                                        ptrdiff_t dst_stride, \
@@ -143,82 +201,21 @@ UNI_W_MC(qpel, hv, 64);
 
 #undef UNI_W_MC
 
-#define BI_MC(PEL, DIR, WIDTH)                                               \
-void ff_hevc_put_hevc_bi_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,          \
-                                                      ptrdiff_t dst_stride,  \
-                                                      uint8_t *src,          \
-                                                      ptrdiff_t src_stride,  \
-                                                      int16_t *src_16bit,    \
-                                                      int height,            \
-                                                      intptr_t mx,           \
-                                                      intptr_t my,           \
-                                                      int width)
-
-BI_MC(pel, pixels, 4);
-BI_MC(pel, pixels, 6);
-BI_MC(pel, pixels, 8);
-BI_MC(pel, pixels, 12);
-BI_MC(pel, pixels, 16);
-BI_MC(pel, pixels, 24);
-BI_MC(pel, pixels, 32);
-BI_MC(pel, pixels, 48);
-BI_MC(pel, pixels, 64);
-
-BI_MC(qpel, h, 16);
-BI_MC(qpel, h, 24);
-BI_MC(qpel, h, 32);
-BI_MC(qpel, h, 48);
-BI_MC(qpel, h, 64);
-
-BI_MC(qpel, v, 8);
-BI_MC(qpel, v, 16);
-BI_MC(qpel, v, 24);
-BI_MC(qpel, v, 32);
-BI_MC(qpel, v, 48);
-BI_MC(qpel, v, 64);
-
-BI_MC(qpel, hv, 8);
-BI_MC(qpel, hv, 16);
-BI_MC(qpel, hv, 24);
-BI_MC(qpel, hv, 32);
-BI_MC(qpel, hv, 48);
-BI_MC(qpel, hv, 64);
-
-BI_MC(epel, h, 24);
-BI_MC(epel, h, 32);
-
-BI_MC(epel, v, 12);
-BI_MC(epel, v, 16);
-BI_MC(epel, v, 24);
-BI_MC(epel, v, 32);
-
-BI_MC(epel, hv, 6);
-BI_MC(epel, hv, 8);
-BI_MC(epel, hv, 16);
-BI_MC(epel, hv, 24);
-BI_MC(epel, hv, 32);
-
-#undef BI_MC
-
-void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src,
-                                      ptrdiff_t src_stride,
+void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
                                       int32_t beta, int32_t *tc,
-                                      uint8_t *no_p, uint8_t *no_q);
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm);
 
-void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src,
-                                      ptrdiff_t src_stride,
+void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
                                       int32_t beta, int32_t *tc,
-                                      uint8_t *no_p, uint8_t *no_q);
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm);
 
-void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src,
-                                        ptrdiff_t src_stride,
-                                        int32_t *tc, uint8_t *no_p,
-                                        uint8_t *no_q);
+void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm);
 
-void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src,
-                                        ptrdiff_t src_stride,
-                                        int32_t *tc, uint8_t *no_p,
-                                        uint8_t *no_q);
+void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm);
 
 void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
                                    ptrdiff_t stride_dst,
diff --git a/libavcodec/loongarch/hpeldsp_init_loongarch.c b/libavcodec/loongarch/hpeldsp_init_loongarch.c
index 924ccd0f40..1690be5438 100644
--- a/libavcodec/loongarch/hpeldsp_init_loongarch.c
+++ b/libavcodec/loongarch/hpeldsp_init_loongarch.c
@@ -20,7 +20,7 @@
  */
 
 #include "libavutil/loongarch/cpu.h"
-#include "../hpeldsp.h"
+#include "libavcodec/hpeldsp.h"
 #include "libavcodec/loongarch/hpeldsp_lasx.h"
 
 void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags)
@@ -37,12 +37,6 @@ void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags)
         c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_lasx;
         c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_lasx;
         c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_lasx;
-//
-//        c->put_pixels_tab[2][0] = ff_put_pixels4_8_lasx;
-//        c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_lasx;
-//        c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_lasx;
-//        c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_lasx;
-//
         c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_lsx;
         c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_lasx;
         c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_lasx;
@@ -52,20 +46,5 @@ void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags)
         c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_lasx;
         c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_lasx;
         c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_lasx;
-
-//        c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_lasx;
-//        c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_lasx;
-//        c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_lasx;
-//        c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_lasx;
-//
-//        c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_lasx;
-//        c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_lasx;
-//        c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_lasx;
-//        c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_lasx;
-//
-//        c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_lasx;
-//        c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_lasx;
-//        c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_lasx;
-//        c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_lasx;
     }
 }
diff --git a/libavcodec/loongarch/hpeldsp_lasx.c b/libavcodec/loongarch/hpeldsp_lasx.c
index 256c6e79d8..dd2ae173da 100644
--- a/libavcodec/loongarch/hpeldsp_lasx.c
+++ b/libavcodec/loongarch/hpeldsp_lasx.c
@@ -22,96 +22,108 @@
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hpeldsp_lasx.h"
 
-static inline void
+static av_always_inline void
 put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
                      int dst_stride, int src_stride1, int src_stride2, int h)
 {
+    int stride1_2, stride1_3, stride1_4;
+    int stride2_2, stride2_3, stride2_4;
     __asm__ volatile (
-        "1:                                       \n\t"
-        "vld      $vr0,    %[src1], 0             \n\t"
-        "add.d    %[src1], %[src1], %[srcStride1] \n\t"
-        "vld      $vr1,    %[src1], 0             \n\t"
-        "add.d    %[src1], %[src1], %[srcStride1] \n\t"
-        "vld      $vr2,    %[src1], 0             \n\t"
-        "add.d    %[src1], %[src1], %[srcStride1] \n\t"
-        "vld      $vr3,    %[src1], 0             \n\t"
-        "add.d    %[src1], %[src1], %[srcStride1] \n\t"
-
-        "vld      $vr4,    %[src2], 0             \n\t"
-        "add.d    %[src2], %[src2], %[srcStride2] \n\t"
-        "vld      $vr5,    %[src2], 0             \n\t"
-        "add.d    %[src2], %[src2], %[srcStride2] \n\t"
-        "vld      $vr6,    %[src2], 0             \n\t"
-        "add.d    %[src2], %[src2], %[srcStride2] \n\t"
-        "vld      $vr7,    %[src2], 0             \n\t"
-        "add.d    %[src2], %[src2], %[srcStride2] \n\t"
-
-        "addi.d   %[h],    %[h],    -4            \n\t"
-
-        "vavgr.bu $vr0,    $vr4,    $vr0          \n\t"
-        "vavgr.bu $vr1,    $vr5,    $vr1          \n\t"
-        "vavgr.bu $vr2,    $vr6,    $vr2          \n\t"
-        "vavgr.bu $vr3,    $vr7,    $vr3          \n\t"
-        "vstelm.d $vr0,    %[dst],  0,  0         \n\t"
-        "add.d    %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vstelm.d $vr1,    %[dst],  0,  0         \n\t"
-        "add.d    %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vstelm.d $vr2,    %[dst],  0,  0         \n\t"
-        "add.d    %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vstelm.d $vr3,    %[dst],  0,  0         \n\t"
-        "add.d    %[dst],  %[dst],  %[dstStride]  \n\t"
-        "bnez     %[h],             1b            \n\t"
+        "slli.d   %[stride1_2],  %[srcStride1],     1             \n\t"
+        "slli.d   %[stride2_2],  %[srcStride2],     1             \n\t"
+        "add.d    %[stride1_3],  %[stride1_2],      %[srcStride1] \n\t"
+        "add.d    %[stride2_3],  %[stride2_2],      %[srcStride2] \n\t"
+        "slli.d   %[stride1_4],  %[stride1_2],      1             \n\t"
+        "slli.d   %[stride2_4],  %[stride2_2],      1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src1],           0             \n\t"
+        "vldx     $vr1,          %[src1],           %[srcStride1] \n\t"
+        "vldx     $vr2,          %[src1],           %[stride1_2]  \n\t"
+        "vldx     $vr3,          %[src1],           %[stride1_3]  \n\t"
+        "add.d    %[src1],       %[src1],           %[stride1_4]  \n\t"
+
+        "vld      $vr4,          %[src2],           0             \n\t"
+        "vldx     $vr5,          %[src2],           %[srcStride2] \n\t"
+        "vldx     $vr6,          %[src2],           %[stride2_2]  \n\t"
+        "vldx     $vr7,          %[src2],           %[stride2_3]  \n\t"
+        "add.d    %[src2],       %[src2],           %[stride2_4]  \n\t"
+
+        "addi.d   %[h],          %[h],              -4            \n\t"
+
+        "vavgr.bu $vr0,          $vr4,              $vr0          \n\t"
+        "vavgr.bu $vr1,          $vr5,              $vr1          \n\t"
+        "vavgr.bu $vr2,          $vr6,              $vr2          \n\t"
+        "vavgr.bu $vr3,          $vr7,              $vr3          \n\t"
+        "vstelm.d $vr0,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr1,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr2,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr3,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "bnez     %[h],                             1b            \n\t"
 
         : [dst]"+&r"(dst), [src2]"+&r"(src2), [src1]"+&r"(src1),
-          [h]"+&r"(h)
+          [h]"+&r"(h), [stride1_2]"=&r"(stride1_2),
+          [stride1_3]"=&r"(stride1_3), [stride1_4]"=&r"(stride1_4),
+          [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3),
+          [stride2_4]"=&r"(stride2_4)
         : [dstStride]"r"(dst_stride), [srcStride1]"r"(src_stride1),
           [srcStride2]"r"(src_stride2)
         : "memory"
     );
 }
 
-static inline void
+static av_always_inline void
 put_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
                       int dst_stride, int src_stride1, int src_stride2, int h)
 {
+    int stride1_2, stride1_3, stride1_4;
+    int stride2_2, stride2_3, stride2_4;
+    int dststride2, dststride3, dststride4;
     __asm__ volatile (
-        "1:                                      \n\t"
-        "vld     $vr0,    %[src1], 0             \n\t"
-        "add.d   %[src1], %[src1], %[srcStride1] \n\t"
-        "vld     $vr1,    %[src1], 0             \n\t"
-        "add.d   %[src1], %[src1], %[srcStride1] \n\t"
-        "vld     $vr2,    %[src1], 0             \n\t"
-        "add.d   %[src1], %[src1], %[srcStride1] \n\t"
-        "vld     $vr3,    %[src1], 0             \n\t"
-        "add.d   %[src1], %[src1], %[srcStride1] \n\t"
-
-        "vld     $vr4,    %[src2], 0             \n\t"
-        "add.d   %[src2], %[src2], %[srcStride2] \n\t"
-        "vld     $vr5,    %[src2], 0             \n\t"
-        "add.d   %[src2], %[src2], %[srcStride2] \n\t"
-        "vld     $vr6,    %[src2], 0             \n\t"
-        "add.d   %[src2], %[src2], %[srcStride2] \n\t"
-        "vld     $vr7,    %[src2], 0             \n\t"
-        "add.d   %[src2], %[src2], %[srcStride2] \n\t"
-
-        "addi.d  %[h],    %[h],    -4            \n\t"
-
-        "vavgr.bu $vr0,   $vr4,    $vr0          \n\t"
-        "vavgr.bu $vr1,   $vr5,    $vr1          \n\t"
-        "vavgr.bu $vr2,   $vr6,    $vr2          \n\t"
-        "vavgr.bu $vr3,   $vr7,    $vr3          \n\t"
-        "vst     $vr0,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr1,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr2,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "vst     $vr3,    %[dst],  0             \n\t"
-        "add.d   %[dst],  %[dst],  %[dstStride]  \n\t"
-        "bnez    %[h],             1b            \n\t"
+        "slli.d   %[stride1_2],  %[srcStride1],     1             \n\t"
+        "slli.d   %[stride2_2],  %[srcStride2],     1             \n\t"
+        "slli.d   %[dststride2], %[dstStride],      1             \n\t"
+        "add.d    %[stride1_3],  %[stride1_2],      %[srcStride1] \n\t"
+        "add.d    %[stride2_3],  %[stride2_2],      %[srcStride2] \n\t"
+        "add.d    %[dststride3], %[dststride2],     %[dstStride]  \n\t"
+        "slli.d   %[stride1_4],  %[stride1_2],      1             \n\t"
+        "slli.d   %[stride2_4],  %[stride2_2],      1             \n\t"
+        "slli.d   %[dststride4], %[dststride2],     1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src1],           0             \n\t"
+        "vldx     $vr1,          %[src1],           %[srcStride1] \n\t"
+        "vldx     $vr2,          %[src1],           %[stride1_2]  \n\t"
+        "vldx     $vr3,          %[src1],           %[stride1_3]  \n\t"
+        "add.d    %[src1],       %[src1],           %[stride1_4]  \n\t"
+
+        "vld      $vr4,          %[src2],           0             \n\t"
+        "vldx     $vr5,          %[src2],           %[srcStride2] \n\t"
+        "vldx     $vr6,          %[src2],           %[stride2_2]  \n\t"
+        "vldx     $vr7,          %[src2],           %[stride2_3]  \n\t"
+        "add.d    %[src2],       %[src2],           %[stride2_4]  \n\t"
+
+        "addi.d   %[h],          %[h],              -4            \n\t"
+
+        "vavgr.bu $vr0,          $vr4,              $vr0          \n\t"
+        "vavgr.bu $vr1,          $vr5,              $vr1          \n\t"
+        "vavgr.bu $vr2,          $vr6,              $vr2          \n\t"
+        "vavgr.bu $vr3,          $vr7,              $vr3          \n\t"
+        "vst      $vr0,          %[dst],            0             \n\t"
+        "vstx     $vr1,          %[dst],            %[dstStride]  \n\t"
+        "vstx     $vr2,          %[dst],            %[dststride2] \n\t"
+        "vstx     $vr3,          %[dst],            %[dststride3] \n\t"
+        "add.d    %[dst],        %[dst],            %[dststride4] \n\t"
+        "bnez     %[h],                             1b            \n\t"
 
         : [dst]"+&r"(dst), [src2]"+&r"(src2), [src1]"+&r"(src1),
-          [h]"+&r"(h)
+          [h]"+&r"(h), [stride1_2]"=&r"(stride1_2),
+          [stride1_3]"=&r"(stride1_3), [stride1_4]"=&r"(stride1_4),
+          [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3),
+          [stride2_4]"=&r"(stride2_4), [dststride2]"=&r"(dststride2),
+          [dststride3]"=&r"(dststride3), [dststride4]"=&r"(dststride4)
         : [dstStride]"r"(dst_stride), [srcStride1]"r"(src_stride1),
           [srcStride2]"r"(src_stride2)
         : "memory"
@@ -124,63 +136,57 @@ void ff_put_pixels8_8_lasx(uint8_t *block, const uint8_t *pixels,
     uint64_t tmp[8];
     int h_8 = h >> 3;
     int res = h & 7;
+    ptrdiff_t stride2, stride3, stride4;
 
     __asm__ volatile (
-        "beqz       %[h_8],                2f          \n\t"
-        "1:                                            \n\t"
-        "ld.d       %[tmp0],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp1],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp2],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp3],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp4],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp5],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp6],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "ld.d       %[tmp7],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-
-        "addi.d     %[h_8],     %[h_8],    -1          \n\t"
-
-        "st.d       %[tmp0],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp1],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp2],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp3],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp4],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp5],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp6],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "st.d       %[tmp7],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "bnez       %[h_8],     1b                     \n\t"
-
-        "2:                                            \n\t"
-        "beqz       %[res],     4f                     \n\t"
-        "3:                                            \n\t"
-        "ld.d       %[tmp0],    %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "addi.d     %[res],     %[res],    -1          \n\t"
-        "st.d       %[tmp0],    %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "bnez       %[res],     3b                     \n\t"
-        "4:                                            \n\t"
+        "beqz     %[h_8],                           2f            \n\t"
+        "slli.d   %[stride2],    %[stride],         1             \n\t"
+        "add.d    %[stride3],    %[stride2],        %[stride]     \n\t"
+        "slli.d   %[stride4],    %[stride2],        1             \n\t"
+        "1:                                                       \n\t"
+        "ld.d     %[tmp0],       %[src],            0x0           \n\t"
+        "ldx.d    %[tmp1],       %[src],            %[stride]     \n\t"
+        "ldx.d    %[tmp2],       %[src],            %[stride2]    \n\t"
+        "ldx.d    %[tmp3],       %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+        "ld.d     %[tmp4],       %[src],            0x0           \n\t"
+        "ldx.d    %[tmp5],       %[src],            %[stride]     \n\t"
+        "ldx.d    %[tmp6],       %[src],            %[stride2]    \n\t"
+        "ldx.d    %[tmp7],       %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+
+        "addi.d   %[h_8],        %[h_8],            -1            \n\t"
+
+        "st.d     %[tmp0],       %[dst],            0x0           \n\t"
+        "stx.d    %[tmp1],       %[dst],            %[stride]     \n\t"
+        "stx.d    %[tmp2],       %[dst],            %[stride2]    \n\t"
+        "stx.d    %[tmp3],       %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "st.d     %[tmp4],       %[dst],            0x0           \n\t"
+        "stx.d    %[tmp5],       %[dst],            %[stride]     \n\t"
+        "stx.d    %[tmp6],       %[dst],            %[stride2]    \n\t"
+        "stx.d    %[tmp7],       %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "bnez     %[h_8],        1b                               \n\t"
+
+        "2:                                                       \n\t"
+        "beqz     %[res],        4f                               \n\t"
+        "3:                                                       \n\t"
+        "ld.d     %[tmp0],       %[src],            0x0           \n\t"
+        "add.d    %[src],        %[src],            %[stride]     \n\t"
+        "addi.d   %[res],        %[res],            -1            \n\t"
+        "st.d     %[tmp0],       %[dst],            0x0           \n\t"
+        "add.d    %[dst],        %[dst],            %[stride]     \n\t"
+        "bnez     %[res],        3b                               \n\t"
+        "4:                                                       \n\t"
         : [tmp0]"=&r"(tmp[0]),        [tmp1]"=&r"(tmp[1]),
           [tmp2]"=&r"(tmp[2]),        [tmp3]"=&r"(tmp[3]),
           [tmp4]"=&r"(tmp[4]),        [tmp5]"=&r"(tmp[5]),
           [tmp6]"=&r"(tmp[6]),        [tmp7]"=&r"(tmp[7]),
           [dst]"+&r"(block),          [src]"+&r"(pixels),
-          [h_8]"+&r"(h_8),            [res]"+&r"(res)
+          [h_8]"+&r"(h_8),            [res]"+&r"(res),
+          [stride2]"=&r"(stride2),    [stride3]"=&r"(stride3),
+          [stride4]"=&r"(stride4)
         : [stride]"r"(line_size)
         : "memory"
     );
@@ -191,59 +197,53 @@ void ff_put_pixels16_8_lsx(uint8_t *block, const uint8_t *pixels,
 {
     int h_8 = h >> 3;
     int res = h & 7;
+    ptrdiff_t stride2, stride3, stride4;
 
     __asm__ volatile (
-        "beqz       %[h_8],     2f                     \n\t"
-        "1:                                            \n\t"
-        "vld        $vr0,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr1,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr2,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr3,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr4,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr5,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr6,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "vld        $vr7,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-
-        "addi.d     %[h_8],     %[h_8],    -1          \n\t"
-
-        "vst        $vr0,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr1,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr2,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr3,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr4,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr5,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr6,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "vst        $vr7,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "bnez       %[h_8],     1b                     \n\t"
-
-        "2:                                            \n\t"
-        "beqz       %[res],     4f                     \n\t"
-        "3:                                            \n\t"
-        "vld        $vr0,       %[src],    0x0         \n\t"
-        "add.d      %[src],     %[src],    %[stride]   \n\t"
-        "addi.d     %[res],     %[res],    -1          \n\t"
-        "vst        $vr0,       %[dst],    0x0         \n\t"
-        "add.d      %[dst],     %[dst],    %[stride]   \n\t"
-        "bnez       %[res],     3b                     \n\t"
-        "4:                                            \n\t"
+        "beqz     %[h_8],                           2f            \n\t"
+        "slli.d   %[stride2],    %[stride],         1             \n\t"
+        "add.d    %[stride3],    %[stride2],        %[stride]     \n\t"
+        "slli.d   %[stride4],    %[stride2],        1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src],            0x0           \n\t"
+        "vldx     $vr1,          %[src],            %[stride]     \n\t"
+        "vldx     $vr2,          %[src],            %[stride2]    \n\t"
+        "vldx     $vr3,          %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+        "vld      $vr4,          %[src],            0x0           \n\t"
+        "vldx     $vr5,          %[src],            %[stride]     \n\t"
+        "vldx     $vr6,          %[src],            %[stride2]    \n\t"
+        "vldx     $vr7,          %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+
+        "addi.d   %[h_8],        %[h_8],            -1            \n\t"
+
+        "vst      $vr0,          %[dst],            0x0           \n\t"
+        "vstx     $vr1,          %[dst],            %[stride]     \n\t"
+        "vstx     $vr2,          %[dst],            %[stride2]    \n\t"
+        "vstx     $vr3,          %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "vst      $vr4,          %[dst],            0x0           \n\t"
+        "vstx     $vr5,          %[dst],            %[stride]     \n\t"
+        "vstx     $vr6,          %[dst],            %[stride2]    \n\t"
+        "vstx     $vr7,          %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "bnez     %[h_8],        1b                               \n\t"
+
+        "2:                                                       \n\t"
+        "beqz     %[res],        4f                               \n\t"
+        "3:                                                       \n\t"
+        "vld      $vr0,          %[src],            0x0           \n\t"
+        "add.d    %[src],        %[src],            %[stride]     \n\t"
+        "addi.d   %[res],        %[res],            -1            \n\t"
+        "vst      $vr0,          %[dst],            0x0           \n\t"
+        "add.d    %[dst],        %[dst],            %[stride]     \n\t"
+        "bnez     %[res],        3b                               \n\t"
+        "4:                                                       \n\t"
         : [dst]"+&r"(block),          [src]"+&r"(pixels),
-          [h_8]"+&r"(h_8),            [res]"+&r"(res)
+          [h_8]"+&r"(h_8),            [res]"+&r"(res),
+          [stride2]"=&r"(stride2),    [stride3]"=&r"(stride3),
+          [stride4]"=&r"(stride4)
         : [stride]"r"(line_size)
         : "memory"
     );
@@ -285,15 +285,18 @@ static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x -1);
-    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4, 0x20,
-              src7, src6, 0x20, src0, src1, src2, src3);
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x -1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5,
+              src4, 0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
@@ -309,12 +312,14 @@ static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
     __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x - 1);
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
               0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
@@ -332,12 +337,14 @@ static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
     __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x - 1);
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
               0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
@@ -355,12 +362,13 @@ static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
     __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x - 1);
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
               0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
@@ -386,13 +394,16 @@ static void common_hz_bil_no_rnd_8x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x -1);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
               0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
@@ -410,12 +421,13 @@ static void common_hz_bil_no_rnd_8x16_lasx(const uint8_t *src,
     __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (src_stride_4x - 1);
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
               0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
@@ -452,37 +464,39 @@ static void common_vt_bil_no_rnd_16x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src8, src9, src10, src11);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src12, src13, src14, src15);
-    src += src_stride_4x;
-    src16 = __lasx_xvld(src, 0);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src9, src10);
+    src11 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src13, src14);
+    src15 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src16 = __lasx_xvld(_src, 0);
 
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
               0x20, src4, src3, 0x20, src0, src1, src2, src3);
     DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
               0x20, src8, src7, 0x20, src4, src5, src6, src7);
-    DUP4_ARG3(__lasx_xvpermi_q, src9, src8, 0x20, src10, src9, 0x20, src11, src10,
-              0x20, src12, src11, 0x20, src8, src9, src10, src11);
-    DUP4_ARG3(__lasx_xvpermi_q, src13, src12, 0x20, src14, src13, 0x20, src15, src14,
-              0x20, src16, src15, 0x20, src12, src13, src14, src15);
-    src0  = __lasx_xvavg_bu(src0, src1);
-    src2  = __lasx_xvavg_bu(src2, src3);
-    src4  = __lasx_xvavg_bu(src4, src5);
-    src6  = __lasx_xvavg_bu(src6, src7);
-    src8  = __lasx_xvavg_bu(src8, src9);
-    src10 = __lasx_xvavg_bu(src10, src11);
-    src12 = __lasx_xvavg_bu(src12, src13);
-    src14 = __lasx_xvavg_bu(src14, src15);
+    DUP4_ARG3(__lasx_xvpermi_q, src9, src8, 0x20, src10, src9, 0x20, src11,
+              src10, 0x20, src12, src11, 0x20, src8, src9, src10, src11);
+    DUP4_ARG3(__lasx_xvpermi_q, src13, src12, 0x20, src14, src13, 0x20, src15,
+              src14, 0x20, src16, src15, 0x20, src12, src13, src14, src15);
+    DUP4_ARG2(__lasx_xvavg_bu, src0, src1, src2, src3, src4, src5, src6, src7,
+              src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvavg_bu, src8, src9, src10, src11, src12, src13, src14,
+              src15, src8, src10, src12, src14);
 
     __lasx_xvstelm_d(src0, dst, 0, 0);
     __lasx_xvstelm_d(src0, dst, 8, 1);
@@ -541,23 +555,24 @@ static void common_vt_bil_no_rnd_8x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += src_stride_4x;
-    src8 = __lasx_xvld(src, 0);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
 
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
               0x20, src4, src3, 0x20, src0, src1, src2, src3);
     DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
               0x20, src8, src7, 0x20, src4, src5, src6, src7);
-    src0  = __lasx_xvavg_bu(src0, src1);
-    src2  = __lasx_xvavg_bu(src2, src3);
-    src4  = __lasx_xvavg_bu(src4, src5);
-    src6  = __lasx_xvavg_bu(src6, src7);
+    DUP4_ARG2(__lasx_xvavg_bu, src0, src1, src2, src3, src4, src5, src6, src7,
+              src0, src2, src4, src6);
 
     __lasx_xvstelm_d(src0, dst, 0, 0);
     __lasx_xvstelm_d(src0, dst, 8, 1);
@@ -604,27 +619,34 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (1 - src_stride_4x);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src9, src10, src11, src12);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src13, src14, src15, src16);
-    src += (src_stride_4x - 1);
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
-
-    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
-              src3, src7, 0x02, src0, src1, src2, src3);
-    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
-              src11, src15, 0x02, src4, src5, src6, src7);
-    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
-
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+              src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+              src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02,
+              src8, src9);
     DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
               sum0, sum2, sum4, sum6);
     DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
@@ -642,24 +664,12 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
               sum0, sum1, sum2, sum3);
     DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
               sum4, sum5, sum6, sum7);
-    sum0 = __lasx_xvaddi_hu(sum0, 1);
-    sum1 = __lasx_xvaddi_hu(sum1, 1);
-    sum2 = __lasx_xvaddi_hu(sum2, 1);
-    sum3 = __lasx_xvaddi_hu(sum3, 1);
-    sum4 = __lasx_xvaddi_hu(sum4, 1);
-    sum5 = __lasx_xvaddi_hu(sum5, 1);
-    sum6 = __lasx_xvaddi_hu(sum6, 1);
-    sum7 = __lasx_xvaddi_hu(sum7, 1);
-    sum0 = __lasx_xvsrai_h(sum0, 2);
-    sum1 = __lasx_xvsrai_h(sum1, 2);
-    sum2 = __lasx_xvsrai_h(sum2, 2);
-    sum3 = __lasx_xvsrai_h(sum3, 2);
-    sum4 = __lasx_xvsrai_h(sum4, 2);
-    sum5 = __lasx_xvsrai_h(sum5, 2);
-    sum6 = __lasx_xvsrai_h(sum6, 2);
-    sum7 = __lasx_xvsrai_h(sum7, 2);
-    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
               sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
     __lasx_xvstelm_d(sum0, dst, 0, 0);
     __lasx_xvstelm_d(sum0, dst, 8, 1);
     dst += dst_stride;
@@ -685,19 +695,25 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     __lasx_xvstelm_d(sum3, dst, 8, 3);
     dst += dst_stride;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (1 - src_stride_4x);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src9, src10, src11, src12);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src13, src14, src15, src16);
-    src += (src_stride_4x - 1);
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
 
     DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
               src3, src7, 0x02, src0, src1, src2, src3);
@@ -722,24 +738,12 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
               sum0, sum1, sum2, sum3);
     DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
               sum4, sum5, sum6, sum7);
-    sum0 = __lasx_xvaddi_hu(sum0, 1);
-    sum1 = __lasx_xvaddi_hu(sum1, 1);
-    sum2 = __lasx_xvaddi_hu(sum2, 1);
-    sum3 = __lasx_xvaddi_hu(sum3, 1);
-    sum4 = __lasx_xvaddi_hu(sum4, 1);
-    sum5 = __lasx_xvaddi_hu(sum5, 1);
-    sum6 = __lasx_xvaddi_hu(sum6, 1);
-    sum7 = __lasx_xvaddi_hu(sum7, 1);
-    sum0 = __lasx_xvsrai_h(sum0, 2);
-    sum1 = __lasx_xvsrai_h(sum1, 2);
-    sum2 = __lasx_xvsrai_h(sum2, 2);
-    sum3 = __lasx_xvsrai_h(sum3, 2);
-    sum4 = __lasx_xvsrai_h(sum4, 2);
-    sum5 = __lasx_xvsrai_h(sum5, 2);
-    sum6 = __lasx_xvsrai_h(sum6, 2);
-    sum7 = __lasx_xvsrai_h(sum7, 2);
-    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
               sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
     __lasx_xvstelm_d(sum0, dst, 0, 0);
     __lasx_xvstelm_d(sum0, dst, 8, 1);
     dst += dst_stride;
@@ -763,7 +767,6 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     dst += dst_stride;
     __lasx_xvstelm_d(sum3, dst, 0, 2);
     __lasx_xvstelm_d(sum3, dst, 8, 3);
-    dst += dst_stride;
 }
 
 static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
@@ -776,25 +779,32 @@ static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (1 - src_stride_4x);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src9, src10, src11, src12);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src13, src14, src15, src16);
-    src += (src_stride_4x - 1);
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
-
-    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
-              src3, src7, 0x02, src0, src1, src2, src3);
-    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
-              src11, src15, 0x02, src4, src5, src6, src7);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+              src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+              src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
     DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
 
     DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
@@ -814,24 +824,12 @@ static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
               sum0, sum1, sum2, sum3);
     DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
               sum4, sum5, sum6, sum7);
-    sum0 = __lasx_xvaddi_hu(sum0, 1);
-    sum1 = __lasx_xvaddi_hu(sum1, 1);
-    sum2 = __lasx_xvaddi_hu(sum2, 1);
-    sum3 = __lasx_xvaddi_hu(sum3, 1);
-    sum4 = __lasx_xvaddi_hu(sum4, 1);
-    sum5 = __lasx_xvaddi_hu(sum5, 1);
-    sum6 = __lasx_xvaddi_hu(sum6, 1);
-    sum7 = __lasx_xvaddi_hu(sum7, 1);
-    sum0 = __lasx_xvsrai_h(sum0, 2);
-    sum1 = __lasx_xvsrai_h(sum1, 2);
-    sum2 = __lasx_xvsrai_h(sum2, 2);
-    sum3 = __lasx_xvsrai_h(sum3, 2);
-    sum4 = __lasx_xvsrai_h(sum4, 2);
-    sum5 = __lasx_xvsrai_h(sum5, 2);
-    sum6 = __lasx_xvsrai_h(sum6, 2);
-    sum7 = __lasx_xvsrai_h(sum7, 2);
-    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
               sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
     __lasx_xvstelm_d(sum0, dst, 0, 0);
     __lasx_xvstelm_d(sum0, dst, 8, 1);
     dst += dst_stride;
@@ -855,7 +853,6 @@ static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
     dst += dst_stride;
     __lasx_xvstelm_d(sum3, dst, 0, 2);
     __lasx_xvstelm_d(sum3, dst, 8, 3);
-    dst += dst_stride;
 }
 
 void ff_put_no_rnd_pixels16_xy2_8_lasx(uint8_t *block,
@@ -869,7 +866,8 @@ void ff_put_no_rnd_pixels16_xy2_8_lasx(uint8_t *block,
     }
 }
 
-static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride,
+static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src,
+                                          int32_t src_stride,
                                           uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
@@ -880,21 +878,27 @@ static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t dst_stride_4x = dst_stride << 2;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (1 - src_stride_4x);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src8, src9, src10, src11);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src12, src13, src14, src15);
-
-    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7, src6,
-              src0, src1, src2, src3);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src9, src10);
+    src11 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src13, src14);
+    src15 = __lasx_xvldx(_src, src_stride_3x);
+
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7,
+              src6, src0, src1, src2, src3);
     DUP4_ARG2(__lasx_xvpickev_d, src9, src8, src11, src10, src13, src12, src15,
               src14, src4, src5, src6, src7);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
@@ -912,7 +916,8 @@ static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     __lasx_xvstelm_d(src1, dst + dst_stride_3x, 0, 3);
 }
 
-static void common_hz_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
+static void common_hz_bil_no_rnd_4x8_lasx(const uint8_t *src,
+                                          int32_t src_stride,
                                           uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
@@ -920,12 +925,15 @@ static void common_hz_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t src_stride_3x = src_stride_2x + src_stride;
     int32_t dst_stride_2x = dst_stride << 1;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
     DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7, src6,
               src0, src1, src2, src3);
     DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src1);
@@ -956,14 +964,17 @@ static void common_vt_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t dst_stride_4x = dst_stride << 2;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += src_stride_4x;
-    src8 = __lasx_xvld(src, 0);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
 
     DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
               src0, src1, src2, src3);
@@ -993,11 +1004,11 @@ static void common_vt_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t dst_stride_2x = dst_stride << 1;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
 
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    src4 = __lasx_xvld(src, 0);
+    src0 = __lasx_xvld(_src, 0);
+    DUP4_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, _src,
+              src_stride_3x, _src, src_stride_4x, src1, src2, src3, src4);
     DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
               src0, src1, src2, src3);
     DUP2_ARG3(__lasx_xvpermi_q, src2, src0, 0x20, src3, src1, 0x20, src0, src1);
@@ -1030,20 +1041,27 @@ static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t dst_stride_4x = dst_stride << 2;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src4, src5, src6, src7);
-    src += (1 - src_stride_4x);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src9, src10, src11, src12);
-    src += src_stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src13, src14, src15, src16);
-    src += (src_stride_4x - 1);
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
 
     DUP4_ARG2(__lasx_xvilvl_b, src9, src0, src10, src1, src11, src2, src12, src3,
               src0, src1, src2, src3);
@@ -1054,25 +1072,15 @@ static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
               0x20, src4, src3, 0x20, src0, src1, src2, src3);
     DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
               0x20, src8, src7, 0x20, src4, src5, src6, src7);
-    src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-    src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-    src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-    src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-    src4 = __lasx_xvhaddw_hu_bu(src4, src4);
-    src5 = __lasx_xvhaddw_hu_bu(src5, src5);
-    src6 = __lasx_xvhaddw_hu_bu(src6, src6);
-    src7 = __lasx_xvhaddw_hu_bu(src7, src7);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+              src3, src3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src4, src4, src5, src5, src6, src6,
+              src7, src7, src4, src5, src6, src7);
     DUP4_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, src4, src5, src6, src7,
               sum0, sum1, sum2, sum3);
-    sum0 = __lasx_xvaddi_hu(sum0, 1);
-    sum1 = __lasx_xvaddi_hu(sum1, 1);
-    sum2 = __lasx_xvaddi_hu(sum2, 1);
-    sum3 = __lasx_xvaddi_hu(sum3, 1);
-    sum0 = __lasx_xvsrai_h(sum0, 2);
-    sum1 = __lasx_xvsrai_h(sum1, 2);
-    sum2 = __lasx_xvsrai_h(sum2, 2);
-    sum3 = __lasx_xvsrai_h(sum3, 2);
-    DUP2_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum0, sum1);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
+              sum0, sum1, sum2, sum3);
+    DUP2_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum0, sum1);
     __lasx_xvstelm_d(sum0, dst, 0, 0);
     __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
     __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
@@ -1094,30 +1102,29 @@ static void common_hv_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride
     int32_t dst_stride_2x = dst_stride << 1;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src0, src1, src2, src3);
-    src += 1;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-              0, src + src_stride_3x, 0, src5, src6, src7, src8);
-    src += (src_stride_4x - 1);
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src4, src9);
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src5 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src6, src7);
+    src8 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src4, src9);
 
     DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
               src0, src1, src2, src3);
     src4 = __lasx_xvilvl_b(src9, src4);
     DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
               0x20, src4, src3, 0x20, src0, src1, src2, src3);
-    src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-    src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-    src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-    src3 = __lasx_xvhaddw_hu_bu(src3, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+              src3, src3, src0, src1, src2, src3);
     DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
-    sum0 = __lasx_xvsrai_h(sum0, 2);
-    sum1 = __lasx_xvsrai_h(sum1, 2);
-    sum0 = __lasx_xvpickev_b(sum1, sum0);
+    sum0 = __lasx_xvsrani_b_h(sum1, sum0, 2);
     __lasx_xvstelm_d(sum0, dst, 0, 0);
     __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
     __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
@@ -1145,32 +1152,40 @@ static void common_hv_bil_16w_lasx(const uint8_t *src, int32_t src_stride,
     int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        src += src_stride_4x;
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
-        src += (1 - src_stride_4x);
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src9, src10, src11, src12);
-        src += src_stride_4x;
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src13, src14, src15, src16);
-        src += (src_stride_4x - 1);
-        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
-
-        DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
-                  src3, src7, 0x02, src0, src1, src2, src3);
-        DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14,
-                  0x02, src11, src15, 0x02, src4, src5, src6, src7);
-        DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
-
-        DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
-                  sum0, sum2, sum4, sum6);
-        DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
-                  sum1, sum3, sum5, sum7);
+        src0 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+        src3 = __lasx_xvldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+        src4 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+        src7 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (1 - src_stride_4x);
+        src9 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+                  src10, src11);
+        src12 = __lasx_xvldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+        src13 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+                  src14, src15);
+        src16 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (src_stride_4x - 1);
+        DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+        DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+                  src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+        DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+                  src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
+        DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02,
+                   src8, src9);
+
+        DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8,
+                  src3, sum0, sum2, sum4, sum6);
+        DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8,
+                  src3, sum1, sum3, sum5, sum7);
         src8 = __lasx_xvilvl_h(src9, src4);
         src9 = __lasx_xvilvh_h(src9, src4);
 
@@ -1180,16 +1195,12 @@ static void common_hv_bil_16w_lasx(const uint8_t *src, int32_t src_stride,
                   sum7, sum7, src4, src5, src6, src7);
         DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
 
-        DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
-                  sum0, sum1, sum2, sum3);
-        DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
-                  sum4, sum5, sum6, sum7);
-        DUP4_ARG2(__lasx_xvsrari_h, sum0, 2, sum1, 2, sum2, 2, sum3, 2, sum0, sum1,
-                  sum2, sum3);
-        DUP4_ARG2(__lasx_xvsrari_h, sum4, 2, sum5, 2, sum6, 2, sum7, 2, sum4, sum5,
-                  sum6, sum7);
-        DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
-                  sum0, sum1, sum2, sum3);
+        DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3,
+                  src5, sum0, sum1, sum2, sum3);
+        DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7,
+                  src9, sum4, sum5, sum6, sum7);
+        DUP4_ARG3(__lasx_xvsrarni_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5,
+                  sum4, 2, sum7, sum6, 2, sum0, sum1, sum2, sum3);
         __lasx_xvstelm_d(sum0, dst, 0, 0);
         __lasx_xvstelm_d(sum0, dst, 8, 1);
         dst += dst_stride;
@@ -1236,29 +1247,29 @@ static void common_hv_bil_8w_lasx(const uint8_t *src, int32_t src_stride,
     int32_t dst_stride_4x = dst_stride << 2;
     int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
 
-    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src0, src5);
-    src += src_stride;
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src0, src5);
+    _src += src_stride;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src1, src2, src3, src4);
-        src += 1;
-        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                  0, src + src_stride_3x, 0, src6, src7, src8, src9);
-        src += (src_stride_4x - 1);
+        src1 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src2, src3);
+        src4 = __lasx_xvldx(_src, src_stride_3x);
+        _src += 1;
+        src6 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+        src9 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (src_stride_4x - 1);
         DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
                   src0, src1, src2, src3);
         src5 = __lasx_xvilvl_b(src9, src4);
         DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
                   0x20, src5, src3, 0x20, src0, src1, src2, src3);
-        src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-        src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-        src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-        src3 = __lasx_xvhaddw_hu_bu(src3, src3);
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+                  src3, src3, src0, src1, src2, src3);
         DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
-        DUP2_ARG2(__lasx_xvsrari_h, sum0, 2, sum1, 2, sum0, sum1);
-        sum0 = __lasx_xvpickev_b(sum1, sum0);
+        sum0 = __lasx_xvsrarni_b_h(sum1, sum0, 2);
         __lasx_xvstelm_d(sum0, dst, 0, 0);
         __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
         __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
diff --git a/libavcodec/loongarch/idctdsp_lasx.c b/libavcodec/loongarch/idctdsp_lasx.c
index dfcb9b1a4f..1cfab0e028 100644
--- a/libavcodec/loongarch/idctdsp_lasx.c
+++ b/libavcodec/loongarch/idctdsp_lasx.c
@@ -22,16 +22,18 @@
 #include "idctdsp_loongarch.h"
 #include "libavutil/loongarch/loongson_intrinsics.h"
 
-static void put_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
-                                    int32_t stride)
+void ff_put_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t stride)
 {
     __m256i b0, b1, b2, b3;
     __m256i temp0, temp1;
-    int32_t stride_2x = stride << 1;
-    int32_t stride_4x = stride << 2;
-    int32_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
-    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
     DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
     DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
     __lasx_xvstelm_d(temp0, pixels, 0, 0);
@@ -45,21 +47,22 @@ static void put_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
     __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
 
-static void put_signed_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
-                                           int32_t stride)
+void ff_put_signed_pixels_clamped_lasx(const int16_t *block,
+                                       uint8_t *av_restrict pixels,
+                                       ptrdiff_t stride)
 {
     __m256i b0, b1, b2, b3;
     __m256i temp0, temp1;
-    __m256i const_128 = {0x0080008000800080, 0x0080008000800080, 0x0080008000800080, 0x0080008000800080};
-    int32_t stride_2x = stride << 1;
-    int32_t stride_4x = stride << 2;
-    int32_t stride_3x = stride_2x + stride;
+    __m256i const_128 = {0x0080008000800080, 0x0080008000800080,
+                         0x0080008000800080, 0x0080008000800080};
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
-    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
-    b0 = __lasx_xvadd_h(b0, const_128);
-    b1 = __lasx_xvadd_h(b1, const_128);
-    b2 = __lasx_xvadd_h(b2, const_128);
-    b3 = __lasx_xvadd_h(b3, const_128);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvadd_h, b0, const_128, b1, const_128, b2, const_128,
+              b3, const_128, b0, b1, b2, b3);
     DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
     DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
     __lasx_xvstelm_d(temp0, pixels, 0, 0);
@@ -73,18 +76,20 @@ static void put_signed_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels
     __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
 
-static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
-                                    int32_t stride)
+void ff_add_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t stride)
 {
     __m256i b0, b1, b2, b3;
     __m256i p0, p1, p2, p3, p4, p5, p6, p7;
     __m256i temp0, temp1, temp2, temp3;
     uint8_t *pix = pixels;
-    int32_t stride_2x = stride << 1;
-    int32_t stride_4x = stride << 2;
-    int32_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
-    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
     p0   = __lasx_xvldrepl_d(pix, 0);
     pix += stride;
     p1   = __lasx_xvldrepl_d(pix, 0);
@@ -100,13 +105,12 @@ static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
     p6   = __lasx_xvldrepl_d(pix, 0);
     pix += stride;
     p7   = __lasx_xvldrepl_d(pix, 0);
-    temp0 = __lasx_xvpermi_q(p1, p0, 0x20);
-    temp1 = __lasx_xvpermi_q(p3, p2, 0x20);
-    temp2 = __lasx_xvpermi_q(p5, p4, 0x20);
-    temp3 = __lasx_xvpermi_q(p7, p6, 0x20);
+    DUP4_ARG3(__lasx_xvpermi_q, p1, p0, 0x20, p3, p2, 0x20, p5, p4, 0x20,
+              p7, p6, 0x20, temp0, temp1, temp2, temp3);
     DUP4_ARG2(__lasx_xvaddw_h_h_bu, b0, temp0, b1, temp1, b2, temp2, b3, temp3,
               temp0, temp1, temp2, temp3);
-    DUP4_ARG1(__lasx_xvclip255_h, temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_xvclip255_h, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
     DUP2_ARG2(__lasx_xvpickev_b, temp1, temp0, temp3, temp2, temp0, temp1);
     __lasx_xvstelm_d(temp0, pixels, 0, 0);
     __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
@@ -118,24 +122,3 @@ static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
     __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
     __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
-
-void ff_put_pixels_clamped_lasx(const int16_t *block,
-                                uint8_t *av_restrict pixels,
-                                ptrdiff_t line_size)
-{
-    put_pixels_clamped_lasx(block, pixels, line_size);
-}
-
-void ff_put_signed_pixels_clamped_lasx(const int16_t *block,
-                                       uint8_t *av_restrict pixels,
-                                       ptrdiff_t line_size)
-{
-    put_signed_pixels_clamped_lasx(block, pixels, line_size);
-}
-
-void ff_add_pixels_clamped_lasx(const int16_t *block,
-                                uint8_t *av_restrict pixels,
-                                ptrdiff_t line_size)
-{
-    add_pixels_clamped_lasx(block, pixels, line_size);
-}
diff --git a/libavcodec/loongarch/idctdsp_loongarch.h b/libavcodec/loongarch/idctdsp_loongarch.h
index f36e0f334a..cae8e7af58 100644
--- a/libavcodec/loongarch/idctdsp_loongarch.h
+++ b/libavcodec/loongarch/idctdsp_loongarch.h
@@ -23,8 +23,7 @@
 #define AVCODEC_LOONGARCH_IDCTDSP_LOONGARCH_H
 
 #include <stdint.h>
-#include <stddef.h>
-#include "../mpegvideo.h"
+#include "libavcodec/mpegvideo.h"
 
 void ff_simple_idct_lasx(int16_t *block);
 void ff_simple_idct_put_lasx(uint8_t *dest, ptrdiff_t stride_dst, int16_t *block);
diff --git a/libavcodec/loongarch/simple_idct_lasx.c b/libavcodec/loongarch/simple_idct_lasx.c
index 3caf83d846..a0d936b666 100644
--- a/libavcodec/loongarch/simple_idct_lasx.c
+++ b/libavcodec/loongarch/simple_idct_lasx.c
@@ -22,189 +22,163 @@
 #include "libavutil/loongarch/loongson_intrinsics.h"
 #include "idctdsp_loongarch.h"
 
-#define LASX_TRANSPOSE4x16(in_0, in_1, in_2, in_3, out_0, out_1, out_2, out_3)  \
-{                                                                               \
-    __m256i temp_0, temp_1, temp_2, temp_3;                                     \
-    __m256i temp_4, temp_5, temp_6, temp_7;                                     \
-    temp_0 = __lasx_xvpermi_q(in_2, in_0, 0x20);                                \
-    temp_1 = __lasx_xvpermi_q(in_2, in_0, 0x31);                                \
-    temp_2 = __lasx_xvpermi_q(in_3, in_1, 0x20);                                \
-    temp_3 = __lasx_xvpermi_q(in_3, in_1, 0x31);                                \
-    DUP2_ARG2(__lasx_xvilvl_h, temp_1, temp_0, temp_3, temp_2, temp_4, temp_6); \
-    DUP2_ARG2(__lasx_xvilvh_h, temp_1, temp_0, temp_3, temp_2, temp_5, temp_7); \
-    DUP2_ARG2(__lasx_xvilvl_w, temp_6, temp_4, temp_7, temp_5, out_0, out_2);   \
-    DUP2_ARG2(__lasx_xvilvh_w, temp_6, temp_4, temp_7, temp_5, out_1, out_3);   \
+#define LASX_TRANSPOSE4x16(in_0, in_1, in_2, in_3, out_0, out_1, out_2, out_3) \
+{                                                                              \
+    __m256i temp_0, temp_1, temp_2, temp_3;                                    \
+    __m256i temp_4, temp_5, temp_6, temp_7;                                    \
+    DUP4_ARG3(__lasx_xvpermi_q, in_2, in_0, 0x20, in_2, in_0, 0x31, in_3, in_1,\
+              0x20, in_3, in_1, 0x31, temp_0, temp_1, temp_2, temp_3);         \
+    DUP2_ARG2(__lasx_xvilvl_h, temp_1, temp_0, temp_3, temp_2, temp_4, temp_6);\
+    DUP2_ARG2(__lasx_xvilvh_h, temp_1, temp_0, temp_3, temp_2, temp_5, temp_7);\
+    DUP2_ARG2(__lasx_xvilvl_w, temp_6, temp_4, temp_7, temp_5, out_0, out_2);  \
+    DUP2_ARG2(__lasx_xvilvh_w, temp_6, temp_4, temp_7, temp_5, out_1, out_3);  \
 }
 
-#define LASX_IDCTROWCONDDC                                                      \
-    const_val  = 16383 * ((1 << 19) / 16383);                                   \
-    const_val1 = __lasx_xvinsgr2vr_w(const_val0, const_val, 0);                 \
-    const_val1 = __lasx_xvreplve0_w(const_val1);                                \
-    DUP4_ARG2(__lasx_xvld, block, 0, block + 16, 0, block + 32, 0, block + 48,  \
-              0, in0, in1, in2, in3);                                           \
-    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                 \
-    a0 = __lasx_xvpermi_d(in0, 0xD8);                                           \
-    a0 = __lasx_vext2xv_w_h(a0);                                                \
-    temp  = __lasx_xvslli_w(a0, 3);                                             \
-    a1 = __lasx_xvpermi_d(in0, 0x8D);                                           \
-    a1 = __lasx_vext2xv_w_h(a1);                                                \
-    a2 = __lasx_xvpermi_d(in1, 0xD8);                                           \
-    a2 = __lasx_vext2xv_w_h(a2);                                                \
-    a3 = __lasx_xvpermi_d(in1, 0x8D);                                           \
-    a3 = __lasx_vext2xv_w_h(a3);                                                \
-    b0 = __lasx_xvpermi_d(in2, 0xD8);                                           \
-    b0 = __lasx_vext2xv_w_h(b0);                                                \
-    b1 = __lasx_xvpermi_d(in2, 0x8D);                                           \
-    b1 = __lasx_vext2xv_w_h(b1);                                                \
-    b2 = __lasx_xvpermi_d(in3, 0xD8);                                           \
-    b2 = __lasx_vext2xv_w_h(b2);                                                \
-    b3 = __lasx_xvpermi_d(in3, 0x8D);                                           \
-    b3 = __lasx_vext2xv_w_h(b3);                                                \
-    select_vec = a0 | a1 | a2 | a3 | b0 | b1 | b2 | b3;                         \
-    select_vec = __lasx_xvslti_wu(select_vec, 1);                               \
-                                                                                \
-    w2    = __lasx_xvrepl128vei_h(w1, 2);                                       \
-    w3    = __lasx_xvrepl128vei_h(w1, 3);                                       \
-    w4    = __lasx_xvrepl128vei_h(w1, 4);                                       \
-    w5    = __lasx_xvrepl128vei_h(w1, 5);                                       \
-    w6    = __lasx_xvrepl128vei_h(w1, 6);                                       \
-    w7    = __lasx_xvrepl128vei_h(w1, 7);                                       \
-    w1    = __lasx_xvrepl128vei_h(w1, 1);                                       \
-                                                                                \
-    /* part of FUNC6(idctRowCondDC) */                                          \
-    temp0 = __lasx_xvmaddwl_w_h(const_val0, in0, w4);                           \
-    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);              \
-    a0    = __lasx_xvadd_w(temp0, temp1);                                       \
-    a1    = __lasx_xvadd_w(temp0, temp2);                                       \
-    a2    = __lasx_xvsub_w(temp0, temp2);                                       \
-    a3    = __lasx_xvsub_w(temp0, temp1);                                       \
-                                                                                \
-    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                 \
-    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                        \
-    temp1 = __lasx_xvneg_h(w7);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
-    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-    temp1 = __lasx_xvneg_h(w1);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w5);                                         \
-    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-    temp1 = __lasx_xvneg_h(w5);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w7);                                         \
-    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-                                                                                \
-    /* if (AV_RAN64A(row + 4)) */                                               \
-    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                 \
-    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                 \
-    temp1 = __lasx_xvilvl_h(w2, w4);                                            \
-    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                 \
-    temp1 = __lasx_xvneg_h(w4);                                                 \
-    temp2 = __lasx_xvilvl_h(w2, temp1);                                         \
-    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                 \
-    temp1 = __lasx_xvneg_h(w6);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w4);                                         \
-    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                 \
-                                                                                \
-    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                 \
-    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                 \
-    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                   \
-    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                 \
-    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                 \
-    temp1 = __lasx_xvneg_h(w1);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
-    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                 \
-                                                                                \
-    temp0 = __lasx_xvadd_w(a0, b0);                                             \
-    temp1 = __lasx_xvadd_w(a1, b1);                                             \
-    temp2 = __lasx_xvadd_w(a2, b2);                                             \
-    temp3 = __lasx_xvadd_w(a3, b3);                                             \
-    a0    = __lasx_xvsub_w(a0, b0);                                             \
-    a1    = __lasx_xvsub_w(a1, b1);                                             \
-    a2    = __lasx_xvsub_w(a2, b2);                                             \
-    a3    = __lasx_xvsub_w(a3, b3);                                             \
-    DUP4_ARG2(__lasx_xvsrai_w, temp0, 11, temp1, 11, temp2, 11, temp3, 11,      \
-              temp0, temp1, temp2, temp3);                                      \
-    DUP4_ARG2(__lasx_xvsrai_w, a0, 11, a1, 11, a2, 11, a3, 11, a0, a1, a2, a3); \
-    in0   = __lasx_xvbitsel_v(temp0, temp, select_vec);                         \
-    in1   = __lasx_xvbitsel_v(temp1, temp, select_vec);                         \
-    in2   = __lasx_xvbitsel_v(temp2, temp, select_vec);                         \
-    in3   = __lasx_xvbitsel_v(temp3, temp, select_vec);                         \
-    a0    = __lasx_xvbitsel_v(a0, temp, select_vec);                            \
-    a1    = __lasx_xvbitsel_v(a1, temp, select_vec);                            \
-    a2    = __lasx_xvbitsel_v(a2, temp, select_vec);                            \
-    a3    = __lasx_xvbitsel_v(a3, temp, select_vec);                            \
-    in0   = __lasx_xvpickev_h(in1, in0);                                        \
-    in1   = __lasx_xvpickev_h(in3, in2);                                        \
-    in2   = __lasx_xvpickev_h(a2, a3);                                          \
-    in3   = __lasx_xvpickev_h(a0, a1);                                          \
-    in0   = __lasx_xvpermi_d(in0, 0xD8);                                        \
-    in1   = __lasx_xvpermi_d(in1, 0xD8);                                        \
-    in2   = __lasx_xvpermi_d(in2, 0xD8);                                        \
-    in3   = __lasx_xvpermi_d(in3, 0xD8);                                        \
+#define LASX_IDCTROWCONDDC                                                     \
+    const_val  = 16383 * ((1 << 19) / 16383);                                  \
+    const_val1 = __lasx_xvreplgr2vr_w(const_val);                              \
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,          \
+              in0, in1, in2, in3);                                             \
+    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                \
+    a0 = __lasx_xvpermi_d(in0, 0xD8);                                          \
+    a0 = __lasx_vext2xv_w_h(a0);                                               \
+    temp  = __lasx_xvslli_w(a0, 3);                                            \
+    a1 = __lasx_xvpermi_d(in0, 0x8D);                                          \
+    a1 = __lasx_vext2xv_w_h(a1);                                               \
+    a2 = __lasx_xvpermi_d(in1, 0xD8);                                          \
+    a2 = __lasx_vext2xv_w_h(a2);                                               \
+    a3 = __lasx_xvpermi_d(in1, 0x8D);                                          \
+    a3 = __lasx_vext2xv_w_h(a3);                                               \
+    b0 = __lasx_xvpermi_d(in2, 0xD8);                                          \
+    b0 = __lasx_vext2xv_w_h(b0);                                               \
+    b1 = __lasx_xvpermi_d(in2, 0x8D);                                          \
+    b1 = __lasx_vext2xv_w_h(b1);                                               \
+    b2 = __lasx_xvpermi_d(in3, 0xD8);                                          \
+    b2 = __lasx_vext2xv_w_h(b2);                                               \
+    b3 = __lasx_xvpermi_d(in3, 0x8D);                                          \
+    b3 = __lasx_vext2xv_w_h(b3);                                               \
+    select_vec = a0 | a1 | a2 | a3 | b0 | b1 | b2 | b3;                        \
+    select_vec = __lasx_xvslti_wu(select_vec, 1);                              \
+                                                                               \
+    DUP4_ARG2(__lasx_xvrepl128vei_h, w1, 2, w1, 3, w1, 4, w1, 5,               \
+              w2, w3, w4, w5);                                                 \
+    DUP2_ARG2(__lasx_xvrepl128vei_h, w1, 6, w1, 7, w6, w7);                    \
+    w1 = __lasx_xvrepl128vei_h(w1, 1);                                         \
+                                                                               \
+    /* part of FUNC6(idctRowCondDC) */                                         \
+    temp0 = __lasx_xvmaddwl_w_h(const_val0, in0, w4);                          \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);             \
+    a0    = __lasx_xvadd_w(temp0, temp1);                                      \
+    a1    = __lasx_xvadd_w(temp0, temp2);                                      \
+    a2    = __lasx_xvsub_w(temp0, temp2);                                      \
+    a3    = __lasx_xvsub_w(temp0, temp1);                                      \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                       \
+    temp1 = __lasx_xvneg_h(w7);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                        \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w5);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                        \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+                                                                               \
+    /* if (AV_RAN64A(row + 4)) */                                              \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                           \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                \
+    temp1 = __lasx_xvneg_h(w4);                                                \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                        \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w6);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                        \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                  \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                \
+                                                                               \
+    DUP4_ARG2(__lasx_xvadd_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsub_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              a0, a1, a2, a3);                                                 \
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 11, temp1, 11, temp2, 11, temp3, 11,     \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsrai_w, a0, 11, a1, 11, a2, 11, a3, 11, a0, a1, a2, a3);\
+    DUP4_ARG3(__lasx_xvbitsel_v, temp0, temp, select_vec, temp1, temp,         \
+              select_vec, temp2, temp, select_vec, temp3, temp, select_vec,    \
+              in0, in1, in2, in3);                                             \
+    DUP4_ARG3(__lasx_xvbitsel_v, a0, temp, select_vec, a1, temp,               \
+              select_vec, a2, temp, select_vec, a3, temp, select_vec,          \
+              a0, a1, a2, a3);                                                 \
+    DUP4_ARG2(__lasx_xvpickev_h, in1, in0, in3, in2, a2, a3, a0, a1,           \
+              in0, in1, in2, in3);                                             \
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,    \
+              in0, in1, in2, in3);                                             \
 
+#define LASX_IDCTCOLS                                                          \
+    /* part of FUNC6(idctSparaseCol) */                                        \
+    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                \
+    temp0 = __lasx_xvmaddwl_w_h(const_val1, in0, w4);                          \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);             \
+    a0    = __lasx_xvadd_w(temp0, temp1);                                      \
+    a1    = __lasx_xvadd_w(temp0, temp2);                                      \
+    a2    = __lasx_xvsub_w(temp0, temp2);                                      \
+    a3    = __lasx_xvsub_w(temp0, temp1);                                      \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                       \
+    temp1 = __lasx_xvneg_h(w7);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                        \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w5);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                        \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+                                                                               \
+    /* if (AV_RAN64A(row + 4)) */                                              \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                           \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                \
+    temp1 = __lasx_xvneg_h(w4);                                                \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                        \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w6);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                        \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                  \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                \
+                                                                               \
+    DUP4_ARG2(__lasx_xvadd_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsub_w, a3, b3, a2, b2, a1, b1, a0, b0,                  \
+              a3, a2, a1, a0);                                                 \
+    DUP4_ARG3(__lasx_xvsrani_h_w, temp1, temp0, 20, temp3, temp2, 20, a2, a3,  \
+              20, a0, a1, 20, in0, in1, in2, in3);                             \
 
-#define LASX_IDCTCOLS                                                           \
-    /* part of FUNC6(idctSparaseCol) */                                         \
-    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                 \
-    temp0 = __lasx_xvmaddwl_w_h(const_val1, in0, w4);                           \
-    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);              \
-    a0    = __lasx_xvadd_w(temp0, temp1);                                       \
-    a1    = __lasx_xvadd_w(temp0, temp2);                                       \
-    a2    = __lasx_xvsub_w(temp0, temp2);                                       \
-    a3    = __lasx_xvsub_w(temp0, temp1);                                       \
-                                                                                \
-    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                 \
-    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                        \
-    temp1 = __lasx_xvneg_h(w7);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
-    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-    temp1 = __lasx_xvneg_h(w1);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w5);                                         \
-    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-    temp1 = __lasx_xvneg_h(w5);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w7);                                         \
-    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
-                                                                                \
-    /* if (AV_RAN64A(row + 4)) */                                               \
-    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                 \
-    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                 \
-    temp1 = __lasx_xvilvl_h(w2, w4);                                            \
-    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                 \
-    temp1 = __lasx_xvneg_h(w4);                                                 \
-    temp2 = __lasx_xvilvl_h(w2, temp1);                                         \
-    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                 \
-    temp1 = __lasx_xvneg_h(w6);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w4);                                         \
-    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                 \
-                                                                                \
-    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                 \
-    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                 \
-    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                   \
-    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                 \
-    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                 \
-    temp1 = __lasx_xvneg_h(w1);                                                 \
-    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
-    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                 \
-                                                                                \
-    temp0 = __lasx_xvadd_w(a0, b0);                                             \
-    temp1 = __lasx_xvadd_w(a1, b1);                                             \
-    temp2 = __lasx_xvadd_w(a2, b2);                                             \
-    temp3 = __lasx_xvadd_w(a3, b3);                                             \
-    a3    = __lasx_xvsub_w(a3, b3);                                             \
-    a2    = __lasx_xvsub_w(a2, b2);                                             \
-    a1    = __lasx_xvsub_w(a1, b1);                                             \
-    a0    = __lasx_xvsub_w(a0, b0);                                             \
-    DUP4_ARG2(__lasx_xvsrai_w, temp0, 20, temp1, 20, temp2, 20, temp3, 20,      \
-              temp0, temp1, temp2, temp3);                                      \
-    DUP4_ARG2(__lasx_xvsrai_w, a0, 20, a1, 20, a2, 20, a3, 20, a0, a1, a2, a3); \
-    in0   = __lasx_xvpickev_h(temp1, temp0);                                    \
-    in1   = __lasx_xvpickev_h(temp3, temp2);                                    \
-    in2   = __lasx_xvpickev_h(a2, a3);                                          \
-    in3   = __lasx_xvpickev_h(a0, a1);                                          \
-
-
-static void simple_idct_lasx(int16_t *block)
+void ff_simple_idct_lasx(int16_t *block)
 {
     int32_t const_val = 1 << 10;
-    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF, 0x4B42539F58C50000, 0x11A822A332493FFF};
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
     __m256i in0, in1, in2, in3;
     __m256i w2, w3, w4, w5, w6, w7;
     __m256i a0, a1, a2, a3;
@@ -215,24 +189,23 @@ static void simple_idct_lasx(int16_t *block)
 
     LASX_IDCTROWCONDDC
     LASX_IDCTCOLS
-    in0   = __lasx_xvpermi_d(in0, 0xD8);
-    in1   = __lasx_xvpermi_d(in1, 0xD8);
-    in2   = __lasx_xvpermi_d(in2, 0xD8);
-    in3   = __lasx_xvpermi_d(in3, 0xD8);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
     __lasx_xvst(in0, block, 0);
     __lasx_xvst(in1, block, 32);
     __lasx_xvst(in2, block, 64);
     __lasx_xvst(in3, block, 96);
 }
 
-static void simple_idct_put_lasx(uint8_t *dst, int32_t dst_stride,
-                                 int16_t *block)
+void ff_simple_idct_put_lasx(uint8_t *dst, ptrdiff_t dst_stride,
+                             int16_t *block)
 {
     int32_t const_val = 1 << 10;
-    int32_t dst_stride_2x = dst_stride << 1;
-    int32_t dst_stride_4x = dst_stride << 2;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
-    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF, 0x4B42539F58C50000, 0x11A822A332493FFF};
+    ptrdiff_t dst_stride_2x = dst_stride << 1;
+    ptrdiff_t dst_stride_4x = dst_stride << 2;
+    ptrdiff_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
     __m256i in0, in1, in2, in3;
     __m256i w2, w3, w4, w5, w6, w7;
     __m256i a0, a1, a2, a3;
@@ -243,10 +216,8 @@ static void simple_idct_put_lasx(uint8_t *dst, int32_t dst_stride,
 
     LASX_IDCTROWCONDDC
     LASX_IDCTCOLS
-    in0   = __lasx_xvpermi_d(in0, 0xD8);
-    in1   = __lasx_xvpermi_d(in1, 0xD8);
-    in2   = __lasx_xvpermi_d(in2, 0xD8);
-    in3   = __lasx_xvpermi_d(in3, 0xD8);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
     DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
     DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
     __lasx_xvstelm_d(in0, dst, 0, 0);
@@ -260,17 +231,19 @@ static void simple_idct_put_lasx(uint8_t *dst, int32_t dst_stride,
     __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
 }
 
-static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
-                                 int16_t *block)
+void ff_simple_idct_add_lasx(uint8_t *dst, ptrdiff_t dst_stride,
+                             int16_t *block)
 {
     int32_t const_val = 1 << 10;
     uint8_t *dst1 = dst;
-    int32_t dst_stride_2x = dst_stride << 1;
-    int32_t dst_stride_4x = dst_stride << 2;
-    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    ptrdiff_t dst_stride_2x = dst_stride << 1;
+    ptrdiff_t dst_stride_4x = dst_stride << 2;
+    ptrdiff_t dst_stride_3x = dst_stride_2x + dst_stride;
 
-    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF, 0x4B42539F58C50000, 0x11A822A332493FFF};
-    __m256i sh = {0x0003000200010000, 0x000B000A00090008, 0x0007000600050004, 0x000F000E000D000C};
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
+    __m256i sh = {0x0003000200010000, 0x000B000A00090008,
+                  0x0007000600050004, 0x000F000E000D000C};
     __m256i in0, in1, in2, in3;
     __m256i w2, w3, w4, w5, w6, w7;
     __m256i a0, a1, a2, a3;
@@ -304,18 +277,12 @@ static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
     dst1 += dst_stride;
     b3    = __lasx_xvldrepl_d(dst1, 0);
     b3    = __lasx_vext2xv_hu_bu(b3);
-    temp0 = __lasx_xvshuf_h(sh, a1, a0);
-    temp1 = __lasx_xvshuf_h(sh, a3, a2);
-    temp2 = __lasx_xvshuf_h(sh, b1, b0);
-    temp3 = __lasx_xvshuf_h(sh, b3, b2);
-    in0   = __lasx_xvadd_h(temp0, in0);
-    in1   = __lasx_xvadd_h(temp1, in1);
-    in2   = __lasx_xvadd_h(temp2, in2);
-    in3   = __lasx_xvadd_h(temp3, in3);
-    in0   = __lasx_xvpermi_d(in0, 0xD8);
-    in1   = __lasx_xvpermi_d(in1, 0xD8);
-    in2   = __lasx_xvpermi_d(in2, 0xD8);
-    in3   = __lasx_xvpermi_d(in3, 0xD8);
+    DUP4_ARG3(__lasx_xvshuf_h, sh, a1, a0, sh, a3, a2, sh, b1, b0, sh, b3, b2,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvadd_h, temp0, in0, temp1, in1, temp2, in2, temp3, in3,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
     DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
     DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
     __lasx_xvstelm_d(in0, dst, 0, 0);
@@ -328,18 +295,3 @@ static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
     __lasx_xvstelm_d(in1, dst + dst_stride_2x, 0, 1);
     __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
 }
-
-void ff_simple_idct_lasx(int16_t *block)
-{
-    simple_idct_lasx(block);
-}
-
-void ff_simple_idct_put_lasx(uint8_t *dst, ptrdiff_t dst_stride, int16_t *block)
-{
-    simple_idct_put_lasx(dst, dst_stride, block);
-}
-
-void ff_simple_idct_add_lasx(uint8_t *dst, ptrdiff_t dst_stride, int16_t *block)
-{
-    simple_idct_add_lasx(dst, dst_stride, block);
-}
diff --git a/libavcodec/loongarch/vc1dsp_lasx.c b/libavcodec/loongarch/vc1dsp_lasx.c
index 4f4088fa1d..40b8668f2b 100644
--- a/libavcodec/loongarch/vc1dsp_lasx.c
+++ b/libavcodec/loongarch/vc1dsp_lasx.c
@@ -28,28 +28,40 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     int32_t con_64   = 64;
     __m256i in0, in1, in2, in3;
     __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4, t5, t6, t7, t8;
-    __m256i const_1  = {0x000c000c000c000c, 0x000c000c000c000c, 0x000c000c000c000c, 0x000c000c000c000c};
-    __m256i const_2  = {0xfff4000cfff4000c, 0xfff4000cfff4000c, 0xfff4000cfff4000c, 0xfff4000cfff4000c};
-    __m256i const_3  = {0x0006001000060010, 0x0006001000060010, 0x0006001000060010, 0x0006001000060010};
-    __m256i const_4  = {0xfff00006fff00006, 0xfff00006fff00006, 0xfff00006fff00006, 0xfff00006fff00006};
-    __m256i const_5  = {0x000f0010000f0010, 0x000f0010000f0010, 0x000f0010000f0010, 0x000f0010000f0010};
-    __m256i const_6  = {0x0004000900040009, 0x0004000900040009, 0x0004000900040009, 0x0004000900040009};
-    __m256i const_7  = {0xfffc000ffffc000f, 0xfffc000ffffc000f, 0xfffc000ffffc000f, 0xfffc000ffffc000f};
-    __m256i const_8  = {0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0};
-    __m256i const_9  = {0xfff00009fff00009, 0xfff00009fff00009, 0xfff00009fff00009, 0xfff00009fff00009};
-    __m256i const_10 = {0x000f0004000f0004, 0x000f0004000f0004, 0x000f0004000f0004, 0x000f0004000f0004};
-    __m256i const_11 = {0xfff70004fff70004, 0xfff70004fff70004, 0xfff70004fff70004, 0xfff70004fff70004};
-    __m256i const_12 = {0xfff0000ffff0000f, 0xfff0000ffff0000f, 0xfff0000ffff0000f, 0xfff0000ffff0000f};
-
-    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, in0, in1, in2, in3);
-    in0 = __lasx_xvpermi_d(in0, 0xD8);
-    in1 = __lasx_xvpermi_d(in1, 0xD8);
-    in2 = __lasx_xvpermi_d(in2, 0xD8);
-    in3 = __lasx_xvpermi_d(in3, 0xD8);
+    __m256i const_1  = {0x000c000c000c000c, 0x000c000c000c000c,
+                        0x000c000c000c000c, 0x000c000c000c000c};
+    __m256i const_2  = {0xfff4000cfff4000c, 0xfff4000cfff4000c,
+                        0xfff4000cfff4000c, 0xfff4000cfff4000c};
+    __m256i const_3  = {0x0006001000060010, 0x0006001000060010,
+                        0x0006001000060010, 0x0006001000060010};
+    __m256i const_4  = {0xfff00006fff00006, 0xfff00006fff00006,
+                        0xfff00006fff00006, 0xfff00006fff00006};
+    __m256i const_5  = {0x000f0010000f0010, 0x000f0010000f0010,
+                        0x000f0010000f0010, 0x000f0010000f0010};
+    __m256i const_6  = {0x0004000900040009, 0x0004000900040009,
+                        0x0004000900040009, 0x0004000900040009};
+    __m256i const_7  = {0xfffc000ffffc000f, 0xfffc000ffffc000f,
+                        0xfffc000ffffc000f, 0xfffc000ffffc000f};
+    __m256i const_8  = {0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0,
+                        0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0};
+    __m256i const_9  = {0xfff00009fff00009, 0xfff00009fff00009,
+                        0xfff00009fff00009, 0xfff00009fff00009};
+    __m256i const_10 = {0x000f0004000f0004, 0x000f0004000f0004,
+                        0x000f0004000f0004, 0x000f0004000f0004};
+    __m256i const_11 = {0xfff70004fff70004, 0xfff70004fff70004,
+                        0xfff70004fff70004, 0xfff70004fff70004};
+    __m256i const_12 = {0xfff0000ffff0000f, 0xfff0000ffff0000f,
+                        0xfff0000ffff0000f, 0xfff0000ffff0000f};
+
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
     /* first loops */
     DUP2_ARG2(__lasx_xvilvl_h, in2, in0, in3, in1, temp0, temp1);
     t2 = __lasx_xvreplgr2vr_w(con_4);
-    DUP2_ARG3(__lasx_xvdp2add_w_h, t2, temp0, const_1, t2, temp0, const_2, t1, t2);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t2, temp0, const_1, t2, temp0,
+              const_2, t1, t2);
     DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
 
     t5 = __lasx_xvadd_w(t1, t3);
@@ -67,32 +79,26 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     temp2 = __lasx_xvdp2_w_h(const_11, temp0);
     t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
 
-    temp0 = __lasx_xvadd_w(t1, t5);
-    temp1 = __lasx_xvadd_w(t6, t2);
-    temp2 = __lasx_xvadd_w(t7, t3);
-    temp3 = __lasx_xvadd_w(t8, t4);
-    in0   = __lasx_xvsub_w(t8, t4);
-    in1   = __lasx_xvsub_w(t7, t3);
-    in2   = __lasx_xvsub_w(t6, t2);
-    in3   = __lasx_xvsub_w(t5, t1);
+    DUP4_ARG2(__lasx_xvadd_w, t1, t5, t6, t2, t7, t3, t8, t4,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsub_w, t8, t4, t7, t3, t6, t2, t5, t1,
+              in0, in1, in2, in3);
     DUP4_ARG2(__lasx_xvsrai_w, temp0, 3, temp1, 3, temp2, 3, temp3, 3,
               temp0, temp1, temp2, temp3);
-    DUP4_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in2, 3, in3, 3, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in2, 3, in3, 3,
+              in0, in1, in2, in3);
 
     /* second loops */
-    temp0 = __lasx_xvpackev_h(temp1, temp0);
-    temp1 = __lasx_xvpackev_h(temp3, temp2);
-    temp2 = __lasx_xvpackev_h(in1, in0);
-    temp3 = __lasx_xvpackev_h(in3, in2);
+    DUP4_ARG2(__lasx_xvpackev_h, temp1, temp0, temp3, temp2, in1, in0,
+              in3, in2, temp0, temp1, temp2, temp3);
     DUP2_ARG2(__lasx_xvilvl_w, temp1, temp0, temp3, temp2, t1, t3);
     DUP2_ARG2(__lasx_xvilvh_w, temp1, temp0, temp3, temp2, t2, t4);
-    in0   = __lasx_xvpermi_q(t3, t1, 0x20);
-    in1   = __lasx_xvpermi_q(t3, t1, 0x31);
-    in2   = __lasx_xvpermi_q(t4, t2, 0x20);
-    in3   = __lasx_xvpermi_q(t4, t2, 0x31);
+    DUP4_ARG3(__lasx_xvpermi_q, t3, t1, 0x20, t3, t1, 0x31, t4, t2, 0x20,
+              t4, t2, 0x31, in0, in1, in2, in3);
     DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in3, in2, temp0, temp1);
     t3    = __lasx_xvreplgr2vr_w(con_64);
-    DUP2_ARG3(__lasx_xvdp2add_w_h, t3, temp0, const_1, t3, temp0, const_2, t1, t2);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t3, temp0, const_1, t3, temp0,
+              const_2, t1, t2);
     DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
 
     t5    = __lasx_xvadd_w(t1, t3);
@@ -110,108 +116,91 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     temp2 = __lasx_xvdp2_w_h(const_11, temp0);
     t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
 
-    temp0 = __lasx_xvadd_w(t5, t1);
-    temp1 = __lasx_xvadd_w(t6, t2);
-    temp2 = __lasx_xvadd_w(t7, t3);
-    temp3 = __lasx_xvadd_w(t8, t4);
-    in0   = __lasx_xvsub_w(t8, t4);
-    in1   = __lasx_xvsub_w(t7, t3);
-    in2   = __lasx_xvsub_w(t6, t2);
-    in3   = __lasx_xvsub_w(t5, t1);
-    in0   = __lasx_xvaddi_wu(in0, 1);
-    in1   = __lasx_xvaddi_wu(in1, 1);
-    in2   = __lasx_xvaddi_wu(in2, 1);
-    in3   = __lasx_xvaddi_wu(in3, 1);
-    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+    DUP4_ARG2(__lasx_xvadd_w, t5, t1, t6, t2, t7, t3, t8, t4,
               temp0, temp1, temp2, temp3);
-    DUP4_ARG2(__lasx_xvsrai_w, in0, 7, in1, 7, in2, 7, in3, 7, in0, in1, in2, in3);
-    t1 = __lasx_xvpickev_h(temp1, temp0);
-    t2 = __lasx_xvpickev_h(temp3, temp2);
-    t3 = __lasx_xvpickev_h(in1, in0);
-    t4 = __lasx_xvpickev_h(in3, in2);
-    in0 = __lasx_xvpermi_d(t1, 0xD8);
-    in1 = __lasx_xvpermi_d(t2, 0xD8);
-    in2 = __lasx_xvpermi_d(t3, 0xD8);
-    in3 = __lasx_xvpermi_d(t4, 0xD8);
+    DUP4_ARG2(__lasx_xvsub_w, t8, t4, t7, t3, t6, t2, t5, t1,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvaddi_wu, in0, 1, in1, 1, in2, 1, in3, 1,
+              in0, in1, in2, in3);
+    DUP4_ARG3(__lasx_xvsrani_h_w, temp1, temp0, 7, temp3, temp2, 7,
+              in1, in0, 7, in3, in2, 7, t1, t2, t3, t4);
+    DUP4_ARG2(__lasx_xvpermi_d, t1, 0xD8, t2, 0xD8, t3, 0xD8, t4, 0xD8,
+              in0, in1, in2, in3);
     __lasx_xvst(in0, block, 0);
     __lasx_xvst(in1, block, 32);
     __lasx_xvst(in2, block, 64);
     __lasx_xvst(in3, block, 96);
 }
 
-void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
 {
     int dc = block[0];
-    uint8_t *dst   = dest + stride;
-    __m256i in0, in1, const_dc, temp0;
-    __m256i zero   = {0};
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i const_dc, temp0, temp1, temp2, temp3;
+    __m256i reg0, reg1, reg2, reg3;
 
     dc = (3 * dc +  1) >> 1;
     dc = (3 * dc + 16) >> 5;
 
     const_dc = __lasx_xvreplgr2vr_h(dc);
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
-    __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
-    dest = dst + stride;
-    dst  = dest + stride;
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
-    __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
-    dest = dst + stride;
-    dst  = dest + stride;
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
-    __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
-    dest = dst + stride;
-    dst  = dest + stride;
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvldrepl_d, dst, 0, dst + stride, 0, dst + stride2,
+              0, dst + stride3, 0, in4, in5, in6, in7);
+
+    DUP4_ARG2(__lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+
+    DUP4_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, temp2,
+              const_dc, temp3, const_dc, reg0, reg1, reg2, reg3);
+    DUP2_ARG3(__lasx_xvssrarni_bu_h, reg1, reg0, 0, reg3, reg2, 0,
+              temp0, temp1);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
+    __lasx_xvstelm_d(temp1, dst, 0, 0);
+    __lasx_xvstelm_d(temp1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, dst + stride2, 0, 1);
+    __lasx_xvstelm_d(temp1, dst + stride3, 0, 3);
 }
 
 void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
-    uint8_t *dst = dest;
-    ptrdiff_t stride_2x = stride << 1;
-    ptrdiff_t stride_3x = stride_2x + stride;
-    __m256i shift    = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
-    __m256i const_64 = {0x0000004000000040, 0x0000004000000040, 0x0000004000000040, 0x0000004000000040};
-    __m256i const_1  = {0x00060010000C000C, 0x00060010000C000C, 0x00060010000C000C, 0x00060010000C000C};
-    __m256i const_2  = {0xFFF00006FFF4000C, 0xFFF00006FFF4000C, 0xFFF00006FFF4000C, 0xFFF00006FFF4000C};
-    __m256i const_3  = {0x0004000F00090010, 0x0004000F00090010, 0x0004000F00090010, 0x0004000F00090010};
-    __m256i const_4  = {0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F};
-    __m256i const_5  = {0x000FFFF000040009, 0x000FFFF000040009, 0x000FFFF000040009, 0x000FFFF000040009};
-    __m256i const_6  = {0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004};
-    __m256i const_7  = {0x0000000000000004, 0x0000000000000004, 0x0000000000000004, 0x0000000000000004};
-    __m256i const_8  = {0x0011001100110011, 0x0011001100110011, 0x0011001100110011, 0x0011001100110011};
-    __m256i const_9  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
-    __m256i const_10 = {0x000A0016000A0016, 0x000A0016000A0016, 0x000A0016000A0016, 0x000A0016000A0016};
-    __m256i const_11 = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6, 0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    __m256i shift    = {0x0000000400000000, 0x0000000500000001,
+                        0x0000000600000002, 0x0000000700000003};
+    __m256i const_64 = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
+    __m256i const_1  = {0x00060010000C000C, 0x00060010000C000C,
+                        0x00060010000C000C, 0x00060010000C000C};
+    __m256i const_2  = {0xFFF00006FFF4000C, 0xFFF00006FFF4000C,
+                        0xFFF00006FFF4000C, 0xFFF00006FFF4000C};
+    __m256i const_3  = {0x0004000F00090010, 0x0004000F00090010,
+                        0x0004000F00090010, 0x0004000F00090010};
+    __m256i const_4  = {0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F,
+                        0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F};
+    __m256i const_5  = {0x000FFFF000040009, 0x000FFFF000040009,
+                        0x000FFFF000040009, 0x000FFFF000040009};
+    __m256i const_6  = {0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004,
+                        0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004};
+    __m256i const_7  = {0x0000000000000004, 0x0000000000000004,
+                        0x0000000000000004, 0x0000000000000004};
+    __m256i const_8  = {0x0011001100110011, 0x0011001100110011,
+                        0x0011001100110011, 0x0011001100110011};
+    __m256i const_9  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011,
+                        0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
+    __m256i const_10 = {0x000A0016000A0016, 0x000A0016000A0016,
+                        0x000A0016000A0016, 0x000A0016000A0016};
+    __m256i const_11 = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6,
+                        0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
     __m256i in0, in1;
     __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
 
@@ -232,8 +221,8 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t4    = __lasx_xvsub_w(temp0, temp1);
     t4    = __lasx_xvpermi_d(t4, 0xB1);
 
-    DUP4_ARG2(__lasx_xvdp4_d_h, temp3, const_3, temp3, const_4, temp3, const_5, temp3,
-              const_6, t1, t2, temp0, temp1);
+    DUP4_ARG2(__lasx_xvdp4_d_h, temp3, const_3, temp3, const_4, temp3,
+              const_5, temp3, const_6, t1, t2, temp0, temp1);
     temp2 = __lasx_xvpickev_w(t2, t1);
     temp3 = __lasx_xvpickev_w(temp1, temp0);
 
@@ -241,10 +230,8 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t2    = __lasx_xvadd_w(temp3, t4);
     temp0 = __lasx_xvsub_w(t4, temp3);
     temp1 = __lasx_xvsub_w(t3, temp2);
-    DUP4_ARG2(__lasx_xvsrai_w, t1, 3, t2, 3, temp0, 3, temp1, 3, t1, t2, t3, t4);
     /* second loops */
-    temp2 = __lasx_xvpickev_h(t2, t1);
-    temp3 = __lasx_xvpickev_h(t4, t3);
+    DUP2_ARG3(__lasx_xvsrani_h_w, t2, t1, 3, temp1, temp0, 3, temp2, temp3);
     temp3 = __lasx_xvshuf4i_h(temp3, 0x4E);
     temp0 = __lasx_xvpermi_q(temp3, temp2, 0x20);
     temp1 = __lasx_xvpermi_q(temp3, temp2, 0x31);
@@ -255,141 +242,124 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp1 = __lasx_xvsub_w(t2, t4);
     temp2 = __lasx_xvadd_w(t2, t4);
     temp3 = __lasx_xvsub_w(t1, t3);
-    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7, t1, t2, t3, t4);
-
-    temp0 = __lasx_xvldrepl_d(dst, 0);
-    temp0 = __lasx_vext2xv_wu_bu(temp0);
-    dst  += stride;
-    temp1 = __lasx_xvldrepl_d(dst, 0);
-    temp1 = __lasx_vext2xv_wu_bu(temp1);
-    dst  += stride;
-    temp2 = __lasx_xvldrepl_d(dst, 0);
-    temp2 = __lasx_vext2xv_wu_bu(temp2);
-    dst  += stride;
-    temp3 = __lasx_xvldrepl_d(dst, 0);
-    temp3 = __lasx_vext2xv_wu_bu(temp3);
-    t1    = __lasx_xvadd_w(temp0, t1);
-    t2    = __lasx_xvadd_w(temp1, t2);
-    t3    = __lasx_xvadd_w(temp2, t3);
-    t4    = __lasx_xvadd_w(temp3, t4);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+              t1, t2, t3, t4);
+
+    temp0 = __lasx_xvldrepl_d(dest, 0);
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2, 0,
+              dest + stride3, 0, temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_vext2xv_wu_bu, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvadd_w, temp0, t1, temp1, t2, temp2, t3, temp3, t4,
+              t1, t2, t3, t4);
     DUP4_ARG1(__lasx_xvclip255_w, t1, t2, t3, t4, t1, t2, t3, t4);
     DUP2_ARG2(__lasx_xvpickev_h, t2, t1, t4, t3, temp0, temp1);
     temp2 = __lasx_xvpickev_b(temp1, temp0);
     temp0 = __lasx_xvperm_w(temp2, shift);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dest + stride, 0, 1);
-    __lasx_xvstelm_d(temp0, dest + stride_2x, 0, 2);
-    __lasx_xvstelm_d(temp0, dest + stride_3x, 0, 3);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
 }
 
-void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
 {
     int dc = block[0];
-    uint8_t *dst   = dest + stride;
-    __m256i in0, in1, const_dc, temp0;
-    __m256i zero = {0};
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    __m256i in0, in1, in2, in3;
+    __m256i const_dc, temp0, temp1, reg0, reg1;
 
     dc = (3  * dc + 1) >> 1;
     dc = (17 * dc + 64) >> 7;
     const_dc = __lasx_xvreplgr2vr_h(dc);
 
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvilvl_d, in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, temp0, temp1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, reg0, reg1);
+    temp0 = __lasx_xvssrarni_bu_h(reg1, reg0, 0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
-    dest = dst + stride;
-    dst  = dest + stride;
-    in0   = __lasx_xvldrepl_d(dest, 0);
-    in1   = __lasx_xvldrepl_d(dst, 0);
-    in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
-    __lasx_xvstelm_d(temp0, dest, 0, 0);
-    __lasx_xvstelm_d(temp0, dst, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
 }
 
-void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
 {
     int dc = block[0];
-    uint8_t *dst1 = dest + stride;
-    uint8_t *dst2 = dst1 + stride;
-    uint8_t *dst3 = dst2 + stride;
-    __m256i in0, in1, in2, in3, const_dc, temp0, temp1;
-    __m256i zero = {0};
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i const_dc, temp0, temp1, temp2, temp3, reg0, reg1;
 
     dc = (17 * dc +  4) >> 3;
     dc = (12 * dc + 64) >> 7;
     const_dc = __lasx_xvreplgr2vr_h(dc);
 
-    in0   = __lasx_xvldrepl_w(dest, 0);
-    in1   = __lasx_xvldrepl_w(dst1, 0);
-    in2   = __lasx_xvldrepl_w(dst2, 0);
-    in3   = __lasx_xvldrepl_w(dst3, 0);
-    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
-    in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
-    __lasx_xvstelm_w(temp0, dest, 0, 0);
-    __lasx_xvstelm_w(temp0, dst1, 0, 1);
-    __lasx_xvstelm_w(temp0, dst2, 0, 4);
-    __lasx_xvstelm_w(temp0, dst3, 0, 5);
-
-    dest = dst3 + stride;
-    dst1 = dest + stride;
-    dst2 = dst1 + stride;
-    dst3 = dst2 + stride;
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvldrepl_w, dst, 0, dst + stride, 0, dst + stride2,
+              0, dst + stride3, 0, in4, in5, in6, in7);
 
-    in0   = __lasx_xvldrepl_w(dest, 0);
-    in1   = __lasx_xvldrepl_w(dst1, 0);
-    in2   = __lasx_xvldrepl_w(dst2, 0);
-    in3   = __lasx_xvldrepl_w(dst3, 0);
-    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
-    in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
-    temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
+    DUP4_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, in5, in4, in7, in6,
+              temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvilvl_d, temp1, temp0, temp3, temp2, reg0, reg1);
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, reg0, reg1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, reg0, reg1);
+    temp0 = __lasx_xvssrarni_bu_h(reg1, reg0, 0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
-    __lasx_xvstelm_w(temp0, dst1, 0, 1);
-    __lasx_xvstelm_w(temp0, dst2, 0, 4);
-    __lasx_xvstelm_w(temp0, dst3, 0, 5);
-
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 1);
+    __lasx_xvstelm_w(temp0, dest + stride2, 0, 4);
+    __lasx_xvstelm_w(temp0, dest + stride3, 0, 5);
+    __lasx_xvstelm_w(temp0, dst, 0, 2);
+    __lasx_xvstelm_w(temp0, dst + stride, 0, 3);
+    __lasx_xvstelm_w(temp0, dst + stride2, 0, 6);
+    __lasx_xvstelm_w(temp0, dst + stride3, 0, 7);
 }
 
 void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
-    uint8_t *dst = dest;
-    ptrdiff_t stride_2x = stride << 1;
-    ptrdiff_t stride_4x = stride << 2;
-    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
     __m256i in0, in1, in2, in3;
     __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
 
-    __m256i const_1  = {0x0011001100110011, 0x0011001100110011, 0x0011001100110011, 0x0011001100110011};
-    __m256i const_2  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
-    __m256i const_3  = {0x000A0016000A0016, 0x000A0016000A0016, 0x000A0016000A0016, 0x000A0016000A0016};
-    __m256i const_4  = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6, 0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
-    __m256i const_5  = {0x0000000400000004, 0x0000000400000004, 0x0000000400000004, 0x0000000400000004};
-    __m256i const_6  = {0x0000004000000040, 0x0000004000000040, 0x0000004000000040, 0x0000004000000040};
-    __m256i const_7  = {0x000C000C000C000C, 0X000C000C000C000C, 0xFFF4000CFFF4000C, 0xFFF4000CFFF4000C};
-    __m256i const_8  = {0x0006001000060010, 0x0006001000060010, 0xFFF00006FFF00006, 0xFFF00006FFF00006};
-    __m256i const_9  = {0x0009001000090010, 0x0009001000090010, 0x0004000F0004000F, 0x0004000F0004000F};
-    __m256i const_10 = {0xFFF0000FFFF0000F, 0xFFF0000FFFF0000F, 0xFFF7FFFCFFF7FFFC, 0xFFF7FFFCFFF7FFFC};
-    __m256i const_11 = {0x0004000900040009, 0x0004000900040009, 0x000FFFF0000FFFF0, 0x000FFFF0000FFFF0};
-    __m256i const_12 = {0x000F0004000F0004, 0x000F0004000F0004, 0xFFF0FFF7FFF0FFF7, 0xFFF0FFF7FFF0FFF7};
-    __m256i shift    = {0x0000000400000000, 0x0000000600000002, 0x0000000500000001, 0x0000000700000003};
+    __m256i const_1  = {0x0011001100110011, 0x0011001100110011,
+                        0x0011001100110011, 0x0011001100110011};
+    __m256i const_2  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011,
+                        0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
+    __m256i const_3  = {0x000A0016000A0016, 0x000A0016000A0016,
+                        0x000A0016000A0016, 0x000A0016000A0016};
+    __m256i const_4  = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6,
+                        0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
+    __m256i const_5  = {0x0000000400000004, 0x0000000400000004,
+                        0x0000000400000004, 0x0000000400000004};
+    __m256i const_6  = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
+    __m256i const_7  = {0x000C000C000C000C, 0X000C000C000C000C,
+                        0xFFF4000CFFF4000C, 0xFFF4000CFFF4000C};
+    __m256i const_8  = {0x0006001000060010, 0x0006001000060010,
+                        0xFFF00006FFF00006, 0xFFF00006FFF00006};
+    __m256i const_9  = {0x0009001000090010, 0x0009001000090010,
+                        0x0004000F0004000F, 0x0004000F0004000F};
+    __m256i const_10 = {0xFFF0000FFFF0000F, 0xFFF0000FFFF0000F,
+                        0xFFF7FFFCFFF7FFFC, 0xFFF7FFFCFFF7FFFC};
+    __m256i const_11 = {0x0004000900040009, 0x0004000900040009,
+                        0x000FFFF0000FFFF0, 0x000FFFF0000FFFF0};
+    __m256i const_12 = {0x000F0004000F0004, 0x000F0004000F0004,
+                        0xFFF0FFF7FFF0FFF7, 0xFFF0FFF7FFF0FFF7};
+    __m256i shift    = {0x0000000400000000, 0x0000000600000002,
+                        0x0000000500000001, 0x0000000700000003};
 
     /* first loops */
-    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              in0, in1, in2, in3);
     in0   = __lasx_xvilvl_d(in1, in0);
     in1   = __lasx_xvilvl_d(in3, in2);
     temp0 = __lasx_xvpickev_h(in1, in0);
@@ -423,8 +393,8 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t4    = __lasx_xvsub_w(t1, t2);
     t4    = __lasx_xvpermi_d(t4, 0x4E);
 
-    DUP4_ARG2(__lasx_xvdp2_w_h, temp1, const_9, temp1, const_10, temp1, const_11,
-              temp1, const_12, t1, t2, temp2, temp3);
+    DUP4_ARG2(__lasx_xvdp2_w_h, temp1, const_9, temp1, const_10, temp1,
+              const_11, temp1, const_12, t1, t2, temp2, temp3);
 
     temp0 = __lasx_xvpermi_q(t2, t1, 0x20);
     temp1 = __lasx_xvpermi_q(t2, t1, 0x31);
@@ -441,48 +411,33 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
               temp0, temp1, temp2, temp3);
 
-    const_1 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_2 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_3 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_4 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_5 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_6 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_7 = __lasx_xvldrepl_w(dst, 0);
-    dst += stride;
-    const_8 = __lasx_xvldrepl_w(dst, 0);
-
-    DUP4_ARG2(__lasx_xvilvl_w, const_2, const_1, const_4, const_3, const_5, const_6,
-              const_7, const_8, const_1, const_2, const_3, const_4);
-    const_1 = __lasx_vext2xv_wu_bu(const_1);
-    const_2 = __lasx_vext2xv_wu_bu(const_2);
-    const_3 = __lasx_vext2xv_wu_bu(const_3);
-    const_4 = __lasx_vext2xv_wu_bu(const_4);
-
-    temp0   = __lasx_xvadd_w(temp0, const_1);
-    temp1   = __lasx_xvadd_w(temp1, const_2);
-    temp2   = __lasx_xvadd_w(temp2, const_3);
-    temp3   = __lasx_xvadd_w(temp3, const_4);
-    DUP4_ARG1(__lasx_xvclip255_w, temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dest + stride, 0, dest + stride2, 0,
+              dest + stride3, 0, const_1, const_2, const_3, const_4);
+    DUP4_ARG2(__lasx_xvldrepl_w, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, const_5, const_6, const_7, const_8);
+
+    DUP4_ARG2(__lasx_xvilvl_w, const_2, const_1, const_4, const_3, const_5,
+              const_6, const_7, const_8, const_1, const_2, const_3, const_4);
+    DUP4_ARG1(__lasx_vext2xv_wu_bu, const_1, const_2, const_3, const_4,
+              const_1, const_2, const_3, const_4);
+    DUP4_ARG2(__lasx_xvadd_w, temp0, const_1, temp1, const_2, temp2, const_3,
+              temp3, const_4, temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_xvclip255_w, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
     DUP2_ARG2(__lasx_xvpickev_h, temp1, temp0, temp3, temp2, temp0, temp1);
     temp0   = __lasx_xvpickev_b(temp1, temp0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
     __lasx_xvstelm_w(temp0, dest + stride, 0, 4);
-    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 1);
-    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 5);
-    dest += stride_4x;
-    __lasx_xvstelm_w(temp0, dest, 0, 6);
-    __lasx_xvstelm_w(temp0, dest + stride, 0, 2);
-    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 7);
-    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 3);
+    __lasx_xvstelm_w(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_w(temp0, dest + stride3, 0, 5);
+    __lasx_xvstelm_w(temp0, dst, 0, 6);
+    __lasx_xvstelm_w(temp0, dst + stride, 0, 2);
+    __lasx_xvstelm_w(temp0, dst + stride2, 0, 7);
+    __lasx_xvstelm_w(temp0, dst + stride3, 0, 3);
 }
 
-void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
 {
     int dc = block[0];
     uint8_t *dst1 = dest + stride;
@@ -495,16 +450,13 @@ void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     dc = (17 * dc + 64) >> 7;
     const_dc = __lasx_xvreplgr2vr_h(dc);
 
-    in0   = __lasx_xvldrepl_w(dest, 0);
-    in1   = __lasx_xvldrepl_w(dst1, 0);
-    in2   = __lasx_xvldrepl_w(dst2, 0);
-    in3   = __lasx_xvldrepl_w(dst3, 0);
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dst1, 0, dst2, 0, dst3, 0,
+              in0, in1, in2, in3);
     DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
     in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
     temp0 = __lasx_xvilvl_b(zero, in0);
-    temp0 = __lasx_xvadd_h(temp0, const_dc);
-    in0 = __lasx_xvclip255_h(temp0);
-    temp0 = __lasx_xvpickev_b(in0, in0);
+    in0   = __lasx_xvadd_h(temp0, const_dc);
+    temp0 = __lasx_xvssrarni_bu_h(in0, in0, 0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
     __lasx_xvstelm_w(temp0, dst1, 0, 1);
     __lasx_xvstelm_w(temp0, dst2, 0, 4);
@@ -513,15 +465,18 @@ void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
 
 void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
-    uint8_t *dst = dest + stride;
-    ptrdiff_t stride_2x = stride << 1;
-    ptrdiff_t stride_3x = stride_2x + stride;
+    uint8_t *dst1 = dest + stride;
+    uint8_t *dst2 = dst1 + stride;
+    uint8_t *dst3 = dst2 + stride;
     __m256i in0, in1, in2, in3;
     __m256i temp0, temp1, temp2, temp3, t1, t2;
 
-    __m256i const_1  = {0x0011001100110011, 0xFFEF0011FFEF0011, 0x0011001100110011, 0xFFEF0011FFEF0011};
-    __m256i const_2  = {0x000A0016000A0016, 0x0016FFF60016FFF6, 0x000A0016000A0016, 0x0016FFF60016FFF6};
-    __m256i const_64 = {0x0000004000000040, 0x0000004000000040, 0x0000004000000040, 0x0000004000000040};
+    __m256i const_1  = {0x0011001100110011, 0xFFEF0011FFEF0011,
+                        0x0011001100110011, 0xFFEF0011FFEF0011};
+    __m256i const_2  = {0x000A0016000A0016, 0x0016FFF60016FFF6,
+                        0x000A0016000A0016, 0x0016FFF60016FFF6};
+    __m256i const_64 = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
 
     DUP2_ARG2(__lasx_xvld, block, 0, block, 32, in0, in1);
     /* first loops */
@@ -547,12 +502,8 @@ void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp0   = __lasx_xvshuf4i_w(in0, 0x9C);
     temp1   = __lasx_xvshuf4i_w(in1, 0x9C);
 
-    in0     = __lasx_xvldrepl_w(dest, 0);
-    in1     = __lasx_xvldrepl_w(dst, 0);
-    dst    += stride;
-    in2     = __lasx_xvldrepl_w(dst, 0);
-    dst    += stride;
-    in3     = __lasx_xvldrepl_w(dst, 0);
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dst1, 0, dst2, 0, dst3, 0,
+              in0, in1, in2, in3);
     temp2   = __lasx_xvilvl_w(in2, in0);
     temp2   = __lasx_vext2xv_wu_bu(temp2);
     temp3   = __lasx_xvilvl_w(in1, in3);
@@ -563,9 +514,9 @@ void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp1   = __lasx_xvpickev_h(temp1, temp0);
     temp0   = __lasx_xvpickev_b(temp1, temp1);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
-    __lasx_xvstelm_w(temp0, dest + stride, 0, 5);
-    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 4);
-    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 1);
+    __lasx_xvstelm_w(temp0, dst1, 0, 5);
+    __lasx_xvstelm_w(temp0, dst2, 0, 4);
+    __lasx_xvstelm_w(temp0, dst3, 0, 1);
 }
 
 static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
@@ -576,7 +527,8 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     __m256i t0, t1, t2, t3, t4, t5, t6, t7;
     __m256i temp0, temp1, const_para1_2, const_para0_3;
     __m256i const_r, const_sh;
-    __m256i sh = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
+    __m256i sh = {0x0000000400000000, 0x0000000500000001,
+                  0x0000000600000002, 0x0000000700000003};
     static const uint8_t para_value[][4] = {{4, 3, 53, 18},
                                             {1, 1, 9, 9},
                                             {3, 4, 18, 53}};
@@ -584,25 +536,23 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     int shift = (shift_value[hmode] + shift_value[vmode]) >> 1;
     int r     = (1 << (shift - 1)) + rnd - 1;
     const uint8_t *para_v = para_value[vmode - 1];
-    ptrdiff_t stride_2x = stride << 1;
-    ptrdiff_t stride_4x = stride << 2;
-    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride4 = stride << 2;
+    ptrdiff_t stride3 = stride2 + stride;
 
     const_r  = __lasx_xvreplgr2vr_h(r);
     const_sh = __lasx_xvreplgr2vr_h(shift);
     src -= 1, src -= stride;
     const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
     const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
-    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0, src + stride_3x,
-              0, in0, in1, in2, in3);
-    in0   = __lasx_xvpermi_d(in0, 0xD8);
-    in1   = __lasx_xvpermi_d(in1, 0xD8);
-    in2   = __lasx_xvpermi_d(in2, 0xD8);
-    in3   = __lasx_xvpermi_d(in3, 0xD8);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
     DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
     t0 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
     t0 = __lasx_xvdp2sub_h_bu(t0, temp1, const_para0_3);
-    src  += (stride << 2);
+    src  += stride4;
     in0   = __lasx_xvld(src, 0);
     in0   = __lasx_xvpermi_d(in0, 0xD8);
     DUP2_ARG2(__lasx_xvilvl_b, in3, in2, in0, in1, temp0, temp1);
@@ -644,19 +594,16 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     DUP2_ARG2(__lasx_xvilvl_b, in1, in0, in2, in3, temp0, temp1);
     t7 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
     t7 = __lasx_xvdp2sub_h_bu(t7, temp1, const_para0_3);
-    DUP4_ARG2(__lasx_xvadd_h, t0, const_r, t1, const_r, t2, const_r, t3, const_r,
-              t0, t1, t2, t3);
-    DUP4_ARG2(__lasx_xvadd_h, t4, const_r, t5, const_r, t6, const_r, t7, const_r,
-              t4, t5, t6, t7);
-    t0    = __lasx_xvsra_h(t0, const_sh);
-    t1    = __lasx_xvsra_h(t1, const_sh);
-    t2    = __lasx_xvsra_h(t2, const_sh);
-    t3    = __lasx_xvsra_h(t3, const_sh);
-    t4    = __lasx_xvsra_h(t4, const_sh);
-    t5    = __lasx_xvsra_h(t5, const_sh);
-    t6    = __lasx_xvsra_h(t6, const_sh);
-    t7    = __lasx_xvsra_h(t7, const_sh);
-    LASX_TRANSPOSE8x8_H(t0, t1, t2, t3, t4, t5, t6, t7, t0, t1, t2, t3, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvadd_h, t0, const_r, t1, const_r, t2, const_r, t3,
+              const_r, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvadd_h, t4, const_r, t5, const_r, t6, const_r, t7,
+              const_r, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvsra_h, t0, const_sh, t1, const_sh, t2, const_sh,
+              t3, const_sh, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvsra_h, t4, const_sh, t5, const_sh, t6, const_sh,
+              t7, const_sh, t4, t5, t6, t7);
+    LASX_TRANSPOSE8x8_H(t0, t1, t2, t3, t4, t5, t6, t7, t0,
+                        t1, t2, t3, t4, t5, t6, t7);
     para_v  = para_value[hmode - 1];
     const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
     const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
@@ -664,16 +611,11 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     const_para1_2 = __lasx_vext2xv_h_b(const_para1_2);
     r       = 64 - rnd;
     const_r = __lasx_xvreplgr2vr_w(r);
-    in0     = __lasx_xvpermi_d(t0, 0x72);
-    in1     = __lasx_xvpermi_d(t1, 0x72);
-    in2     = __lasx_xvpermi_d(t2, 0x72);
-    t0      = __lasx_xvpermi_d(t0, 0xD8);
-    t1      = __lasx_xvpermi_d(t1, 0xD8);
-    t2      = __lasx_xvpermi_d(t2, 0xD8);
-    t3      = __lasx_xvpermi_d(t3, 0xD8);
-    t4      = __lasx_xvpermi_d(t4, 0xD8);
-    t5      = __lasx_xvpermi_d(t5, 0xD8);
-    t6      = __lasx_xvpermi_d(t6, 0xD8);
+    DUP4_ARG2(__lasx_xvpermi_d, t0, 0x72, t1, 0x72, t2, 0x72, t0, 0xD8,
+              in0, in1, in2, t0);
+    DUP4_ARG2(__lasx_xvpermi_d, t1, 0xD8, t2, 0xD8, t3, 0xD8, t4, 0xD8,
+              t1, t2, t3, t4);
+    DUP2_ARG2(__lasx_xvpermi_d, t5, 0xD8, t6, 0xD8, t5, t6);
     t7      = __lasx_xvpermi_d(t7, 0xD8);
     DUP2_ARG2(__lasx_xvilvl_h, t2, t1, t3, t0, temp0, temp1);
     t0 = __lasx_xvdp2_w_h(temp0, const_para1_2);
@@ -699,33 +641,30 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in2, t7, temp0, temp1);
     t7 = __lasx_xvdp2_w_h(temp0, const_para1_2);
     t7 = __lasx_xvdp2sub_w_h(t7, temp1, const_para0_3);
-    t0    = __lasx_xvadd_w(t0, const_r);
-    t1    = __lasx_xvadd_w(t1, const_r);
-    t2    = __lasx_xvadd_w(t2, const_r);
-    t3    = __lasx_xvadd_w(t3, const_r);
-    t4    = __lasx_xvadd_w(t4, const_r);
-    t5    = __lasx_xvadd_w(t5, const_r);
-    t6    = __lasx_xvadd_w(t6, const_r);
-    t7    = __lasx_xvadd_w(t7, const_r);
+    DUP4_ARG2(__lasx_xvadd_w, t0, const_r, t1, const_r, t2, const_r,
+              t3, const_r, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvadd_w, t4, const_r, t5, const_r, t6, const_r,
+              t7, const_r, t4, t5, t6, t7);
     DUP4_ARG2(__lasx_xvsrai_w, t0, 7, t1, 7, t2, 7, t3, 7, t0, t1, t2, t3);
     DUP4_ARG2(__lasx_xvsrai_w, t4, 7, t5, 7, t6, 7, t7, 7, t4, t5, t6, t7);
     LASX_TRANSPOSE8x8_W(t0, t1, t2, t3, t4, t5, t6, t7,
                         t0, t1, t2, t3, t4, t5, t6, t7);
     DUP4_ARG1(__lasx_xvclip255_w, t0, t1, t2, t3, t0, t1, t2, t3);
     DUP4_ARG1(__lasx_xvclip255_w, t4, t5, t6, t7, t4, t5, t6, t7);
-    DUP4_ARG2(__lasx_xvpickev_h, t1, t0, t3, t2, t5, t4, t7, t6, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvpickev_h, t1, t0, t3, t2, t5, t4, t7, t6,
+              t0, t1, t2, t3);
     DUP2_ARG2(__lasx_xvpickev_b, t1, t0, t3, t2, t0, t1);
     t0 = __lasx_xvperm_w(t0, sh);
     t1 = __lasx_xvperm_w(t1, sh);
     __lasx_xvstelm_d(t0, dst, 0, 0);
     __lasx_xvstelm_d(t0, dst + stride, 0, 1);
-    __lasx_xvstelm_d(t0, dst + stride_2x, 0, 2);
-    __lasx_xvstelm_d(t0, dst + stride_3x, 0, 3);
-    dst += stride_4x;
+    __lasx_xvstelm_d(t0, dst + stride2, 0, 2);
+    __lasx_xvstelm_d(t0, dst + stride3, 0, 3);
+    dst += stride4;
     __lasx_xvstelm_d(t1, dst, 0, 0);
     __lasx_xvstelm_d(t1, dst + stride, 0, 1);
-    __lasx_xvstelm_d(t1, dst + stride_2x, 0, 2);
-    __lasx_xvstelm_d(t1, dst + stride_3x, 0, 3);
+    __lasx_xvstelm_d(t1, dst + stride2, 0, 2);
+    __lasx_xvstelm_d(t1, dst + stride3, 0, 3);
 }
 
 #define PUT_VC1_MSPEL_MC_LASX(hmode, vmode)                                   \
@@ -783,10 +722,8 @@ void ff_put_no_rnd_vc1_chroma_mc8_lasx(uint8_t *dst /* align 8 */,
 
         DUP4_ARG1(__lasx_vext2xv_hu_bu, src00, src01, src10, src11,
                   src00, src01, src10, src11);
-        src00 = __lasx_xvmul_h(src00, A);
-        src01 = __lasx_xvmul_h(src01, B);
-        src10 = __lasx_xvmul_h(src10, C);
-        src11 = __lasx_xvmul_h(src11, D);
+        DUP4_ARG2(__lasx_xvmul_h, src00, A, src01, B, src10, C, src11, D,
+                  src00, src01, src10, src11);
         src00 = __lasx_xvadd_h(src00, src01);
         src10 = __lasx_xvadd_h(src10, src11);
         src00 = __lasx_xvadd_h(src00, src10);
@@ -876,35 +813,38 @@ static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
     __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;
     __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;
     __m256i t0, t1, t2, t3, t4, t5, t6, t7;
-    ptrdiff_t stride_2x = stride << 1;
-    ptrdiff_t stride_4x = stride << 2;
-    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride4 = stride << 2;
+    ptrdiff_t stride3 = stride2 + stride;
     static const uint16_t para_value[][2] = {{0x0304, 0x1235},
                                             {0x0101, 0x0909},
                                             {0x0403, 0x3512}};
     const uint16_t *para_v = para_value[hmode - 1];
     static const int shift_value[] = {0, 6, 4, 6};
     static int add_value[3];
+    uint8_t *_src = (uint8_t*)src - 1;
     add_value[2] = add_value[0] = 32 - rnd, add_value[1] = 8 - rnd;
 
     const_r  = __lasx_xvreplgr2vr_h(add_value[hmode - 1]);
     const_sh = __lasx_xvreplgr2vr_h(shift_value[hmode]);
     const_para0_3 = __lasx_xvreplgr2vr_h(*para_v);
     const_para1_2 = __lasx_xvreplgr2vr_h(*(para_v + 1));
-    src -= 1;
-
-    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
-              src + stride_3x, 0, in0, in1, in2, in3);
-    src += stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
-              src + stride_3x, 0, in4, in5, in6, in7);
-    src += stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
-              src + stride_3x, 0, in8, in9, in10, in11);
-    src += stride_4x;
-    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
-              src + stride_3x, 0, in12, in13, in14, in15);
-    src -= stride_4x;
+
+    in0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in1, in2);
+    in3 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in5, in6);
+    in7 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in9, in10);
+    in11 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in13, in14);
+    in15 = __lasx_xvldx(_src, stride3);
     DUP4_ARG2(__lasx_xvilvl_b, in2, in0, in3, in1, in6, in4, in7, in5,
               tmp0_m, tmp1_m, tmp2_m, tmp3_m);
     DUP4_ARG2(__lasx_xvilvl_b, in10, in8, in11, in9, in14, in12, in15, in13,
@@ -941,22 +881,14 @@ static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
     DUP2_ARG3(__lasx_xvpermi_q, out0, out0, 0x31, out1, out1, 0x31, out16, out17);
     out18 = __lasx_xvpermi_q(out2, out2, 0x31);
 
-    out0  = __lasx_xvpermi_d(out0, 0xD8);
-    out1  = __lasx_xvpermi_d(out1, 0xD8);
-    out2  = __lasx_xvpermi_d(out2, 0xD8);
-    out3  = __lasx_xvpermi_d(out3, 0xD8);
-    out4  = __lasx_xvpermi_d(out4, 0xD8);
-    out5  = __lasx_xvpermi_d(out5, 0xD8);
-    out6  = __lasx_xvpermi_d(out6, 0xD8);
-    out7  = __lasx_xvpermi_d(out7, 0xD8);
-    out8  = __lasx_xvpermi_d(out8, 0xD8);
-    out9  = __lasx_xvpermi_d(out9, 0xD8);
-    out10 = __lasx_xvpermi_d(out10, 0xD8);
-    out11 = __lasx_xvpermi_d(out11, 0xD8);
-    out12 = __lasx_xvpermi_d(out12, 0xD8);
-    out13 = __lasx_xvpermi_d(out13, 0xD8);
-    out14 = __lasx_xvpermi_d(out14, 0xD8);
-    out15 = __lasx_xvpermi_d(out15, 0xD8);
+    DUP4_ARG2(__lasx_xvpermi_d, out0, 0xD8, out1, 0xD8, out2, 0xD8, out3, 0xD8,
+              out0, out1, out2, out3);
+    DUP4_ARG2(__lasx_xvpermi_d, out4, 0xD8, out5, 0xD8, out6, 0xD8, out7, 0xD8,
+              out4, out5, out6, out7);
+    DUP4_ARG2(__lasx_xvpermi_d, out8, 0xD8, out9, 0xD8, out10, 0xD8, out11,
+              0xD8, out8, out9, out10, out11);
+    DUP4_ARG2(__lasx_xvpermi_d, out12, 0xD8, out13, 0xD8, out14, 0xD8, out15,
+              0xD8, out12, out13, out14, out15);
     out16 = __lasx_xvpermi_d(out16, 0xD8);
     out17 = __lasx_xvpermi_d(out17, 0xD8);
     out18 = __lasx_xvpermi_d(out18, 0xD8);
diff --git a/libavcodec/loongarch/videodsp_init.c b/libavcodec/loongarch/videodsp_init.c
index bfe35ad90f..6cbb7763ff 100644
--- a/libavcodec/loongarch/videodsp_init.c
+++ b/libavcodec/loongarch/videodsp_init.c
@@ -19,9 +19,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "config.h"
-#include "libavutil/attributes.h"
 #include "libavcodec/videodsp.h"
+#include "libavutil/attributes.h"
 
 static void prefetch_loongarch(uint8_t *mem, ptrdiff_t stride, int h)
 {
diff --git a/libavcodec/loongarch/vp8dsp_init_loongarch.c b/libavcodec/loongarch/vp8dsp_init_loongarch.c
index 7f36fb90c1..63da15b198 100644
--- a/libavcodec/loongarch/vp8dsp_init_loongarch.c
+++ b/libavcodec/loongarch/vp8dsp_init_loongarch.c
@@ -26,6 +26,7 @@
 
 #include "libavutil/loongarch/cpu.h"
 #include "libavcodec/vp8dsp.h"
+#include "libavutil/attributes.h"
 #include "vp8dsp_loongarch.h"
 
 #define VP8_MC_LOONGARCH_FUNC(IDX, SIZE)                                          \
diff --git a/libavcodec/loongarch/vp9_idct_lsx.c b/libavcodec/loongarch/vp9_idct_lsx.c
index b977687237..88805814c6 100644
--- a/libavcodec/loongarch/vp9_idct_lsx.c
+++ b/libavcodec/loongarch/vp9_idct_lsx.c
@@ -28,42 +28,42 @@
 #define ALLOC_ALIGNED(align) __attribute__ ((aligned(align)))
 #define ROUND_POWER_OF_TWO(value, n) (((value) + (1 << ((n) - 1))) >> (n))
 
-static const int32_t cospi_1_64 = 16364;
-static const int32_t cospi_2_64 = 16305;
-static const int32_t cospi_3_64 = 16207;
-static const int32_t cospi_4_64 = 16069;
-static const int32_t cospi_5_64 = 15893;
-static const int32_t cospi_6_64 = 15679;
-static const int32_t cospi_7_64 = 15426;
-static const int32_t cospi_8_64 = 15137;
-static const int32_t cospi_9_64 = 14811;
-static const int32_t cospi_10_64 = 14449;
-static const int32_t cospi_11_64 = 14053;
-static const int32_t cospi_12_64 = 13623;
-static const int32_t cospi_13_64 = 13160;
-static const int32_t cospi_14_64 = 12665;
-static const int32_t cospi_15_64 = 12140;
-static const int32_t cospi_16_64 = 11585;
-static const int32_t cospi_17_64 = 11003;
-static const int32_t cospi_18_64 = 10394;
-static const int32_t cospi_19_64 = 9760;
-static const int32_t cospi_20_64 = 9102;
-static const int32_t cospi_21_64 = 8423;
-static const int32_t cospi_22_64 = 7723;
-static const int32_t cospi_23_64 = 7005;
-static const int32_t cospi_24_64 = 6270;
-static const int32_t cospi_25_64 = 5520;
-static const int32_t cospi_26_64 = 4756;
-static const int32_t cospi_27_64 = 3981;
-static const int32_t cospi_28_64 = 3196;
-static const int32_t cospi_29_64 = 2404;
-static const int32_t cospi_30_64 = 1606;
-static const int32_t cospi_31_64 = 804;
-
-static const int32_t sinpi_1_9 = 5283;
-static const int32_t sinpi_2_9 = 9929;
-static const int32_t sinpi_3_9 = 13377;
-static const int32_t sinpi_4_9 = 15212;
+const int32_t cospi_1_64 = 16364;
+const int32_t cospi_2_64 = 16305;
+const int32_t cospi_3_64 = 16207;
+const int32_t cospi_4_64 = 16069;
+const int32_t cospi_5_64 = 15893;
+const int32_t cospi_6_64 = 15679;
+const int32_t cospi_7_64 = 15426;
+const int32_t cospi_8_64 = 15137;
+const int32_t cospi_9_64 = 14811;
+const int32_t cospi_10_64 = 14449;
+const int32_t cospi_11_64 = 14053;
+const int32_t cospi_12_64 = 13623;
+const int32_t cospi_13_64 = 13160;
+const int32_t cospi_14_64 = 12665;
+const int32_t cospi_15_64 = 12140;
+const int32_t cospi_16_64 = 11585;
+const int32_t cospi_17_64 = 11003;
+const int32_t cospi_18_64 = 10394;
+const int32_t cospi_19_64 = 9760;
+const int32_t cospi_20_64 = 9102;
+const int32_t cospi_21_64 = 8423;
+const int32_t cospi_22_64 = 7723;
+const int32_t cospi_23_64 = 7005;
+const int32_t cospi_24_64 = 6270;
+const int32_t cospi_25_64 = 5520;
+const int32_t cospi_26_64 = 4756;
+const int32_t cospi_27_64 = 3981;
+const int32_t cospi_28_64 = 3196;
+const int32_t cospi_29_64 = 2404;
+const int32_t cospi_30_64 = 1606;
+const int32_t cospi_31_64 = 804;
+
+const int32_t sinpi_1_9 = 5283;
+const int32_t sinpi_2_9 = 9929;
+const int32_t sinpi_3_9 = 13377;
+const int32_t sinpi_4_9 = 15212;
 
 #define VP9_DOTP_CONST_PAIR(reg0, reg1, cnst0, cnst1, out0, out1)  \
 {                                                                  \
@@ -155,28 +155,29 @@ static const int32_t sinpi_4_9 = 15212;
 }
 
 /* multiply and add macro */
-#define VP9_MADD(inp0, inp1, inp2, inp3, cst0, cst1, cst2, cst3,         \
-                 out0, out1, out2, out3)                                 \
-{                                                                        \
-    __m128i madd_s0_m, madd_s1_m, madd_s2_m, madd_s3_m;                  \
-    __m128i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                              \
-                                                                         \
-    madd_s1_m = __lsx_vilvl_h(inp1, inp0);                               \
-    madd_s0_m = __lsx_vilvh_h(inp1, inp0);                               \
-    madd_s3_m = __lsx_vilvl_h(inp3, inp2);                               \
-    madd_s2_m = __lsx_vilvh_h(inp3, inp2);                               \
-    DUP4_ARG2(__lsx_vdp2_w_h, madd_s1_m, cst0, madd_s0_m, cst0,          \
-              madd_s1_m, cst1, madd_s0_m, cst1, tmp0_m, tmp1_m, tmp2_m, tmp3_m);\
-    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                       \
-              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,           \
-              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);      \
-    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1);     \
-    DUP4_ARG2(__lsx_vdp2_w_h, madd_s3_m, cst2, madd_s2_m, cst2,                 \
-              madd_s3_m, cst3, madd_s2_m, cst3, tmp0_m, tmp1_m, tmp2_m, tmp3_m);\
-    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                       \
-              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,           \
-              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);      \
-    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out2, out3);     \
+#define VP9_MADD(inp0, inp1, inp2, inp3, cst0, cst1, cst2, cst3,            \
+                 out0, out1, out2, out3)                                    \
+{                                                                           \
+    __m128i madd_s0_m, madd_s1_m, madd_s2_m, madd_s3_m;                     \
+    __m128i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+                                                                            \
+    madd_s1_m = __lsx_vilvl_h(inp1, inp0);                                  \
+    madd_s0_m = __lsx_vilvh_h(inp1, inp0);                                  \
+    madd_s3_m = __lsx_vilvl_h(inp3, inp2);                                  \
+    madd_s2_m = __lsx_vilvh_h(inp3, inp2);                                  \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s1_m, cst0, madd_s0_m, cst0,             \
+              madd_s1_m, cst1, madd_s0_m, cst1, tmp0_m, tmp1_m,             \
+              tmp2_m, tmp3_m);                                              \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS, tmp1_m,           \
+              VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS, tmp3_m,       \
+              VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);          \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1); \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s3_m, cst2, madd_s2_m, cst2, madd_s3_m,  \
+              cst3, madd_s2_m, cst3, tmp0_m, tmp1_m, tmp2_m, tmp3_m);       \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                   \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,       \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);  \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out2, out3); \
 }
 
 #define VP9_SET_CONST_PAIR(mask_h, idx1_h, idx2_h)                           \
@@ -255,8 +256,10 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements of 8x8 block */
-    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48, in0, in1, in2, in3);
-    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              in4, in5, in6, in7);
     __lsx_vst(zero, input, 0);
     __lsx_vst(zero, input, 16);
     __lsx_vst(zero, input, 32);
@@ -265,7 +268,8 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     __lsx_vst(zero, input, 80);
     __lsx_vst(zero, input, 96);
     __lsx_vst(zero, input, 112);
-    DUP4_ARG2(__lsx_vilvl_d,in1, in0, in3, in2, in5, in4, in7, in6, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vilvl_d,in1, in0, in3, in2, in5, in4, in7,
+              in6, in0, in1, in2, in3);
 
     /* stage1 */
     DUP2_ARG2(__lsx_vilvh_h, in3, in0, in2, in1, s0, s1);
@@ -273,9 +277,11 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     k1 = VP9_SET_COSPI_PAIR(cospi_4_64, cospi_28_64);
     k2 = VP9_SET_COSPI_PAIR(-cospi_20_64, cospi_12_64);
     k3 = VP9_SET_COSPI_PAIR(cospi_12_64, cospi_20_64);
-    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3, tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1, VP9_DCT_CONST_BITS,
-              tmp2, VP9_DCT_CONST_BITS, tmp3, VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp2, VP9_DCT_CONST_BITS, tmp3,
+              VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
     DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
               s0, s1, s2, s3);
     LSX_BUTTERFLY_4_H(s0, s1, s3, s2, s4, s7, s6, s5);
@@ -288,8 +294,9 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     k3 = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);
     DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
                   tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1, VP9_DCT_CONST_BITS,
-              tmp2, VP9_DCT_CONST_BITS, tmp3, VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp2, VP9_DCT_CONST_BITS, tmp3,
+              VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
     DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
               s0, s1, s2, s3);
     LSX_BUTTERFLY_4_H(s0, s1, s2, s3, m0, m1, m2, m3);
@@ -312,8 +319,10 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
                    in0, in1, in2, in3, in4, in5, in6, in7);
 
     /* final rounding (add 2^4, divide by 2^5) and shift */
-    DUP4_ARG2(__lsx_vsrari_h, in0 , 5, in1, 5, in2, 5, in3, 5, in0, in1, in2, in3);
-    DUP4_ARG2(__lsx_vsrari_h, in4 , 5, in5, 5, in6, 5, in7, 5, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vsrari_h, in0 , 5, in1, 5, in2, 5, in3, 5,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4 , 5, in5, 5, in6, 5, in7, 5,
+              in4, in5, in6, in7);
 
     /* add block and store 8x8 */
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
@@ -350,8 +359,10 @@ static void vp9_idct8x8_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
                    in0, in1, in2, in3, in4, in5, in6, in7);
     /* final rounding (add 2^4, divide by 2^5) and shift */
-    DUP4_ARG2(__lsx_vsrari_h, in0, 5, in1, 5, in2, 5, in3, 5, in0, in1, in2, in3);
-    DUP4_ARG2(__lsx_vsrari_h, in4, 5, in5, 5, in6, 5, in7, 5, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vsrari_h, in0, 5, in1, 5, in2, 5, in3, 5,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4, 5, in5, 5, in6, 5, in7, 5,
+              in4, in5, in6, in7);
     /* add block and store 8x8 */
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
     dst += (4 * dst_stride);
@@ -374,8 +385,8 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
               reg4, reg5, reg6, reg7);
     DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
               reg8, reg9, reg10, reg11);
-    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input, 32*15,
-              reg12, reg13, reg14, reg15);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input,
+              32*15, reg12, reg13, reg14, reg15);
 
     __lsx_vst(zero, input, 32*0);
     __lsx_vst(zero, input, 32*1);
@@ -499,8 +510,8 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
               reg4, reg5, reg6, reg7);
     DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
               reg8, reg9, reg10, reg11);
-    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input, 32*15,
-              reg12, reg13, reg14, reg15);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input,
+              32*15, reg12, reg13, reg14, reg15);
 
     __lsx_vst(zero, input, 32*0);
     __lsx_vst(zero, input, 32*1);
@@ -630,6 +641,9 @@ static void vp9_idct16x16_1_add_lsx(int16_t *input, uint8_t *dst,
     int16_t out;
     __m128i vec, res0, res1, res2, res3, res4, res5, res6, res7;
     __m128i dst0, dst1, dst2, dst3, tmp0, tmp1, tmp2, tmp3;
+    int32_t stride2 = dst_stride << 1;
+    int32_t stride3 = stride2 + dst_stride;
+    int32_t stride4 = stride2 << 1;
 
     out = ROUND_POWER_OF_TWO((input[0] * cospi_16_64), VP9_DCT_CONST_BITS);
     out = ROUND_POWER_OF_TWO((out * cospi_16_64), VP9_DCT_CONST_BITS);
@@ -638,8 +652,9 @@ static void vp9_idct16x16_1_add_lsx(int16_t *input, uint8_t *dst,
     vec = __lsx_vreplgr2vr_h(out);
 
     for (i = 4; i--;) {
-        DUP4_ARG2(__lsx_vld, dst, 0, dst + dst_stride, 0, dst + dst_stride * 2, 0,
-                  dst + dst_stride * 3, 0, dst0, dst1, dst2, dst3);
+        dst0 = __lsx_vld(dst, 0);
+        DUP2_ARG2(__lsx_vldx, dst, dst_stride, dst, stride2, dst1, dst2);
+        dst3 = __lsx_vldx(dst, stride3);
         VP9_UNPCK_UB_SH(dst0, res4, res0);
         VP9_UNPCK_UB_SH(dst1, res5, res1);
         VP9_UNPCK_UB_SH(dst2, res6, res2);
@@ -648,15 +663,17 @@ static void vp9_idct16x16_1_add_lsx(int16_t *input, uint8_t *dst,
                   res0, res1, res2, res3);
         DUP4_ARG2(__lsx_vadd_h, res4, vec, res5, vec, res6, vec, res7, vec,
                   res4, res5, res6, res7);
-        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
-        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7, res4, res5, res6, res7);
-        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6, res2, res7, res3,
-                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3,
+                  res0, res1, res2, res3);
+        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7,
+                  res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6,
+                  res2, res7, res3, tmp0, tmp1, tmp2, tmp3);
         __lsx_vst(tmp0, dst, 0);
-        __lsx_vst(tmp1, dst + dst_stride, 0);
-        __lsx_vst(tmp2, dst + dst_stride * 2, 0);
-        __lsx_vst(tmp3, dst + dst_stride * 3, 0);
-        dst += dst_stride << 2;
+        __lsx_vstx(tmp1, dst, dst_stride);
+        __lsx_vstx(tmp2, dst, stride2);
+        __lsx_vstx(tmp3, dst, stride3);
+        dst += stride4;
     }
 }
 
@@ -1087,7 +1104,8 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     DUP4_ARG2(__lsx_vld, tmp_odd_buf, 0, tmp_odd_buf, 16,
               tmp_odd_buf, 32, tmp_odd_buf, 48, reg0, reg1, reg2, reg3);
     DUP4_ARG2(__lsx_vld, tmp_odd_buf, 8 * 16, tmp_odd_buf, 8 * 16 + 16,
-              tmp_odd_buf, 8 * 16 + 32, tmp_odd_buf, 8 * 16 + 48, reg4, reg5, reg6, reg7);
+              tmp_odd_buf, 8 * 16 + 32, tmp_odd_buf, 8 * 16 + 48,
+              reg4, reg5, reg6, reg7);
 
     DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
                   loc0, loc1, loc2, loc3);
@@ -1107,9 +1125,11 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
 
     /* Load 8 & Store 8 */
     DUP4_ARG2(__lsx_vld, tmp_odd_buf, 4 * 16, tmp_odd_buf, 4 * 16 + 16,
-              tmp_odd_buf, 4 * 16 + 32, tmp_odd_buf, 4 * 16 + 48, reg1, reg2, reg0, reg3);
+              tmp_odd_buf, 4 * 16 + 32, tmp_odd_buf, 4 * 16 + 48,
+              reg1, reg2, reg0, reg3);
     DUP4_ARG2(__lsx_vld, tmp_odd_buf, 12 * 16, tmp_odd_buf, 12 * 16 + 16,
-              tmp_odd_buf, 12 * 16 + 32, tmp_odd_buf, 12 * 16 + 48, reg4, reg5, reg6, reg7);
+              tmp_odd_buf, 12 * 16 + 32, tmp_odd_buf, 12 * 16 + 48,
+              reg4, reg5, reg6, reg7);
 
     DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
               loc0, loc1, loc2, loc3);
@@ -1252,6 +1272,7 @@ static void vp9_idct32x32_1_add_lsx(int16_t *input, uint8_t *dst,
 {
     int32_t i;
     int16_t out;
+    uint8_t *dst_tmp = dst + dst_stride;
     __m128i zero = __lsx_vldi(0);
     __m128i dst0, dst1, dst2, dst3, tmp0, tmp1, tmp2, tmp3;
     __m128i res0, res1, res2, res3, res4, res5, res6, res7, vec;
@@ -1265,7 +1286,7 @@ static void vp9_idct32x32_1_add_lsx(int16_t *input, uint8_t *dst,
 
     for (i = 16; i--;) {
         DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
-        DUP2_ARG2(__lsx_vld, dst + dst_stride, 0, dst + dst_stride, 16, dst2, dst3);
+        DUP2_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst2, dst3);
 
         DUP4_ARG2(__lsx_vilvl_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
                   res0, res1, res2, res3);
@@ -1282,10 +1303,10 @@ static void vp9_idct32x32_1_add_lsx(int16_t *input, uint8_t *dst,
 
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst, 16);
-        dst += dst_stride;
-        __lsx_vst(tmp2, dst, 0);
-        __lsx_vst(tmp3, dst, 16);
-        dst += dst_stride;
+        __lsx_vst(tmp2, dst_tmp, 0);
+        __lsx_vst(tmp3, dst_tmp, 16);
+        dst = dst_tmp + dst_stride;
+        dst_tmp = dst + dst_stride;
     }
 }
 
@@ -1388,4 +1409,3 @@ void ff_idct_idct_32x32_add_lsx(uint8_t *dst, ptrdiff_t stride,
         vp9_idct32x32_colcol_addblk_lsx(block, dst, stride);
     }
 }
-
diff --git a/libavcodec/loongarch/vp9_intra_lsx.c b/libavcodec/loongarch/vp9_intra_lsx.c
index b3d4f88158..d3f32646f3 100644
--- a/libavcodec/loongarch/vp9_intra_lsx.c
+++ b/libavcodec/loongarch/vp9_intra_lsx.c
@@ -24,28 +24,22 @@
 #include "vp9dsp_loongarch.h"
 
 #define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4,   \
-                 _dst5, _dst6, _dst7, _dst, _stride)  \
+                 _dst5, _dst6, _dst7, _dst, _stride,  \
+                 _stride2, _stride3, _stride4)        \
 {                                                     \
     __lsx_vst(_dst0, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst1, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst2, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst3, _dst, 0);                        \
-    _dst += _stride;                                  \
+    __lsx_vstx(_dst1, _dst, _stride);                 \
+    __lsx_vstx(_dst2, _dst, _stride2);                \
+    __lsx_vstx(_dst3, _dst, _stride3);                \
+    _dst += _stride4;                                 \
     __lsx_vst(_dst4, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst5, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst6, _dst, 0);                        \
-    _dst += _stride;                                  \
-    __lsx_vst(_dst7, _dst, 0);                        \
-    _dst += _stride;                                  \
+    __lsx_vstx(_dst5, _dst, _stride);                 \
+    __lsx_vstx(_dst6, _dst, _stride2);                \
+    __lsx_vstx(_dst7, _dst, _stride3);                \
 }
 
 #define LSX_ST_8X16(_dst0, _dst1, _dst2, _dst3, _dst4,   \
-                 _dst5, _dst6, _dst7, _dst, _stride)     \
+                    _dst5, _dst6, _dst7, _dst, _stride)  \
 {                                                        \
     __lsx_vst(_dst0, _dst, 0);                           \
     __lsx_vst(_dst0, _dst, 16);                          \
@@ -77,10 +71,15 @@ void ff_vert_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *left,
                        const uint8_t *src)
 {
     __m128i src0;
-
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
     src0 = __lsx_vld(src, 0);
-    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst, dst_stride);
-    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst, dst_stride);
+    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst,
+             dst_stride, stride2, stride3, stride4);
 }
 
 void ff_vert_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *left,
@@ -102,6 +101,9 @@ void ff_hor_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
 {
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i src8, src9, src10, src11, src12, src13, src14, src15;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
 
     src15 = __lsx_vldrepl_b(src, 0);
     src14 = __lsx_vldrepl_b(src, 1);
@@ -119,8 +121,11 @@ void ff_hor_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
     src2  = __lsx_vldrepl_b(src, 13);
     src1  = __lsx_vldrepl_b(src, 14);
     src0  = __lsx_vldrepl_b(src, 15);
-    LSX_ST_8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);
-    LSX_ST_8(src8, src9, src10, src11, src12, src13, src14, src15, dst, dst_stride);
+    LSX_ST_8(src0, src1, src2, src3, src4, src5, src6, src7, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(src8, src9, src10, src11, src12, src13, src14, src15, dst,
+             dst_stride, stride2, stride3, stride4);
 }
 
 void ff_hor_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
@@ -163,10 +168,14 @@ void ff_hor_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
     src2  = __lsx_vldrepl_b(src, 29);
     src1  = __lsx_vldrepl_b(src, 30);
     src0  = __lsx_vldrepl_b(src, 31);
-    LSX_ST_8X16(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);
-    LSX_ST_8X16(src8, src9, src10, src11, src12, src13, src14, src15, dst, dst_stride);
-    LSX_ST_8X16(src16, src17, src18, src19, src20, src21, src22, src23, dst, dst_stride);
-    LSX_ST_8X16(src24, src25, src26, src27, src28, src29, src30, src31, dst, dst_stride);
+    LSX_ST_8X16(src0, src1, src2, src3, src4, src5, src6, src7,
+                dst, dst_stride);
+    LSX_ST_8X16(src8, src9, src10, src11, src12, src13, src14, src15,
+                dst, dst_stride);
+    LSX_ST_8X16(src16, src17, src18, src19, src20, src21, src22, src23,
+                dst, dst_stride);
+    LSX_ST_8X16(src24, src25, src26, src27, src28, src29, src30, src31,
+                dst, dst_stride);
 }
 
 void ff_dc_4x4_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src_left,
@@ -282,6 +291,9 @@ void ff_dc_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
                      const uint8_t *src_left, const uint8_t *src_top)
 {
     __m128i tmp0, tmp1, dst0;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
 
     tmp0 = __lsx_vld(src_top, 0);
     tmp1 = __lsx_vld(src_left, 0);
@@ -292,8 +304,11 @@ void ff_dc_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
     dst0 = __lsx_vhaddw_qu_du(dst0, dst0);
     dst0 = __lsx_vsrari_w(dst0, 5);
     dst0 = __lsx_vreplvei_b(dst0, 0);
-    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
-    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,
+             dst_stride, stride2, stride3, stride4);
 }
 
 #define INTRA_DC_TL_16X16(dir)                                                \
@@ -302,6 +317,9 @@ void ff_dc_##dir##_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,              \
                              const uint8_t *top)                              \
 {                                                                             \
     __m128i tmp0, dst0;                                                       \
+    ptrdiff_t stride2 = dst_stride << 1;                                      \
+    ptrdiff_t stride3 = stride2 + dst_stride;                                 \
+    ptrdiff_t stride4 = stride2 << 1;                                         \
                                                                               \
     tmp0 = __lsx_vld(dir, 0);                                                 \
     dst0 = __lsx_vhaddw_hu_bu(tmp0, tmp0);                                    \
@@ -310,8 +328,11 @@ void ff_dc_##dir##_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,              \
     dst0 = __lsx_vhaddw_qu_du(dst0, dst0);                                    \
     dst0 = __lsx_vsrari_w(dst0, 4);                                           \
     dst0 = __lsx_vreplvei_b(dst0, 0);                                         \
-    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
-    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,             \
+             dst_stride, stride2, stride3, stride4);                          \
+    dst += stride4;                                                           \
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,             \
+             dst_stride, stride2, stride3, stride4);                          \
 }
 
 INTRA_DC_TL_16X16(top);
@@ -333,31 +354,39 @@ void ff_dc_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
     dst0 = __lsx_vhaddw_qu_du(dst0, dst0);
     dst0 = __lsx_vsrari_w(dst0, 6);
     dst0 = __lsx_vreplvei_b(dst0, 0);
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
 }
 
-#define INTRA_DC_TL_32X32(dir)                                                   \
-void ff_dc_##dir##_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,                 \
-                             const uint8_t *left,                                \
-                             const uint8_t *top)                                 \
-{                                                                                \
-    __m128i tmp0, tmp1, dst0;                                                    \
-                                                                                 \
-    DUP2_ARG2(__lsx_vld, dir, 0, dir, 16, tmp0, tmp1);                           \
-    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);           \
-    dst0 = __lsx_vadd_h(tmp0, tmp1);                                             \
-    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                       \
-    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                       \
-    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);                                       \
-    dst0 = __lsx_vsrari_w(dst0, 5);                                              \
-    dst0 = __lsx_vreplvei_b(dst0, 0);                                            \
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
-    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst, dst_stride);\
+#define INTRA_DC_TL_32X32(dir)                                               \
+void ff_dc_##dir##_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,             \
+                             const uint8_t *left,                            \
+                             const uint8_t *top)                             \
+{                                                                            \
+    __m128i tmp0, tmp1, dst0;                                                \
+                                                                             \
+    DUP2_ARG2(__lsx_vld, dir, 0, dir, 16, tmp0, tmp1);                       \
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);       \
+    dst0 = __lsx_vadd_h(tmp0, tmp1);                                         \
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                   \
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                   \
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);                                   \
+    dst0 = __lsx_vsrari_w(dst0, 5);                                          \
+    dst0 = __lsx_vreplvei_b(dst0, 0);                                        \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
 }
 
 INTRA_DC_TL_32X32(top);
@@ -368,9 +397,15 @@ void ff_dc_##val##_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,       \
                              const uint8_t *left, const uint8_t *top)  \
 {                                                                      \
     __m128i out = __lsx_vldi(val);                                     \
+    ptrdiff_t stride2 = dst_stride << 1;                               \
+    ptrdiff_t stride3 = stride2 + dst_stride;                          \
+    ptrdiff_t stride4 = stride2 << 1;                                  \
                                                                        \
-    LSX_ST_8(out, out, out, out, out, out, out, out, dst, dst_stride); \
-    LSX_ST_8(out, out, out, out, out, out, out, out, dst, dst_stride); \
+    LSX_ST_8(out, out, out, out, out, out, out, out, dst,              \
+             dst_stride, stride2, stride3, stride4);                   \
+    dst += stride4;                                                    \
+    LSX_ST_8(out, out, out, out, out, out, out, out, dst,              \
+             dst_stride, stride2, stride3, stride4);                   \
 }
 
 INTRA_PREDICT_VALDC_16X16_LSX(127);
@@ -449,8 +484,10 @@ void ff_tm_8x8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
               src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
               src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vpickev_b, src1, src0, src3, src2, src5, src4, src7, src6,
               src0, src1, src2, src3);
     __lsx_vstelm_d(src0, dst, 0, 0);
@@ -478,6 +515,9 @@ void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
     __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i reg0, reg1;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
 
     reg0 = __lsx_vreplgr2vr_h(top_left);
     reg1 = __lsx_vld(src_top_ptr, 0);
@@ -485,26 +525,28 @@ void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
               3, tmp15, tmp14, tmp13, tmp12);
     DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
               7, tmp11, tmp10, tmp9, tmp8);
-    DUP4_ARG2(__lsx_vldrepl_b, src_left, 8, src_left, 9, src_left, 10, src_left,
-              11, tmp7, tmp6, tmp5, tmp4);
-    DUP4_ARG2(__lsx_vldrepl_b, src_left, 12, src_left, 13, src_left, 14, src_left,
-              15, tmp3, tmp2, tmp1, tmp0);
-    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-              src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 8, src_left, 9, src_left, 10,
+              src_left, 11, tmp7, tmp6, tmp5, tmp4);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 12, src_left, 13, src_left, 14,
+              src_left, 15, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3,
+              reg1, src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
               src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
               src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-              tmp0, tmp1, tmp2, tmp3);
-    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
               src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
               src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7,
+              reg1, src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
               src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
@@ -515,32 +557,39 @@ void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
               src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
               tmp4, tmp5, tmp6, tmp7);
-    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
-              src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
-              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11,
+              reg1, src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
               src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
               src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-              tmp8, tmp9, tmp10, tmp11);
-    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
               src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
               src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp8, tmp9, tmp10, tmp11);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1,
+              tmp15, reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1,
+              tmp15, reg1, src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
               src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
               src4, src5, src6, src7);
-    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
-    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
     DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
               tmp12, tmp13, tmp14, tmp15);
-    LSX_ST_8(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, dst, dst_stride);
-    LSX_ST_8(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15, dst, dst_stride);
+    LSX_ST_8(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15, dst,
+             dst_stride, stride2, stride3, stride4);
 }
 
 void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
@@ -557,25 +606,25 @@ void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 
     src_left += 28;
     for (loop_cnt = 8; loop_cnt--;) {
-        DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left, 3,
-                  tmp3, tmp2, tmp1, tmp0);
+        DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2,
+                  src_left, 3, tmp3, tmp2, tmp1, tmp0);
         src_left -= 4;
-        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src4, src5, src6, src7);
-        DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
-                  dst0, dst1, dst2, dst3);
-        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
-                  dst4, dst5, dst6, dst7);
-        DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
-                  dst0, dst1, dst2, dst3);
-        DUP4_ARG2(__lsx_vssub_hu, dst4, reg0, dst5, reg0, dst6, reg0, dst7, reg0,
-                  dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1,
+                  tmp3, reg1, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1,
+                  tmp3, reg1, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3,
+                  reg0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7,
+                  reg0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2,
+                  tmp3, reg2, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2,
+                  tmp3, reg2, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3,
+                  reg0, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vssub_hu, dst4, reg0, dst5, reg0, dst6, reg0, dst7,
+                  reg0, dst4, dst5, dst6, dst7);
         DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
                   src0, src1, src2, src3);
         DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
@@ -584,10 +633,10 @@ void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
                   dst0, dst1, dst2, dst3);
         DUP4_ARG2(__lsx_vsat_hu, dst4, 7, dst5, 7, dst6, 7, dst7, 7,
                   dst4, dst5, dst6, dst7);
-        DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                  src0, src1, src2, src3);
-        DUP4_ARG2(__lsx_vpackev_b, dst4, dst0, dst5, dst1, dst6, dst2, dst7, dst3,
-                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7,
+                  src3, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vpackev_b, dst4, dst0, dst5, dst1, dst6, dst2, dst7,
+                  dst3, dst0, dst1, dst2, dst3);
         __lsx_vst(src0, dst, 0);
         __lsx_vst(dst0, dst, 16);
         dst += dst_stride;
diff --git a/libavcodec/loongarch/vp9_lpf_lsx.c b/libavcodec/loongarch/vp9_lpf_lsx.c
index 58619e6950..8e1915b888 100644
--- a/libavcodec/loongarch/vp9_lpf_lsx.c
+++ b/libavcodec/loongarch/vp9_lpf_lsx.c
@@ -24,43 +24,32 @@
 #include "libavutil/common.h"
 #include "vp9dsp_loongarch.h"
 
-#define LSX_LD_8(_src, _stride, _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7) \
+#define LSX_LD_8(_src, _stride, _stride2, _stride3, _stride4, _in0, _in1, _in2, \
+                 _in3, _in4, _in5, _in6, _in7)                                  \
 {                                                                               \
     _in0 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in1 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in2 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in3 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
+    _in1 = __lsx_vldx(_src, _stride);                                           \
+    _in2 = __lsx_vldx(_src, _stride2);                                          \
+    _in3 = __lsx_vldx(_src, _stride3);                                          \
+    _src += _stride4;                                                           \
     _in4 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in5 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in6 = __lsx_vld(_src, 0);                                                  \
-    _src += _stride;                                                            \
-    _in7 = __lsx_vld(_src, 0);                                                  \
+    _in5 = __lsx_vldx(_src, _stride);                                           \
+    _in6 = __lsx_vldx(_src, _stride2);                                          \
+    _in7 = __lsx_vldx(_src, _stride3);                                          \
 }
 
-#define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4, _dst5,                      \
-                  _dst6, _dst7, _dst, _stride)                                  \
+#define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4, _dst5, _dst6, _dst7,        \
+                 _dst, _stride, _stride2, _stride3, _stride4)                   \
 {                                                                               \
     __lsx_vst(_dst0, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst1, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst2, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst3, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
+    __lsx_vstx(_dst1, _dst, _stride);                                           \
+    __lsx_vstx(_dst2, _dst, _stride2);                                          \
+    __lsx_vstx(_dst3, _dst, _stride3);                                          \
+    _dst += _stride4;                                                           \
     __lsx_vst(_dst4, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst5, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst6, _dst, 0);                                                  \
-    _dst += _stride;                                                            \
-    __lsx_vst(_dst7, _dst, 0);                                                  \
+    __lsx_vstx(_dst5, _dst, _stride);                                           \
+    __lsx_vstx(_dst6, _dst, _stride2);                                          \
+    __lsx_vstx(_dst7, _dst, _stride3);                                          \
 }
 
 #define VP9_LPF_FILTER4_4W(p1_src, p0_src, q0_src, q1_src, mask_src, hev_src, \
@@ -248,10 +237,11 @@ void ff_loop_filter_v_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i mask, hev, flat, thresh, b_limit, limit;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0, p1_out, p0_out, q0_out, q1_out;
 
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -281,10 +271,11 @@ void ff_loop_filter_v_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i limit0, thresh1, b_limit1, limit1;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
 
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh0 = __lsx_vreplgr2vr_b(thresh_ptr);
     thresh1 = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -324,10 +315,11 @@ void ff_loop_filter_v_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i p3_l, p2_l, p1_l, p0_l, q3_l, q2_l, q1_l, q0_l;
     __m128i zero = __lsx_vldi(0);
 
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -399,10 +391,11 @@ void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -445,11 +438,11 @@ void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h, p1_filt8_l,
-                  p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l, p2_filt8_l,
-                  p1_filt8_l, p0_filt8_l, q0_filt8_l);
-        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h, q2_filt8_l,
-                  q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -460,12 +453,12 @@ void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
 
 
-        __lsx_vst(p2_out, dst - stride3, 0);
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
-        __lsx_vst(q2_out, dst + stride2, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
     }
 }
 
@@ -486,10 +479,11 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -514,10 +508,10 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
+        __lsx_vstx(q1_out, dst, stride);
     } else {
         DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
                   p3_l, p2_l, p1_l, p0_l);
@@ -527,11 +521,11 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* convert 16 bit output data into 8 bit */
-        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l, p1_filt8_l,
-                  p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l, p2_filt8_l, p1_filt8_l,
-                  p0_filt8_l, q0_filt8_l);
-        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l, q2_filt8_l,
-                  q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -541,12 +535,12 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
         q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
 
-        __lsx_vst(p2_out, dst - stride3, 0);
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
-        __lsx_vst(q2_out, dst + stride2, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
     }
 }
 
@@ -567,10 +561,11 @@ void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = { 0 };
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -595,10 +590,10 @@ void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
+        __lsx_vstx(q1_out, dst, stride);
     } else {
         DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
                   p3_h, p2_h, p1_h, p0_h);
@@ -622,12 +617,12 @@ void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_h, flat);
         q2_out = __lsx_vbitsel_v(q2, q2_filt8_h, flat);
 
-        __lsx_vst(p2_out, dst - stride3, 0);
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
-        __lsx_vst(q2_out, dst + stride2, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
     }
 }
 
@@ -652,10 +647,11 @@ static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -670,10 +666,10 @@ static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        __lsx_vst(p1_out, dst - stride2, 0);
-        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
         __lsx_vst(q0_out, dst, 0);
-        __lsx_vst(q1_out, dst + stride, 0);
+        __lsx_vstx(q1_out, dst, stride);
         return 1;
     } else {
         DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
@@ -717,7 +713,8 @@ static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
     }
 }
 
-static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride, uint8_t *filter48)
+static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride,
+                               uint8_t *filter48)
 {
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
@@ -740,14 +737,19 @@ static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride, uint8_t *filter48
 
     flat = __lsx_vld(filter48, 96);
 
-    DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
-              dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
-    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp + stride, 0, dst_tmp + stride2, 0,
-              dst_tmp + stride3, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3,
-              0, q0, q1, q2, q3);
-    DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2, 0,
-              dst_tmp1 + stride3, 0, q4, q5, q6, q7);
+    DUP4_ARG2(__lsx_vldx, dst_tmp, -stride4, dst_tmp, -stride3, dst_tmp,
+              -stride2, dst_tmp, -stride, p7, p6, p5, p4);
+    p3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp, stride3);
+
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    q4 = __lsx_vld(dst_tmp1, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp1, stride, dst_tmp1, stride2, q5, q6);
+    q7 = __lsx_vldx(dst_tmp1, stride3);
     VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
 
     /* if flat2 is zero for all pixels, then no need to calculate other filter */
@@ -756,12 +758,12 @@ static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride, uint8_t *filter48
                   48, p2, p1, p0, q0);
         DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
 
-        __lsx_vst(p2, dst - stride3, 0);
-        __lsx_vst(p1, dst - stride2, 0);
-        __lsx_vst(p0, dst - stride, 0);
+        __lsx_vstx(p2, dst, -stride3);
+        __lsx_vstx(p1, dst, -stride2);
+        __lsx_vstx(p0, dst, -stride);
         __lsx_vst(q0, dst, 0);
-        __lsx_vst(q1, dst + stride, 0);
-        __lsx_vst(q2, dst + stride2, 0);
+        __lsx_vstx(q1, dst, stride);
+        __lsx_vstx(q2, dst, stride2);
     } else {
         dst = dst_tmp - stride3;
 
@@ -1113,10 +1115,11 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i tmp0, tmp1, tmp2;
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-              dst - stride, 0, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-              dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -1164,8 +1167,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
         /* load 16 vector elements */
         DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
                   dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
-        DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2,
-                  0, dst_tmp1 + stride3, 0, q4, q5, q6, q7);
+        DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0,
+                dst_tmp1 + stride2, 0, dst_tmp1 + stride3, 0, q4, q5, q6, q7);
 
         VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
 
@@ -1352,15 +1355,18 @@ void ff_loop_filter_h_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst + stride4;
+    uint8_t *dst_tmp1 = dst - 4;
+    uint8_t *dst_tmp2 = dst_tmp1 + stride4;
     __m128i mask, hev, flat, limit, thresh, b_limit;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i vec0, vec1, vec2, vec3;
 
-    DUP4_ARG2(__lsx_vld, dst, -4, dst + stride, -4, dst + stride2, -4,
-              dst + stride3, -4, p3, p2, p1, p0);
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, q0, q1, q2, q3);
+    p3 = __lsx_vld(dst_tmp1, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp1, stride, dst_tmp1, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp1, stride3);
+    q0 = __lsx_vld(dst_tmp2, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp2, stride, dst_tmp2, stride2, q1, q2);
+    q3 = __lsx_vldx(dst_tmp2, stride3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -1395,7 +1401,7 @@ void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst;
+    uint8_t *dst_tmp = dst - 4;
     __m128i mask, hev, flat;
     __m128i thresh0, b_limit0, limit0, thresh1, b_limit1, limit1;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
@@ -1403,17 +1409,21 @@ void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i row8, row9, row10, row11, row12, row13, row14, row15;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
 
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row0, row1, row2, row3);
+    row0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row1, row2);
+    row3 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row4, row5, row6, row7);
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row8, row9, row10, row11);
+    row8 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row9, row10);
+    row11 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row12, row13, row14, row15);
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
 
     LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
                         row8, row9, row10, row11, row12, row13, row14, row15,
@@ -1471,7 +1481,7 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst;
+    uint8_t *dst_tmp = dst - 4;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i p1_out, p0_out, q0_out, q1_out;
     __m128i flat, mask, hev, thresh, b_limit, limit;
@@ -1482,11 +1492,13 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, p3, p2, p1, p0);
+    p3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, q0, q1, q2, q3);
+    q0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q1, q2);
+    q3 = __lsx_vldx(dst_tmp, stride3);
 
     LSX_TRANSPOSE8x8_B(p3, p2, p1, p0, q0, q1, q2, q3,
                        p3, p2, p1, p0, q0, q1, q2, q3);
@@ -1587,7 +1599,7 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst;
+    uint8_t *dst_tmp = dst - 4;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i p1_out, p0_out, q0_out, q1_out;
     __m128i flat, mask, hev, thresh, b_limit, limit;
@@ -1601,18 +1613,21 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i zero = __lsx_vldi(0);
 
-
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, p0, p1, p2, p3);
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row4, row5, row6, row7);
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, q3, q2, q1, q0);
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row12, row13, row14, row15);
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -1687,11 +1702,11 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h, p1_filt8_l,
-                  p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l, p2_filt8_l, p1_filt8_l,
-                  p0_filt8_l, q0_filt8_l);
-        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h, q2_filt8_l,
-                  q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -1769,7 +1784,7 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst;
+    uint8_t *dst_tmp = dst - 4;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i p1_out, p0_out, q0_out, q1_out;
     __m128i flat, mask, hev, thresh, b_limit, limit;
@@ -1780,17 +1795,21 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i zero = __lsx_vldi(0);
 
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, p0, p1, p2, p3);
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row4, row5, row6, row7);
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, q3, q2, q1, q0);
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row12, row13, row14, row15);
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -1940,7 +1959,7 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     ptrdiff_t stride2 = stride << 1;
     ptrdiff_t stride3 = stride2 + stride;
     ptrdiff_t stride4 = stride2 << 1;
-    uint8_t *dst_tmp = dst;
+    uint8_t *dst_tmp = dst - 4;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i p1_out, p0_out, q0_out, q1_out;
     __m128i flat, mask, hev, thresh, b_limit, limit;
@@ -1951,17 +1970,21 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i zero = __lsx_vldi(0);
 
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, p0, p1, p2, p3);
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row4, row5, row6, row7);
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, q3, q2, q1, q0);
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
     dst_tmp += stride4;
-    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-              dst_tmp + stride3, -4, row12, row13, row14, row15);
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -2104,14 +2127,17 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     }
 }
 
-static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
-                                       uint8_t *output, int32_t out_pitch)
+static void vp9_transpose_16x8_to_8x16(uint8_t *input, ptrdiff_t in_pitch,
+                                       uint8_t *output)
 {
     __m128i p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    ptrdiff_t in_pitch2 = in_pitch << 1;
+    ptrdiff_t in_pitch3 = in_pitch2 + in_pitch;
+    ptrdiff_t in_pitch4 = in_pitch2 << 1;
 
-    LSX_LD_8(input, in_pitch,
+    LSX_LD_8(input, in_pitch, in_pitch2, in_pitch3, in_pitch4,
              p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org);
     /* 8x8 transpose */
     LSX_TRANSPOSE8x8_B(p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org,
@@ -2125,23 +2151,45 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     DUP2_ARG2(__lsx_vilvh_w, tmp6, tmp4, tmp7, tmp5, q2, q6);
     DUP4_ARG2(__lsx_vbsrl_v, q0, 8, q2, 8, q4, 8, q6, 8, q1, q3, q5, q7);
 
-    LSX_ST_8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_pitch);
-    output += out_pitch;
-    LSX_ST_8(q0, q1, q2, q3, q4, q5, q6, q7, output, out_pitch);
+    __lsx_vst(p7, output, 0);
+    __lsx_vst(p6, output, 16);
+    __lsx_vst(p5, output, 32);
+    __lsx_vst(p4, output, 48);
+    __lsx_vst(p3, output, 64);
+    __lsx_vst(p2, output, 80);
+    __lsx_vst(p1, output, 96);
+    __lsx_vst(p0, output, 112);
+    __lsx_vst(q0, output, 128);
+    __lsx_vst(q1, output, 144);
+    __lsx_vst(q2, output, 160);
+    __lsx_vst(q3, output, 176);
+    __lsx_vst(q4, output, 192);
+    __lsx_vst(q5, output, 208);
+    __lsx_vst(q6, output, 224);
+    __lsx_vst(q7, output, 240);
 }
 
-static void vp9_transpose_8x16_to_16x8(uint8_t *input, int32_t in_pitch,
-                                       uint8_t *output, int32_t out_pitch)
+static void vp9_transpose_8x16_to_16x8(uint8_t *input, uint8_t *output,
+                                       ptrdiff_t out_pitch)
 {
     __m128i p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o;
     __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
-
-    LSX_LD_8(input, in_pitch, p7, p6, p5, p4, p3, p2, p1, p0);
-    input += in_pitch;
-    LSX_LD_8(input, in_pitch, q0, q1, q2, q3, q4, q5, q6, q7);
+    ptrdiff_t out_pitch2 = out_pitch << 1;
+    ptrdiff_t out_pitch3 = out_pitch2 + out_pitch;
+    ptrdiff_t out_pitch4 = out_pitch2 << 1;
+
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, input, 128, input, 144, input, 160, input, 176,
+              q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, input, 192, input, 208, input, 224, input, 240,
+              q4, q5, q6, q7);
     LSX_TRANSPOSE16x8_B(p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5,
                         q6, q7, p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o);
-    LSX_ST_8(p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o, output, out_pitch);
+    LSX_ST_8(p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o,
+             output, out_pitch, out_pitch2, out_pitch3, out_pitch4);
 }
 
 static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
@@ -2152,10 +2200,17 @@ static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
     __m128i tmp0, tmp1, tmp4, tmp5, tmp6, tmp7;
     __m128i tmp2, tmp3;
     __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
-
-    LSX_LD_8(input, in_stride, row0, row1, row2, row3, row4, row5, row6, row7);
-    input += in_stride;
-    LSX_LD_8(input, in_stride,
+    int32_t in_stride2 = in_stride << 1;
+    int32_t in_stride3 = in_stride2 + in_stride;
+    int32_t in_stride4 = in_stride2 << 1;
+    int32_t out_stride2 = out_stride << 1;
+    int32_t out_stride3 = out_stride2 + out_stride;
+    int32_t out_stride4 = out_stride2 << 1;
+
+    LSX_LD_8(input, in_stride, in_stride2, in_stride3, in_stride4,
+             row0, row1, row2, row3, row4, row5, row6, row7);
+    input += in_stride4;
+    LSX_LD_8(input, in_stride, in_stride2, in_stride3, in_stride4,
              row8, row9, row10, row11, row12, row13, row14, row15);
 
     LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
@@ -2197,9 +2252,11 @@ static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
     q3 = __lsx_vpackev_w(tmp3, tmp2);
     q7 = __lsx_vpackod_w(tmp3, tmp2);
 
-    LSX_ST_8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_stride);
-    output += out_stride;
-    LSX_ST_8(q0, q1, q2, q3, q4, q5, q6, q7, output, out_stride);
+    LSX_ST_8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_stride,
+             out_stride2, out_stride3, out_stride4);
+    output += out_stride4;
+    LSX_ST_8(q0, q1, q2, q3, q4, q5, q6, q7, output, out_stride,
+             out_stride2, out_stride3, out_stride4);
 }
 
 static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
@@ -2218,7 +2275,8 @@ static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, src, -64, src, -48, src, -32, src, -16, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, src, -64, src, -48, src, -32, src, -16,
+              p3, p2, p1, p0);
     DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
@@ -2560,7 +2618,7 @@ void ff_loop_filter_h_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
     uint8_t transposed_input[16 * 24] __attribute__ ((aligned(16)));
     uint8_t *filter48 = &transposed_input[16 * 16];
 
-    vp9_transpose_16x8_to_8x16(dst - 8, stride, transposed_input, 16);
+    vp9_transpose_16x8_to_8x16(dst - 8, stride, transposed_input);
 
     early_exit = vp9_vt_lpf_t4_and_t8_8w((transposed_input + 16 * 8),
                                          &filter48[0], dst, stride,
@@ -2571,7 +2629,7 @@ void ff_loop_filter_h_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
                                        &filter48[0]);
 
         if (0 == early_exit) {
-            vp9_transpose_8x16_to_16x8(transposed_input, 16, dst - 8, stride);
+            vp9_transpose_8x16_to_16x8(transposed_input, dst - 8, stride);
         }
     }
 }
@@ -2598,7 +2656,8 @@ static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *dst, uint8_t *filter48,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    DUP4_ARG2(__lsx_vld, dst, -64, dst, -48, dst, -32, dst, -16, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, -64, dst, -48, dst, -32, dst, -16,
+              p3, p2, p1, p0);
     DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
@@ -3072,12 +3131,11 @@ void ff_loop_filter_h_16_16_lsx(uint8_t *dst, ptrdiff_t stride,
                                           b_limit_ptr, limit_ptr, thresh_ptr);
 
     if (0 == early_exit) {
-        early_exit = vp9_vt_lpf_t16_16w((transposed_input + 16 * 8), dst, stride,
-                                        &filter48[0]);
+        early_exit = vp9_vt_lpf_t16_16w((transposed_input + 16 * 8), dst,
+                                         stride, &filter48[0]);
 
         if (0 == early_exit) {
             vp9_transpose_16x16(transposed_input, 16, (dst - 8), stride);
         }
     }
 }
-
diff --git a/libavcodec/loongarch/vp9_mc_lsx.c b/libavcodec/loongarch/vp9_mc_lsx.c
index 9d91675781..c6746fd87f 100644
--- a/libavcodec/loongarch/vp9_mc_lsx.c
+++ b/libavcodec/loongarch/vp9_mc_lsx.c
@@ -33,86 +33,89 @@ static const uint8_t mc_filt_mask_arr[16 * 3] = {
 };
 
 
-#define HORIZ_8TAP_4WID_4VECS_FILT(_src0, _src1, _src2, _src3,                         \
-                                   _mask0, _mask1, _mask2, _mask3,                     \
-                                   _filter0, _filter1, _filter2, _filter3,             \
-                                   _out0, _out1)                                       \
-{                                                                                      \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                    \
-    __m128i _reg0, _reg1, _reg2, _reg3;                                                \
-                                                                                       \
-    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src3, _src2, _mask0,               \
-              _tmp0, _tmp1);                                                           \
-    DUP2_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _reg0, _reg1);         \
-    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask1, _src3, _src2, _mask1, _tmp2, _tmp3);\
-    DUP2_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp2, _filter1, _reg1, _tmp3, _filter1,       \
-              _reg0, _reg1);                                                           \
-    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask2, _src3, _src2, _mask2, _tmp4, _tmp5);\
-    DUP2_ARG2(__lsx_vdp2_h_b, _tmp4, _filter2, _tmp5, _filter2, _reg2, _reg3);         \
-    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask3, _src3, _src2, _mask3, _tmp6, _tmp7);\
-    DUP2_ARG3(__lsx_vdp2add_h_b, _reg2, _tmp6, _filter3, _reg3, _tmp7, _filter3,       \
-              _reg2, _reg3);                                                           \
-    DUP2_ARG2(__lsx_vsadd_h, _reg0, _reg2, _reg1, _reg3, _out0, _out1);                \
+#define HORIZ_8TAP_4WID_4VECS_FILT(_src0, _src1, _src2, _src3,                 \
+                                   _mask0, _mask1, _mask2, _mask3,             \
+                                   _filter0, _filter1, _filter2, _filter3,     \
+                                   _out0, _out1)                               \
+{                                                                              \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;            \
+    __m128i _reg0, _reg1, _reg2, _reg3;                                        \
+                                                                               \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src3, _src2, _mask0,       \
+              _tmp0, _tmp1);                                                   \
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _reg0, _reg1); \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask1, _src3, _src2, _mask1,       \
+               _tmp2, _tmp3);                                                  \
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp2, _filter1, _reg1, _tmp3,         \
+              _filter1, _reg0, _reg1);                                         \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask2, _src3, _src2, _mask2,       \
+               _tmp4, _tmp5);                                                  \
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp4, _filter2, _tmp5, _filter2, _reg2, _reg3); \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask3, _src3, _src2, _mask3,       \
+               _tmp6, _tmp7);                                                  \
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg2, _tmp6, _filter3, _reg3, _tmp7,         \
+              _filter3, _reg2, _reg3);                                         \
+    DUP2_ARG2(__lsx_vsadd_h, _reg0, _reg2, _reg1, _reg3, _out0, _out1);        \
 }
 
-#define HORIZ_8TAP_8WID_4VECS_FILT(_src0, _src1, _src2, _src3,                        \
-                                   _mask0, _mask1, _mask2, _mask3,                    \
-                                   _filter0, _filter1, _filter2, _filter3,            \
-                                   _out0, _out1, _out2, _out3)                        \
-{                                                                                     \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                   \
-    __m128i _reg0, _reg1, _reg2, _reg3, _reg4, _reg5, _reg6, _reg7;                   \
-                                                                                      \
-    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask0, _src1, _src1, _mask0, _src2,       \
-              _src2, _mask0, _src3, _src3, _mask0, _tmp0, _tmp1, _tmp2, _tmp3);       \
-    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _tmp2, _filter0,      \
-              _tmp3, _filter0, _reg0, _reg1, _reg2, _reg3);                           \
-    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask2, _src1, _src1, _mask2, _src2,       \
-              _src2, _mask2, _src3, _src3, _mask2, _tmp0, _tmp1, _tmp2, _tmp3);       \
-    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter2, _tmp1, _filter2, _tmp2, _filter2,      \
-              _tmp3, _filter2, _reg4, _reg5, _reg6, _reg7);                           \
-    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask1, _src1, _src1, _mask1, _src2,       \
-              _src2, _mask1, _src3, _src3, _mask1, _tmp4, _tmp5, _tmp6, _tmp7);       \
-    DUP4_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp4, _filter1, _reg1, _tmp5, _filter1,      \
-              _reg2, _tmp6, _filter1, _reg3, _tmp7, _filter1, _reg0, _reg1, _reg2,    \
-              _reg3);                                                                 \
-    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask3, _src1, _src1, _mask3, _src2,       \
-              _src2, _mask3, _src3, _src3, _mask3, _tmp4, _tmp5, _tmp6, _tmp7);       \
-    DUP4_ARG3(__lsx_vdp2add_h_b, _reg4, _tmp4, _filter3, _reg5, _tmp5, _filter3,      \
-              _reg6, _tmp6, _filter3, _reg7, _tmp7, _filter3, _reg4, _reg5, _reg6,    \
-              _reg7);                                                                 \
-    DUP4_ARG2(__lsx_vsadd_h, _reg0, _reg4, _reg1, _reg5, _reg2, _reg6, _reg3, _reg7,  \
-              _out0, _out1, _out2, _out3);                                            \
+#define HORIZ_8TAP_8WID_4VECS_FILT(_src0, _src1, _src2, _src3,                 \
+                                   _mask0, _mask1, _mask2, _mask3,             \
+                                   _filter0, _filter1, _filter2, _filter3,     \
+                                   _out0, _out1, _out2, _out3)                 \
+{                                                                              \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;            \
+    __m128i _reg0, _reg1, _reg2, _reg3, _reg4, _reg5, _reg6, _reg7;            \
+                                                                               \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask0, _src1, _src1, _mask0, _src2,\
+              _src2, _mask0, _src3, _src3, _mask0, _tmp0, _tmp1, _tmp2, _tmp3);\
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _tmp2,         \
+              _filter0, _tmp3, _filter0, _reg0, _reg1, _reg2, _reg3);          \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask2, _src1, _src1, _mask2, _src2,\
+              _src2, _mask2, _src3, _src3, _mask2, _tmp0, _tmp1, _tmp2, _tmp3);\
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter2, _tmp1, _filter2, _tmp2,         \
+              _filter2, _tmp3, _filter2, _reg4, _reg5, _reg6, _reg7);          \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask1, _src1, _src1, _mask1, _src2,\
+              _src2, _mask1, _src3, _src3, _mask1, _tmp4, _tmp5, _tmp6, _tmp7);\
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp4, _filter1, _reg1, _tmp5,         \
+              _filter1, _reg2, _tmp6, _filter1, _reg3, _tmp7, _filter1, _reg0, \
+              _reg1, _reg2, _reg3);                                            \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask3, _src1, _src1, _mask3, _src2,\
+              _src2, _mask3, _src3, _src3, _mask3, _tmp4, _tmp5, _tmp6, _tmp7);\
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg4, _tmp4, _filter3, _reg5, _tmp5,         \
+              _filter3, _reg6, _tmp6, _filter3, _reg7, _tmp7, _filter3, _reg4, \
+              _reg5, _reg6, _reg7);                                            \
+    DUP4_ARG2(__lsx_vsadd_h, _reg0, _reg4, _reg1, _reg5, _reg2, _reg6, _reg3,  \
+              _reg7, _out0, _out1, _out2, _out3);                              \
 }
 
-#define FILT_8TAP_DPADD_S_H(_reg0, _reg1, _reg2, _reg3,                            \
-                             _filter0, _filter1, _filter2, _filter3)               \
-( {                                                                                \
-    __m128i _vec0, _vec1;                                                          \
-                                                                                   \
-    _vec0 = __lsx_vdp2_h_b(_reg0, _filter0);                                       \
-    _vec0 = __lsx_vdp2add_h_b(_vec0, _reg1, _filter1);                             \
-    _vec1 = __lsx_vdp2_h_b(_reg2, _filter2);                                       \
-    _vec1 = __lsx_vdp2add_h_b(_vec1, _reg3, _filter3);                             \
-    _vec0 = __lsx_vsadd_h(_vec0, _vec1);                                           \
-                                                                                   \
-    _vec0;                                                                         \
+#define FILT_8TAP_DPADD_S_H(_reg0, _reg1, _reg2, _reg3,                        \
+                             _filter0, _filter1, _filter2, _filter3)           \
+( {                                                                            \
+    __m128i _vec0, _vec1;                                                      \
+                                                                               \
+    _vec0 = __lsx_vdp2_h_b(_reg0, _filter0);                                   \
+    _vec0 = __lsx_vdp2add_h_b(_vec0, _reg1, _filter1);                         \
+    _vec1 = __lsx_vdp2_h_b(_reg2, _filter2);                                   \
+    _vec1 = __lsx_vdp2add_h_b(_vec1, _reg3, _filter3);                         \
+    _vec0 = __lsx_vsadd_h(_vec0, _vec1);                                       \
+                                                                               \
+    _vec0;                                                                     \
 } )
 
-#define HORIZ_8TAP_FILT(_src0, _src1, _mask0, _mask1, _mask2, _mask3,              \
-                        _filt_h0, _filt_h1, _filt_h2, _filt_h3)                    \
-( {                                                                                \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3;                                            \
-    __m128i _out;                                                                  \
-                                                                                   \
-    DUP4_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src1, _src0, _mask1, _src1,    \
-              _src0, _mask2, _src1, _src0, _mask3, _tmp0, _tmp1, _tmp2, _tmp3);    \
-    _out = FILT_8TAP_DPADD_S_H(_tmp0, _tmp1, _tmp2, _tmp3, _filt_h0, _filt_h1,     \
-                               _filt_h2, _filt_h3);                                \
-    _out = __lsx_vsrari_h(_out, 7);                                                \
-    _out = __lsx_vsat_h(_out, 7);                                                  \
-                                                                                   \
-    _out;                                                                          \
+#define HORIZ_8TAP_FILT(_src0, _src1, _mask0, _mask1, _mask2, _mask3,          \
+                        _filt_h0, _filt_h1, _filt_h2, _filt_h3)                \
+( {                                                                            \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3;                                        \
+    __m128i _out;                                                              \
+                                                                               \
+    DUP4_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src1, _src0, _mask1, _src1,\
+              _src0, _mask2, _src1, _src0, _mask3, _tmp0, _tmp1, _tmp2, _tmp3);\
+    _out = FILT_8TAP_DPADD_S_H(_tmp0, _tmp1, _tmp2, _tmp3, _filt_h0, _filt_h1, \
+                               _filt_h2, _filt_h3);                            \
+    _out = __lsx_vsrari_h(_out, 7);                                            \
+    _out = __lsx_vsat_h(_out, 7);                                              \
+                                                                               \
+    _out;                                                                      \
 } )
 
 #define LSX_LD_4(_src, _stride, _src0, _src1, _src2, _src3)               \
@@ -126,39 +129,6 @@ static const uint8_t mc_filt_mask_arr[16 * 3] = {
     _src3 = __lsx_vld(_src, 0);                                           \
 }
 
-#define LSX_ST_4(_dst0, _dst1, _dst2, _dst3, _dst, _stride)               \
-{                                                                         \
-    __lsx_vst(_dst0, _dst, 0);                                            \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst1, _dst, 0);                                            \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst2, _dst, 0);                                            \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst3, _dst, 0);                                            \
-}
-
-#define LSX_LD_4_16(_src, _stride, _src0, _src1, _src2, _src3)            \
-{                                                                         \
-    _src0 = __lsx_vld(_src, 16);                                          \
-    _src += _stride;                                                      \
-    _src1 = __lsx_vld(_src, 16);                                          \
-    _src += _stride;                                                      \
-    _src2 = __lsx_vld(_src, 16);                                          \
-    _src += _stride;                                                      \
-    _src3 = __lsx_vld(_src, 16);                                          \
-}
-
-#define LSX_ST_4_16(_dst0, _dst1, _dst2, _dst3, _dst, _stride)            \
-{                                                                         \
-    __lsx_vst(_dst0, _dst, 16);                                           \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst1, _dst, 16);                                           \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst2, _dst, 16);                                           \
-    _dst += _stride;                                                      \
-    __lsx_vst(_dst3, _dst, 16);                                           \
-}
-
 static void common_hz_8t_4x4_lsx(const uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter)
@@ -178,8 +148,8 @@ static void common_hz_8t_4x4_lsx(const uint8_t *src, int32_t src_stride,
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
               src0, src1, src2, src3);
-    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                               filter0, filter1, filter2, filter3, out0, out1);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out0, out1);
     out = __lsx_vssrarni_b_h(out1, out0, 7);
     out = __lsx_vxori_b(out, 128);
     __lsx_vstelm_w(out, dst, 0, 0);
@@ -195,34 +165,36 @@ static void common_hz_8t_4x8_lsx(const uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter)
 {
-    int32_t stride = src_stride << 1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
     __m128i src0, src1, src2, src3;
     __m128i filter0, filter1, filter2, filter3;
     __m128i mask0, mask1, mask2, mask3;
     __m128i out0, out1, out2, out3;
+    uint8_t *_src = (uint8_t*)src - 3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
-    src -= 3;
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
 
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src += stride;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
     DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
               src0, src1, src2, src3);
-    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                               filter0, filter1, filter2, filter3, out0, out1);
-    src += stride;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-    src += stride;
-    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out0, out1);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
     DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
               src0, src1, src2, src3);
-    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                               filter0, filter1, filter2, filter3, out2, out3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out2, out3);
     DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
     DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
     __lsx_vstelm_w(out0, dst, 0, 0);
@@ -272,8 +244,8 @@ static void common_hz_8t_8x4_lsx(const uint8_t *src, int32_t src_stride,
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
               src0, src1, src2, src3);
-    HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                               filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+    HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+         mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
     DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
     DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
     __lsx_vstelm_d(out0, dst, 0, 0);
@@ -290,27 +262,30 @@ static void common_hz_8t_8x8mult_lsx(const uint8_t *src, int32_t src_stride,
                                      const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt = height >> 2;
-    int32_t stride = src_stride << 1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
     __m128i src0, src1, src2, src3;
     __m128i filter0, filter1, filter2, filter3;
     __m128i mask0, mask1, mask2, mask3;
     __m128i out0, out1, out2, out3;
+    uint8_t* _src = (uint8_t*)src - 3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
-    src -= 3;
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-        src += stride;
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
@@ -321,7 +296,6 @@ static void common_hz_8t_8x8mult_lsx(const uint8_t *src, int32_t src_stride,
         dst += dst_stride;
         __lsx_vstelm_d(out1, dst, 0, 1);
         dst += dst_stride;
-        src += stride;
     }
 }
 
@@ -332,7 +306,8 @@ static void common_hz_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
     if (height == 4) {
         common_hz_8t_8x4_lsx(src, src_stride, dst, dst_stride, filter);
     } else {
-        common_hz_8t_8x8mult_lsx(src, src_stride, dst, dst_stride, filter, height);
+        common_hz_8t_8x8mult_lsx(src, src_stride, dst, dst_stride,
+                                 filter, height);
     }
 }
 
@@ -355,12 +330,13 @@ static void common_hz_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
               filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
-        DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
+        const uint8_t* _src = src + src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 8, _src, 8, src1, src3);
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
@@ -396,8 +372,8 @@ static void common_hz_8t_32w_lsx(const uint8_t *src, int32_t src_stride,
         src += src_stride;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
@@ -411,8 +387,8 @@ static void common_hz_8t_32w_lsx(const uint8_t *src, int32_t src_stride,
         dst += dst_stride;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
@@ -445,8 +421,8 @@ static void common_hz_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
         src1 = __lsx_vshuf_b(src2, src0, shuff);
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
@@ -457,8 +433,8 @@ static void common_hz_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
         src1 = __lsx_vshuf_b(src2, src0, shuff);
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 32);
@@ -473,23 +449,25 @@ static void common_vt_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
                                 const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt = height >> 2;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
     __m128i reg0, reg1, reg2, reg3, reg4;
     __m128i filter0, filter1, filter2, filter3;
     __m128i out0, out1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
     DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0,
               tmp1, tmp2, tmp3);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
@@ -499,14 +477,18 @@ static void common_vt_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
     reg2 = __lsx_vxori_b(reg2, 128);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0,
-                  tmp1, tmp2, tmp3);
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
         DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
         DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
-        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1, filter2, filter3);
-        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1, filter2, filter3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1,
+                                   filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1,
+                                   filter2, filter3);
         out0 = __lsx_vssrarni_b_h(out1, out0, 7);
         out0 = __lsx_vxori_b(out0, 128);
         __lsx_vstelm_w(out0, dst, 0, 0);
@@ -535,36 +517,47 @@ static void common_vt_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i reg0, reg1, reg2, reg3, reg4, reg5;
     __m128i filter0, filter1, filter2, filter3;
     __m128i out0, out1, out2, out3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg0, reg1, reg2, reg3);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  tmp0, tmp1, tmp2, tmp3);
-        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1, filter2, filter3);
-        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1, filter2, filter3);
-        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1, filter2, filter3);
-        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1, filter2, filter3);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1,
+                                   filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1,
+                                   filter2, filter3);
+        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1,
+                                   filter2, filter3);
+        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1,
+                                   filter2, filter3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
@@ -596,52 +589,64 @@ static void common_vt_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i reg0, reg1, reg2, reg3, reg4, reg5;
     __m128i reg6, reg7, reg8, reg9, reg10, reg11;
     __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1,
-              reg2, reg3);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg0, reg1, reg2, reg3);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
-    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1, reg6, reg7,
-              reg8, reg9);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg6, reg7, reg8, reg9);
     DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
         DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
                   src0, src1, src2, src3);
         DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
                   src4, src5, src7, src8);
-        tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
-        tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
-        tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
-        tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
+        tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1,
+                                   filter2, filter3);
+        tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1,
+                                   filter2, filter3);
+        tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1,
+                                   filter2, filter3);
+        tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1,
+                                   filter2, filter3);
         DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
         DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         dst += dst_stride;
         __lsx_vst(tmp1, dst, 0);
         dst += dst_stride;
-        tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0, filter1, filter2, filter3);
-        tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
-        tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
-        tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
+        tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0, filter1,
+                                   filter2, filter3);
+        tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1,
+                                   filter2, filter3);
+        tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1,
+                                   filter2, filter3);
+        tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1,
+                                   filter2, filter3);
         DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
         DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
@@ -670,7 +675,7 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
                                       const int8_t *filter, int32_t height,
                                       int32_t width)
 {
-    const uint8_t *src_tmp;
+    uint8_t *src_tmp;
     uint8_t *dst_tmp;
     uint32_t cnt = width >> 4;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
@@ -678,26 +683,34 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
     __m128i reg0, reg1, reg2, reg3, reg4, reg5;
     __m128i reg6, reg7, reg8, reg9, reg10, reg11;
     __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
     for (;cnt--;) {
         uint32_t loop_cnt = height >> 2;
 
-        src_tmp = src;
+        src_tmp = _src;
         dst_tmp = dst;
 
-        LSX_LD_4(src_tmp, src_stride, src0, src1, src2, src3);
-        src_tmp += src_stride;
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride3);
+        src_tmp += src_stride4;
         src4 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        src5 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        src6 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src5, src6);
+        src_tmp += src_stride3;
+
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
         DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
@@ -708,34 +721,44 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
         DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
         for (;loop_cnt--;) {
-            LSX_LD_4(src_tmp, src_stride, src7, src8, src9, src10);
-            src_tmp += src_stride;
-            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src0, src1, src2, src3);
-            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src4, src5, src7, src8);
-            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
-            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
-            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
-            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
-            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride3);
+            src_tmp += src_stride4;
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                      128, src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src4, src5, src7, src8);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
             DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             __lsx_vst(tmp0, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            __lsx_vst(tmp1, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0, filter1, filter2, filter3);
-            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
-            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
-            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
-            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            __lsx_vstx(tmp1, dst_tmp, dst_stride);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
             DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-            __lsx_vst(tmp0, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            __lsx_vst(tmp1, dst_tmp, 0);
-            dst_tmp += dst_stride;
+            __lsx_vstx(tmp0, dst_tmp, dst_stride2);
+            __lsx_vstx(tmp1, dst_tmp, dst_stride3);
+            dst_tmp += dst_stride4;
 
             reg0 = reg2;
             reg1 = src0;
@@ -751,8 +774,8 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
             reg11 = src8;
             src6 = src10;
         }
-        src += 16;
-        dst += 16;
+        _src += 16;
+        dst  += 16;
     }
 }
 
@@ -767,7 +790,8 @@ static void common_vt_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
                                  uint8_t *dst, int32_t dst_stride,
                                  const int8_t *filter, int32_t height)
 {
-    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height, 64);
+    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride,
+                              filter, height, 64);
 }
 
 static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
@@ -784,23 +808,26 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
     __m128i out0, out1;
     __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3 - 3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
-    src -= (3 + 3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
               filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
@@ -819,8 +846,10 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
     tmp2 = __lsx_vpackev_b(tmp5, tmp4);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
         tmp3 = HORIZ_8TAP_FILT(src7, src8, mask0, mask1, mask2, mask3, filt_hz0,
@@ -829,8 +858,8 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
         tmp4 = __lsx_vpackev_b(tmp3, tmp4);
         out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp4, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
-        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3, filt_hz0,
-                               filt_hz1, filt_hz2, filt_hz3);
+        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3,
+                               filt_hz0, filt_hz1, filt_hz2, filt_hz3);
         src0 = __lsx_vshuf_b(src1, tmp3, shuff);
         src0 = __lsx_vpackev_b(src1, src0);
         out1 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp4, src0, filt_vt0, filt_vt1,
@@ -866,24 +895,26 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i mask0, mask1, mask2, mask3;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
     __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3 - 3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
-    src -= (3 + 3 * src_stride);
-    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-              src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
@@ -909,8 +940,11 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
     DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
         src7 = HORIZ_8TAP_FILT(src7, src7, mask0, mask1, mask2, mask3, filt_hz0,
@@ -928,8 +962,8 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
         src1 = __lsx_vpackev_b(src9, src8);
         src3 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp3, src1, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
-        src10 = HORIZ_8TAP_FILT(src10, src10, mask0, mask1, mask2, mask3, filt_hz0,
-                               filt_hz1, filt_hz2, filt_hz3);
+        src10 = HORIZ_8TAP_FILT(src10, src10, mask0, mask1, mask2, mask3,
+                                filt_hz0, filt_hz1, filt_hz2, filt_hz3);
         src2 = __lsx_vpackev_b(src10, src9);
         src4 = FILT_8TAP_DPADD_S_H(tmp5, tmp6, src0, src2, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
@@ -1035,12 +1069,24 @@ static void copy_width16_lsx(const uint8_t *src, int32_t src_stride,
 {
     int32_t cnt = height >> 2;
     __m128i src0, src1, src2, src3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src;
 
     for (;cnt--;) {
-        LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-        src += src_stride;
-        LSX_ST_4(src0, src1, src2, src3, dst, dst_stride);
-        dst += dst_stride;
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        __lsx_vst(src0, dst, 0);
+        __lsx_vstx(src1, dst, dst_stride);
+        __lsx_vstx(src2, dst, dst_stride2);
+        __lsx_vstx(src3, dst, dst_stride3);
+        dst += dst_stride4;
     }
 }
 
@@ -1049,20 +1095,41 @@ static void copy_width32_lsx(const uint8_t *src, int32_t src_stride,
                              int32_t height)
 {
     int32_t cnt = height >> 2;
-    const uint8_t *src_tmp = src;
-    uint8_t *dst_tmp = dst;
+    uint8_t *src_tmp1 = (uint8_t*)src;
+    uint8_t *dst_tmp1 = dst;
+    uint8_t *src_tmp2 = src_tmp1 + 16;
+    uint8_t *dst_tmp2 = dst_tmp1 + 16;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
 
     for (;cnt--;) {
-        LSX_LD_4(src_tmp, src_stride, src0, src1, src2, src3);
-        src_tmp += src_stride;
-
-        LSX_LD_4_16(src, src_stride, src4, src5, src6, src7);
-        src += src_stride;
-        LSX_ST_4(src0, src1, src2, src3, dst_tmp, dst_stride);
-        dst_tmp += dst_stride;
-        LSX_ST_4_16(src4, src5, src6, src7, dst, dst_stride);
-        dst += dst_stride;
+        src0 = __lsx_vld(src_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp1, src_stride, src_tmp1, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp1, src_stride3);
+        src_tmp1 += src_stride4;
+
+        src4 = __lsx_vld(src_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp2, src_stride, src_tmp2, src_stride2,
+                  src5, src6);
+        src7 = __lsx_vldx(src_tmp2, src_stride3);
+        src_tmp2 += src_stride4;
+
+        __lsx_vst(src0, dst_tmp1, 0);
+        __lsx_vstx(src1, dst_tmp1, dst_stride);
+        __lsx_vstx(src2, dst_tmp1, dst_stride2);
+        __lsx_vstx(src3, dst_tmp1, dst_stride3);
+        dst_tmp1 += dst_stride4;
+        __lsx_vst(src4, dst_tmp2, 0);
+        __lsx_vstx(src5, dst_tmp2, dst_stride);
+        __lsx_vstx(src6, dst_tmp2, dst_stride2);
+        __lsx_vstx(src7, dst_tmp2, dst_stride3);
+        dst_tmp2 += dst_stride4;
     }
 }
 
@@ -1075,13 +1142,17 @@ static void copy_width64_lsx(const uint8_t *src, int32_t src_stride,
     __m128i src8, src9, src10, src11, src12, src13, src14, src15;
 
     for (;cnt--;) {
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
         src += src_stride;
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
         src += src_stride;
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src8, src9, src10, src11);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src8, src9, src10, src11);
         src += src_stride;
-        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src12, src13, src14, src15);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src12, src13, src14, src15);
         src += src_stride;
         __lsx_vst(src0, dst, 0);
         __lsx_vst(src1, dst, 16);
@@ -1248,21 +1319,26 @@ static void common_hz_8t_and_aver_dst_8w_lsx(const uint8_t *src,
     __m128i mask0, mask1, mask2, mask3;
     __m128i tmp0, tmp1, tmp2, tmp3;
     __m128i dst0, dst1, dst2, dst3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride2 + src_stride;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src - 3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
-    src -= 3;
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-        src += src_stride;
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, tmp0, tmp1, tmp2, tmp3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+              mask3,filter0, filter1, filter2, filter3, tmp0, tmp1, tmp2, tmp3);
         dst0 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
         dst1 = __lsx_vldrepl_d(dst_tmp, 0);
@@ -1293,6 +1369,7 @@ static void common_hz_8t_and_aver_dst_16w_lsx(const uint8_t *src,
                                               int32_t height)
 {
     int32_t loop_cnt = height >> 1;
+    int32_t dst_stride2 = dst_stride << 1;
     uint8_t *dst_tmp = dst;
     __m128i src0, src1, src2, src3, filter0, filter1, filter2, filter3;
     __m128i mask0, mask1, mask2, mask3, dst0, dst1, dst2, dst3;
@@ -1312,9 +1389,8 @@ static void common_hz_8t_and_aver_dst_16w_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
         src += src_stride;
         dst0 = __lsx_vld(dst_tmp, 0);
-        dst_tmp += dst_stride;
-        dst1 = __lsx_vld(dst_tmp, 0);
-        dst_tmp += dst_stride;
+        dst1 = __lsx_vldx(dst_tmp, dst_stride);
+        dst_tmp += dst_stride2;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
         DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
@@ -1339,9 +1415,8 @@ static void common_hz_8t_and_aver_dst_16w_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vxori_b, dst2, 128, dst3, 128, dst2, dst3);
         DUP2_ARG2(__lsx_vavgr_bu, dst0, dst2, dst1, dst3, dst0, dst1);
         __lsx_vst(dst0, dst, 0);
-        dst += dst_stride;
-        __lsx_vst(dst1, dst, 0);
-        dst += dst_stride;
+        __lsx_vstx(dst1, dst, dst_stride);
+        dst += dst_stride2;
     }
 }
 
@@ -1375,22 +1450,22 @@ static void common_hz_8t_and_aver_dst_32w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
-                  mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
-                  mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
-                  mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
-        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
-                  mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
-        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
-                  filter0, tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
-                  filter2, tmp8, tmp9, tmp10, tmp11);
-        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
-                  tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
-        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
-                  tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2,
+                  src2, mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2,
+                  src2, mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2,
+                  src2, mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2,
+                  src2, mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0,
+                  tmp3, filter0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2,
+                  tmp11, filter2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1,
+             tmp2, tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3,
+        tmp10, tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
         DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
                   tmp0, tmp1, tmp2, tmp3);
         DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
@@ -1429,8 +1504,8 @@ static void common_hz_8t_and_aver_dst_64w_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
@@ -1443,8 +1518,8 @@ static void common_hz_8t_and_aver_dst_64w_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vld, dst, 32, dst, 48, dst0, dst1);
         DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
                   src0, src1, src2, src3);
-        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
-                                   filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
@@ -1468,20 +1543,22 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
     __m128i reg0, reg1, reg2, reg3, reg4;
     __m128i filter0, filter1, filter2, filter3;
     __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0, tmp1,
-              tmp2, tmp3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              tmp0, tmp1, tmp2, tmp3);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
     DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
     reg2 = __lsx_vilvl_d(tmp5, tmp2);
@@ -1489,8 +1566,10 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
     reg2 = __lsx_vxori_b(reg2, 128);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         src0 = __lsx_vldrepl_w(dst_tmp, 0);
         dst_tmp += dst_stride;
         src1 = __lsx_vldrepl_w(dst_tmp, 0);
@@ -1501,12 +1580,14 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
         src0 = __lsx_vilvl_d(src1, src0);
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0,
-                  tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
         DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
         DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
-        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1, filter2, filter3);
-        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1, filter2, filter3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0,
+                                   filter1, filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0,
+                                   filter1, filter2, filter3);
         out0 = __lsx_vssrarni_b_h(out1, out0, 7);
         out0 = __lsx_vxori_b(out0, 128);
         out0 = __lsx_vavgr_bu(out0, src0);
@@ -1538,28 +1619,34 @@ static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
     __m128i reg0, reg1, reg2, reg3, reg4, reg5;
     __m128i filter0, filter1, filter2, filter3;
     __m128i out0, out1, out2, out3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2,
+              src1, reg0, reg1, reg2, reg3);
     DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         src0 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
         src1 = __lsx_vldrepl_d(dst_tmp, 0);
@@ -1571,12 +1658,16 @@ static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vilvl_d, src1, src0, src3, src2, src0, src1);
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
-        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                  tmp0, tmp1, tmp2, tmp3);
-        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1, filter2, filter3);
-        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1, filter2, filter3);
-        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1, filter2, filter3);
-        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1, filter2, filter3);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0,
+                                   filter1, filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0,
+                                   filter1, filter2, filter3);
+        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0,
+                                   filter1, filter2, filter3);
+        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0,
+                                   filter1, filter2, filter3);
         DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
         DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         DUP2_ARG2(__lsx_vavgr_bu, out0, src0, out1, src1, out0, out1);
@@ -1607,34 +1698,39 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
                                                    int32_t height,
                                                    int32_t width)
 {
-    const uint8_t *src_tmp;
-    uint8_t *dst_tmp;
+    uint8_t *src_tmp;
     uint32_t cnt = width >> 4;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     __m128i filter0, filter1, filter2, filter3;
     __m128i reg0, reg1, reg2, reg3, reg4, reg5;
     __m128i reg6, reg7, reg8, reg9, reg10, reg11;
     __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src - src_stride3;
 
-    src -= (3 * src_stride);
     DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
               filter0, filter1, filter2, filter3);
     for (;cnt--;) {
         uint32_t loop_cnt = height >> 2;
         uint8_t *dst_reg = dst;
 
-        src_tmp = src;
-        dst_tmp = dst;
-
-        LSX_LD_4(src_tmp, src_stride, src0, src1, src2, src3);
-        src_tmp += src_stride;
+        src_tmp = _src;
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride3);
+        src_tmp += src_stride4;
         src4 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        src5 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        src6 = __lsx_vld(src_tmp, 0);
-        src_tmp += src_stride;
-        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src5, src6);
+        src_tmp += src_stride3;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
         DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
@@ -1645,44 +1741,50 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
         DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
         for (;loop_cnt--;) {
-            LSX_LD_4(src_tmp, src_stride, src7, src8, src9, src10);
-            src_tmp += src_stride;
-            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src0, src1, src2, src3);
-            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src4, src5, src7, src8);
-            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
-            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
-            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
-            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
-            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride3);
+            src_tmp += src_stride4;
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                      128, src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src4, src5, src7, src8);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
             DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             tmp2 = __lsx_vld(dst_reg, 0);
-            dst_reg += dst_stride;
-            tmp3 = __lsx_vld(dst_reg, 0);
-            dst_reg += dst_stride;
+            tmp3 = __lsx_vldx(dst_reg, dst_stride);
             DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
-            __lsx_vst(tmp0, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            __lsx_vst(tmp1, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0, filter1, filter2, filter3);
-            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
-            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
-            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
-            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            __lsx_vst(tmp0, dst_reg, 0);
+            __lsx_vstx(tmp1, dst_reg, dst_stride);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
             DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-            tmp2 = __lsx_vld(dst_reg, 0);
-            dst_reg += dst_stride;
-            tmp3 = __lsx_vld(dst_reg, 0);
-            dst_reg += dst_stride;
+            tmp2 = __lsx_vldx(dst_reg, dst_stride2);
+            tmp3 = __lsx_vldx(dst_reg, dst_stride3);
             DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
-            __lsx_vst(tmp0, dst_tmp, 0);
-            dst_tmp += dst_stride;
-            __lsx_vst(tmp1, dst_tmp, 0);
-            dst_tmp += dst_stride;
+            __lsx_vstx(tmp0, dst_reg, dst_stride2);
+            __lsx_vstx(tmp1, dst_reg, dst_stride3);
+            dst_reg += dst_stride4;
 
             reg0 = reg2;
             reg1 = src0;
@@ -1698,8 +1800,8 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
             reg11 = src8;
             src6 = src10;
         }
-        src += 16;
-        dst += 16;
+        _src += 16;
+        dst  += 16;
     }
 }
 
@@ -1750,23 +1852,27 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
     __m128i out0, out1;
     __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - 3 - src_stride3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
-    src -= (3 + 3 * src_stride);
-    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
@@ -1785,8 +1891,10 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
     tmp2 = __lsx_vpackev_b(tmp5, tmp4);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
         src2 = __lsx_vldrepl_w(dst_tmp, 0);
         dst_tmp += dst_stride;
         src3 = __lsx_vldrepl_w(dst_tmp, 0);
@@ -1805,8 +1913,8 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
         tmp4 = __lsx_vpackev_b(tmp3, tmp4);
         out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp4, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
-        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3, filt_hz0,
-                               filt_hz1, filt_hz2, filt_hz3);
+        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3,
+                               filt_hz0, filt_hz1, filt_hz2, filt_hz3);
         src0 = __lsx_vshuf_b(src1, tmp3, shuff);
         src0 = __lsx_vpackev_b(src1, src0);
         out1 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp4, src0, filt_vt0, filt_vt1,
@@ -1846,23 +1954,26 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
     __m128i mask0, mask1, mask2, mask3;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
     __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - 3 - src_stride3;
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
-    src -= (3 + 3 * src_stride);
-    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
     DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
-    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += src_stride;
-    src4 = __lsx_vld(src, 0);
-    src += src_stride;
-    src5 = __lsx_vld(src, 0);
-    src += src_stride;
-    src6 = __lsx_vld(src, 0);
-    src += src_stride;
-    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
@@ -1888,8 +1999,10 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
     DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
 
     for (;loop_cnt--;) {
-        LSX_LD_4(src, src_stride, src7, src8, src9, src10);
-        src += src_stride;
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
 
         DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
                   src7, src8, src9, src10);
@@ -2049,19 +2162,33 @@ static void avg_width16_lsx(const uint8_t *src, int32_t src_stride,
                             int32_t height)
 {
     int32_t cnt = height >> 2;
-    uint8_t *dst_tmp = dst;
     __m128i src0, src1, src2, src3;
     __m128i dst0, dst1, dst2, dst3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src;
 
     for (;cnt--;) {
-        LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-        src += src_stride;
-        LSX_LD_4(dst_tmp, dst_stride, dst0, dst1, dst2, dst3);
-        dst_tmp += dst_stride;
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+
+        dst0 = __lsx_vld(dst, 0);
+        DUP2_ARG2(__lsx_vldx, dst, dst_stride, dst, dst_stride2,
+                  dst1, dst2);
+        dst3 = __lsx_vldx(dst, dst_stride3);
         DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
                   src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
-        LSX_ST_4(dst0, dst1, dst2, dst3, dst, dst_stride);
-        dst += dst_stride;
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride);
+        __lsx_vstx(dst2, dst, dst_stride2);
+        __lsx_vstx(dst3, dst, dst_stride3);
+        dst += dst_stride4;
     }
 }
 
@@ -2070,29 +2197,56 @@ static void avg_width32_lsx(const uint8_t *src, int32_t src_stride,
                             int32_t height)
 {
     int32_t cnt = height >> 2;
-    const uint8_t *src_tmp = src;
-    uint8_t *dst_tmp, *dst_tmp1, *dst_tmp2;
+    uint8_t *src_tmp1 = (uint8_t*)src;
+    uint8_t *src_tmp2 = src_tmp1 + 16;
+    uint8_t *dst_tmp1, *dst_tmp2;
     __m128i src0, src1, src2, src3, src4, src5, src6, src7;
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-
-    dst_tmp = dst_tmp1 = dst_tmp2 = dst;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+
+    dst_tmp1 = dst;
+    dst_tmp2 = dst + 16;
     for (;cnt--;) {
-        LSX_LD_4(src, src_stride, src0, src2, src4, src6);
-        src += src_stride;
-        LSX_LD_4_16(src_tmp, src_stride, src1, src3, src5, src7);
-        src_tmp += src_stride;
-        LSX_LD_4(dst_tmp1, dst_stride, dst0, dst2, dst4, dst6);
-        dst_tmp1 += dst_stride;
-        LSX_LD_4_16(dst_tmp2, dst_stride, dst1, dst3, dst5, dst7);
-        dst_tmp2 += dst_stride;
+        src0 = __lsx_vld(src_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp1, src_stride, src_tmp1, src_stride2,
+                  src2, src4);
+        src6 = __lsx_vldx(src_tmp1, src_stride3);
+        src_tmp1 += src_stride4;
+
+        src1 = __lsx_vld(src_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp2, src_stride, src_tmp2, src_stride2,
+                  src3, src5);
+        src7 = __lsx_vldx(src_tmp2, src_stride3);
+        src_tmp2 += src_stride4;
+
+        dst0 = __lsx_vld(dst_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, dst_tmp1, dst_stride, dst_tmp1, dst_stride2,
+                  dst2, dst4);
+        dst6 = __lsx_vldx(dst_tmp1, dst_stride3);
+        dst1 = __lsx_vld(dst_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, dst_tmp2, dst_stride, dst_tmp2, dst_stride2,
+                  dst3, dst5);
+        dst7 = __lsx_vldx(dst_tmp2, dst_stride3);
+
         DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
                   src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
         DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
                   src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
-        LSX_ST_4(dst0, dst2, dst4, dst6, dst_tmp, dst_stride);
-        dst_tmp += dst_stride;
-        LSX_ST_4_16(dst1, dst3, dst5, dst7, dst, dst_stride);
-        dst += dst_stride;
+        __lsx_vst(dst0, dst_tmp1, 0);
+        __lsx_vstx(dst2, dst_tmp1, dst_stride);
+        __lsx_vstx(dst4, dst_tmp1, dst_stride2);
+        __lsx_vstx(dst6, dst_tmp1, dst_stride3);
+        dst_tmp1 += dst_stride4;
+        __lsx_vst(dst1, dst_tmp2, 0);
+        __lsx_vstx(dst3, dst_tmp2, dst_stride);
+        __lsx_vstx(dst5, dst_tmp2, dst_stride2);
+        __lsx_vstx(dst7, dst_tmp2, dst_stride3);
+        dst_tmp2 += dst_stride4;
     }
 }
 
diff --git a/libavcodec/loongarch/vp9dsp_init_loongarch.c b/libavcodec/loongarch/vp9dsp_init_loongarch.c
index f85abde11d..e49625ad5f 100644
--- a/libavcodec/loongarch/vp9dsp_init_loongarch.c
+++ b/libavcodec/loongarch/vp9dsp_init_loongarch.c
@@ -37,10 +37,10 @@
     init_subpel1(1, idx, idxh, idxv, 32, dir, type);  \
     init_subpel1(2, idx, idxh, idxv, 16, dir, type);  \
     init_subpel1(3, idx, idxh, idxv,  8, dir, type);  \
-    init_subpel1(4, idx, idxh, idxv,  4, dir, type)
+    init_subpel1(4, idx, idxh, idxv,  4, dir, type);
 
 #define init_subpel3(idx, type)         \
-    init_subpel2(idx, 1, 0, h, type)    \
+    init_subpel2(idx, 1, 0, h, type);   \
     init_subpel2(idx, 0, 1, v, type);   \
     init_subpel2(idx, 1, 1, hv, type);
 
@@ -48,32 +48,11 @@
     dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][0][0] = ff_##type##sz##_lsx;  \
     dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][0][0] = ff_##type##sz##_lsx;  \
     dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][0][0] = ff_##type##sz##_lsx;  \
-    dsp->mc[idx1][FILTER_BILINEAR    ][idx2][0][0] = ff_##type##sz##_lsx
+    dsp->mc[idx1][FILTER_BILINEAR    ][idx2][0][0] = ff_##type##sz##_lsx;
 
 #define init_copy(idx, sz)                    \
     init_fpel(idx, 0, sz, copy);              \
-    init_fpel(idx, 1, sz, avg)
-
-static av_cold void vp9dsp_mc_init_lsx(VP9DSPContext *dsp, int bpp)
-{
-    int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags)) {
-        if (bpp == 8) {
-            init_subpel3(0, put);
-            init_subpel3(1, avg);
-            init_copy(0, 64);
-            init_copy(1, 32);
-            init_copy(2, 16);
-            init_copy(3, 8);
-        }
-    }
-}
-
-#undef init_subpel1
-#undef init_subpel2
-#undef init_subpel3
-#undef init_copy
-#undef init_fpel
+    init_fpel(idx, 1, sz, avg);
 
 #define init_intra_pred1_lsx(tx, sz)                            \
     dsp->intra_pred[tx][VERT_PRED]    = ff_vert_##sz##_lsx;     \
@@ -92,51 +71,33 @@ static av_cold void vp9dsp_mc_init_lsx(VP9DSPContext *dsp, int bpp)
     dsp->intra_pred[tx][TOP_DC_PRED]  = ff_dc_top_##sz##_lsx;   \
     dsp->intra_pred[tx][TM_VP8_PRED]  = ff_tm_##sz##_lsx;       \
 
-static av_cold void vp9dsp_intrapred_init_lsx(VP9DSPContext *dsp, int bpp)
-{
-    int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags)) {
-        if (bpp == 8) {
-            init_intra_pred1_lsx(TX_16X16, 16x16);
-            init_intra_pred1_lsx(TX_32X32, 32x32);
-            init_intra_pred2_lsx(TX_4X4, 4x4);
-            init_intra_pred2_lsx(TX_8X8, 8x8);
-        }
-    }
-}
-
-#undef init_intra_pred_lsx
-#undef init_intra_pred_lsx
-
 #define init_idct(tx, nm)                        \
     dsp->itxfm_add[tx][DCT_DCT]   =              \
     dsp->itxfm_add[tx][ADST_DCT]  =              \
     dsp->itxfm_add[tx][DCT_ADST]  =              \
-    dsp->itxfm_add[tx][ADST_ADST] = nm##_add_lsx
+    dsp->itxfm_add[tx][ADST_ADST] = nm##_add_lsx;
 
 #define init_itxfm(tx, sz)                                     \
-    dsp->itxfm_add[tx][DCT_DCT] = ff_idct_idct_##sz##_add_lsx
+    dsp->itxfm_add[tx][DCT_DCT] = ff_idct_idct_##sz##_add_lsx;
 
-static av_cold void vp9dsp_itxfm_init_lsx(VP9DSPContext *dsp, int bpp)
+av_cold void ff_vp9dsp_init_loongarch(VP9DSPContext *dsp, int bpp)
 {
     int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags)) {
+    if (have_lsx(cpu_flags))
         if (bpp == 8) {
+            init_subpel3(0, put);
+            init_subpel3(1, avg);
+            init_copy(0, 64);
+            init_copy(1, 32);
+            init_copy(2, 16);
+            init_copy(3, 8);
+            init_intra_pred1_lsx(TX_16X16, 16x16);
+            init_intra_pred1_lsx(TX_32X32, 32x32);
+            init_intra_pred2_lsx(TX_4X4, 4x4);
+            init_intra_pred2_lsx(TX_8X8, 8x8);
             init_itxfm(TX_8X8, 8x8);
             init_itxfm(TX_16X16, 16x16);
             init_idct(TX_32X32, ff_idct_idct_32x32);
-        }
-    }
-}
-
-#undef init_idct
-#undef init_itxfm
-
-static av_cold void vp9dsp_loopfilter_init_lsx(VP9DSPContext *dsp, int bpp)
-{
-    int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags)) {
-        if (bpp == 8) {
             dsp->loop_filter_8[0][0] = ff_loop_filter_h_4_8_lsx;
             dsp->loop_filter_8[0][1] = ff_loop_filter_v_4_8_lsx;
             dsp->loop_filter_8[1][0] = ff_loop_filter_h_8_8_lsx;
@@ -156,23 +117,14 @@ static av_cold void vp9dsp_loopfilter_init_lsx(VP9DSPContext *dsp, int bpp)
             dsp->loop_filter_mix2[1][1][0] = ff_loop_filter_h_88_16_lsx;
             dsp->loop_filter_mix2[1][1][1] = ff_loop_filter_v_88_16_lsx;
         }
-    }
 }
 
-static av_cold void vp9dsp_init_lsx(VP9DSPContext *dsp, int bpp)
-{
-    int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags)) {
-        vp9dsp_mc_init_lsx(dsp, bpp);
-        vp9dsp_intrapred_init_lsx(dsp, bpp);
-        vp9dsp_loopfilter_init_lsx(dsp, bpp);
-        vp9dsp_itxfm_init_lsx(dsp, bpp);
-    }
-}
-
-av_cold void ff_vp9dsp_init_loongarch(VP9DSPContext *dsp, int bpp)
-{
-    int cpu_flags = av_get_cpu_flags();
-    if (have_lsx(cpu_flags))
-        vp9dsp_init_lsx(dsp, bpp);
-}
+#undef init_subpel1
+#undef init_subpel2
+#undef init_subpel3
+#undef init_copy
+#undef init_fpel
+#undef init_intra_pred1_lsx
+#undef init_intra_pred2_lsx
+#undef init_idct
+#undef init_itxfm
diff --git a/libavutil/loongarch/loongson_intrinsics.h b/libavutil/loongarch/loongson_intrinsics.h
index 865d6ae9bd..eb256863c8 100644
--- a/libavutil/loongarch/loongson_intrinsics.h
+++ b/libavutil/loongarch/loongson_intrinsics.h
@@ -33,8 +33,7 @@
  *                Xiwei Gu   <guxiwei-hf@loongson.cn>
  *                Lu Wang    <wanglu@loongson.cn>
  *
- * This file is maintained in LSOM project, don't change it directly.
- * You can get the latest version of this header from: ***
+ * This file is a header file for loongarch builtin extension.
  *
  */
 
@@ -43,71 +42,50 @@
 
 /**
  * MAJOR version: Macro usage changes.
- * MINOR version: Add new functions, or bug fix.
+ * MINOR version: Add new functions, or bug fixes.
  * MICRO version: Comment changes or implementation changes.
  */
 #define LSOM_VERSION_MAJOR 1
-#define LSOM_VERSION_MINOR 0
+#define LSOM_VERSION_MINOR 1
 #define LSOM_VERSION_MICRO 0
 
 #define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0); \
-    _OUT1 = _INS(_IN1); \
-}
+  {                                               \
+    _OUT0 = _INS(_IN0);                           \
+    _OUT1 = _INS(_IN1);                           \
+  }
 
 #define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0, _IN1); \
-    _OUT1 = _INS(_IN2, _IN3); \
-}
+  {                                                           \
+    _OUT0 = _INS(_IN0, _IN1);                                 \
+    _OUT1 = _INS(_IN2, _IN3);                                 \
+  }
 
 #define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0, _IN1, _IN2); \
-    _OUT1 = _INS(_IN3, _IN4, _IN5); \
-}
+  {                                                                       \
+    _OUT0 = _INS(_IN0, _IN1, _IN2);                                       \
+    _OUT1 = _INS(_IN3, _IN4, _IN5);                                       \
+  }
 
 #define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1); \
-    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3); \
-}
-
-#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                  _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
-    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
-}
-
-#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                  _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
-    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
-}
-
-/*
- * =============================================================================
- * Description : Print out elements in vector.
- * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
- *               Outputs -
- * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
- *               '_enter' is TRUE, prefix "\nVP:" will be added first.
- * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
- *               VP:1,2,3,4,
- * =============================================================================
- */
-#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
-{                                                     \
-    RTYPE _tmp0 = (RTYPE)in0;                         \
-    int _i = 0;                                       \
-    if (enter)                                        \
-        printf("\nVP:");                              \
-    for(_i = 0; _i < element_num; _i++)               \
-        printf("%d,",_tmp0[_i]);                      \
-}
+  {                                                                         \
+    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1);                              \
+    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3);                              \
+  }
+
+#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, _OUT0, \
+                  _OUT1, _OUT2, _OUT3)                                         \
+  {                                                                            \
+    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1);                     \
+    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3);                     \
+  }
+
+#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, _IN8, \
+                  _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3)             \
+  {                                                                           \
+    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1);        \
+    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3);      \
+  }
 
 #ifdef __loongarch_sx
 #include <lsxintrin.h>
@@ -116,11 +94,11 @@
  * Description : Dot product & addition of byte vector elements
  * Arguments   : Inputs  - in_c, in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Signed byte elements from in_h are multiplied by
  *               signed byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
- *               Then the results plus to signed half word elements from in_c.
+ *               Then the results plus to signed half-word elements from in_c.
  * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
  *        in_c : 1,2,3,4, 1,2,3,4
  *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
@@ -128,13 +106,13 @@
  *         out : 23,40,41,26, 23,40,41,26
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h,
+                                        __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
+  out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -142,35 +120,61 @@ static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l
  * Description : Dot product & addition of byte vector elements
  * Arguments   : Inputs  - in_c, in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Unsigned byte elements from in_h are multiplied by
  *               unsigned byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
- *               The results plus to signed half word elements from in_c.
- * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *               The results plus to signed half-word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_bu(in_c, in_h, in_l)
  *        in_c : 1,2,3,4, 1,2,3,4
  *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
  *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
  *         out : 23,40,41,26, 23,40,41,26
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h,
+                                         __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half-word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_bu_b(in_c, in_h, in_l)
+ *        in_c : 1,1,1,1, 1,1,1,1
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : -1,-2,-3,-4, -5,-6,-7,-8, 1,2,3,4, 5,6,7,8
+ *         out : -4,-24,-60,-112, 6,26,62,114
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu_b(__m128i in_c, __m128i in_h,
+                                           __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
+  out = __lsx_vmaddwev_h_bu_b(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
 }
 
 /*
  * =============================================================================
- * Description : Dot product & addition of half word vector elements
+ * Description : Dot product & addition of half-word vector elements
  * Arguments   : Inputs  - in_c, in_h, in_l
  *               Outputs - out
- *               Retrun Type - __m128i
- * Details     : Signed half word elements from in_h are multiplied by
- *               signed half word elements from in_l, and then added adjacent to
+ *               Return Type - __m128i
+ * Details     : Signed half-word elements from in_h are multiplied by
+ *               signed half-word elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
  *               Then the results plus to signed word elements from in_c.
  * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
@@ -180,13 +184,13 @@ static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_
  *         out : 23,40,41,26
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h,
+                                        __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
+  out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -194,7 +198,7 @@ static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l
  * Description : Dot product of byte vector elements
  * Arguments   : Inputs  - in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Signed byte elements from in_h are multiplied by
  *               signed byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
@@ -204,13 +208,12 @@ static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l
  *         out : 22,38,38,22, 22,38,38,22
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmulwev_h_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
+  out = __lsx_vmulwev_h_b(in_h, in_l);
+  out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -218,7 +221,7 @@ static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
  * Description : Dot product of byte vector elements
  * Arguments   : Inputs  - in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Unsigned byte elements from in_h are multiplied by
  *               unsigned byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
@@ -228,13 +231,12 @@ static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
  *         out : 22,38,38,22, 22,38,38,22
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmulwev_h_bu(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
+  out = __lsx_vmulwev_h_bu(in_h, in_l);
+  out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -242,7 +244,7 @@ static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
  * Description : Dot product of byte vector elements
  * Arguments   : Inputs  - in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Unsigned byte elements from in_h are multiplied by
  *               signed byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
@@ -252,13 +254,12 @@ static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
  *         out : 22,38,38,22, 22,38,38,6
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
-    return out;
+  out = __lsx_vmulwev_h_bu_b(in_h, in_l);
+  out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -266,7 +267,7 @@ static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
  * Description : Dot product of byte vector elements
  * Arguments   : Inputs  - in_h, in_l
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Signed byte elements from in_h are multiplied by
  *               signed byte elements from in_l, and then added adjacent to
  *               each other to get results with the twice size of input.
@@ -276,19 +277,19 @@ static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
  *         out : 22,38,38,22
  * =============================================================================
  */
-static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
+static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l) {
+  __m128i out;
 
-    out = __lsx_vmulwev_w_h(in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
+  out = __lsx_vmulwev_w_h(in_h, in_l);
+  out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+  return out;
 }
 
 /*
  * =============================================================================
  * Description : Clip all halfword elements of input vector between min & max
- *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
+ *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) :
+ *               (_in))
  * Arguments   : Inputs  - _in  (input vector)
  *                       - min  (min threshold)
  *                       - max  (max threshold)
@@ -301,13 +302,12 @@ static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
  *         out : 1,2,9,9, 1,9,9,9
  * =============================================================================
  */
-static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
-{
-    __m128i out;
+static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max) {
+  __m128i out;
 
-    out = __lsx_vmax_h(min, _in);
-    out = __lsx_vmin_h(max, out);
-    return out;
+  out = __lsx_vmax_h(min, _in);
+  out = __lsx_vmin_h(max, out);
+  return out;
 }
 
 /*
@@ -315,20 +315,19 @@ static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
  * Description : Set each element of vector between 0 and 255
  * Arguments   : Inputs  - _in
  *               Outputs - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Signed byte elements from _in are clamped between 0 and 255.
  * Example     : out = __lsx_vclip255_h(_in)
  *         _in : -8,255,280,249, -8,255,280,249
  *         out : 0,255,255,249, 0,255,255,249
  * =============================================================================
  */
-static inline __m128i __lsx_vclip255_h(__m128i _in)
-{
-    __m128i out;
+static inline __m128i __lsx_vclip255_h(__m128i _in) {
+  __m128i out;
 
-    out = __lsx_vmaxi_h(_in, 0);
-    out = __lsx_vsat_hu(out, 7);
-    return out;
+  out = __lsx_vmaxi_h(_in, 0);
+  out = __lsx_vsat_hu(out, 7);
+  return out;
 }
 
 /*
@@ -336,20 +335,19 @@ static inline __m128i __lsx_vclip255_h(__m128i _in)
  * Description : Set each element of vector between 0 and 255
  * Arguments   : Inputs  - _in
  *               Outputs - out
- *               Retrun Type - word
+ *               Return Type - word
  * Details     : Signed byte elements from _in are clamped between 0 and 255.
  * Example     : out = __lsx_vclip255_w(_in)
  *         _in : -8,255,280,249
  *         out : 0,255,255,249
  * =============================================================================
  */
-static inline __m128i __lsx_vclip255_w(__m128i _in)
-{
-    __m128i out;
+static inline __m128i __lsx_vclip255_w(__m128i _in) {
+  __m128i out;
 
-    out = __lsx_vmaxi_w(_in, 0);
-    out = __lsx_vsat_wu(out, 7);
-    return out;
+  out = __lsx_vmaxi_w(_in, 0);
+  out = __lsx_vsat_wu(out, 7);
+  return out;
 }
 
 /*
@@ -365,12 +363,12 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *   _in1(out) : 1,2,3,4
  * =============================================================================
  */
-#define LSX_SWAP(_in0, _in1)                                            \
-{                                                                       \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-}                                                                       \
+#define LSX_SWAP(_in0, _in1)         \
+  {                                  \
+    _in0 = __lsx_vxor_v(_in0, _in1); \
+    _in1 = __lsx_vxor_v(_in0, _in1); \
+    _in0 = __lsx_vxor_v(_in0, _in1); \
+  }
 
 /*
  * =============================================================================
@@ -386,25 +384,27 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  * =============================================================================
  */
 #define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                              \
+  {                                                                            \
     __m128i _t0, _t1, _t2, _t3;                                                \
                                                                                \
-    _t0   = __lsx_vilvl_w(_in1, _in0);                                         \
-    _t1   = __lsx_vilvh_w(_in1, _in0);                                         \
-    _t2   = __lsx_vilvl_w(_in3, _in2);                                         \
-    _t3   = __lsx_vilvh_w(_in3, _in2);                                         \
+    _t0 = __lsx_vilvl_w(_in1, _in0);                                           \
+    _t1 = __lsx_vilvh_w(_in1, _in0);                                           \
+    _t2 = __lsx_vilvl_w(_in3, _in2);                                           \
+    _t3 = __lsx_vilvh_w(_in3, _in2);                                           \
     _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
     _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
     _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
     _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
-}
+  }
 
 /*
  * =============================================================================
  * Description : Transpose 8x8 block with byte elements in vectors
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
+ * Details     : The rows of the matrix become columns, and the columns
+ *               become rows.
  * Example     : LSX_TRANSPOSE8x8_B
  *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
  *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
@@ -425,34 +425,35 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
  * =============================================================================
  */
-#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-   __m128i zero = {0};                                                            \
-   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                      \
-   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-                                                                                  \
-   _t0 = __lsx_vilvl_b(_in2, _in0);                                               \
-   _t1 = __lsx_vilvl_b(_in3, _in1);                                               \
-   _t2 = __lsx_vilvl_b(_in6, _in4);                                               \
-   _t3 = __lsx_vilvl_b(_in7, _in5);                                               \
-   _t4 = __lsx_vilvl_b(_t1, _t0);                                                 \
-   _t5 = __lsx_vilvh_b(_t1, _t0);                                                 \
-   _t6 = __lsx_vilvl_b(_t3, _t2);                                                 \
-   _t7 = __lsx_vilvh_b(_t3, _t2);                                                 \
-   _out0 = __lsx_vilvl_w(_t6, _t4);                                               \
-   _out2 = __lsx_vilvh_w(_t6, _t4);                                               \
-   _out4 = __lsx_vilvl_w(_t7, _t5);                                               \
-   _out6 = __lsx_vilvh_w(_t7, _t5);                                               \
-   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                     \
-   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                     \
-   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                     \
-   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                     \
-}
+#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    __m128i zero = { 0 };                                                   \
+    __m128i shuf8 = { 0x0F0E0D0C0B0A0908, 0x1716151413121110 };             \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                         \
+                                                                            \
+    _t0 = __lsx_vilvl_b(_in2, _in0);                                        \
+    _t1 = __lsx_vilvl_b(_in3, _in1);                                        \
+    _t2 = __lsx_vilvl_b(_in6, _in4);                                        \
+    _t3 = __lsx_vilvl_b(_in7, _in5);                                        \
+    _t4 = __lsx_vilvl_b(_t1, _t0);                                          \
+    _t5 = __lsx_vilvh_b(_t1, _t0);                                          \
+    _t6 = __lsx_vilvl_b(_t3, _t2);                                          \
+    _t7 = __lsx_vilvh_b(_t3, _t2);                                          \
+    _out0 = __lsx_vilvl_w(_t6, _t4);                                        \
+    _out2 = __lsx_vilvh_w(_t6, _t4);                                        \
+    _out4 = __lsx_vilvl_w(_t7, _t5);                                        \
+    _out6 = __lsx_vilvh_w(_t7, _t5);                                        \
+    _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                              \
+    _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                              \
+    _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                              \
+    _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                              \
+  }
 
 /*
  * =============================================================================
- * Description : Transpose 8x8 block with half word elements in vectors
+ * Description : Transpose 8x8 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
  *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
  * Details     :
@@ -467,37 +468,38 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
  * =============================================================================
  */
-#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                     \
-                                                                                  \
-    _s0 = __lsx_vilvl_h(_in6, _in4);                                              \
-    _s1 = __lsx_vilvl_h(_in7, _in5);                                              \
-    _t0 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t1 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvh_h(_in6, _in4);                                              \
-    _s1 = __lsx_vilvh_h(_in7, _in5);                                              \
-    _t2 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t3 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvl_h(_in2, _in0);                                              \
-    _s1 = __lsx_vilvl_h(_in3, _in1);                                              \
-    _t4 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t5 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvh_h(_in2, _in0);                                              \
-    _s1 = __lsx_vilvh_h(_in3, _in1);                                              \
-    _t6 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t7 = __lsx_vilvh_h(_s1, _s0);                                                \
-                                                                                  \
-    _out0 = __lsx_vpickev_d(_t0, _t4);                                            \
-    _out2 = __lsx_vpickev_d(_t1, _t5);                                            \
-    _out4 = __lsx_vpickev_d(_t2, _t6);                                            \
-    _out6 = __lsx_vpickev_d(_t3, _t7);                                            \
-    _out1 = __lsx_vpickod_d(_t0, _t4);                                            \
-    _out3 = __lsx_vpickod_d(_t1, _t5);                                            \
-    _out5 = __lsx_vpickod_d(_t2, _t6);                                            \
-    _out7 = __lsx_vpickod_d(_t3, _t7);                                            \
-}
+#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;               \
+                                                                            \
+    _s0 = __lsx_vilvl_h(_in6, _in4);                                        \
+    _s1 = __lsx_vilvl_h(_in7, _in5);                                        \
+    _t0 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t1 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvh_h(_in6, _in4);                                        \
+    _s1 = __lsx_vilvh_h(_in7, _in5);                                        \
+    _t2 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t3 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvl_h(_in2, _in0);                                        \
+    _s1 = __lsx_vilvl_h(_in3, _in1);                                        \
+    _t4 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t5 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvh_h(_in2, _in0);                                        \
+    _s1 = __lsx_vilvh_h(_in3, _in1);                                        \
+    _t6 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t7 = __lsx_vilvh_h(_s1, _s0);                                          \
+                                                                            \
+    _out0 = __lsx_vpickev_d(_t0, _t4);                                      \
+    _out2 = __lsx_vpickev_d(_t1, _t5);                                      \
+    _out4 = __lsx_vpickev_d(_t2, _t6);                                      \
+    _out6 = __lsx_vpickev_d(_t3, _t7);                                      \
+    _out1 = __lsx_vpickod_d(_t0, _t4);                                      \
+    _out3 = __lsx_vpickod_d(_t1, _t5);                                      \
+    _out5 = __lsx_vpickod_d(_t2, _t6);                                      \
+    _out7 = __lsx_vpickod_d(_t3, _t7);                                      \
+  }
 
 /*
  * =============================================================================
@@ -505,7 +507,8 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
  *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
  *               Return Type - as per RTYPE
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
  * Example     : LSX_TRANSPOSE8x4_B
  *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
  *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
@@ -522,26 +525,26 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
  * =============================================================================
  */
-#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,       \
-                           _out0, _out1, _out2, _out3)                           \
-{                                                                                \
-    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
-                                                                                 \
-    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
-    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
-                                                                                 \
-    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
-    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
-                                                                                 \
-    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
-    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
-    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
-    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
-}
+#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, \
+                           _out0, _out1, _out2, _out3)                     \
+  {                                                                        \
+    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                            \
+                                                                           \
+    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                 \
+    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                 \
+    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                 \
+    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                 \
+                                                                           \
+    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                             \
+    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                             \
+                                                                           \
+    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                               \
+    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                               \
+    _out1 = __lsx_vilvh_d(_out2, _out0);                                   \
+    _out3 = __lsx_vilvh_d(_out0, _out2);                                   \
+  }
 
 /*
  * =============================================================================
@@ -569,29 +572,30 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *              120,121,122,123,124,125,126,127
  * =============================================================================
  */
-#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
-                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,       \
-              _tmp0, _tmp1, _tmp2, _tmp3);                                         \
-    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,        \
-              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                                  \
-    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);                \
-    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);                \
-    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);                \
-    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);                \
-    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                    \
-    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                    \
-    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                    \
-    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                    \
-    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);            \
-    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);            \
-    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);            \
-    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);            \
-}
+#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                            _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                            _out6, _out7)                                    \
+  {                                                                          \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;          \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                          \
+    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5, \
+              _tmp0, _tmp1, _tmp2, _tmp3);                                   \
+    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,  \
+              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                            \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);          \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);          \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);          \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);          \
+    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);              \
+    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);              \
+    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);              \
+    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);              \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);      \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);      \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);      \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);      \
+  }
 
 /*
  * =============================================================================
@@ -607,33 +611,33 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  * =============================================================================
  */
 #define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
+  {                                                                           \
     _out0 = __lsx_vadd_b(_in0, _in3);                                         \
     _out1 = __lsx_vadd_b(_in1, _in2);                                         \
     _out2 = __lsx_vsub_b(_in1, _in2);                                         \
     _out3 = __lsx_vsub_b(_in0, _in3);                                         \
-}
+  }
 #define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
+  {                                                                           \
     _out0 = __lsx_vadd_h(_in0, _in3);                                         \
     _out1 = __lsx_vadd_h(_in1, _in2);                                         \
     _out2 = __lsx_vsub_h(_in1, _in2);                                         \
     _out3 = __lsx_vsub_h(_in0, _in3);                                         \
-}
+  }
 #define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
+  {                                                                           \
     _out0 = __lsx_vadd_w(_in0, _in3);                                         \
     _out1 = __lsx_vadd_w(_in1, _in2);                                         \
     _out2 = __lsx_vsub_w(_in1, _in2);                                         \
     _out3 = __lsx_vsub_w(_in0, _in3);                                         \
-}
+  }
 #define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
+  {                                                                           \
     _out0 = __lsx_vadd_d(_in0, _in3);                                         \
     _out1 = __lsx_vadd_d(_in1, _in2);                                         \
     _out2 = __lsx_vsub_d(_in1, _in2);                                         \
     _out3 = __lsx_vsub_d(_in0, _in3);                                         \
-}
+  }
 
 /*
  * =============================================================================
@@ -652,59 +656,63 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  *              _out7 = _in0 - _in7;
  * =============================================================================
  */
-#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_b(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_b(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_b(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_b(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_b(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_b(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_b(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_b(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_h(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_h(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_h(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_h(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_h(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_h(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_h(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_h(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_w(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_w(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_w(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_w(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_w(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_w(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_w(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_w(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_d(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_d(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_d(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_d(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_d(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_d(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_d(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_d(_in0, _in7);                                            \
-}
-
-#endif //LSX
+#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_b(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_b(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_b(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_b(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_b(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_b(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_b(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_b(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_h(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_h(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_h(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_h(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_h(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_h(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_h(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_h(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_w(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_w(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_w(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_w(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_w(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_w(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_w(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_w(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_d(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_d(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_d(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_d(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_d(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_d(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_d(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_d(_in0, _in7);                                      \
+  }
+
+#endif  // LSX
 
 #ifdef __loongarch_asx
 #include <lasxintrin.h>
@@ -722,13 +730,12 @@ static inline __m128i __lsx_vclip255_w(__m128i _in)
  * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmulwev_h_bu(in_h, in_l);
-    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmulwev_h_bu(in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -740,18 +747,17 @@ static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
  * Details     : Signed byte elements from in_h are multiplied with
  *               signed byte elements from in_l producing a result
  *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
+ *               Then this multiplication results of adjacent odd-even elements
  *               are added to the out vector
  * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmulwev_h_b(in_h, in_l);
-    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmulwev_h_b(in_h, in_l);
+  out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -771,13 +777,12 @@ static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
  *         out : 22,38,38,22, 22,38,38,22
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -785,22 +790,21 @@ static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
  * Description : Dot product of word vector elements
  * Arguments   : Inputs - in_h, in_l
  *               Output - out
- *               Retrun Type - signed double
+ *               Return Type - signed double
  * Details     : Signed word elements from in_h are multiplied with
  *               signed word elements from in_l producing a result
- *               twice the size of input i.e. signed double word.
+ *               twice the size of input i.e. signed double-word.
  *               Then this multiplied results of adjacent odd-even elements
  *               are added to the out vector.
  * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmulwev_d_w(in_h, in_l);
-    out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmulwev_d_w(in_h, in_l);
+  out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -817,13 +821,12 @@ static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -831,7 +834,7 @@ static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
  * Description : Dot product & addition of byte vector elements
  * Arguments   : Inputs - in_h, in_l
  *               Output - out
- *               Retrun Type - halfword
+ *               Return Type - halfword
  * Details     : Signed byte elements from in_h are multiplied with
  *               signed byte elements from in_l producing a result
  *               twice the size of input i.e. signed halfword.
@@ -840,13 +843,59 @@ static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_bu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_h_bu(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_bu_b(__m256i in_c, __m256i in_h,
+                                             __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_h_bu_b(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -867,13 +916,13 @@ static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_
  *         out : 23,40,41,26, 23,40,41,26
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -890,13 +939,13 @@ static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in
  * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -913,13 +962,13 @@ static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i i
  * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h,
+                                             __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
-    return out;
+  out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+  return out;
 }
 
 /*
@@ -937,14 +986,14 @@ static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i
  * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_h_bu(in_h, in_l);
-    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
-    out = __lasx_xvsub_h(in_c, out);
-    return out;
+static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_h_bu(in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  out = __lasx_xvsub_h(in_c, out);
+  return out;
 }
 
 /*
@@ -966,14 +1015,14 @@ static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i i
  *         out : -7,-3,0,0, 0,-1,0,-1
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    out = __lasx_xvsub_w(in_c, out);
-    return out;
+static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  out = __lasx_xvsub_w(in_c, out);
+  return out;
 }
 
 /*
@@ -982,10 +1031,10 @@ static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in
  * Arguments   : Inputs - in_h, in_l
  *               Output - out
  *               Return Type - signed word
- * Details     : Signed halfword elements from in_h are iniplied with
+ * Details     : Signed halfword elements from in_h are multiplied with
  *               signed halfword elements from in_l producing a result
  *               four times the size of input i.e. signed doubleword.
- *               Then this iniplication results of four adjacent elements
+ *               Then this multiplication results of four adjacent elements
  *               are added together and stored to the out vector.
  * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
  *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
@@ -993,14 +1042,13 @@ static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in
  *         out : -2,0,1,1
  * =============================================================================
  */
-static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    out = __lasx_xvhaddw_d_w(out, out);
-    return out;
+static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  out = __lasx_xvhaddw_d_w(out, out);
+  return out;
 }
 
 /*
@@ -1015,13 +1063,12 @@ static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvilvh_b(in_h, in_l);
-    out = __lasx_xvhaddw_h_b(out, out);
-    return out;
+  out = __lasx_xvilvh_b(in_h, in_l);
+  out = __lasx_xvhaddw_h_b(out, out);
+  return out;
 }
 
 /*
@@ -1039,13 +1086,12 @@ static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
  *         out : 1,0,0,-1, 1,0,0, 2
  * =============================================================================
  */
- static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvilvh_h(in_h, in_l);
-    out = __lasx_xvhaddw_w_h(out, out);
-    return out;
+  out = __lasx_xvilvh_h(in_h, in_l);
+  out = __lasx_xvhaddw_w_h(out, out);
+  return out;
 }
 
 /*
@@ -1060,13 +1106,12 @@ static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvilvl_b(in_h, in_l);
-    out = __lasx_xvhaddw_h_b(out, out);
-    return out;
+  out = __lasx_xvilvl_b(in_h, in_l);
+  out = __lasx_xvhaddw_h_b(out, out);
+  return out;
 }
 
 /*
@@ -1084,13 +1129,12 @@ static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
  *         out : 5,-1,4,2, 1,0,2,-1
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvilvl_h(in_h, in_l);
-    out = __lasx_xvhaddw_w_h(out, out);
-    return out;
+  out = __lasx_xvilvl_h(in_h, in_l);
+  out = __lasx_xvhaddw_w_h(out, out);
+  return out;
 }
 
 /*
@@ -1105,13 +1149,12 @@ static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvilvl_b(in_h, in_l);
-    out = __lasx_xvhaddw_hu_bu(out, out);
-    return out;
+  out = __lasx_xvilvl_b(in_h, in_l);
+  out = __lasx_xvhaddw_hu_bu(out, out);
+  return out;
 }
 
 /*
@@ -1125,13 +1168,12 @@ static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
  * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvsllwil_hu_bu(in_l, 0);
-    out = __lasx_xvadd_h(in_h, out);
-    return out;
+  out = __lasx_xvsllwil_hu_bu(in_l, 0);
+  out = __lasx_xvadd_h(in_h, out);
+  return out;
 }
 
 /*
@@ -1148,13 +1190,12 @@ static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
  *         out : 2, 0,1,2, -1,0,1,1,
  * =============================================================================
  */
-static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
+static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
 
-    out = __lasx_xvsllwil_w_h(in_l, 0);
-    out = __lasx_xvadd_w(in_h, out);
-    return out;
+  out = __lasx_xvsllwil_w_h(in_l, 0);
+  out = __lasx_xvadd_w(in_h, out);
+  return out;
 }
 
 /*
@@ -1175,15 +1216,15 @@ static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
  *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
  * =============================================================================
  */
-static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
-    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
-    tmp0 = __lasx_xvmul_w(tmp0, tmp1);
-    out  = __lasx_xvadd_w(tmp0, in_c);
-    return out;
+static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+  tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+  tmp0 = __lasx_xvmul_w(tmp0, tmp1);
+  out = __lasx_xvadd_w(tmp0, in_c);
+  return out;
 }
 
 /*
@@ -1199,15 +1240,15 @@ static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in
  * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
  * =============================================================================
  */
-static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvilvh_h(in_h, in_h);
-    tmp1 = __lasx_xvilvh_h(in_l, in_l);
-    tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
-    out  = __lasx_xvadd_w(tmp0, in_c);
-    return out;
+static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvilvh_h(in_h, in_h);
+  tmp1 = __lasx_xvilvh_h(in_l, in_l);
+  tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
+  out = __lasx_xvadd_w(tmp0, in_c);
+  return out;
 }
 
 /*
@@ -1225,14 +1266,13 @@ static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in
  *         out : 6,1,3,0, 0,0,1,0
  * =============================================================================
  */
-static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
-    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
-    out  = __lasx_xvmul_w(tmp0, tmp1);
-    return out;
+static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+  tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+  out = __lasx_xvmul_w(tmp0, tmp1);
+  return out;
 }
 
 /*
@@ -1250,39 +1290,39 @@ static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
  *         out : 0,0,0,0, 0,0,0,1
  * =============================================================================
  */
-static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvilvh_h(in_h, in_h);
-    tmp1 = __lasx_xvilvh_h(in_l, in_l);
-    out  = __lasx_xvmulwev_w_h(tmp0, tmp1);
-    return out;
+static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvilvh_h(in_h, in_h);
+  tmp1 = __lasx_xvilvh_h(in_l, in_l);
+  out = __lasx_xvmulwev_w_h(tmp0, tmp1);
+  return out;
 }
 
 /*
  * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added saturately after being doubled.
+ * Description : The low half of the vector elements are added to the high half
+ *               after being doubled, then saturated.
  * Arguments   : Inputs - in_h, in_l
  *               Output - out
- * Details     : The in_h vector adds the in_l vector saturately after the lower
- *               half of the two-fold zero extension (unsigned byte to unsigned
- *               halfword) and the results are stored to the out vector.
+ * Details     : The in_h vector adds the in_l vector after the lower half of
+ *               the two-fold zero extension (unsigned byte to unsigned
+ *               halfword) and then saturated. The results are stored to the out
+ *               vector.
  * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
  *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
- *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
- *         out : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
+ *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1,
+ *               0,0,0,1
+ *        out  : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
  * =============================================================================
  */
-static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp1, out;
-    __m256i zero = {0};
-
-    tmp1 = __lasx_xvilvl_b(zero, in_l);
-    out  = __lasx_xvsadd_hu(in_h, tmp1);
-    return out;
+static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l) {
+  __m256i tmp1, out;
+  __m256i zero = { 0 };
+
+  tmp1 = __lasx_xvilvl_b(zero, in_l);
+  out = __lasx_xvsadd_hu(in_h, tmp1);
+  return out;
 }
 
 /*
@@ -1301,13 +1341,12 @@ static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
  *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
  * =============================================================================
  */
-static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
-{
-    __m256i out;
+static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max) {
+  __m256i out;
 
-    out = __lasx_xvmax_h(min, in);
-    out = __lasx_xvmin_h(max, out);
-    return out;
+  out = __lasx_xvmax_h(min, in);
+  out = __lasx_xvmin_h(max, out);
+  return out;
 }
 
 /*
@@ -1317,16 +1356,15 @@ static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
  * Arguments   : Inputs  - in   (input vector)
  *               Outputs - out  (output vector with clipped elements)
  *               Return Type - signed halfword
- * Example     : See out = __lasx_xvclamp255_w(in)
+ * Example     : See out = __lasx_xvclip255_w(in)
  * =============================================================================
  */
-static inline __m256i __lasx_xvclip255_h(__m256i in)
-{
-    __m256i out;
+static inline __m256i __lasx_xvclip255_h(__m256i in) {
+  __m256i out;
 
-    out = __lasx_xvmaxi_h(in, 0);
-    out = __lasx_xvsat_hu(out, 7);
-    return out;
+  out = __lasx_xvmaxi_h(in, 0);
+  out = __lasx_xvsat_hu(out, 7);
+  return out;
 }
 
 /*
@@ -1336,25 +1374,24 @@ static inline __m256i __lasx_xvclip255_h(__m256i in)
  * Arguments   : Inputs - in   (input vector)
  *               Output - out  (output vector with clipped elements)
  *               Return Type - signed word
- * Example     : out = __lasx_xvclamp255_w(in)
+ * Example     : out = __lasx_xvclip255_w(in)
  *          in : -8,255,280,249, -8,255,280,249
  *         out :  0,255,255,249,  0,255,255,249
  * =============================================================================
  */
-static inline __m256i __lasx_xvclip255_w(__m256i in)
-{
-    __m256i out;
+static inline __m256i __lasx_xvclip255_w(__m256i in) {
+  __m256i out;
 
-    out = __lasx_xvmaxi_w(in, 0);
-    out = __lasx_xvsat_wu(out, 7);
-    return out;
+  out = __lasx_xvmaxi_w(in, 0);
+  out = __lasx_xvsat_wu(out, 7);
+  return out;
 }
 
 /*
  * =============================================================================
  * Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
- *               if 'indx >= 8' use xvsplati_h_*.
+ *               elements in output vector. If 'idx < 8' use xvsplati_l_*,
+ *               if 'idx >= 8' use xvsplati_h_*.
  * Arguments   : Inputs - in, idx
  *               Output - out
  * Details     : Idx element value from in vector is replicated to all
@@ -1366,20 +1403,19 @@ static inline __m256i __lasx_xvclip255_w(__m256i in)
  *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
  * =============================================================================
  */
-static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
-{
-    __m256i out;
+static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx) {
+  __m256i out;
 
-    out = __lasx_xvpermi_q(in, in, 0x02);
-    out = __lasx_xvreplve_h(out, idx);
-    return out;
+  out = __lasx_xvpermi_q(in, in, 0x02);
+  out = __lasx_xvreplve_h(out, idx);
+  return out;
 }
 
 /*
  * =============================================================================
  * Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
- *               if 'indx >= 8' use xvsplati_h_*.
+ *               elements in output vector. If 'idx < 8' use xvsplati_l_*,
+ *               if 'idx >= 8' use xvsplati_h_*.
  * Arguments   : Inputs - in, idx
  *               Output - out
  * Details     : Idx element value from in vector is replicated to all
@@ -1391,103 +1427,104 @@ static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
  *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
  * =============================================================================
  */
+static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx) {
+  __m256i out;
 
-static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
-{
-    __m256i out;
-
-    out = __lasx_xvpermi_q(in, in, 0x13);
-    out = __lasx_xvreplve_h(out, idx);
-    return out;
+  out = __lasx_xvpermi_q(in, in, 0x13);
+  out = __lasx_xvreplve_h(out, idx);
+  return out;
 }
 
 /*
  * =============================================================================
- * Description : Transpose 4x4 block with double word elements in vectors
+ * Description : Transpose 4x4 block with double-word elements in vectors
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3
  *               Outputs - _out0, _out1, _out2, _out3
  * Example     : LASX_TRANSPOSE4x4_D
- *         _in0 : 1,2,3,4
- *         _in1 : 1,2,3,4
- *         _in2 : 1,2,3,4
- *         _in3 : 1,2,3,4
+ *        _in0 : 1,2,3,4
+ *        _in1 : 1,2,3,4
+ *        _in2 : 1,2,3,4
+ *        _in3 : 1,2,3,4
  *
- *        _out0 : 1,1,1,1
- *        _out1 : 2,2,2,2
- *        _out2 : 3,3,3,3
- *        _out3 : 4,4,4,4
+ *       _out0 : 1,1,1,1
+ *       _out1 : 2,2,2,2
+ *       _out2 : 3,3,3,3
+ *       _out3 : 4,4,4,4
  * =============================================================================
  */
-#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                               \
-    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                         \
-    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                        \
-    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                        \
-    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                        \
-    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                        \
-    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                               \
-    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                               \
-    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                               \
-    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                               \
-}
+#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, \
+                            _out3)                                       \
+  {                                                                      \
+    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                  \
+    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                 \
+    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                 \
+    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                 \
+    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                 \
+    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                        \
+    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                        \
+    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                        \
+    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                        \
+  }
 
 /*
  * =============================================================================
  * Description : Transpose 8x8 block with word elements in vectors
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
  * Example     : LASX_TRANSPOSE8x8_W
- *         _in0 : 1,2,3,4,5,6,7,8
- *         _in1 : 2,2,3,4,5,6,7,8
- *         _in2 : 3,2,3,4,5,6,7,8
- *         _in3 : 4,2,3,4,5,6,7,8
- *         _in4 : 5,2,3,4,5,6,7,8
- *         _in5 : 6,2,3,4,5,6,7,8
- *         _in6 : 7,2,3,4,5,6,7,8
- *         _in7 : 8,2,3,4,5,6,7,8
+ *        _in0 : 1,2,3,4,5,6,7,8
+ *        _in1 : 2,2,3,4,5,6,7,8
+ *        _in2 : 3,2,3,4,5,6,7,8
+ *        _in3 : 4,2,3,4,5,6,7,8
+ *        _in4 : 5,2,3,4,5,6,7,8
+ *        _in5 : 6,2,3,4,5,6,7,8
+ *        _in6 : 7,2,3,4,5,6,7,8
+ *        _in7 : 8,2,3,4,5,6,7,8
  *
- *        _out0 : 1,2,3,4,5,6,7,8
- *        _out1 : 2,2,2,2,2,2,2,2
- *        _out2 : 3,3,3,3,3,3,3,3
- *        _out3 : 4,4,4,4,4,4,4,4
- *        _out4 : 5,5,5,5,5,5,5,5
- *        _out5 : 6,6,6,6,6,6,6,6
- *        _out6 : 7,7,7,7,7,7,7,7
- *        _out7 : 8,8,8,8,8,8,8,8
+ *       _out0 : 1,2,3,4,5,6,7,8
+ *       _out1 : 2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8
  * =============================================================================
  */
-#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_w(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvl_w(_in3, _in1);                                          \
-    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_w(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvh_w(_in3, _in1);                                          \
-    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvl_w(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvl_w(_in7, _in5);                                          \
-    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_w(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvh_w(_in7, _in5);                                          \
-    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                               \
-    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                               \
-    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                               \
-    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                               \
-    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                               \
-    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                               \
-    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                               \
-    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                               \
-}
+#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _s0_m, _s1_m;                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+                                                                             \
+    _s0_m = __lasx_xvilvl_w(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvl_w(_in3, _in1);                                     \
+    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_w(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvh_w(_in3, _in1);                                     \
+    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvl_w(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvl_w(_in7, _in5);                                     \
+    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_w(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvh_w(_in7, _in5);                                     \
+    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                        \
+    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                        \
+    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                        \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                        \
+    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                        \
+    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                        \
+    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                        \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                        \
+  }
 
 /*
  * =============================================================================
@@ -1495,53 +1532,54 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
  *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
  *                         (input 16x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
  * Example     : See LASX_TRANSPOSE16x8_H
  * =============================================================================
  */
-#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
-                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                                    \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
-    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                         \
-    _t0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                         \
-    _t1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                         \
-    _t2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                         \
-    _t3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                         \
-    _t4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                         \
-    _t5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                         \
-    _t6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                         \
-    _t7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                         \
-    _tmp0_m = __lasx_xvilvl_w(_t2, _t0);                                             \
-    _tmp2_m = __lasx_xvilvh_w(_t2, _t0);                                             \
-    _tmp4_m = __lasx_xvilvl_w(_t3, _t1);                                             \
-    _tmp6_m = __lasx_xvilvh_w(_t3, _t1);                                             \
-    _tmp1_m = __lasx_xvilvl_w(_t6, _t4);                                             \
-    _tmp3_m = __lasx_xvilvh_w(_t6, _t4);                                             \
-    _tmp5_m = __lasx_xvilvl_w(_t7, _t5);                                             \
-    _tmp7_m = __lasx_xvilvh_w(_t7, _t5);                                             \
-    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                       \
-    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                       \
-    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                       \
-    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                       \
-    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                       \
-    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                       \
-    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                       \
-    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                       \
-}
+#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                             _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                             _out6, _out7)                                    \
+  {                                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                               \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                               \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                  \
+    _out0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                \
+    _out1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                \
+    _out2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                \
+    _out3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                \
+    _out4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                \
+    _out5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                \
+    _out6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                \
+    _out7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                \
+    _tmp0_m = __lasx_xvilvl_w(_out2, _out0);                                  \
+    _tmp2_m = __lasx_xvilvh_w(_out2, _out0);                                  \
+    _tmp4_m = __lasx_xvilvl_w(_out3, _out1);                                  \
+    _tmp6_m = __lasx_xvilvh_w(_out3, _out1);                                  \
+    _tmp1_m = __lasx_xvilvl_w(_out6, _out4);                                  \
+    _tmp3_m = __lasx_xvilvh_w(_out6, _out4);                                  \
+    _tmp5_m = __lasx_xvilvl_w(_out7, _out5);                                  \
+    _tmp7_m = __lasx_xvilvh_w(_out7, _out5);                                  \
+    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                \
+    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                \
+    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                \
+    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                \
+    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                \
+    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                \
+    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                \
+    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                \
+  }
 
 /*
  * =============================================================================
@@ -1549,9 +1587,10 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
  *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
  *                         (input 16x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
  * Example     : LASX_TRANSPOSE16x8_H
  *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
  *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
@@ -1580,72 +1619,73 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
  * =============================================================================
  */
-#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
-                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-   {                                                                                 \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
-    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                         \
-    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
-    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
-    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
-    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
-    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
-    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
-    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
-    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
-    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
-    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
-    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
-    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
-    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
-    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
-    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
-    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
-    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
-    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
-    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
-    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                         \
-    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
-    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
-    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
-    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
-    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
-    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
-    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
-    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
-    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
-    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
-    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
-    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
-    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
-    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
-    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
-    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
-    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
-    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
-    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
-    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
-}
+#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                             _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                             _out6, _out7)                                    \
+  {                                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                               \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                               \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                           \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                  \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                  \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                  \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                  \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                  \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                  \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                  \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                  \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                  \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                      \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                      \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                      \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                      \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                      \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                      \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                      \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                      \
+    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                         \
+    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                         \
+    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                         \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                         \
+                                                                              \
+    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                  \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                  \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                  \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                  \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                  \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                  \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                  \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                  \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                  \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                      \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                      \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                      \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                      \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                      \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                      \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                      \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                      \
+    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                         \
+    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                         \
+    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                         \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                         \
+  }
 
 /*
  * =============================================================================
@@ -1653,61 +1693,65 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3
  *               Outputs - _out0, _out1, _out2, _out3
  *               Return Type - signed halfword
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
  * Example     : See LASX_TRANSPOSE8x8_H
  * =============================================================================
  */
-#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)     \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-                                                                                    \
-    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                            \
-    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                            \
-    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                                          \
-    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                                          \
-    _out1 = __lasx_xvilvh_d(_out0, _out0);                                          \
-    _out3 = __lasx_xvilvh_d(_out2, _out2);                                          \
-}
+#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, \
+                            _out3)                                       \
+  {                                                                      \
+    __m256i _s0_m, _s1_m;                                                \
+                                                                         \
+    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                 \
+    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                 \
+    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                               \
+    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                               \
+    _out1 = __lasx_xvilvh_d(_out0, _out0);                               \
+    _out3 = __lasx_xvilvh_d(_out2, _out2);                               \
+  }
 
 /*
  * =============================================================================
  * Description : Transpose input 8x8 byte block
  * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
  *                         (input 8x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x8 byte block)
  * Example     : See LASX_TRANSPOSE8x8_H
  * =============================================================================
  */
-#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
-{                                                                                   \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                          \
-    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                          \
-    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                          \
-    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                          \
-    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                    \
-    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                    \
-    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                    \
-    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                    \
-    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                                      \
-    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                                      \
-    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                                      \
-    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                                      \
-    _out1 = __lasx_xvbsrl_v(_out0, 8);                                              \
-    _out3 = __lasx_xvbsrl_v(_out2, 8);                                              \
-    _out5 = __lasx_xvbsrl_v(_out4, 8);                                              \
-    _out7 = __lasx_xvbsrl_v(_out6, 8);                                              \
-}
+#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                   \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                   \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                   \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                   \
+    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                             \
+    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                             \
+    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                             \
+    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                               \
+    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                               \
+    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                               \
+    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                               \
+    _out1 = __lasx_xvbsrl_v(_out0, 8);                                       \
+    _out3 = __lasx_xvbsrl_v(_out2, 8);                                       \
+    _out5 = __lasx_xvbsrl_v(_out4, 8);                                       \
+    _out7 = __lasx_xvbsrl_v(_out6, 8);                                       \
+  }
 
 /*
  * =============================================================================
  * Description : Transpose 8x8 block with halfword elements in vectors.
  * Arguments   : Inputs  - _in0, _in1, ~
  *               Outputs - _out0, _out1, ~
- * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
  * Example     : LASX_TRANSPOSE8x8_H
  *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
  *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
@@ -1728,40 +1772,41 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
  * =============================================================================
  */
-#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_h(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvl_h(_in7, _in5);                                          \
-    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_h(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvh_h(_in7, _in5);                                          \
-    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_h(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvl_h(_in3, _in1);                                          \
-    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_h(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvh_h(_in3, _in1);                                          \
-    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-                                                                                    \
-    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                                    \
-    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                                    \
-    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                                    \
-    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                                    \
-    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                                    \
-    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                                    \
-    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                                    \
-    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                                    \
-}
+#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _s0_m, _s1_m;                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+                                                                             \
+    _s0_m = __lasx_xvilvl_h(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvl_h(_in7, _in5);                                     \
+    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_h(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvh_h(_in7, _in5);                                     \
+    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+                                                                             \
+    _s0_m = __lasx_xvilvl_h(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvl_h(_in3, _in1);                                     \
+    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_h(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvh_h(_in3, _in1);                                     \
+    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+                                                                             \
+    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                             \
+    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                             \
+    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                             \
+    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                             \
+    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                             \
+    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                             \
+    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                             \
+    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                             \
+  }
 
 /*
  * =============================================================================
@@ -1776,34 +1821,34 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  *               _out3 = _in0 - _in3;
  * =============================================================================
  */
-#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_b(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_b(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_b(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_b(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_h(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_h(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_h(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_h(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_w(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_w(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_w(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_w(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_d(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_d(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_d(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_d(_in0, _in3);                                         \
-}
+#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_b(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_b(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_b(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_b(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_h(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_h(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_h(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_h(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_w(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_w(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_w(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_w(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_d(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_d(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_d(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_d(_in0, _in3);                                        \
+  }
 
 /*
  * =============================================================================
@@ -1822,60 +1867,82 @@ static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
  *               _out7 = _in0 - _in7;
  * =============================================================================
  */
-#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_b(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_b(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_b(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_b(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_b(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_b(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_b(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_b(_in0, _in7);                                           \
-}
-
-#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_h(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_h(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_h(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_h(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_h(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_h(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_h(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_h(_in0, _in7);                                           \
-}
+#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_b(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_b(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_b(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_b(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_b(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_b(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_b(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_b(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_h(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_h(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_h(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_h(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_h(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_h(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_h(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_h(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_w(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_w(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_w(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_w(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_w(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_w(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_w(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_w(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_d(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_d(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_d(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_d(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_d(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_d(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_d(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_d(_in0, _in7);                                     \
+  }
+
+#endif  // LASX
 
-#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_w(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_w(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_w(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_w(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_w(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_w(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_w(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_w(_in0, _in7);                                           \
-}
-
-#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_d(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_d(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_d(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_d(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_d(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_d(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_d(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_d(_in0, _in7);                                           \
-}
-
-#endif //LASX
+/*
+ * =============================================================================
+ * Description : Print out elements in vector.
+ * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
+ *               Outputs -
+ * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
+ *               '_enter' is TRUE, prefix "\nVP:" will be added first.
+ * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
+ *               VP:1,2,3,4,
+ * =============================================================================
+ */
+#define VECT_PRINT(RTYPE, element_num, in0, enter)                 \
+  {                                                                \
+    RTYPE _tmp0 = (RTYPE)in0;                                      \
+    int _i = 0;                                                    \
+    if (enter) printf("\nVP:");                                    \
+    for (_i = 0; _i < element_num; _i++) printf("%d,", _tmp0[_i]); \
+  }
 
 #endif /* LOONGSON_INTRINSICS_H */
 #endif /* AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H */
-
diff --git a/libswscale/loongarch/yuv2rgb_lasx.c b/libswscale/loongarch/yuv2rgb_lasx.c
index c1f805e7a1..4fd7873aea 100644
--- a/libswscale/loongarch/yuv2rgb_lasx.c
+++ b/libswscale/loongarch/yuv2rgb_lasx.c
@@ -36,11 +36,13 @@
     __m256i vr_coeff = __lasx_xvreplgr2vr_d(c->vrCoeff);     \
 
 #define LOAD_YUV_16                                          \
-    m_y  = __lasx_xvld(py + (w << 4), 0);                    \
-    m_u  = __lasx_xvldrepl_d(pu + (w << 3), 0);              \
-    m_v  = __lasx_xvldrepl_d(pv + (w << 3), 0);              \
-    m_y = __lasx_vext2xv_hu_bu(m_y);                         \
-    DUP2_ARG1(__lasx_vext2xv_hu_bu, m_u, m_v, m_u, m_v);     \
+    m_y1 = __lasx_xvld(py_1, 0);                             \
+    m_y2 = __lasx_xvld(py_2, 0);                             \
+    m_u  = __lasx_xvldrepl_d(pu, 0);                         \
+    m_v  = __lasx_xvldrepl_d(pv, 0);                         \
+    DUP2_ARG2(__lasx_xvilvl_b, m_u, m_u, m_v, m_v, m_u, m_v);\
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, m_y1, m_y2, m_u, m_v,    \
+              m_y1, m_y2, m_u, m_v);                         \
 
 /* YUV2RGB method
  * The conversion method is as follows:
@@ -53,60 +55,62 @@
  */
 
 #define YUV2RGB                                                            \
-    m_y = __lasx_xvslli_h(m_y, 3);                                         \
-    m_u = __lasx_xvslli_h(m_u, 3);                                         \
-    m_v = __lasx_xvslli_h(m_v, 3);                                         \
-    m_y = __lasx_xvsub_h(m_y, y_offset);                                   \
-    m_u = __lasx_xvsub_h(m_u, u_offset);                                   \
-    m_v = __lasx_xvsub_h(m_v, v_offset);                                   \
-    m_u = __lasx_xvshuf_h(shuf1, m_u, m_u);                                \
-    m_v = __lasx_xvshuf_h(shuf1, m_v, m_v);                                \
-    y_1 = __lasx_xvmuh_h(m_y, y_coeff);                                    \
-    u2g = __lasx_xvmuh_h(m_u, ug_coeff);                                   \
-    u2b = __lasx_xvmuh_h(m_u, ub_coeff);                                   \
-    v2r = __lasx_xvmuh_h(m_v, vr_coeff);                                   \
-    v2g = __lasx_xvmuh_h(m_v, vg_coeff);                                   \
-    r   = __lasx_xvsadd_h(y_1, v2r);                                       \
-    v2g = __lasx_xvsadd_h(v2g, u2g);                                       \
-    g   = __lasx_xvsadd_h(v2g, y_1);                                       \
-    b   = __lasx_xvsadd_h(y_1, u2b);                                       \
-    DUP2_ARG1(__lasx_xvclip255_h, r, g, r, g);                             \
-    b = __lasx_xvclip255_h(b);                                             \
-
-#define RGB_PACK_16(r, g, b, rgb_l, rgb_h)                                 \
+    m_y1 = __lasx_xvslli_h(m_y1, 3);                                       \
+    m_y2 = __lasx_xvslli_h(m_y2, 3);                                       \
+    m_u  = __lasx_xvslli_h(m_u, 3);                                        \
+    m_v  = __lasx_xvslli_h(m_v, 3);                                        \
+    m_y1 = __lasx_xvsub_h(m_y1, y_offset);                                 \
+    m_y2 = __lasx_xvsub_h(m_y2, y_offset);                                 \
+    m_u  = __lasx_xvsub_h(m_u, u_offset);                                  \
+    m_v  = __lasx_xvsub_h(m_v, v_offset);                                  \
+    y_1  = __lasx_xvmuh_h(m_y1, y_coeff);                                  \
+    y_2  = __lasx_xvmuh_h(m_y2, y_coeff);                                  \
+    u2g  = __lasx_xvmuh_h(m_u, ug_coeff);                                  \
+    u2b  = __lasx_xvmuh_h(m_u, ub_coeff);                                  \
+    v2r  = __lasx_xvmuh_h(m_v, vr_coeff);                                  \
+    v2g  = __lasx_xvmuh_h(m_v, vg_coeff);                                  \
+    r1   = __lasx_xvsadd_h(y_1, v2r);                                      \
+    v2g  = __lasx_xvsadd_h(v2g, u2g);                                      \
+    g1   = __lasx_xvsadd_h(y_1, v2g);                                      \
+    b1   = __lasx_xvsadd_h(y_1, u2b);                                      \
+    r2   = __lasx_xvsadd_h(y_2, v2r);                                      \
+    g2   = __lasx_xvsadd_h(y_2, v2g);                                      \
+    b2   = __lasx_xvsadd_h(y_2, u2b);                                      \
+    DUP4_ARG1(__lasx_xvclip255_h, r1, g1, b1, r2, r1, g1, b1, r2);         \
+    DUP2_ARG1(__lasx_xvclip255_h, g2, b2, g2, b2);                         \
+
+#define RGB_PACK(r, g, b, rgb_l, rgb_h)                                    \
 {                                                                          \
     __m256i rg;                                                            \
     rg = __lasx_xvpackev_b(g, r);                                          \
     DUP2_ARG3(__lasx_xvshuf_b, b, rg, shuf2, b, rg, shuf3, rgb_l, rgb_h);  \
 }
 
-#define RGB_PACK_32(r, g, b, a, rgb_l, rgb_h)                              \
+#define RGB32_PACK(a, r, g, b, rgb_l, rgb_h)                               \
 {                                                                          \
-    __m256i rg, ba;                                                        \
-    rgb_l = __lasx_xvpackev_b(g, r);                                       \
-    rgb_h = __lasx_xvpackev_b(a, b);                                       \
-    rg = __lasx_xvilvl_h(rgb_h, rgb_l);                                    \
-    ba = __lasx_xvilvh_h(rgb_h, rgb_l);                                    \
-    rgb_l = __lasx_xvpermi_q(ba, rg, 0x20);                                \
-    rgb_h = __lasx_xvpermi_q(ba, rg, 0x31);                                \
+    __m256i ra, bg, tmp0, tmp1;                                            \
+    ra    = __lasx_xvpackev_b(r, a);                                       \
+    bg    = __lasx_xvpackev_b(b, g);                                       \
+    tmp0  = __lasx_xvilvl_h(bg, ra);                                       \
+    tmp1  = __lasx_xvilvh_h(bg, ra);                                       \
+    rgb_l = __lasx_xvpermi_q(tmp1, tmp0, 0x20);                            \
+    rgb_h = __lasx_xvpermi_q(tmp1, tmp0, 0x31);                            \
 }
 
-#define RGB_STORE_32(rgb_l, rgb_h, iamge, w)                               \
-{                                                                          \
-    uint8_t *index = image + (w * 64);                                     \
-    __lasx_xvst(rgb_l, index, 0);                                          \
-    __lasx_xvst(rgb_h, index + 32, 0);                                     \
+#define RGB_STORE(rgb_l, rgb_h, image)                                       \
+{                                                                            \
+    __lasx_xvstelm_d(rgb_l, image, 0,  0);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 8,  1);                                   \
+    __lasx_xvstelm_d(rgb_h, image, 16, 0);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 24, 2);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 32, 3);                                   \
+    __lasx_xvstelm_d(rgb_h, image, 40, 2);                                   \
 }
 
-#define RGB_STORE(rgb_l, rgb_h, image, w)                                      \
-{                                                                              \
-    uint8_t *index = image + (w * 48);                                         \
-    __lasx_xvstelm_d(rgb_l, (index), 0,  0);                                   \
-    __lasx_xvstelm_d(rgb_l, (index), 8,  1);                                   \
-    __lasx_xvstelm_d(rgb_h, (index), 16, 0);                                   \
-    __lasx_xvstelm_d(rgb_l, (index), 24, 2);                                   \
-    __lasx_xvstelm_d(rgb_l, (index), 32, 3);                                   \
-    __lasx_xvstelm_d(rgb_h, (index), 40, 2);                                   \
+#define RGB32_STORE(rgb_l, rgb_h, image)                                     \
+{                                                                            \
+    __lasx_xvst(rgb_l, image, 0);                                            \
+    __lasx_xvst(rgb_h, image, 32);                                           \
 }
 
 #define YUV2RGBFUNC(func_name, dst_type, alpha)                                     \
@@ -114,97 +118,213 @@
                          int srcStride[], int srcSliceY, int srcSliceH,             \
                          uint8_t *dst[], int dstStride[])                           \
 {                                                                                   \
-    int w, y, h_size, vshift;                                                       \
-    __m256i m_y, m_u, m_v;                                                          \
-    __m256i y_1, u2g, v2g, u2b, v2r, rgb_l, rgb_h;                                  \
-    __m256i r, g, b;                                                                \
+    int x, y, h_size, vshift, res;                                                  \
+    __m256i m_y1, m_y2, m_u, m_v;                                                   \
+    __m256i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m256i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
     __m256i shuf2 = {0x0504120302100100, 0x0A18090816070614,                        \
                      0x0504120302100100, 0x0A18090816070614};                       \
     __m256i shuf3 = {0x1E0F0E1C0D0C1A0B, 0x0101010101010101,                        \
                      0x1E0F0E1C0D0C1A0B, 0x0101010101010101};                       \
-    __m256i shuf1 = {0x0001000100000000, 0x0003000300020002,                        \
-                     0x0005000500040004, 0x0007000700060006};                       \
     YUV2RGB_LOAD_COE                                                                \
                                                                                     \
     h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
     vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
-    for (y = 0; y < srcSliceH; y++) {                                               \
-        dst_type *image   = dst[0] + (y + srcSliceY) * dstStride[0];                \
-        const uint8_t *py = src[0] +               y * srcStride[0];                \
-        const uint8_t *pu = src[1] +   (y >> vshift) * srcStride[1];                \
-        const uint8_t *pv = src[2] +   (y >> vshift) * srcStride[2];                \
-        for (w = 0; w < h_size; w ++) {                                             \
-
-
-#define END_FUNC()                                                                  \
-        }                                                                           \
-    }                                                                               \
-    return srcSliceH;                                                               \
-}
-
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (y + srcSliceY) * dstStride[0]);\
+        dst_type *image2    = (dst_type *)(image1 +                   dstStride[0]);\
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
 
-#define YUV2RGBFUNC_32(func_name, dst_type, alpha)                                  \
+#define YUV2RGBFUNC32(func_name, dst_type, alpha)                                   \
            int func_name(SwsContext *c, const uint8_t *src[],                       \
                          int srcStride[], int srcSliceY, int srcSliceH,             \
                          uint8_t *dst[], int dstStride[])                           \
 {                                                                                   \
-    int w, y, h_size, vshift, a = -1;                                               \
-    __m256i m_y, m_u, m_v;                                                          \
-    __m256i y_1, u2g, v2g, u2b, v2r, rgb_l, rgb_h;                                  \
-    __m256i r, g, b;                                                                \
-    __m256i alp = __lasx_xvreplgr2vr_w(a);                                          \
-    __m256i shuf1 = {0x0001000100000000, 0x0003000300020002,                        \
-                     0x0005000500040004, 0x0007000700060006};                       \
+    int x, y, h_size, vshift, res;                                                  \
+    __m256i m_y1, m_y2, m_u, m_v;                                                   \
+    __m256i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m256i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
+    __m256i a = __lasx_xvldi(0xFF);                                                 \
+                                                                                    \
     YUV2RGB_LOAD_COE                                                                \
                                                                                     \
     h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
     vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
-    for (y = 0; y < srcSliceH; y++) {                                               \
-        dst_type *image   = dst[0] + (y + srcSliceY) * dstStride[0];                \
-        const uint8_t *py = src[0] +               y * srcStride[0];                \
-        const uint8_t *pu = src[1] +   (y >> vshift) * srcStride[1];                \
-        const uint8_t *pv = src[2] +   (y >> vshift) * srcStride[2];                \
-        for (w = 0; w < h_size; w ++) {                                             \
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        int yd = y + srcSliceY;                                                     \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (yd)     * dstStride[0]);       \
+        dst_type *image2    = (dst_type *)(dst[0] + (yd + 1) * dstStride[0]);       \
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
+
+#define DEALYUV2RGBREMAIN                                                           \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 48;                                                           \
+            image2 += 48;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                       \
 
+#define DEALYUV2RGBREMAIN32                                                         \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 16;                                                           \
+            image2 += 16;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                       \
+
+
+#define PUTRGB24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = r[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = b[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = r[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = b[Y];
+
+#define PUTBGR24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = b[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = r[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = b[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = r[Y];
+
+#define PUTRGB(dst, src)                    \
+    Y      = src[0];                        \
+    dst[0] = r[Y] + g[Y] + b[Y];            \
+    Y      = src[1];                        \
+    dst[1] = r[Y] + g[Y] + b[Y];            \
+
+#define ENDRES                              \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 6;                            \
+    image2 += 6;                            \
+
+#define ENDRES32                            \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 2;                            \
+    image2 += 2;                            \
+
+#define END_FUNC()                          \
+        }                                   \
+    }                                       \
+    return srcSliceH;                       \
+}
 
 YUV2RGBFUNC(yuv420_rgb24_lasx, uint8_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_16(r, g, b, rgb_l, rgb_h);
-    RGB_STORE(rgb_l, rgb_h, image, w);
+    RGB_PACK(r1, g1, b1, rgb1_l, rgb1_h);
+    RGB_PACK(r2, g2, b2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN
+    PUTRGB24(image1, py_1);
+    PUTRGB24(image2, py_2);
+    ENDRES
     END_FUNC()
 
 YUV2RGBFUNC(yuv420_bgr24_lasx, uint8_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_16(b, g, r, rgb_l, rgb_h);
-    RGB_STORE(rgb_l, rgb_h, image, w);
+    RGB_PACK(b1, g1, r1, rgb1_l, rgb1_h);
+    RGB_PACK(b2, g2, r2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN
+    PUTBGR24(image1, py_1);
+    PUTBGR24(image2, py_2);
+    ENDRES
     END_FUNC()
 
-YUV2RGBFUNC_32(yuv420_rgba32_lasx, uint8_t, 0)
+YUV2RGBFUNC32(yuv420_rgba32_lasx, uint32_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_32(r, g, b, alp, rgb_l, rgb_h);
-    RGB_STORE_32(rgb_l, rgb_h, image, w);
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
     END_FUNC()
 
-YUV2RGBFUNC_32(yuv420_bgra32_lasx, uint8_t, 0)
+YUV2RGBFUNC32(yuv420_bgra32_lasx, uint32_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_32(b, g, r, alp, rgb_l, rgb_h);
-    RGB_STORE_32(rgb_l, rgb_h, image, w);
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
     END_FUNC()
 
-YUV2RGBFUNC_32(yuv420_argb32_lasx, uint8_t, 0)
+YUV2RGBFUNC32(yuv420_argb32_lasx, uint32_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_32(alp, r, g, b, rgb_l, rgb_h);
-    RGB_STORE_32(rgb_l, rgb_h, image, w);
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
     END_FUNC()
 
-YUV2RGBFUNC_32(yuv420_abgr32_lasx, uint8_t, 0)
+YUV2RGBFUNC32(yuv420_abgr32_lasx, uint32_t, 0)
     LOAD_YUV_16
     YUV2RGB
-    RGB_PACK_32(alp, b, g, r, rgb_l, rgb_h);
-    RGB_STORE_32(rgb_l, rgb_h, image, w);
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
     END_FUNC()
