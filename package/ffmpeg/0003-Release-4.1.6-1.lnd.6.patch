From 39ed8565dff31986dee2b332a3147ca73d2c840e Mon Sep 17 00:00:00 2001
From: Hao Chen <chenhao@loongson.cn>
Date: Mon, 18 Oct 2021 17:57:57 +0800
Subject: [PATCH] Release 4.1.6-1.lnd.6

Patch 7752dd6==>f7f7601.

Change-Id: Iedb389ea2b5df6b4fdd24c7b0ba15cbaf3566560
---
 libavcodec/loongarch/Makefile                |    3 +-
 libavcodec/loongarch/h264_deblock_lasx.c     |   71 +-
 libavcodec/loongarch/h264chroma_lasx.c       |  594 +--
 libavcodec/loongarch/h264dsp_lasx.c          | 1053 ++++--
 libavcodec/loongarch/h264idct_lasx.c         |  219 +-
 libavcodec/loongarch/h264qpel_lasx.c         |  483 ++-
 libavcodec/loongarch/hevc_idct_lsx.c         |  387 +-
 libavcodec/loongarch/hevc_lpf_sao_lsx.c      | 1811 +++++----
 libavcodec/loongarch/hevc_macros_lsx.h       |   24 +-
 libavcodec/loongarch/hevc_mc_bi_lsx.c        | 1777 +++++----
 libavcodec/loongarch/hevc_mc_uni_lsx.c       |  681 ++--
 libavcodec/loongarch/hevc_mc_uniw_lsx.c      |  103 +-
 libavcodec/loongarch/hevcdsp_lsx.c           | 2411 ++++++------
 libavcodec/loongarch/hpeldsp_lasx.c          |  903 +++--
 libavcodec/loongarch/hpeldsp_lasx.h          |    1 +
 libavcodec/loongarch/idctdsp_lasx.c          |   71 +-
 libavcodec/loongarch/simple_idct_lasx.c      |  169 +-
 libavcodec/loongarch/vc1dsp_lasx.c           |  602 +--
 libavcodec/loongarch/vp8_lpf_lsx.c           |  228 +-
 libavcodec/loongarch/vp8_mc_lsx.c            |  951 +++++
 libavcodec/loongarch/vp8dsp_init_loongarch.c |   21 +
 libavcodec/loongarch/vp8dsp_loongarch.h      |   51 +
 libavcodec/loongarch/vp9_idct_lsx.c          |  618 ++-
 libavcodec/loongarch/vp9_intra_lsx.c         |  268 +-
 libavcodec/loongarch/vp9_lpf_lsx.c           |  621 ++-
 libavcodec/loongarch/vp9_mc_lsx.c            |  842 +++--
 libavutil/loongarch/generic_macros_lasx.h    | 3561 ------------------
 libavutil/loongarch/generic_macros_lsx.h     |  670 ----
 libavutil/loongarch/loongson_intrinsics.h    | 1881 +++++++++
 libswscale/loongarch/input_lasx.c            |   44 +-
 libswscale/loongarch/output_lasx.c           |  259 +-
 libswscale/loongarch/rgb2rgb_lasx.c          |    7 +-
 libswscale/loongarch/swscale_lasx.c          |  663 ++--
 libswscale/loongarch/yuv2rgb_lasx.c          |   21 +-
 34 files changed, 10728 insertions(+), 11341 deletions(-)
 create mode 100644 libavcodec/loongarch/vp8_mc_lsx.c
 delete mode 100644 libavutil/loongarch/generic_macros_lasx.h
 delete mode 100644 libavutil/loongarch/generic_macros_lsx.h
 create mode 100644 libavutil/loongarch/loongson_intrinsics.h

diff --git a/libavcodec/loongarch/Makefile b/libavcodec/loongarch/Makefile
index 2a8d7e430f..d82eef4fd0 100644
--- a/libavcodec/loongarch/Makefile
+++ b/libavcodec/loongarch/Makefile
@@ -27,4 +27,5 @@ LSX-OBJS-$(CONFIG_VP9_DECODER)        += loongarch/vp9_mc_lsx.o      \
                                          loongarch/vp9_intra_lsx.o \
                                          loongarch/vp9_lpf_lsx.o \
                                          loongarch/vp9_idct_lsx.o
-LSX-OBJS-$(CONFIG_VP8_DECODER)        += loongarch/vp8_lpf_lsx.o
+LSX-OBJS-$(CONFIG_VP8_DECODER)        += loongarch/vp8_mc_lsx.o \
+                                         loongarch/vp8_lpf_lsx.o
diff --git a/libavcodec/loongarch/h264_deblock_lasx.c b/libavcodec/loongarch/h264_deblock_lasx.c
index 73706739a0..886cc66d95 100644
--- a/libavcodec/loongarch/h264_deblock_lasx.c
+++ b/libavcodec/loongarch/h264_deblock_lasx.c
@@ -21,7 +21,7 @@
 
 #include "libavcodec/bit_depth_template.c"
 #include "h264dsp_lasx.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 #define H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv, dir, \
                                                  d_idx, mask_dir)           \
@@ -42,10 +42,10 @@ do {                                                                        \
         out &= mask_dir; \
         if (!(mask_mv & b_idx)) { \
             if (bidir) { \
-                ref2 = LASX_LD(ref_t + d_idx_12); \
-                ref3 = LASX_LD(ref_t + d_idx_52); \
-                ref0 = LASX_LD(ref_t + 12); \
-                ref1 = LASX_LD(ref_t + 52); \
+                ref2 = __lasx_xvldx(ref_t, d_idx_12); \
+                ref3 = __lasx_xvldx(ref_t, d_idx_52); \
+                ref0 = __lasx_xvld(ref_t, 12); \
+                ref1 = __lasx_xvld(ref_t, 52); \
                 ref2 = __lasx_xvilvl_w(ref3, ref2); \
                 ref0 = __lasx_xvilvl_w(ref0, ref0); \
                 ref1 = __lasx_xvilvl_w(ref1, ref1); \
@@ -54,51 +54,53 @@ do {                                                                        \
                 ref1 = __lasx_xvsub_b(ref1, ref3); \
                 ref0 = __lasx_xvor_v(ref0, ref1); \
 \
-                tmp2 = LASX_LD(mv_t + d_idx_x4_48);   \
-                tmp3 = LASX_LD(mv_t + 48); \
-                tmp4 = LASX_LD(mv_t + 208); \
-                tmp5 = LASX_LD(mv_t + 208 + d_idx_x4); \
-                LASX_PCKEV_Q_2(tmp2, tmp2, tmp5, tmp5, tmp2, tmp5); \
-                LASX_PCKEV_Q(tmp4, tmp3, tmp3); \
+                tmp2 = __lasx_xvldx(mv_t, d_idx_x4_48);   \
+                tmp3 = __lasx_xvld(mv_t, 48); \
+                tmp4 = __lasx_xvld(mv_t, 208); \
+                tmp5 = __lasx_xvld(mv_t + d_idx_x4, 208); \
+                DUP2_ARG3(__lasx_xvpermi_q, tmp2, tmp2, 0x20, tmp5, tmp5, 0x20, tmp2, tmp5); \
+                tmp3 =  __lasx_xvpermi_q(tmp4, tmp3, 0x20); \
                 tmp2 = __lasx_xvsub_h(tmp2, tmp3); \
                 tmp5 = __lasx_xvsub_h(tmp5, tmp3); \
-                LASX_SAT_H_2(tmp2, tmp5, tmp2, tmp5, 7); \
-                LASX_PCKEV_B(tmp5, tmp2, tmp0); \
+                DUP2_ARG2(__lasx_xvsat_h, tmp2, 7, tmp5, 7, tmp2, tmp5); \
+                tmp0 = __lasx_xvpickev_b(tmp5, tmp2); \
+                tmp0 = __lasx_xvpermi_d(tmp0, 0xd8); \
                 tmp0 = __lasx_xvadd_b(tmp0, cnst_1); \
                 tmp0 = __lasx_xvssub_bu(tmp0, cnst_0); \
-                LASX_SAT_H(tmp0, tmp0, 7); \
-                LASX_PCKEV_B(tmp0, tmp0, tmp0); \
-                LASX_PCKOD_D_128SV(tmp0, tmp0, tmp1); \
+                tmp0 = __lasx_xvsat_h(tmp0, 7); \
+                tmp0 = __lasx_xvpickev_b(tmp0, tmp0); \
+                tmp0 = __lasx_xvpermi_d(tmp0, 0xd8); \
+                tmp1 = __lasx_xvpickod_d(tmp0, tmp0); \
                 out = __lasx_xvor_v(ref0, tmp0); \
                 tmp1 = __lasx_xvshuf4i_w(tmp1, 0xB1); \
                 out = __lasx_xvor_v(out, tmp1); \
                 tmp0 = __lasx_xvshuf4i_w(out, 0xB1); \
                 out = __lasx_xvmin_bu(out, tmp0); \
             } else { \
-                ref0 = LASX_LD(ref_t + d_idx_12); \
-                ref3 = LASX_LD(ref_t + 12); \
-                tmp2 = LASX_LD(mv_t + d_idx_x4_48); \
-                tmp3 = LASX_LD(mv_t + 48); \
+                ref0 = __lasx_xvldx(ref_t, d_idx_12); \
+                ref3 = __lasx_xvld(ref_t, 12); \
+                tmp2 = __lasx_xvldx(mv_t, d_idx_x4_48); \
+                tmp3 = __lasx_xvld(mv_t, 48); \
                 tmp4 = __lasx_xvsub_h(tmp3, tmp2); \
-                LASX_SAT_H(tmp4, tmp1, 7); \
+                tmp1 = __lasx_xvsat_h(tmp4, 7); \
                 tmp1 = __lasx_xvpickev_b(tmp1, tmp1); \
                 tmp1 = __lasx_xvadd_b(tmp1, cnst_1); \
                 out = __lasx_xvssub_bu(tmp1, cnst_0); \
-                LASX_SAT_H(out, out, 7); \
+                out = __lasx_xvsat_h(out, 7); \
                 out = __lasx_xvpickev_b(out, out); \
                 ref0 = __lasx_xvsub_b(ref3, ref0); \
                 out = __lasx_xvor_v(out, ref0); \
             } \
         } \
-        tmp0 = LASX_LD(nnz_t + 12); \
-        tmp1 = LASX_LD(nnz_t + d_idx_12); \
+        tmp0 = __lasx_xvld(nnz_t, 12); \
+        tmp1 = __lasx_xvldx(nnz_t, d_idx_12); \
         tmp0 = __lasx_xvor_v(tmp0, tmp1); \
         tmp0 = __lasx_xvmin_bu(tmp0, cnst_2); \
         out  = __lasx_xvmin_bu(out, cnst_2); \
         tmp0 = __lasx_xvslli_h(tmp0, 1); \
         tmp0 = __lasx_xvmax_bu(out, tmp0); \
-        LASX_UNPCK_L_HU_BU(tmp0, tmp0); \
-        LASX_ST_D(tmp0, 0,  bS_t + dir_x32); \
+        tmp0 = __lasx_vext2xv_hu_bu(tmp0); \
+        __lasx_xvstelm_d(tmp0, bS_t + dir_x32, 0, 0); \
         ref_t += step; \
         mv_t  += step_x4; \
         nnz_t += step; \
@@ -124,8 +126,7 @@ void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
         cnst_1 = __lasx_xvreplgr2vr_d(cnst4);
         cnst_2 = __lasx_xvldi(0x01);
     } else {
-        cnst_0 = __lasx_xvldi(0x06);
-        cnst_1 = __lasx_xvldi(0x03);
+        DUP2_ARG1(__lasx_xvldi, 0x06, 0x03, cnst_0, cnst_1);
         cnst_2 = __lasx_xvldi(0x01);
     }
     step  <<= 3;
@@ -134,11 +135,11 @@ void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
     H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv1, 1, -8, zero);
     H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(32, 8, mask_mv0, 0, -1, one);
 
-    LASX_LD_2((int8_t*)bS, 16, tmp0, tmp1);
-    LASX_ILVH_D_2_128SV(tmp0, tmp0, tmp1, tmp1, tmp2, tmp3);
-    LASX_TRANSPOSE4x4_H_128SV(tmp0, tmp2, tmp1, tmp3, tmp2, tmp3, tmp4, tmp5);
-     __lasx_xvstelm_d(tmp2, (int8_t*)bS, 0, 0);
-     __lasx_xvstelm_d(tmp3, (int8_t*)bS + 8, 0, 0);
-     __lasx_xvstelm_d(tmp4, (int8_t*)bS + 16, 0, 0);
-     __lasx_xvstelm_d(tmp5, (int8_t*)bS + 24, 0, 0);
+    DUP2_ARG2(__lasx_xvld, (int8_t*)bS, 0, (int8_t*)bS, 16, tmp0, tmp1);
+    DUP2_ARG2(__lasx_xvilvh_d, tmp0, tmp0, tmp1, tmp1, tmp2, tmp3);
+    LASX_TRANSPOSE4x4_H(tmp0, tmp2, tmp1, tmp3, tmp2, tmp3, tmp4, tmp5);
+    __lasx_xvstelm_d(tmp2, (int8_t*)bS, 0, 0);
+    __lasx_xvstelm_d(tmp3, (int8_t*)bS + 8, 0, 0);
+    __lasx_xvstelm_d(tmp4, (int8_t*)bS + 16, 0, 0);
+    __lasx_xvstelm_d(tmp5, (int8_t*)bS + 24, 0, 0);
 }
diff --git a/libavcodec/loongarch/h264chroma_lasx.c b/libavcodec/loongarch/h264chroma_lasx.c
index 4fbe99ba73..82cef77a95 100644
--- a/libavcodec/loongarch/h264chroma_lasx.c
+++ b/libavcodec/loongarch/h264chroma_lasx.c
@@ -24,7 +24,7 @@
 #include "h264chroma_lasx.h"
 #include "libavutil/attributes.h"
 #include "libavutil/avassert.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 static const uint8_t chroma_mask_arr[32] = {
     0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
@@ -36,6 +36,8 @@ static av_always_inline void avc_chroma_hv_8x4_lasx(uint8_t *src, uint8_t *dst,
                              uint32_t coef_hor1, uint32_t coef_ver0,
                              uint32_t coef_ver1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i src0, src1, src2, src3, src4, out;
     __m256i res_hz0, res_hz1, res_hz2, res_vt0, res_vt1;
     __m256i mask;
@@ -45,28 +47,31 @@ static av_always_inline void avc_chroma_hv_8x4_lasx(uint8_t *src, uint8_t *dst,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h(coef_ver0);
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h(coef_ver1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    src0 = LASX_LD(src);
-    LASX_LD_4(src + stride, stride, src1, src2, src3, src4);
-    LASX_PCKEV_Q_2(src2, src1, src4, src3, src1, src3);
-    LASX_SHUF_B_128SV(src0, src0, mask, src0);
-    LASX_SHUF_B_2_128SV(src1, src1, src3, src3, mask, mask, src1, src3);
-    LASX_DP2_H_BU(src0, coeff_hz_vec, res_hz0);
-    LASX_DP2_H_BU(src1, coeff_hz_vec, res_hz1);
-    LASX_DP2_H_BU(src3, coeff_hz_vec, res_hz2);
+    DUP2_ARG2(__lasx_xvld, chroma_mask_arr, 0, src, 0, mask, src0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    DUP2_ARG3(__lasx_xvpermi_q, src2, src1, 0x20, src4, src3, 0x20, src1, src3);
+    src0 = __lasx_xvshuf_b(src0, src0, mask);
+    DUP2_ARG3(__lasx_xvshuf_b, src1, src1, mask, src3, src3, mask, src1, src3);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, res_hz0, res_hz1);
+    res_hz2 = __lasx_xvdp2_h_bu(src3, coeff_hz_vec);
     res_vt0 = __lasx_xvmul_h(res_hz1, coeff_vt_vec0);
     res_vt1 = __lasx_xvmul_h(res_hz2, coeff_vt_vec0);
-    LASX_PCKEV_Q(res_hz1, res_hz0, res_hz0);
+    res_hz0 = __lasx_xvpermi_q(res_hz1, res_hz0, 0x20);
     res_hz1 = __lasx_xvpermi_q(res_hz1, res_hz2, 0x3);
     res_hz0 = __lasx_xvmul_h(res_hz0, coeff_vt_vec1);
     res_hz1 = __lasx_xvmul_h(res_hz1, coeff_vt_vec1);
     res_vt0 = __lasx_xvadd_h(res_vt0, res_hz0);
     res_vt1 = __lasx_xvadd_h(res_vt1, res_hz1);
-    LASX_SRARI_H_2(res_vt0, res_vt1, res_vt0, res_vt1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res_vt0, 6, res_vt1, 6, res_vt0, res_vt1);
     res_vt0 = __lasx_xvsat_hu(res_vt0, 7);
     res_vt1 = __lasx_xvsat_hu(res_vt1, 7);
-    LASX_PCKEV_B_128SV(res_vt1, res_vt0, out);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    out = __lasx_xvpickev_b(res_vt1, res_vt0);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hv_8x8_lasx(uint8_t *src, uint8_t *dst,
@@ -74,6 +79,9 @@ static av_always_inline void avc_chroma_hv_8x8_lasx(uint8_t *src, uint8_t *dst,
                              uint32_t coef_hor1, uint32_t coef_ver0,
                              uint32_t coef_ver1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i out0, out1;
     __m256i res_hz0, res_hz1, res_hz2, res_hz3, res_hz4;
@@ -85,25 +93,26 @@ static av_always_inline void avc_chroma_hv_8x8_lasx(uint8_t *src, uint8_t *dst,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h(coef_ver0);
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h(coef_ver1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    src0 = LASX_LD(src);
-    LASX_LD_8(src + stride, stride, src1, src2, src3, src4,
+    DUP2_ARG2(__lasx_xvld, chroma_mask_arr, 0, src, 0, mask, src0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
               src5, src6, src7, src8);
-    LASX_PCKEV_Q_4(src2, src1, src4, src3, src6, src5, src8, src7,
-                   src1, src3, src5, src7);
-    LASX_SHUF_B_128SV(src0, src0, mask, src0);
-    LASX_SHUF_B_4_128SV(src1, src1, src3, src3, src5, src5, src7, src7,
-                        mask, mask, mask, mask, src1, src3, src5, src7);
-    LASX_DP2_H_BU(src0, coeff_hz_vec, res_hz0);
-    LASX_DP2_H_BU(src1, coeff_hz_vec, res_hz1);
-    LASX_DP2_H_BU(src3, coeff_hz_vec, res_hz2);
-    LASX_DP2_H_BU(src5, coeff_hz_vec, res_hz3);
-    LASX_DP2_H_BU(src7, coeff_hz_vec, res_hz4);
+    DUP4_ARG3(__lasx_xvpermi_q, src2, src1, 0x20, src4, src3, 0x20, src6, src5, 0x20,
+              src8, src7, 0x20, src1, src3, src5, src7);
+    src0 = __lasx_xvshuf_b(src0, src0, mask);
+    DUP4_ARG3(__lasx_xvshuf_b, src1, src1, mask, src3, src3, mask, src5, src5, mask, src7,
+              src7, mask, src1, src3, src5, src7);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src3,
+              coeff_hz_vec, src5, coeff_hz_vec, res_hz0, res_hz1, res_hz2, res_hz3);
+    res_hz4 = __lasx_xvdp2_h_bu(src7, coeff_hz_vec);
     res_vt0 = __lasx_xvmul_h(res_hz1, coeff_vt_vec0);
     res_vt1 = __lasx_xvmul_h(res_hz2, coeff_vt_vec0);
     res_vt2 = __lasx_xvmul_h(res_hz3, coeff_vt_vec0);
     res_vt3 = __lasx_xvmul_h(res_hz4, coeff_vt_vec0);
-    LASX_PCKEV_Q(res_hz1, res_hz0, res_hz0);
+    res_hz0 = __lasx_xvpermi_q(res_hz1, res_hz0, 0x20);
     res_hz1 = __lasx_xvpermi_q(res_hz1, res_hz2, 0x3);
     res_hz2 = __lasx_xvpermi_q(res_hz2, res_hz3, 0x3);
     res_hz3 = __lasx_xvpermi_q(res_hz3, res_hz4, 0x3);
@@ -115,20 +124,29 @@ static av_always_inline void avc_chroma_hv_8x8_lasx(uint8_t *src, uint8_t *dst,
     res_vt1 = __lasx_xvadd_h(res_vt1, res_hz1);
     res_vt2 = __lasx_xvadd_h(res_vt2, res_hz2);
     res_vt3 = __lasx_xvadd_h(res_vt3, res_hz3);
-    LASX_SRARI_H_4(res_vt0, res_vt1, res_vt2, res_vt3,
-                   res_vt0, res_vt1, res_vt2, res_vt3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res_vt0, 6, res_vt1, 6, res_vt2, 6, res_vt3, 6,
+              res_vt0, res_vt1, res_vt2, res_vt3);
     res_vt0 = __lasx_xvsat_hu(res_vt0, 7);
     res_vt1 = __lasx_xvsat_hu(res_vt1, 7);
     res_vt2 = __lasx_xvsat_hu(res_vt2, 7);
     res_vt3 = __lasx_xvsat_hu(res_vt3, 7);
-    LASX_PCKEV_B_2_128SV(res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride);
+    DUP2_ARG2(__lasx_xvpickev_b, res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hz_8x4_lasx(uint8_t *src, uint8_t *dst,
                              ptrdiff_t stride, uint32_t coeff0, uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i src0, src1, src2, src3, out;
     __m256i res0, res1;
     __m256i mask;
@@ -136,24 +154,31 @@ static av_always_inline void avc_chroma_hz_8x4_lasx(uint8_t *src, uint8_t *dst,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    LASX_LD_4(src, stride, src0, src1, src2, src3);
-    LASX_PCKEV_Q_2(src1, src0, src3, src2, src0, src2);
-    LASX_SHUF_B_2_128SV(src0, src0, src2, src2, mask, mask, src0, src2);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
+    mask = __lasx_xvld(chroma_mask_arr, 0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src2);
+    DUP2_ARG3(__lasx_xvshuf_b, src0, src0, mask, src2, src2, mask, src0, src2);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, res0, res1);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
-    LASX_SRARI_H_2(res0, res1, res0, res1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res0, res1);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
-    LASX_PCKEV_B_128SV(res1, res0, out);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    out = __lasx_xvpickev_b(res1, res0);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
+
 }
 
 static av_always_inline void avc_chroma_hz_8x8_lasx(uint8_t *src, uint8_t *dst,
                              ptrdiff_t stride, uint32_t coeff0, uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i out0, out1;
     __m256i res0, res1, res2, res3;
@@ -162,29 +187,37 @@ static av_always_inline void avc_chroma_hz_8x8_lasx(uint8_t *src, uint8_t *dst,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    LASX_LD_8(src, stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src2, src4, src6);
-    LASX_SHUF_B_4_128SV(src0, src0, src2, src2, src4, src4, src6, src6,
-                        mask, mask, mask, mask, src0, src2, src4, src6);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
-    LASX_DP2_H_BU(src4, coeff_vec, res2);
-    LASX_DP2_H_BU(src6, coeff_vec, res3);
+    mask = __lasx_xvld(chroma_mask_arr, 0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src0, src1, src2, src3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4, 0x20,
+              src7, src6, 0x20, src0, src2, src4, src6);
+    DUP4_ARG3(__lasx_xvshuf_b, src0, src0, mask, src2, src2, mask, src4, src4, mask,
+              src6, src6, mask, src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, src4, coeff_vec, src6,
+              coeff_vec, res0, res1, res2, res3);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
     res2 = __lasx_xvslli_h(res2, 3);
     res3 = __lasx_xvslli_h(res3, 3);
-    LASX_SRARI_H_4(res0, res1, res2, res3,
-                   res0, res1, res2, res3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1, res2, res3);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
     res2 = __lasx_xvsat_hu(res2, 7);
     res3 = __lasx_xvsat_hu(res3, 7);
-    LASX_PCKEV_B_2_128SV(res1, res0, res3, res2, out0, out1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride)
+    DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, out0, out1);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hz_nonmult_lasx(uint8_t *src,
@@ -192,6 +225,9 @@ static av_always_inline void avc_chroma_hz_nonmult_lasx(uint8_t *src,
                              uint32_t coeff1, int32_t height)
 {
     uint32_t row;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i src0, src1, src2, src3, out;
     __m256i res0, res1;
     __m256i mask;
@@ -199,36 +235,39 @@ static av_always_inline void avc_chroma_hz_nonmult_lasx(uint8_t *src,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
+    mask = __lasx_xvld(chroma_mask_arr, 0);
 
     for (row = height >> 2; row--;) {
-        LASX_LD_4(src, stride, src0, src1, src2, src3);
-        src += (stride * 4);
-        LASX_PCKEV_Q_2(src1, src0, src3, src2, src0, src2);
-        LASX_SHUF_B_2_128SV(src0, src0, src2, src2, mask, mask, src0, src2);
-        LASX_DP2_H_BU(src0, coeff_vec, res0);
-        LASX_DP2_H_BU(src2, coeff_vec, res1);
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+                  src0, src1, src2, src3);
+        src += stride_4x;
+        DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src2);
+        DUP2_ARG3(__lasx_xvshuf_b, src0, src0, mask, src2, src2, mask, src0, src2);
+        DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, res0, res1);
         res0 = __lasx_xvslli_h(res0, 3);
         res1 = __lasx_xvslli_h(res1, 3);
-        LASX_SRARI_H_2(res0, res1, res0, res1, 6);
+        DUP2_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res0, res1);
         res0 = __lasx_xvsat_hu(res0, 7);
         res1 = __lasx_xvsat_hu(res1, 7);
-        LASX_PCKEV_B_128SV(res1, res0, out);
-        LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
-        dst += (stride * 4);
+        out = __lasx_xvpickev_b(res1, res0);
+        __lasx_xvstelm_d(out, dst, 0, 0);
+        __lasx_xvstelm_d(out, dst + stride, 0, 2);
+        __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+        __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
+        dst += stride_4x;
     }
 
     if ((height & 3)) {
         for (row = (height & 3); row--;) {
-            src0 = LASX_LD(src);
+            src0 = __lasx_xvld(src, 0);
             src += stride;
-            LASX_SHUF_B_128SV(src0, src0, mask, src0);
-            LASX_DP2_H_BU(src0, coeff_vec, res0);
+            src0 = __lasx_xvshuf_b(src0, src0, mask);
+            res0 = __lasx_xvdp2_h_bu(src0, coeff_vec);
             res0 = __lasx_xvslli_h(res0, 3);
-            LASX_SRARI_H(res0, res0, 6);
+            res0 = __lasx_xvsrari_h(res0, 6);
             res0 = __lasx_xvsat_hu(res0, 7);
-            LASX_PCKEV_B_128SV(res0, res0, out);
-            LASX_ST_D(out, 0, dst);
+            out  = __lasx_xvpickev_b(res0, res0);
+            __lasx_xvstelm_d(out, dst, 0, 0);
             dst += stride;
         }
     }
@@ -237,31 +276,40 @@ static av_always_inline void avc_chroma_hz_nonmult_lasx(uint8_t *src,
 static av_always_inline void avc_chroma_vt_8x4_lasx(uint8_t *src, uint8_t *dst,
                              ptrdiff_t stride, uint32_t coeff0, uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i src0, src1, src2, src3, src4, out;
     __m256i res0, res1;
     __m256i coeff_vec0 = __lasx_xvreplgr2vr_b(coeff0);
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    src0 = LASX_LD(src);
-    LASX_LD_4(src + stride, stride, src1, src2, src3, src4);
-    LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src0, src1, src2, src3);
-    LASX_ILVL_B_2_128SV(src1, src0, src3, src2, src0, src2);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
+    src0 = __lasx_xvld(src, 0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2, 0x20,
+              src4, src3, 0x20, src0, src1, src2, src3);
+    DUP2_ARG2(__lasx_xvilvl_b, src1, src0, src3, src2, src0, src2);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, res0, res1);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
-    LASX_SRARI_H_2(res0, res1, res0, res1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res0, res1);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
-    LASX_PCKEV_B_128SV(res1, res0, out);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    out = __lasx_xvpickev_b(res1, res0);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_vt_8x8_lasx(uint8_t *src, uint8_t *dst,
                              ptrdiff_t stride, uint32_t coeff0, uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i out0, out1;
     __m256i res0, res1, res2, res3;
@@ -269,31 +317,40 @@ static av_always_inline void avc_chroma_vt_8x8_lasx(uint8_t *src, uint8_t *dst,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    src0 = LASX_LD(src);
-    LASX_LD_8(src+stride, stride, src1, src2, src3, src4,
+    src0 = __lasx_xvld(src, 0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
               src5, src6, src7, src8);
-    LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src0, src1, src2, src3);
-    LASX_PCKEV_Q_4(src5, src4, src6, src5, src7, src6, src8, src7,
-                   src4, src5, src6, src7);
-    LASX_ILVL_B_4_128SV(src1, src0, src3, src2, src5, src4, src7, src6,
-                        src0, src2, src4, src6);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
-    LASX_DP2_H_BU(src4, coeff_vec, res2);
-    LASX_DP2_H_BU(src6, coeff_vec, res3);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2, 0x20,
+              src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6, 0x20,
+              src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvilvl_b, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, src4, coeff_vec,
+              src6, coeff_vec, res0, res1, res2, res3);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
     res2 = __lasx_xvslli_h(res2, 3);
     res3 = __lasx_xvslli_h(res3, 3);
-    LASX_SRARI_H_4(res0, res1, res2, res3, res0, res1, res2, res3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1, res2, res3);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
     res2 = __lasx_xvsat_hu(res2, 7);
     res3 = __lasx_xvsat_hu(res3, 7);
-    LASX_PCKEV_B_2_128SV(res1, res0, res3, res2, out0, out1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride);
+    DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, out0, out1);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void copy_width8x8_lasx(uint8_t *src, uint8_t *dst,
@@ -439,6 +496,9 @@ static av_always_inline void avc_chroma_hv_and_aver_dst_8x4_lasx(uint8_t *src,
                              uint32_t coef_hor1, uint32_t coef_ver0,
                              uint32_t coef_ver1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tp0, tp1, tp2, tp3;
     __m256i src0, src1, src2, src3, src4, out;
     __m256i res_hz0, res_hz1, res_hz2, res_vt0, res_vt1;
@@ -449,32 +509,35 @@ static av_always_inline void avc_chroma_hv_and_aver_dst_8x4_lasx(uint8_t *src,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h(coef_ver0);
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h(coef_ver1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    src0 = LASX_LD(src);
-    LASX_LD_4(src + stride, stride, src1, src2, src3, src4);
-    LASX_PCKEV_Q_2(src2, src1, src4, src3, src1, src3);
-    LASX_SHUF_B_128SV(src0, src0, mask, src0);
-    LASX_SHUF_B_2_128SV(src1, src1, src3, src3, mask, mask, src1, src3);
-    LASX_DP2_H_BU(src0, coeff_hz_vec, res_hz0);
-    LASX_DP2_H_BU(src1, coeff_hz_vec, res_hz1);
-    LASX_DP2_H_BU(src3, coeff_hz_vec, res_hz2);
+    DUP2_ARG2(__lasx_xvld, chroma_mask_arr, 0, src, 0, mask, src0);
+    DUP4_ARG2(__lasx_xvldx, src, stride, src, stride_2x, src, stride_3x, src, stride_4x,
+              src1, src2, src3, src4);
+    DUP2_ARG3(__lasx_xvpermi_q, src2, src1, 0x20, src4, src3, 0x20, src1, src3);
+    src0 = __lasx_xvshuf_b(src0, src0, mask);
+    DUP2_ARG3(__lasx_xvshuf_b, src1, src1, mask, src3, src3, mask, src1, src3);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, res_hz0, res_hz1);
+    res_hz2 = __lasx_xvdp2_h_bu(src3, coeff_hz_vec);
     res_vt0 = __lasx_xvmul_h(res_hz1, coeff_vt_vec0);
     res_vt1 = __lasx_xvmul_h(res_hz2, coeff_vt_vec0);
-    LASX_PCKEV_Q(res_hz1, res_hz0, res_hz0);
+    res_hz0 = __lasx_xvpermi_q(res_hz1, res_hz0, 0x20);
     res_hz1 = __lasx_xvpermi_q(res_hz1, res_hz2, 0x3);
     res_hz0 = __lasx_xvmul_h(res_hz0, coeff_vt_vec1);
     res_hz1 = __lasx_xvmul_h(res_hz1, coeff_vt_vec1);
     res_vt0 = __lasx_xvadd_h(res_vt0, res_hz0);
     res_vt1 = __lasx_xvadd_h(res_vt1, res_hz1);
-    LASX_SRARI_H_2(res_vt0, res_vt1, res_vt0, res_vt1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res_vt0, 6, res_vt1, 6, res_vt0, res_vt1);
     res_vt0 = __lasx_xvsat_hu(res_vt0, 7);
     res_vt1 = __lasx_xvsat_hu(res_vt1, 7);
-    LASX_PCKEV_B_128SV(res_vt1, res_vt0, out);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, tp0);
+    out = __lasx_xvpickev_b(res_vt1, res_vt0);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    tp0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out = __lasx_xvavgr_bu(out, tp0);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hv_and_aver_dst_8x8_lasx(uint8_t *src,
@@ -482,6 +545,9 @@ static av_always_inline void avc_chroma_hv_and_aver_dst_8x8_lasx(uint8_t *src,
                              uint32_t coef_hor1, uint32_t coef_ver0,
                              uint32_t coef_ver1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tp0, tp1, tp2, tp3, dst0, dst1;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i out0, out1;
@@ -494,25 +560,26 @@ static av_always_inline void avc_chroma_hv_and_aver_dst_8x8_lasx(uint8_t *src,
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h(coef_ver1);
     __m256i coeff_hz_vec = __lasx_xvilvl_b(coeff_hz_vec0, coeff_hz_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    src0 = LASX_LD(src);
-    LASX_LD_8(src + stride, stride, src1, src2, src3, src4,
+    DUP2_ARG2(__lasx_xvld, chroma_mask_arr, 0, src, 0, mask, src0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
               src5, src6, src7, src8);
-    LASX_PCKEV_Q_4(src2, src1, src4, src3, src6, src5, src8, src7,
-                   src1, src3, src5, src7);
-    LASX_SHUF_B_128SV(src0, src0, mask, src0);
-    LASX_SHUF_B_4_128SV(src1, src1, src3, src3, src5, src5, src7, src7,
-                        mask, mask, mask, mask, src1, src3, src5, src7);
-    LASX_DP2_H_BU(src0, coeff_hz_vec, res_hz0);
-    LASX_DP2_H_BU(src1, coeff_hz_vec, res_hz1);
-    LASX_DP2_H_BU(src3, coeff_hz_vec, res_hz2);
-    LASX_DP2_H_BU(src5, coeff_hz_vec, res_hz3);
-    LASX_DP2_H_BU(src7, coeff_hz_vec, res_hz4);
+    DUP4_ARG3(__lasx_xvpermi_q, src2, src1, 0x20, src4, src3, 0x20, src6, src5, 0x20,
+              src8, src7, 0x20, src1, src3, src5, src7);
+    src0 = __lasx_xvshuf_b(src0, src0, mask);
+    DUP4_ARG3(__lasx_xvshuf_b, src1, src1, mask, src3, src3, mask, src5, src5, mask, src7,
+              src7, mask, src1, src3, src5, src7);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src3,
+              coeff_hz_vec, src5, coeff_hz_vec, res_hz0, res_hz1, res_hz2, res_hz3);
+    res_hz4 = __lasx_xvdp2_h_bu(src7, coeff_hz_vec);
     res_vt0 = __lasx_xvmul_h(res_hz1, coeff_vt_vec0);
     res_vt1 = __lasx_xvmul_h(res_hz2, coeff_vt_vec0);
     res_vt2 = __lasx_xvmul_h(res_hz3, coeff_vt_vec0);
     res_vt3 = __lasx_xvmul_h(res_hz4, coeff_vt_vec0);
-    LASX_PCKEV_Q(res_hz1, res_hz0, res_hz0);
+    res_hz0 = __lasx_xvpermi_q(res_hz1, res_hz0, 0x20);
     res_hz1 = __lasx_xvpermi_q(res_hz1, res_hz2, 0x3);
     res_hz2 = __lasx_xvpermi_q(res_hz2, res_hz3, 0x3);
     res_hz3 = __lasx_xvpermi_q(res_hz3, res_hz4, 0x3);
@@ -524,29 +591,42 @@ static av_always_inline void avc_chroma_hv_and_aver_dst_8x8_lasx(uint8_t *src,
     res_vt1 = __lasx_xvadd_h(res_vt1, res_hz1);
     res_vt2 = __lasx_xvadd_h(res_vt2, res_hz2);
     res_vt3 = __lasx_xvadd_h(res_vt3, res_hz3);
-    LASX_SRARI_H_4(res_vt0, res_vt1, res_vt2, res_vt3,
-                   res_vt0, res_vt1, res_vt2, res_vt3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res_vt0, 6, res_vt1, 6, res_vt2, 6, res_vt3, 6,
+              res_vt0, res_vt1, res_vt2, res_vt3);
     res_vt0 = __lasx_xvsat_hu(res_vt0, 7);
     res_vt1 = __lasx_xvsat_hu(res_vt1, 7);
     res_vt2 = __lasx_xvsat_hu(res_vt2, 7);
     res_vt3 = __lasx_xvsat_hu(res_vt3, 7);
-    LASX_PCKEV_B_2_128SV(res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst0);
-    LASX_LD_4(dst + 4 * stride, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst1);
+    DUP2_ARG2(__lasx_xvpickev_b, res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
+    dst += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    dst -= stride_4x;
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst1 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out0 = __lasx_xvavgr_bu(out0, dst0);
     out1 = __lasx_xvavgr_bu(out1, dst1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hz_and_aver_dst_8x4_lasx(uint8_t *src,
                              uint8_t *dst, ptrdiff_t stride, uint32_t coeff0,
                              uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i tp0, tp1, tp2, tp3;
     __m256i src0, src1, src2, src3, out;
     __m256i res0, res1;
@@ -555,29 +635,36 @@ static av_always_inline void avc_chroma_hz_and_aver_dst_8x4_lasx(uint8_t *src,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    LASX_LD_4(src, stride, src0, src1, src2, src3);
-    LASX_PCKEV_Q_2(src1, src0, src3, src2, src0, src2);
-    LASX_SHUF_B_2_128SV(src0, src0, src2, src2, mask, mask, src0, src2);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
+    mask = __lasx_xvld(chroma_mask_arr, 0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src2);
+    DUP2_ARG3(__lasx_xvshuf_b, src0, src0, mask, src2, src2, mask, src0, src2);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, res0, res1);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
-    LASX_SRARI_H_2(res0, res1, res0, res1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res0, res1);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
-    LASX_PCKEV_B_128SV(res1, res0, out);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, tp0);
+    out = __lasx_xvpickev_b(res1, res0);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    tp0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out = __lasx_xvavgr_bu(out, tp0);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_hz_and_aver_dst_8x8_lasx(uint8_t *src,
                              uint8_t *dst, ptrdiff_t stride, uint32_t coeff0,
                              uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tp0, tp1, tp2, tp3, dst0, dst1;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i out0, out1;
@@ -587,43 +674,58 @@ static av_always_inline void avc_chroma_hz_and_aver_dst_8x8_lasx(uint8_t *src,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    mask = LASX_LD(chroma_mask_arr);
-    LASX_LD_8(src, stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src2, src4, src6);
-    LASX_SHUF_B_4_128SV(src0, src0, src2, src2, src4, src4, src6, src6,
-                        mask, mask, mask, mask, src0, src2, src4, src6);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
-    LASX_DP2_H_BU(src4, coeff_vec, res2);
-    LASX_DP2_H_BU(src6, coeff_vec, res3);
+    mask = __lasx_xvld(chroma_mask_arr, 0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src0, src1, src2, src3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4, 0x20,
+              src7, src6, 0x20, src0, src2, src4, src6);
+    DUP4_ARG3(__lasx_xvshuf_b, src0, src0, mask, src2, src2, mask, src4, src4,
+              mask, src6, src6, mask, src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, src4, coeff_vec, src6,
+              coeff_vec, res0, res1, res2, res3);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
     res2 = __lasx_xvslli_h(res2, 3);
     res3 = __lasx_xvslli_h(res3, 3);
-    LASX_SRARI_H_4(res0, res1, res2, res3,
-                   res0, res1, res2, res3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1, res2, res3);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
     res2 = __lasx_xvsat_hu(res2, 7);
     res3 = __lasx_xvsat_hu(res3, 7);
-    LASX_PCKEV_B_2_128SV(res1, res0, res3, res2, out0, out1);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst0);
-    LASX_LD_4(dst + 4 * stride, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst1);
+    DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, out0, out1);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
+    dst += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    dst -= stride_4x;
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst1 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out0 = __lasx_xvavgr_bu(out0, dst0);
     out1 = __lasx_xvavgr_bu(out1, dst1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride)
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_vt_and_aver_dst_8x4_lasx(uint8_t *src,
                              uint8_t *dst, ptrdiff_t stride, uint32_t coeff0,
                              uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tp0, tp1, tp2, tp3;
     __m256i src0, src1, src2, src3, src4, out;
     __m256i res0, res1;
@@ -631,30 +733,37 @@ static av_always_inline void avc_chroma_vt_and_aver_dst_8x4_lasx(uint8_t *src,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    src0 = LASX_LD(src);
-    LASX_LD_4(src + stride, stride, src1, src2, src3, src4);
-    LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src0, src1, src2, src3);
-    LASX_ILVL_B_2_128SV(src1, src0, src3, src2, src0, src2);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
+    src0 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvldx, src, stride, src, stride_2x, src, stride_3x, src, stride_4x,
+              src1, src2, src3, src4);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2, 0x20,
+              src4, src3, 0x20, src0, src1, src2, src3);
+    DUP2_ARG2(__lasx_xvilvl_b, src1, src0, src3, src2, src0, src2);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, res0, res1);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
-    LASX_SRARI_H_2(res0, res1, res0, res1, 6);
+    DUP2_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res0, res1)
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
-    LASX_PCKEV_B_128SV(res1, res0, out);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, tp0);
+    out  = __lasx_xvpickev_b(res1, res0);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    tp0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out = __lasx_xvavgr_bu(out, tp0);
-    LASX_ST_D_4(out, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(out, dst, 0, 0);
+    __lasx_xvstelm_d(out, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avc_chroma_vt_and_aver_dst_8x8_lasx(uint8_t *src,
                              uint8_t *dst, ptrdiff_t stride, uint32_t coeff0,
                              uint32_t coeff1)
 {
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
     __m256i tp0, tp1, tp2, tp3, dst0, dst1;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i out0, out1;
@@ -663,39 +772,53 @@ static av_always_inline void avc_chroma_vt_and_aver_dst_8x8_lasx(uint8_t *src,
     __m256i coeff_vec1 = __lasx_xvreplgr2vr_b(coeff1);
     __m256i coeff_vec = __lasx_xvilvl_b(coeff_vec0, coeff_vec1);
 
-    src0 = LASX_LD(src);
-    LASX_LD_8(src + stride, stride, src1, src2, src3, src4,
+    src0 = __lasx_xvld(src, 0);
+    src += stride;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
+              src1, src2, src3, src4);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x, src, stride_3x,
               src5, src6, src7, src8);
-    LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src0, src1, src2, src3);
-    LASX_PCKEV_Q_4(src5, src4, src6, src5, src7, src6, src8, src7,
-                   src4, src5, src6, src7);
-    LASX_ILVL_B_4_128SV(src1, src0, src3, src2, src5, src4, src7, src6,
-                        src0, src2, src4, src6);
-    LASX_DP2_H_BU(src0, coeff_vec, res0);
-    LASX_DP2_H_BU(src2, coeff_vec, res1);
-    LASX_DP2_H_BU(src4, coeff_vec, res2);
-    LASX_DP2_H_BU(src6, coeff_vec, res3);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2, 0x20,
+              src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6, 0x20,
+              src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvilvl_b, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvdp2_h_bu, src0, coeff_vec, src2, coeff_vec, src4, coeff_vec, src6,
+              coeff_vec, res0, res1, res2, res3);
     res0 = __lasx_xvslli_h(res0, 3);
     res1 = __lasx_xvslli_h(res1, 3);
     res2 = __lasx_xvslli_h(res2, 3);
     res3 = __lasx_xvslli_h(res3, 3);
-    LASX_SRARI_H_4(res0, res1, res2, res3, res0, res1, res2, res3, 6);
+    DUP4_ARG2(__lasx_xvsrari_h, res0, 6, res1, 6, res2, 6, res3, 6,
+                   res0, res1, res2, res3);
     res0 = __lasx_xvsat_hu(res0, 7);
     res1 = __lasx_xvsat_hu(res1, 7);
     res2 = __lasx_xvsat_hu(res2, 7);
     res3 = __lasx_xvsat_hu(res3, 7);
-    LASX_PCKEV_B_2_128SV(res1, res0, res3, res2, out0, out1);
-    LASX_LD_4(dst, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst0);
-    LASX_LD_4(dst + 4 * stride, stride, tp0, tp1, tp2, tp3);
-    LASX_ILVL_D_2_128SV(tp2, tp0, tp3, tp1, tp0, tp2);
-    LASX_PCKEV_Q(tp2, tp0, dst1);
+    DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, out0, out1);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst0 = __lasx_xvpermi_q(tp2, tp0, 0x20);
+    dst += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+              tp0, tp1, tp2, tp3);
+    dst -= stride_4x;
+    DUP2_ARG2(__lasx_xvilvl_d, tp2, tp0, tp3, tp1, tp0, tp2);
+    dst1 = __lasx_xvpermi_q(tp2, tp0, 0x20);
     out0 = __lasx_xvavgr_bu(out0, dst0);
     out1 = __lasx_xvavgr_bu(out1, dst1);
-    LASX_ST_D_4(out0, 0, 2, 1, 3, dst, stride);
-    LASX_ST_D_4(out1, 0, 2, 1, 3, dst + stride * 4, stride);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out0, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(out1, dst + stride_2x, 0, 1);
+    __lasx_xvstelm_d(out1, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avg_width8x8_lasx(uint8_t *src, uint8_t *dst,
@@ -703,18 +826,18 @@ static av_always_inline void avg_width8x8_lasx(uint8_t *src, uint8_t *dst,
 {
     __m256i src0, src1, src2, src3;
     __m256i dst0, dst1, dst2, dst3;
-    ptrdiff_t stride_x2 = stride << 1;
-    ptrdiff_t stride_x3 = stride_x2 + stride;
-    ptrdiff_t stride_x4 = stride << 2;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
 
     src0 = __lasx_xvldrepl_d(src, 0);
     src1 = __lasx_xvldrepl_d(src + stride, 0);
-    src2 = __lasx_xvldrepl_d(src + stride_x2, 0);
-    src3 = __lasx_xvldrepl_d(src + stride_x3, 0);
+    src2 = __lasx_xvldrepl_d(src + stride_2x, 0);
+    src3 = __lasx_xvldrepl_d(src + stride_3x, 0);
     dst0 = __lasx_xvldrepl_d(dst, 0);
     dst1 = __lasx_xvldrepl_d(dst + stride, 0);
-    dst2 = __lasx_xvldrepl_d(dst + stride_x2, 0);
-    dst3 = __lasx_xvldrepl_d(dst + stride_x3, 0);
+    dst2 = __lasx_xvldrepl_d(dst + stride_2x, 0);
+    dst3 = __lasx_xvldrepl_d(dst + stride_3x, 0);
     src0 = __lasx_xvpackev_d(src1,src0);
     src2 = __lasx_xvpackev_d(src3,src2);
     src0 = __lasx_xvpermi_q(src0, src2, 0x02);
@@ -722,18 +845,21 @@ static av_always_inline void avg_width8x8_lasx(uint8_t *src, uint8_t *dst,
     dst2 = __lasx_xvpackev_d(dst3,dst2);
     dst0 = __lasx_xvpermi_q(dst0, dst2, 0x02);
     dst0 = __lasx_xvavgr_bu(src0, dst0);
-    LASX_ST_D_4(dst0, 0, 1, 2, 3, dst, stride);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
 
-    src += stride_x4;
-    dst += stride_x4;
+    src += stride_4x;
+    dst += stride_4x;
     src0 = __lasx_xvldrepl_d(src, 0);
     src1 = __lasx_xvldrepl_d(src + stride, 0);
-    src2 = __lasx_xvldrepl_d(src + stride_x2, 0);
-    src3 = __lasx_xvldrepl_d(src + stride_x3, 0);
+    src2 = __lasx_xvldrepl_d(src + stride_2x, 0);
+    src3 = __lasx_xvldrepl_d(src + stride_3x, 0);
     dst0 = __lasx_xvldrepl_d(dst, 0);
     dst1 = __lasx_xvldrepl_d(dst + stride, 0);
-    dst2 = __lasx_xvldrepl_d(dst + stride_x2, 0);
-    dst3 = __lasx_xvldrepl_d(dst + stride_x3, 0);
+    dst2 = __lasx_xvldrepl_d(dst + stride_2x, 0);
+    dst3 = __lasx_xvldrepl_d(dst + stride_3x, 0);
     src0 = __lasx_xvpackev_d(src1,src0);
     src2 = __lasx_xvpackev_d(src3,src2);
     src0 = __lasx_xvpermi_q(src0, src2, 0x02);
@@ -741,7 +867,10 @@ static av_always_inline void avg_width8x8_lasx(uint8_t *src, uint8_t *dst,
     dst2 = __lasx_xvpackev_d(dst3,dst2);
     dst0 = __lasx_xvpermi_q(dst0, dst2, 0x02);
     dst0 = __lasx_xvavgr_bu(src0, dst0);
-    LASX_ST_D_4(dst0, 0, 1, 2, 3, dst, stride);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
 }
 
 static av_always_inline void avg_width8x4_lasx(uint8_t *src, uint8_t *dst,
@@ -749,17 +878,17 @@ static av_always_inline void avg_width8x4_lasx(uint8_t *src, uint8_t *dst,
 {
     __m256i src0, src1, src2, src3;
     __m256i dst0, dst1, dst2, dst3;
-    ptrdiff_t stride_x2 = stride << 1;
-    ptrdiff_t stride_x3 = stride_x2 + stride;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     src0 = __lasx_xvldrepl_d(src, 0);
     src1 = __lasx_xvldrepl_d(src + stride, 0);
-    src2 = __lasx_xvldrepl_d(src + stride_x2, 0);
-    src3 = __lasx_xvldrepl_d(src + stride_x3, 0);
+    src2 = __lasx_xvldrepl_d(src + stride_2x, 0);
+    src3 = __lasx_xvldrepl_d(src + stride_3x, 0);
     dst0 = __lasx_xvldrepl_d(dst, 0);
     dst1 = __lasx_xvldrepl_d(dst + stride, 0);
-    dst2 = __lasx_xvldrepl_d(dst + stride_x2, 0);
-    dst3 = __lasx_xvldrepl_d(dst + stride_x3, 0);
+    dst2 = __lasx_xvldrepl_d(dst + stride_2x, 0);
+    dst3 = __lasx_xvldrepl_d(dst + stride_3x, 0);
     src0 = __lasx_xvpackev_d(src1,src0);
     src2 = __lasx_xvpackev_d(src3,src2);
     src0 = __lasx_xvpermi_q(src0, src2, 0x02);
@@ -767,7 +896,10 @@ static av_always_inline void avg_width8x4_lasx(uint8_t *src, uint8_t *dst,
     dst2 = __lasx_xvpackev_d(dst3,dst2);
     dst0 = __lasx_xvpermi_q(dst0, dst2, 0x02);
     dst0 = __lasx_xvavgr_bu(src0, dst0);
-    LASX_ST_D_4(dst0, 0, 1, 2, 3, dst, stride);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
 }
 
 static void avc_chroma_hv_and_aver_dst_8w_lasx(uint8_t *src, uint8_t *dst,
diff --git a/libavcodec/loongarch/h264dsp_lasx.c b/libavcodec/loongarch/h264dsp_lasx.c
index 21efa69672..873c109413 100644
--- a/libavcodec/loongarch/h264dsp_lasx.c
+++ b/libavcodec/loongarch/h264dsp_lasx.c
@@ -22,7 +22,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "h264dsp_lasx.h"
 
 #define AVC_LPF_P1_OR_Q1(p0_or_q0_org_in, q0_or_p0_org_in,   \
@@ -36,7 +36,7 @@
     temp = __lasx_xvslli_h(p1_or_q1_org_in, 1);              \
     clip3 = __lasx_xvsub_h(clip3, temp);                     \
     clip3 = __lasx_xvavg_h(p2_or_q2_org_in, clip3);          \
-    LASX_CLIP_H(clip3, neg_tc_in, tc_in);                    \
+    clip3 = __lasx_xvclip_h(clip3, neg_tc_in, tc_in);        \
     p1_or_q1_out = __lasx_xvadd_h(p1_or_q1_org_in, clip3);   \
 }
 
@@ -55,21 +55,22 @@
     p1_sub_q1 = __lasx_xvaddi_hu(p1_sub_q1, 4);              \
     delta = __lasx_xvadd_h(q0_sub_p0, p1_sub_q1);            \
     delta = __lasx_xvsrai_h(delta, 3);                       \
-                                                             \
-    LASX_CLIP_H(delta, neg_threshold_in, threshold_in);      \
-                                                             \
+    delta = __lasx_xvclip_h(delta, neg_threshold_in,         \
+           threshold_in);                                    \
     p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_org_in, delta);   \
     q0_or_p0_out = __lasx_xvsub_h(q0_or_p0_org_in, delta);   \
                                                              \
-    LASX_CLIP_H_0_255(p0_or_q0_out, p0_or_q0_out);           \
-    LASX_CLIP_H_0_255(q0_or_p0_out, q0_or_p0_out);           \
+    p0_or_q0_out = __lasx_xvclip255_h(p0_or_q0_out);         \
+    q0_or_p0_out = __lasx_xvclip255_h(q0_or_p0_out);         \
 }
 
 void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
-                                   int alpha_in, int beta_in, int8_t *tc)
+                               int alpha_in, int beta_in, int8_t *tc)
 {
     int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
     int img_width_8x = img_width << 3;
+    int img_width_3x = img_width_2x + img_width;
     __m256i tmp_vec0, bs_vec;
     __m256i tc_vec = {0x0101010100000000, 0x0303030302020202, 0x0101010100000000, 0x0303030302020202};
 
@@ -94,10 +95,18 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
             __m256i row0, row1, row2, row3, row4, row5, row6, row7;
             __m256i row8, row9, row10, row11, row12, row13, row14, row15;
 
-            LASX_LD_8(src, img_width, row0, row1, row2, row3,
-                      row4, row5, row6, row7);
-            LASX_LD_8(src_tmp, img_width, row8, row9, row10, row11,
-                      row12, row13, row14, row15);
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row0, row1, row2, row3);
+            src += img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row4, row5, row6, row7);
+            src -= img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
+                      img_width_2x, src_tmp, img_width_3x, row8, row9, row10, row11);
+            src_tmp += img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
+                      img_width_2x, src_tmp, img_width_3x, row12, row13, row14, row15);
+            src_tmp -= img_width_4x;
 
             LASX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
                                  row8, row9, row10, row11,
@@ -141,8 +150,8 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
                 p2_org_h = __lasx_vext2xv_hu_bu(p2_org);
                 AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, p1_org_h, p2_org_h,
                                  neg_tc_h, tc_h, p1_h);
-                LASX_PCKEV_B(p1_h, p1_h, p1_h);
-
+                p1_h = __lasx_xvpickev_b(p1_h, p1_h);
+                p1_h = __lasx_xvpermi_d(p1_h, 0xd8);
                 p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
                 is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
                 tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
@@ -160,7 +169,8 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
                 q2_org_h = __lasx_vext2xv_hu_bu(q2_org);
                 AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, q1_org_h, q2_org_h,
                                  neg_tc_h, tc_h, q1_h);
-                LASX_PCKEV_B(q1_h, q1_h, q1_h);
+                q1_h = __lasx_xvpickev_b(q1_h, q1_h);
+                q1_h = __lasx_xvpermi_d(q1_h, 0xd8);
                 q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
 
                 is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
@@ -176,35 +186,49 @@ void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
                 p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
             }
 
             {
                 __m256i row0, row1, row2, row3, row4, row5, row6, row7;
-
-                LASX_ILVL_B_4(p1_org, p3_org, p0_org, p2_org, q2_org, q0_org, q3_org,
-                              q1_org, row0, row1, row2, row3);
-                LASX_ILVLH_B_2(row1, row0, row3, row2, row5, row4, row7, row6);
-                LASX_ILVLH_W_2(row6, row4, row7, row5, row1, row0, row3, row2);
-                LASX_PCKEV_Q_4(row0, row0, row1, row1, row2, row2, row3, row3,
-                               row4, row5, row6, row7);
-                LASX_ST_D_2(row4, 2, 3, src, img_width);
+                __m256i control = {0x0000000400000000, 0x0000000500000001,
+                                   0x0000000600000002, 0x0000000700000003};
+
+                DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org, 0x02,
+                          p2_org, q1_org, 0x02, p3_org, q0_org, 0x02, p0_org, p1_org,
+                          p2_org, p3_org);
+                DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org, row0, row2);
+                DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org, row1, row3);
+                DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
+                DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
+                DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6, control,
+                          row7, control, row4, row5, row6, row7);
+                __lasx_xvstelm_d(row4, src, 0, 0);
+                __lasx_xvstelm_d(row4, src + img_width, 0, 1);
                 src += img_width_2x;
-                LASX_ST_D_2(row0, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row4, src, 0, 2);
+                __lasx_xvstelm_d(row4, src + img_width, 0, 3);
                 src += img_width_2x;
-                LASX_ST_D_2(row5, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row5, src, 0, 0);
+                __lasx_xvstelm_d(row5, src + img_width, 0, 1);
                 src += img_width_2x;
-                LASX_ST_D_2(row1, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row5, src, 0, 2);
+                __lasx_xvstelm_d(row5, src + img_width, 0, 3);
                 src += img_width_2x;
-                LASX_ST_D_2(row6, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row6, src, 0, 0);
+                __lasx_xvstelm_d(row6, src + img_width, 0, 1);
                 src += img_width_2x;
-                LASX_ST_D_2(row2, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row6, src, 0, 2);
+                __lasx_xvstelm_d(row6, src + img_width, 0, 3);
                 src += img_width_2x;
-                LASX_ST_D_2(row7, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row7, src, 0, 0);
+                __lasx_xvstelm_d(row7, src + img_width, 0, 1);
                 src += img_width_2x;
-                LASX_ST_D_2(row3, 2, 3, src, img_width);
+                __lasx_xvstelm_d(row7, src, 0, 2);
+                __lasx_xvstelm_d(row7, src + img_width, 0, 3);
             }
         }
     }
@@ -235,10 +259,9 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
         alpha = __lasx_xvreplgr2vr_b(alpha_in);
         beta  = __lasx_xvreplgr2vr_b(beta_in);
 
-        p2_org = LASX_LD(data - img_width_3x);
-        p1_org = LASX_LD(data - img_width_2x);
-        p0_org = LASX_LD(data - img_width);
-        LASX_LD_2(data, img_width, q0_org, q1_org);
+        DUP2_ARG2(__lasx_xvldx, data, -img_width_3x, data, -img_width_2x, p2_org, p1_org);
+        p0_org = __lasx_xvldx(data, -img_width);
+        DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
 
         is_bs_greater_than0 = __lasx_xvslt_bu(zero, bs_vec);
         p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
@@ -255,7 +278,7 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
         if (__lasx_xbnz_v(is_less_than)) {
             __m256i neg_tc_h, tc_h, p2_asub_p0, q2_asub_q0;
 
-            q2_org = LASX_LD(data + img_width_2x);
+            q2_org = __lasx_xvldx(data, img_width_2x);
 
             neg_tc_h = __lasx_xvneg_b(tc_vec);
             neg_tc_h = __lasx_vext2xv_h_b(neg_tc_h);
@@ -274,10 +297,11 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
                 p2_org_h = __lasx_vext2xv_hu_bu(p2_org);
                 AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, p1_org_h, p2_org_h,
                                  neg_tc_h, tc_h, p1_h);
-                LASX_PCKEV_B(p1_h, p1_h, p1_h);
+                p1_h = __lasx_xvpickev_b(p1_h, p1_h);
+                p1_h = __lasx_xvpermi_d(p1_h, 0xd8);
                 p1_h   = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
                 p1_org = __lasx_xvpermi_q(p1_org, p1_h, 0x30);
-                LASX_ST(p1_org, data - img_width_2x);
+                __lasx_xvst(p1_org, data - img_width_2x, 0);
 
                 is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
                 tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
@@ -295,10 +319,11 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
                 q2_org_h = __lasx_vext2xv_hu_bu(q2_org);
                 AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, q1_org_h, q2_org_h,
                                  neg_tc_h, tc_h, q1_h);
-                LASX_PCKEV_B(q1_h, q1_h, q1_h);
+                q1_h = __lasx_xvpickev_b(q1_h, q1_h);
+                q1_h = __lasx_xvpermi_d(q1_h, 0xd8);
                 q1_h = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
                 q1_org = __lasx_xvpermi_q(q1_org, q1_h, 0x30);
-                LASX_ST(q1_org, data + img_width);
+                __lasx_xvst(q1_org, data + img_width, 0);
 
                 is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
                 tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
@@ -314,13 +339,14 @@ void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0Xd8, q0_h, 0xd8, p0_h, q0_h);
                 p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
                 p0_org = __lasx_xvpermi_q(p0_org, p0_h, 0x30);
                 q0_org = __lasx_xvpermi_q(q0_org, q0_h, 0x30);
-                LASX_ST(p0_org, data - img_width);
-                LASX_ST(q0_org, data);
+                __lasx_xvst(p0_org, data - img_width, 0);
+                __lasx_xvst(q0_org, data, 0);
             }
         }
     }
@@ -332,6 +358,9 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
     __m256i tmp_vec0, bs_vec;
     __m256i tc_vec = {0x0303020201010000, 0x0303020201010000, 0x0, 0x0};
     __m256i zero = __lasx_xvldi(0);
+    int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
+    int img_width_3x = img_width_2x + img_width;
 
     tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
     tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
@@ -352,13 +381,19 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
         {
             __m256i row0, row1, row2, row3, row4, row5, row6, row7;
 
-            LASX_LD_8(src, img_width, row0, row1, row2, row3,
-                      row4, row5, row6, row7);
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row0, row1, row2, row3);
+            src += img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row4, row5, row6, row7);
+            src -= img_width_4x;
             /* LASX_TRANSPOSE8x4_B */
-            LASX_ILVL_B_4(row2, row0, row3, row1, row6, row4, row7, row5,
-                          p1_org, p0_org, q0_org, q1_org);
-            LASX_ILVL_B_2(p0_org, p1_org, q1_org, q0_org, row0, row1);
-            LASX_ILVLH_W_128SV(row1, row0, row3, row2);
+            DUP4_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row6, row4, row7, row5,
+                      p1_org, p0_org, q0_org, q1_org);
+            row0 = __lasx_xvilvl_b(p0_org, p1_org);
+            row1 = __lasx_xvilvl_b(q1_org, q0_org);
+            row3 = __lasx_xvilvh_w(row1, row0);
+            row2 = __lasx_xvilvl_w(row1, row0);
             p1_org = __lasx_xvpermi_d(row2, 0x00);
             p0_org = __lasx_xvpermi_d(row2, 0x55);
             q0_org = __lasx_xvpermi_d(row3, 0x00);
@@ -396,7 +431,8 @@ void ff_h264_h_lpf_chroma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
                 p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
             }
@@ -446,9 +482,8 @@ void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
         alpha = __lasx_xvreplgr2vr_b(alpha_in);
         beta  = __lasx_xvreplgr2vr_b(beta_in);
 
-        p1_org = LASX_LD(data - img_width_2x);
-        p0_org = LASX_LD(data - img_width);
-        LASX_LD_2(data, img_width, q0_org, q1_org);
+        DUP2_ARG2(__lasx_xvldx, data, -img_width_2x, data, -img_width, p1_org, p0_org);
+        DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
 
         is_bs_greater_than0 = __lasx_xvslt_bu(zero, bs_vec);
         p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
@@ -479,11 +514,12 @@ void ff_h264_v_lpf_chroma_8_lasx(uint8_t *data, int img_width,
 
                 AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
                              neg_thresh_h, tc_h, p0_h, q0_h);
-                LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
                 p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
                 q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
-                LASX_ST_D(p0_h, 0, data - img_width);
-                LASX_ST_D(q0_h, 0, data);
+                __lasx_xvstelm_d(p0_h, data - img_width, 0, 0);
+                __lasx_xvstelm_d(q0_h, data, 0, 0);
             }
         }
     }
@@ -533,6 +569,8 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
                                      int alpha_in, int beta_in)
 {
     int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
+    int img_width_3x = img_width_2x + img_width;
     uint8_t *src = data - 4;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -543,11 +581,18 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         __m256i row0, row1, row2, row3, row4, row5, row6, row7;
         __m256i row8, row9, row10, row11, row12, row13, row14, row15;
 
-        LASX_LD_8(src, img_width, row0, row1, row2, row3,
-                  row4, row5, row6, row7);
-        src += img_width << 3;
-        LASX_LD_8(src, img_width, row8, row9, row10, row11,
-                  row12, row13, row14, row15);
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row0, row1, row2, row3);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row4, row5, row6, row7);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row8, row9, row10, row11);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row12, row13, row14, row15);
+        src += img_width_4x;
 
         LASX_TRANSPOSE16x8_B(row0, row1, row2, row3,
                              row4, row5, row6, row7,
@@ -600,8 +645,10 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
             AVC_LPF_P0P1P2_OR_Q0Q1Q2(p3_org_h, p0_org_h, q0_org_h, p1_org_h,
                                      p2_org_h, q1_org_h, p0_h, p1_h, p2_h);
 
-            LASX_PCKEV_B(p0_h, p0_h, p0_h);
-            LASX_PCKEV_B_2(p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+            p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, p1_h, 0xd8, p2_h, 0xd8, p1_h, p2_h);
             p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than_beta);
             p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
             p2_org = __lasx_xvbitsel_v(p2_org, p2_h, is_less_than_beta);
@@ -609,7 +656,8 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
 
         AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
         /* combine */
-        LASX_PCKEV_B(p0_h, p0_h, p0_h);
+        p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+        p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
         p0_org = __lasx_xvbitsel_v(p0_org, p0_h, negate_is_less_than_beta);
 
         /* if (tmpFlag && (unsigned)ABS(q2-q0) < thresholds->beta_in) */
@@ -630,8 +678,10 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
             AVC_LPF_P0P1P2_OR_Q0Q1Q2(q3_org_h, q0_org_h, p0_org_h, q1_org_h,
                                      q2_org_h, p1_org_h, q0_h, q1_h, q2_h);
 
-            LASX_PCKEV_B(q0_h, q0_h, q0_h);
-            LASX_PCKEV_B_2(q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+            q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, q1_h, 0xd8, q2_h, 0xd8, q1_h, q2_h);
             q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than_beta);
             q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
             q2_org = __lasx_xvbitsel_v(q2_org, q2_h, is_less_than_beta);
@@ -641,35 +691,49 @@ void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
 
         /* combine */
-        LASX_PCKEV_B(q0_h, q0_h, q0_h);
+        q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+        q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
         q0_org = __lasx_xvbitsel_v(q0_org, q0_h, negate_is_less_than_beta);
 
         /* transpose and store */
         {
             __m256i row0, row1, row2, row3, row4, row5, row6, row7;
-
-            LASX_ILVL_B_4(p1_org, p3_org, p0_org, p2_org, q2_org, q0_org, q3_org,
-                          q1_org, row0, row1, row2, row3);
-            LASX_ILVLH_B_2(row1, row0, row3, row2, row5, row4, row7, row6);
-            LASX_ILVLH_W_2(row6, row4, row7, row5, row1, row0, row3, row2);
-            LASX_PCKEV_Q_4(row0, row0, row1, row1, row2, row2, row3, row3,
-                           row4, row5, row6, row7);
+            __m256i control = {0x0000000400000000, 0x0000000500000001,
+                               0x0000000600000002, 0x0000000700000003};
+
+            DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org, 0x02,
+                      p2_org, q1_org, 0x02, p3_org, q0_org, 0x02, p0_org, p1_org, p2_org,
+                      p3_org);
+            DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org, row0, row2);
+            DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org, row1, row3);
+            DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
+            DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
+            DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6, control,
+                      row7, control, row4, row5, row6, row7);
             src = data - 4;
-            LASX_ST_D_2(row4, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row4, src, 0, 0);
+            __lasx_xvstelm_d(row4, src + img_width, 0, 1);
             src += img_width_2x;
-            LASX_ST_D_2(row0, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row4, src, 0, 2);
+            __lasx_xvstelm_d(row4, src + img_width, 0, 3);
             src += img_width_2x;
-            LASX_ST_D_2(row5, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row5, src, 0, 0);
+            __lasx_xvstelm_d(row5, src + img_width, 0, 1);
             src += img_width_2x;
-            LASX_ST_D_2(row1, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row5, src, 0, 2);
+            __lasx_xvstelm_d(row5, src + img_width, 0, 3);
             src += img_width_2x;
-            LASX_ST_D_2(row6, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row6, src, 0, 0);
+            __lasx_xvstelm_d(row6, src + img_width, 0, 1);
             src += img_width_2x;
-            LASX_ST_D_2(row2, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row6, src, 0, 2);
+            __lasx_xvstelm_d(row6, src + img_width, 0, 3);
             src += img_width_2x;
-            LASX_ST_D_2(row7, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row7, src, 0, 0);
+            __lasx_xvstelm_d(row7, src + img_width, 0, 1);
             src += img_width_2x;
-            LASX_ST_D_2(row3, 2, 3, src, img_width);
+            __lasx_xvstelm_d(row7, src, 0, 2);
+            __lasx_xvstelm_d(row7, src + img_width, 0, 3);
         }
     }
 }
@@ -678,13 +742,15 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
                                      int alpha_in, int beta_in)
 {
     int img_width_2x = img_width << 1;
+    int img_width_3x = img_width_2x + img_width;
     uint8_t *src = data - img_width_2x;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
     __m256i p1_org, p0_org, q0_org, q1_org;
     __m256i zero = __lasx_xvldi(0);
 
-    LASX_LD_4(src, img_width, p1_org, p0_org, q0_org, q1_org)
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x, src, img_width_3x,
+              p1_org, p0_org, q0_org, q1_org);
     alpha = __lasx_xvreplgr2vr_b(alpha_in);
     beta  = __lasx_xvreplgr2vr_b(beta_in);
     p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
@@ -701,8 +767,8 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
     if (__lasx_xbnz_v(is_less_than)) {
         __m256i p2_asub_p0, q2_asub_q0, p0_h, q0_h, negate_is_less_than_beta;
         __m256i p1_org_h, p0_org_h, q0_org_h, q1_org_h;
-        __m256i p2_org = LASX_LD(src - img_width);
-        __m256i q2_org = LASX_LD(data + img_width_2x);
+        __m256i p2_org = __lasx_xvldx(src, -img_width);
+        __m256i q2_org = __lasx_xvldx(data, img_width_2x);
         __m256i less_alpha_shift2_add2 = __lasx_xvsrli_b(alpha, 2);
         less_alpha_shift2_add2 = __lasx_xvaddi_bu(less_alpha_shift2_add2, 2);
         less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0, less_alpha_shift2_add2);
@@ -722,7 +788,7 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         /* combine and store */
         if (__lasx_xbnz_v(is_less_than_beta)) {
             __m256i p2_org_h, p3_org_h, p1_h, p2_h;
-            __m256i p3_org = LASX_LD(src - img_width_2x);
+            __m256i p3_org = __lasx_xvldx(src, -img_width_2x);
 
             p2_org_h   = __lasx_vext2xv_hu_bu(p2_org);
             p3_org_h   = __lasx_vext2xv_hu_bu(p3_org);
@@ -730,21 +796,24 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
             AVC_LPF_P0P1P2_OR_Q0Q1Q2(p3_org_h, p0_org_h, q0_org_h, p1_org_h,
                                      p2_org_h, q1_org_h, p0_h, p1_h, p2_h);
 
-            LASX_PCKEV_B(p0_h, p0_h, p0_h);
-            LASX_PCKEV_B_2(p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+            p0_h =  __lasx_xvpermi_d(p0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, p1_h, 0xd8, p2_h, 0xd8, p1_h, p2_h);
             p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than_beta);
             p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
             p2_org = __lasx_xvbitsel_v(p2_org, p2_h, is_less_than_beta);
 
-            LASX_ST(p1_org, src);
-            LASX_ST(p2_org, src - img_width);
+            __lasx_xvst(p1_org, src, 0);
+            __lasx_xvst(p2_org, src - img_width, 0);
         }
 
         AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
         /* combine */
-        LASX_PCKEV_B(p0_h, p0_h, p0_h);
+        p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+        p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
         p0_org = __lasx_xvbitsel_v(p0_org, p0_h, negate_is_less_than_beta);
-        LASX_ST(p0_org, data - img_width);
+        __lasx_xvst(p0_org, data - img_width, 0);
 
         /* if (tmpFlag && (unsigned)ABS(q2-q0) < thresholds->beta_in) */
         q2_asub_q0 = __lasx_xvabsd_bu(q2_org, q0_org);
@@ -757,7 +826,7 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
         /* combine and store */
         if (__lasx_xbnz_v(is_less_than_beta)) {
             __m256i q2_org_h, q3_org_h, q1_h, q2_h;
-            __m256i q3_org = LASX_LD(data + img_width_2x + img_width);
+            __m256i q3_org = __lasx_xvldx(data, img_width_2x + img_width);
 
             q2_org_h   = __lasx_vext2xv_hu_bu(q2_org);
             q3_org_h   = __lasx_vext2xv_hu_bu(q3_org);
@@ -765,23 +834,26 @@ void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
             AVC_LPF_P0P1P2_OR_Q0Q1Q2(q3_org_h, q0_org_h, p0_org_h, q1_org_h,
                                      q2_org_h, p1_org_h, q0_h, q1_h, q2_h);
 
-            LASX_PCKEV_B(q0_h, q0_h, q0_h);
-            LASX_PCKEV_B_2(q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+            q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, q1_h, 0xd8, q2_h, 0xd8, q1_h, q2_h);
             q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than_beta);
             q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
             q2_org = __lasx_xvbitsel_v(q2_org, q2_h, is_less_than_beta);
 
-            LASX_ST(q1_org, data + img_width);
-            LASX_ST(q2_org, data + img_width_2x);
+            __lasx_xvst(q1_org, data + img_width, 0);
+            __lasx_xvst(q2_org, data + img_width_2x, 0);
         }
 
         AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
 
         /* combine */
-        LASX_PCKEV_B(q0_h, q0_h, q0_h);
+        q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+        q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
         q0_org = __lasx_xvbitsel_v(q0_org, q0_h, negate_is_less_than_beta);
 
-        LASX_ST(q0_org, data);
+        __lasx_xvst(q0_org, data, 0);
     }
 }
 
@@ -789,6 +861,9 @@ void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
                                        int alpha_in, int beta_in)
 {
     uint8_t *src = data - 2;
+    int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
+    int img_width_3x = img_width_2x + img_width;
     __m256i p1_org, p0_org, q0_org, q1_org;
     __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
     __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -796,14 +871,19 @@ void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
     {
         __m256i row0, row1, row2, row3, row4, row5, row6, row7;
 
-        LASX_LD_8(src, img_width, row0, row1, row2, row3,
-                  row4, row5, row6, row7);
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x, src,
+                  img_width_3x, row0, row1, row2, row3);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x, src,
+                  img_width_3x, row4, row5, row6, row7);
 
         /* LASX_TRANSPOSE8x4_B */
-        LASX_ILVL_B_4(row2, row0, row3, row1, row6, row4, row7, row5,
-                      p1_org, p0_org, q0_org, q1_org);
-        LASX_ILVL_B_2(p0_org, p1_org, q1_org, q0_org, row0, row1);
-        LASX_ILVLH_W_128SV(row1, row0, row3, row2);
+        DUP4_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row6, row4, row7, row5,
+                  p1_org, p0_org, q0_org, q1_org);
+        row0 = __lasx_xvilvl_b(p0_org, p1_org);
+        row1 = __lasx_xvilvl_b(q1_org, q0_org);
+        row3 = __lasx_xvilvh_w(row1, row0);
+        row2 = __lasx_xvilvl_w(row1, row0);
         p1_org = __lasx_xvpermi_d(row2, 0x00);
         p0_org = __lasx_xvpermi_d(row2, 0x55);
         q0_org = __lasx_xvpermi_d(row3, 0x00);
@@ -833,7 +913,8 @@ void ff_h264_h_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
 
         AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
         AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
-        LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+        DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+        DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
         p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
         q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
     }
@@ -867,9 +948,9 @@ void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
     alpha = __lasx_xvreplgr2vr_b(alpha_in);
     beta  = __lasx_xvreplgr2vr_b(beta_in);
 
-    p1_org = LASX_LD(data - img_width_2x);
-    p0_org = LASX_LD(data - img_width);
-    LASX_LD_2(data, img_width, q0_org, q1_org);
+    p1_org = __lasx_xvldx(data, -img_width_2x);
+    p0_org = __lasx_xvldx(data, -img_width);
+    DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
 
     p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
     p1_asub_p0 = __lasx_xvabsd_bu(p1_org, p0_org);
@@ -891,11 +972,12 @@ void ff_h264_v_lpf_chroma_intra_8_lasx(uint8_t *data, int img_width,
 
         AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
         AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
-        LASX_PCKEV_B_2(p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+        DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h, p0_h, q0_h);
+        DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8, p0_h, q0_h);
         p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
         q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
-        LASX_ST_D(p0_h, 0, data - img_width);
-        LASX_ST_D(q0_h, 0, data);
+        __lasx_xvstelm_d(p0_h, data - img_width, 0, 0);
+        __lasx_xvstelm_d(q0_h, data, 0, 0);
     }
 }
 
@@ -910,6 +992,9 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
     __m256i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     __m256i denom, offset;
+    int stride_2x = stride << 1;
+    int stride_4x = stride << 2;
+    int stride_3x = stride_2x + stride;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
     offset_in  += ((weight_src + weight_dst) << 7);
@@ -921,26 +1006,36 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    src += 8 * stride;
-    LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                   src0, src1, src2, src3);
-    LASX_LD_8(dst, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                   dst0, dst1, dst2, dst3);
-
-    LASX_XORI_B_8_128(src0, src1, src2, src3, dst0, dst1, dst2, dst3);
-    LASX_ILVLH_B_4(dst0, src0, dst1, src1, dst2, src2, dst3, src3,
-                   vec1, vec0, vec3, vec2, vec5, vec4, vec7, vec6);
-
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-    LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
-    LASX_DP2ADD_H_B(offset, wgt, vec2, tmp2);
-    LASX_DP2ADD_H_B(offset, wgt, vec3, tmp3);
-    LASX_DP2ADD_H_B(offset, wgt, vec4, tmp4);
-    LASX_DP2ADD_H_B(offset, wgt, vec5, tmp5);
-    LASX_DP2ADD_H_B(offset, wgt, vec6, tmp6);
-    LASX_DP2ADD_H_B(offset, wgt, vec7, tmp7);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    src += stride_4x;
+    DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+              0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    dst += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    dst -= stride_4x;
+    DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+              0x20, tmp7, tmp6, 0x20, dst0, dst1, dst2, dst3);
+
+    DUP4_ARG2(__lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lasx_xvilvl_b, dst0, src0, dst1, src1, dst2, src2,
+              dst3, src3, vec0, vec2, vec4, vec6);
+    DUP4_ARG2(__lasx_xvilvh_b, dst0, src0, dst1, src1, dst2, src2,
+              dst3, src3, vec1, vec3, vec5, vec7);
+
+    DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+              offset, wgt, vec2, offset, wgt, vec3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec4, offset, wgt, vec5,
+              offset, wgt, vec6, offset, wgt, vec7, tmp4, tmp5, tmp6, tmp7);
 
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
@@ -951,57 +1046,66 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
     tmp6 = __lasx_xvsra_h(tmp6, denom);
     tmp7 = __lasx_xvsra_h(tmp7, denom);
 
-    LASX_CLIP_H_0_255_4(tmp0, tmp1, tmp2, tmp3,
-                        tmp0, tmp1, tmp2, tmp3);
-    LASX_CLIP_H_0_255_4(tmp4, tmp5, tmp6, tmp7,
-                        tmp4, tmp5, tmp6, tmp7);
-    LASX_PCKEV_B_4_128SV(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                         dst0, dst1, dst2, dst3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+              dst0, dst1, dst2, dst3);
     __lasx_xvstelm_d(dst0, dst, 0, 0);
-    __lasx_xvstelm_d(dst0, dst, 8, 2);
+    __lasx_xvstelm_d(dst0, dst, 8, 1);
     dst += stride;
-    __lasx_xvstelm_d(dst0, dst, 0, 1);
+    __lasx_xvstelm_d(dst0, dst, 0, 2);
     __lasx_xvstelm_d(dst0, dst, 8, 3);
     dst += stride;
     __lasx_xvstelm_d(dst1, dst, 0, 0);
-    __lasx_xvstelm_d(dst1, dst, 8, 2);
+    __lasx_xvstelm_d(dst1, dst, 8, 1);
     dst += stride;
-    __lasx_xvstelm_d(dst1, dst, 0, 1);
+    __lasx_xvstelm_d(dst1, dst, 0, 2);
     __lasx_xvstelm_d(dst1, dst, 8, 3);
     dst += stride;
     __lasx_xvstelm_d(dst2, dst, 0, 0);
-    __lasx_xvstelm_d(dst2, dst, 8, 2);
+    __lasx_xvstelm_d(dst2, dst, 8, 1);
     dst += stride;
-    __lasx_xvstelm_d(dst2, dst, 0, 1);
+    __lasx_xvstelm_d(dst2, dst, 0, 2);
     __lasx_xvstelm_d(dst2, dst, 8, 3);
     dst += stride;
     __lasx_xvstelm_d(dst3, dst, 0, 0);
-    __lasx_xvstelm_d(dst3, dst, 8, 2);
+    __lasx_xvstelm_d(dst3, dst, 8, 1);
     dst += stride;
-    __lasx_xvstelm_d(dst3, dst, 0, 1);
+    __lasx_xvstelm_d(dst3, dst, 0, 2);
     __lasx_xvstelm_d(dst3, dst, 8, 3);
     dst += stride;
 
     if (16 == height) {
-        LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-        LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                       src0, src1, src2, src3);
-        LASX_LD_8(dst, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-        LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                       dst0, dst1, dst2, dst3);
-
-        LASX_XORI_B_8_128(src0, src1, src2, src3, dst0, dst1, dst2, dst3);
-        LASX_ILVLH_B_4(dst0, src0, dst1, src1, dst2, src2, dst3, src3,
-                       vec1, vec0, vec3, vec2, vec5, vec4, vec7, vec6);
-
-        LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-        LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
-        LASX_DP2ADD_H_B(offset, wgt, vec2, tmp2);
-        LASX_DP2ADD_H_B(offset, wgt, vec3, tmp3);
-        LASX_DP2ADD_H_B(offset, wgt, vec4, tmp4);
-        LASX_DP2ADD_H_B(offset, wgt, vec5, tmp5);
-        LASX_DP2ADD_H_B(offset, wgt, vec6, tmp6);
-        LASX_DP2ADD_H_B(offset, wgt, vec7, tmp7);
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+                  src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+        src += stride_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+                  src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+        src += stride_4x;
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+                  0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+                  dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
+        dst += stride_4x;
+        DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+                  dst, stride_3x, tmp4, tmp5, tmp6, tmp7);
+        dst -= stride_4x;
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+                  0x20, tmp7, tmp6, 0x20, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG2(__lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lasx_xvilvl_b, dst0, src0, dst1, src1, dst2, src2,
+                  dst3, src3, vec0, vec2, vec4, vec6);
+        DUP4_ARG2(__lasx_xvilvh_b, dst0, src0, dst1, src1, dst2, src2,
+                  dst3, src3, vec1, vec3, vec5, vec7);
+
+        DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+                  offset, wgt, vec2, offset, wgt, vec3, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec4, offset, wgt, vec5,
+                  offset, wgt, vec6, offset, wgt, vec7, tmp4, tmp5, tmp6, tmp7);
 
         tmp0 = __lasx_xvsra_h(tmp0, denom);
         tmp1 = __lasx_xvsra_h(tmp1, denom);
@@ -1012,34 +1116,32 @@ void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
         tmp6 = __lasx_xvsra_h(tmp6, denom);
         tmp7 = __lasx_xvsra_h(tmp7, denom);
 
-        LASX_CLIP_H_0_255_4(tmp0, tmp1, tmp2, tmp3,
-                            tmp0, tmp1, tmp2, tmp3);
-        LASX_CLIP_H_0_255_4(tmp4, tmp5, tmp6, tmp7,
-                            tmp4, tmp5, tmp6, tmp7);
-        LASX_PCKEV_B_4_128SV(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                             dst0, dst1, dst2, dst3);
+        DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+                  dst0, dst1, dst2, dst3);
         __lasx_xvstelm_d(dst0, dst, 0, 0);
-        __lasx_xvstelm_d(dst0, dst, 8, 2);
+        __lasx_xvstelm_d(dst0, dst, 8, 1);
         dst += stride;
-        __lasx_xvstelm_d(dst0, dst, 0, 1);
+        __lasx_xvstelm_d(dst0, dst, 0, 2);
         __lasx_xvstelm_d(dst0, dst, 8, 3);
         dst += stride;
         __lasx_xvstelm_d(dst1, dst, 0, 0);
-        __lasx_xvstelm_d(dst1, dst, 8, 2);
+        __lasx_xvstelm_d(dst1, dst, 8, 1);
         dst += stride;
-        __lasx_xvstelm_d(dst1, dst, 0, 1);
+        __lasx_xvstelm_d(dst1, dst, 0, 2);
         __lasx_xvstelm_d(dst1, dst, 8, 3);
         dst += stride;
         __lasx_xvstelm_d(dst2, dst, 0, 0);
-        __lasx_xvstelm_d(dst2, dst, 8, 2);
+        __lasx_xvstelm_d(dst2, dst, 8, 1);
         dst += stride;
-        __lasx_xvstelm_d(dst2, dst, 0, 1);
+        __lasx_xvstelm_d(dst2, dst, 0, 2);
         __lasx_xvstelm_d(dst2, dst, 8, 3);
         dst += stride;
         __lasx_xvstelm_d(dst3, dst, 0, 0);
-        __lasx_xvstelm_d(dst3, dst, 8, 2);
+        __lasx_xvstelm_d(dst3, dst, 8, 1);
         dst += stride;
-        __lasx_xvstelm_d(dst3, dst, 0, 1);
+        __lasx_xvstelm_d(dst3, dst, 0, 2);
         __lasx_xvstelm_d(dst3, dst, 8, 3);
     }
 }
@@ -1051,6 +1153,8 @@ static void avc_biwgt_8x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     __m256i wgt, vec0, vec1;
     __m256i src0, dst0;
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
     offset_in  += ((weight_src + weight_dst) << 7);
@@ -1062,21 +1166,26 @@ static void avc_biwgt_8x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_4(dst, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst0);
-    LASX_XORI_B_2_128(src0, dst0);
-    LASX_ILVLH_B(dst0, src0, vec1, vec0);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-    LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
+    vec0 = __lasx_xvilvl_b(dst0, src0);
+    vec1 = __lasx_xvilvh_b(dst0, src0);
+    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1, tmp0, tmp1);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
-    LASX_CLIP_H_0_255_2(tmp0, tmp1, tmp0, tmp1);
-    LASX_PCKEV_B_128SV(tmp1, tmp0, dst0)
-    LASX_ST_D_4(dst0, 0, 2, 1, 3, dst, stride);
+    DUP2_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp0, tmp1);
+    dst0 = __lasx_xvpickev_b(tmp1, tmp0);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
 }
 
 static void avc_biwgt_8x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
@@ -1086,7 +1195,9 @@ static void avc_biwgt_8x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     __m256i wgt, vec0, vec1, vec2, vec3;
     __m256i src0, src1, dst0, dst1;
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset;
+    ptrdiff_t stride_2x = stride << 1;
     ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
     uint8_t* dst_tmp = dst;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
@@ -1099,37 +1210,46 @@ static void avc_biwgt_8x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src1);
-
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvld, dst_tmp, 0, dst_tmp + stride, 0, dst_tmp + stride_2x, 0,
+              dst_tmp + stride_3x, 0, tmp0, tmp1, tmp2, tmp3);
     dst_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst0);
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst1);
-
-    LASX_XORI_B_4_128(src0, src1, dst0, dst1);
-    LASX_ILVLH_B_2(dst0, src0, dst1, src1, vec1, vec0, vec3, vec2);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-    LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
-    LASX_DP2ADD_H_B(offset, wgt, vec2, tmp2);
-    LASX_DP2ADD_H_B(offset, wgt, vec3, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst_tmp, 0, dst_tmp, stride, dst_tmp, stride_2x,
+              dst_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+
+    DUP4_ARG2(__lasx_xvxori_b, src0, 128, src1, 128, dst0, 128, dst1, 128,
+              src0, src1, dst0, dst1);
+    DUP2_ARG2(__lasx_xvilvl_b, dst0, src0, dst1, src1, vec0, vec2);
+    DUP2_ARG2(__lasx_xvilvh_b, dst0, src0, dst1, src1, vec1, vec3);
+    DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+              offset, wgt, vec2, offset, wgt, vec3, tmp0, tmp1, tmp2, tmp3);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
     tmp2 = __lasx_xvsra_h(tmp2, denom);
     tmp3 = __lasx_xvsra_h(tmp3, denom);
-    LASX_CLIP_H_0_255_4(tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
-    LASX_PCKEV_B_2_128SV(tmp1, tmp0, tmp3, tmp2, dst0, dst1)
-    LASX_ST_D_4(dst0, 0, 2, 1, 3, dst, stride);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, dst0, dst1);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
     dst += stride_4x;
-    LASX_ST_D_4(dst1, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(dst1, dst, 0, 0);
+    __lasx_xvstelm_d(dst1, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst1, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst1, dst + stride_3x, 0, 3);
 }
 
 static void avc_biwgt_8x16_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
@@ -1139,7 +1259,9 @@ static void avc_biwgt_8x16_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     __m256i wgt, vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m256i src0, src1, src2, src3, dst0, dst1, dst2, dst3;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, denom, offset;
+    ptrdiff_t stride_2x = stride << 1;
     ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
     uint8_t* dst_tmp = dst;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
@@ -1152,49 +1274,58 @@ static void avc_biwgt_8x16_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src1);
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src2);
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src3);
-
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src2 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src3 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+
+    DUP4_ARG2(__lasx_xvldx, dst_tmp, 0, dst_tmp, stride, dst_tmp, stride_2x,
+              dst_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     dst_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst0);
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst_tmp, 0, dst_tmp, stride, dst_tmp, stride_2x,
+              dst_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     dst_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst1);
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst_tmp, 0, dst_tmp, stride, dst_tmp, stride_2x,
+              dst_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     dst_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst2);
-    LASX_LD_4(dst_tmp, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst3);
-
-    LASX_XORI_B_8_128(src0, src1, src2, src3, dst0, dst1, dst2, dst3);
-    LASX_ILVLH_B_4(dst0, src0, dst1, src1, dst2, src2, dst3, src3,
-                  vec1, vec0, vec3, vec2, vec5, vec4, vec7, vec6);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-    LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
-    LASX_DP2ADD_H_B(offset, wgt, vec2, tmp2);
-    LASX_DP2ADD_H_B(offset, wgt, vec3, tmp3);
-    LASX_DP2ADD_H_B(offset, wgt, vec4, tmp4);
-    LASX_DP2ADD_H_B(offset, wgt, vec5, tmp5);
-    LASX_DP2ADD_H_B(offset, wgt, vec6, tmp6);
-    LASX_DP2ADD_H_B(offset, wgt, vec7, tmp7);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst2 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst_tmp, 0, dst_tmp, stride, dst_tmp, stride_2x,
+              dst_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst3 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+
+    DUP4_ARG2(__lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lasx_xvilvl_b, dst0, src0, dst1, src1, dst2, src2,
+              dst3, src3, vec0, vec2, vec4, vec6);
+    DUP4_ARG2(__lasx_xvilvh_b, dst0, src0, dst1, src1, dst2, src2,
+              dst3, src3, vec1, vec3, vec5, vec7);
+    DUP4_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1,
+              offset, wgt, vec2, offset, wgt, vec3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG3(__lasx_xvdp2add_h_b,offset, wgt, vec4, offset, wgt, vec5,
+              offset, wgt, vec6, offset, wgt, vec7, tmp4, tmp5, tmp6, tmp7);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
     tmp2 = __lasx_xvsra_h(tmp2, denom);
@@ -1203,17 +1334,29 @@ static void avc_biwgt_8x16_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     tmp5 = __lasx_xvsra_h(tmp5, denom);
     tmp6 = __lasx_xvsra_h(tmp6, denom);
     tmp7 = __lasx_xvsra_h(tmp7, denom);
-    LASX_CLIP_H_0_255_4(tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
-    LASX_CLIP_H_0_255_4(tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
-    LASX_PCKEV_B_4_128SV(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                         dst0, dst1, dst2, dst3)
-    LASX_ST_D_4(dst0, 0, 2, 1, 3, dst, stride);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_xvclip255_h, tmp4, tmp5, tmp6, tmp7, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG2(__lasx_xvpickev_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+                   dst0, dst1, dst2, dst3)
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + stride_3x, 0, 3);
     dst += stride_4x;
-    LASX_ST_D_4(dst1, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(dst1, dst, 0, 0);
+    __lasx_xvstelm_d(dst1, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst1, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst1, dst + stride_3x, 0, 3);
     dst += stride_4x;
-    LASX_ST_D_4(dst2, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(dst2, dst, 0, 0);
+    __lasx_xvstelm_d(dst2, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst2, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst2, dst + stride_3x, 0, 3);
     dst += stride_4x;
-    LASX_ST_D_4(dst3, 0, 2, 1, 3, dst, stride);
+    __lasx_xvstelm_d(dst3, dst, 0, 0);
+    __lasx_xvstelm_d(dst3, dst + stride, 0, 1);
+    __lasx_xvstelm_d(dst3, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(dst3, dst + stride_3x, 0, 3);
 }
 
 void ff_biweight_h264_pixels8_8_lasx(uint8_t *dst, uint8_t *src,
@@ -1251,17 +1394,18 @@ static void avc_biwgt_4x2_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_2(src, stride, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, src0);
-    LASX_LD_2(dst, stride, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, dst0);
-    LASX_XORI_B_2_128(src0, dst0);
-    LASX_ILVL_B_128SV(dst0, src0, vec0);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
+    DUP2_ARG2(__lasx_xvldx, src, 0, src, stride, tmp0, tmp1);
+    src0 = __lasx_xvilvl_w(tmp1, tmp0);
+    DUP2_ARG2(__lasx_xvldx, dst, 0, dst, stride, tmp0, tmp1);
+    dst0 = __lasx_xvilvl_w(tmp1, tmp0);
+    DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
+    vec0 = __lasx_xvilvl_b(dst0, src0);
+    tmp0 = __lasx_xvdp2add_h_b(offset, wgt, vec0);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
-    LASX_CLIP_H_0_255(tmp0, tmp0);
-    LASX_PCKEV_B_128SV(tmp0, tmp0, tmp0);
-    LASX_ST_W_2(tmp0, 0, 1, dst, stride);
+    tmp0 = __lasx_xvclip255_h(tmp0);
+    tmp0 = __lasx_xvpickev_b(tmp0, tmp0);
+    __lasx_xvstelm_w(tmp0, dst, 0, 0);
+    __lasx_xvstelm_w(tmp0, dst + stride, 0, 1);
 }
 
 static void avc_biwgt_4x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
@@ -1271,6 +1415,8 @@ static void avc_biwgt_4x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     __m256i wgt, vec0;
     __m256i src0, dst0;
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
     offset_in  += ((weight_src + weight_dst) << 7);
@@ -1282,19 +1428,26 @@ static void avc_biwgt_4x4_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, src0);
-    LASX_LD_4(dst, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, dst0);
-    LASX_XORI_B_2_128(src0, dst0);
-    LASX_ILVL_B(dst0, src0, vec0);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
+    src0 = __lasx_xvilvl_w(tmp1, tmp0);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
+    dst0 = __lasx_xvilvl_w(tmp1, tmp0);
+    DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
+    vec0 = __lasx_xvilvl_b(dst0, src0);
+    dst0 = __lasx_xvilvh_b(dst0, src0);
+    vec0 = __lasx_xvpermi_q(vec0, dst0, 0x02);
+    tmp0 = __lasx_xvdp2add_h_b(offset, wgt, vec0);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
-    LASX_CLIP_H_0_255(tmp0, tmp0);
-    LASX_PCKEV_B_128SV(tmp0, tmp0, tmp0);
-    LASX_ST_W_4(tmp0, 0, 1, 4, 5, dst, stride);
+    tmp0 = __lasx_xvclip255_h(tmp0);
+    tmp0 = __lasx_xvpickev_b(tmp0, tmp0);
+    __lasx_xvstelm_w(tmp0, dst, 0, 0);
+    __lasx_xvstelm_w(tmp0, dst + stride, 0, 1);
+    __lasx_xvstelm_w(tmp0, dst + stride_2x, 0, 4);
+    __lasx_xvstelm_w(tmp0, dst + stride_3x, 0, 5);
 }
 
 static void avc_biwgt_4x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
@@ -1304,6 +1457,9 @@ static void avc_biwgt_4x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     __m256i wgt, vec0, vec1;
     __m256i src0, dst0;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, denom, offset;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_in   = (unsigned) ((offset_in + 1) | 1) << log2_denom;
     offset_in  += ((weight_src + weight_dst) << 7);
@@ -1315,25 +1471,42 @@ static void avc_biwgt_4x8_lasx(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_in);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_ILVL_W_4_128SV(tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5,
-                        tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_8(dst, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_ILVL_W_4_128SV(tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5,
-                        tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, dst0);
-    LASX_XORI_B_2_128(src0, dst0);
-    LASX_ILVLH_B(dst0, src0, vec1, vec0);
-    LASX_DP2ADD_H_B(offset, wgt, vec0, tmp0);
-    LASX_DP2ADD_H_B(offset, wgt, vec1, tmp1);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    dst += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x,
+              dst, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    dst -= stride_4x;
+    DUP4_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    dst0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP2_ARG2(__lasx_xvxori_b, src0, 128, dst0, 128, src0, dst0);
+    vec0 = __lasx_xvilvl_b(dst0, src0);
+    vec1 = __lasx_xvilvh_b(dst0, src0);
+    DUP2_ARG3(__lasx_xvdp2add_h_b, offset, wgt, vec0, offset, wgt, vec1, tmp0, tmp1);
     tmp0 = __lasx_xvsra_h(tmp0, denom);
     tmp1 = __lasx_xvsra_h(tmp1, denom);
-    LASX_CLIP_H_0_255_2(tmp0, tmp1, tmp0, tmp1);
-    LASX_PCKEV_B_128SV(tmp1, tmp0, tmp0);
-    LASX_ST_W_8(tmp0, 0, 1, 4, 5, 2, 3, 6, 7, dst, stride);
+    DUP2_ARG1(__lasx_xvclip255_h, tmp0, tmp1, tmp0, tmp1);
+    tmp0 = __lasx_xvpickev_b(tmp1, tmp0);
+    __lasx_xvstelm_w(tmp0, dst, 0, 0);
+    __lasx_xvstelm_w(tmp0, dst + stride, 0, 1);
+    __lasx_xvstelm_w(tmp0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_w(tmp0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_w(tmp0, dst, 0, 4);
+    __lasx_xvstelm_w(tmp0, dst + stride, 0, 5);
+    __lasx_xvstelm_w(tmp0, dst + stride_2x, 0, 6);
+    __lasx_xvstelm_w(tmp0, dst + stride_3x, 0, 7);
 }
 
 void ff_biweight_h264_pixels4_8_lasx(uint8_t *dst, uint8_t *src,
@@ -1358,6 +1531,9 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
                                     int weight_src, int offset_in)
 {
     uint32_t offset_val;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i zero = __lasx_xvldi(0);
     __m256i src0, src1, src2, src3;
     __m256i src0_l, src1_l, src2_l, src3_l, src0_h, src1_h, src2_h, src3_h;
@@ -1370,11 +1546,18 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                   src0, src1, src2, src3);
-    LASX_ILVLH_B_4(zero, src0, zero, src1, zero, src2, zero, src3,
-                   src0_h, src0_l, src1_h, src1_l, src2_h, src2_l, src3_h, src3_l);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    src -= stride_4x;
+    DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+              0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, zero, src2,
+              zero, src3, src0_l, src1_l, src2_l, src3_l);
+    DUP4_ARG2(__lasx_xvilvh_b, zero, src0, zero, src1, zero, src2,
+              zero, src3, src0_h, src1_h, src2_h, src3_h);
     src0_l = __lasx_xvmul_h(wgt, src0_l);
     src0_h = __lasx_xvmul_h(wgt, src0_h);
     src1_l = __lasx_xvmul_h(wgt, src1_l);
@@ -1383,10 +1566,10 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
     src2_h = __lasx_xvmul_h(wgt, src2_h);
     src3_l = __lasx_xvmul_h(wgt, src3_l);
     src3_h = __lasx_xvmul_h(wgt, src3_h);
-    LASX_SADD_H_4(src0_l, offset, src0_h, offset, src1_l, offset,
-                  src1_h, offset, src0_l, src0_h, src1_l, src1_h);
-    LASX_SADD_H_4(src2_l, offset, src2_h, offset, src3_l, offset,
-                  src3_h, offset, src2_l, src2_h, src3_l, src3_h);
+    DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l, offset,
+              src1_h, offset, src0_l, src0_h, src1_l, src1_h);
+    DUP4_ARG2(__lasx_xvsadd_h, src2_l, offset, src2_h, offset, src3_l, offset,
+              src3_h, offset, src2_l, src2_h, src3_l, src3_h);
     src0_l = __lasx_xvmaxi_h(src0_l, 0);
     src0_h = __lasx_xvmaxi_h(src0_h, 0);
     src1_l = __lasx_xvmaxi_h(src1_l, 0);
@@ -1404,37 +1587,43 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
     src3_l = __lasx_xvssrlrn_bu_h(src3_l, denom);
     src3_h = __lasx_xvssrlrn_bu_h(src3_h, denom);
     __lasx_xvstelm_d(src0_l, src, 0, 0);
-    __lasx_xvstelm_d(src0_l, src, 8, 2);
+    __lasx_xvstelm_d(src0_h, src, 8, 0);
     src += stride;
-    __lasx_xvstelm_d(src0_h, src, 0, 0);
+    __lasx_xvstelm_d(src0_l, src, 0, 2);
     __lasx_xvstelm_d(src0_h, src, 8, 2);
     src += stride;
     __lasx_xvstelm_d(src1_l, src, 0, 0);
-    __lasx_xvstelm_d(src1_l, src, 8, 2);
+    __lasx_xvstelm_d(src1_h, src, 8, 0);
     src += stride;
-    __lasx_xvstelm_d(src1_h, src, 0, 0);
+    __lasx_xvstelm_d(src1_l, src, 0, 2);
     __lasx_xvstelm_d(src1_h, src, 8, 2);
     src += stride;
     __lasx_xvstelm_d(src2_l, src, 0, 0);
-    __lasx_xvstelm_d(src2_l, src, 8, 2);
+    __lasx_xvstelm_d(src2_h, src, 8, 0);
     src += stride;
-    __lasx_xvstelm_d(src2_h, src, 0, 0);
+    __lasx_xvstelm_d(src2_l, src, 0, 2);
     __lasx_xvstelm_d(src2_h, src, 8, 2);
     src += stride;
     __lasx_xvstelm_d(src3_l, src, 0, 0);
-    __lasx_xvstelm_d(src3_l, src, 8, 2);
+    __lasx_xvstelm_d(src3_h, src, 8, 0);
     src += stride;
-    __lasx_xvstelm_d(src3_h, src, 0, 0);
+    __lasx_xvstelm_d(src3_l, src, 0, 2);
     __lasx_xvstelm_d(src3_h, src, 8, 2);
     src += stride;
 
     if (16 == height) {
-        LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-        LASX_PCKEV_Q_4(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                       src0, src1, src2, src3);
-        LASX_ILVLH_B_4(zero, src0, zero, src1, zero, src2, zero, src3,
-                       src0_h, src0_l, src1_h, src1_l, src2_h, src2_l,
-                       src3_h, src3_l);
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+                  src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+        src += stride_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+                  src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+        src -= stride_4x;
+        DUP4_ARG3(__lasx_xvpermi_q, tmp1, tmp0, 0x20, tmp3, tmp2, 0x20, tmp5, tmp4,
+                  0x20, tmp7, tmp6, 0x20, src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, zero, src2,
+                  zero, src3, src0_l, src1_l, src2_l, src3_l);
+        DUP4_ARG2(__lasx_xvilvh_b, zero, src0, zero, src1, zero, src2,
+                  zero, src3, src0_h, src1_h, src2_h, src3_h);
         src0_l = __lasx_xvmul_h(wgt, src0_l);
         src0_h = __lasx_xvmul_h(wgt, src0_h);
         src1_l = __lasx_xvmul_h(wgt, src1_l);
@@ -1443,10 +1632,10 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
         src2_h = __lasx_xvmul_h(wgt, src2_h);
         src3_l = __lasx_xvmul_h(wgt, src3_l);
         src3_h = __lasx_xvmul_h(wgt, src3_h);
-        LASX_SADD_H_4(src0_l, offset, src0_h, offset, src1_l, offset,
-                      src1_h, offset, src0_l, src0_h, src1_l, src1_h);
-        LASX_SADD_H_4(src2_l, offset, src2_h, offset, src3_l, offset,
-                      src3_h, offset, src2_l, src2_h, src3_l, src3_h);
+        DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l, offset,
+                  src1_h, offset, src0_l, src0_h, src1_l, src1_h);
+        DUP4_ARG2(__lasx_xvsadd_h, src2_l, offset, src2_h, offset, src3_l, offset,
+                  src3_h, offset, src2_l, src2_h, src3_l, src3_h);
         src0_l = __lasx_xvmaxi_h(src0_l, 0);
         src0_h = __lasx_xvmaxi_h(src0_h, 0);
         src1_l = __lasx_xvmaxi_h(src1_l, 0);
@@ -1464,27 +1653,27 @@ void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
         src3_l = __lasx_xvssrlrn_bu_h(src3_l, denom);
         src3_h = __lasx_xvssrlrn_bu_h(src3_h, denom);
         __lasx_xvstelm_d(src0_l, src, 0, 0);
-        __lasx_xvstelm_d(src0_l, src, 8, 2);
+        __lasx_xvstelm_d(src0_h, src, 8, 0);
         src += stride;
-        __lasx_xvstelm_d(src0_h, src, 0, 0);
+        __lasx_xvstelm_d(src0_l, src, 0, 2);
         __lasx_xvstelm_d(src0_h, src, 8, 2);
         src += stride;
         __lasx_xvstelm_d(src1_l, src, 0, 0);
-        __lasx_xvstelm_d(src1_l, src, 8, 2);
+        __lasx_xvstelm_d(src1_h, src, 8, 0);
         src += stride;
-        __lasx_xvstelm_d(src1_h, src, 0, 0);
+        __lasx_xvstelm_d(src1_l, src, 0, 2);
         __lasx_xvstelm_d(src1_h, src, 8, 2);
         src += stride;
         __lasx_xvstelm_d(src2_l, src, 0, 0);
-        __lasx_xvstelm_d(src2_l, src, 8, 2);
+        __lasx_xvstelm_d(src2_h, src, 8, 0);
         src += stride;
-        __lasx_xvstelm_d(src2_h, src, 0, 0);
+        __lasx_xvstelm_d(src2_l, src, 0, 2);
         __lasx_xvstelm_d(src2_h, src, 8, 2);
         src += stride;
         __lasx_xvstelm_d(src3_l, src, 0, 0);
-        __lasx_xvstelm_d(src3_l, src, 8, 2);
+        __lasx_xvstelm_d(src3_h, src, 8, 0);
         src += stride;
-        __lasx_xvstelm_d(src3_h, src, 0, 0);
+        __lasx_xvstelm_d(src3_l, src, 0, 2);
         __lasx_xvstelm_d(src3_h, src, 8, 2);
     }
 }
@@ -1494,6 +1683,8 @@ static void avc_wgt_8x4_lasx(uint8_t *src, ptrdiff_t stride,
                              int32_t offset_in)
 {
     uint32_t offset_val;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i wgt, zero = __lasx_xvldi(0);
     __m256i src0, src0_h, src0_l;
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset;
@@ -1504,10 +1695,12 @@ static void avc_wgt_8x4_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_ILVLH_B(zero, src0, src0_h, src0_l);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    src0_l = __lasx_xvilvl_b(zero, src0);
+    src0_h = __lasx_xvilvh_b(zero, src0);
     src0_l = __lasx_xvmul_h(wgt, src0_l);
     src0_h = __lasx_xvmul_h(wgt, src0_h);
     src0_l = __lasx_xvsadd_h(src0_l, offset);
@@ -1517,18 +1710,23 @@ static void avc_wgt_8x4_lasx(uint8_t *src, ptrdiff_t stride,
     src0_l = __lasx_xvssrlrn_bu_h(src0_l, denom);
     src0_h = __lasx_xvssrlrn_bu_h(src0_h, denom);
 
-    LASX_PCKEV_D(src0_h, src0_l, src0);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, src, stride);
+    src0 = __lasx_xvpickev_d(src0_h, src0_l);
+    __lasx_xvstelm_d(src0, src, 0, 0);
+    __lasx_xvstelm_d(src0, src + stride, 0, 1);
+    __lasx_xvstelm_d(src0, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, src + stride_3x, 0, 3);
 }
 
 static void avc_wgt_8x8_lasx(uint8_t *src, ptrdiff_t stride, int32_t log2_denom,
                              int32_t src_weight, int32_t offset_in)
 {
-    uint32_t offset_val;
     __m256i src0, src1, src0_h, src0_l, src1_h, src1_l, zero = __lasx_xvldi(0);
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset, wgt;
-    ptrdiff_t stride_4x = stride << 2;
+    uint32_t offset_val;
     uint8_t* src_tmp = src;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_val = (unsigned) offset_in << log2_denom;
 
@@ -1536,20 +1734,23 @@ static void avc_wgt_8x8_lasx(uint8_t *src, ptrdiff_t stride, int32_t log2_denom,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src1);
-    LASX_ILVLH_B_2(zero, src0, zero, src1, src0_h, src0_l, src1_h, src1_l);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP2_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, src0_l, src1_l);
+    DUP2_ARG2(__lasx_xvilvh_b, zero, src0, zero, src1, src0_h, src1_h);
     src0_l = __lasx_xvmul_h(wgt, src0_l);
     src0_h = __lasx_xvmul_h(wgt, src0_h);
     src1_l = __lasx_xvmul_h(wgt, src1_l);
     src1_h = __lasx_xvmul_h(wgt, src1_h);
-    LASX_SADD_H_4(src0_l, offset, src0_h, offset, src1_l, offset,
-                  src1_h, offset, src0_l, src0_h, src1_l, src1_h);
+    DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l, offset,
+              src1_h, offset, src0_l, src0_h, src1_l, src1_h);
     src0_l = __lasx_xvmaxi_h(src0_l, 0);
     src0_h = __lasx_xvmaxi_h(src0_h, 0);
     src1_l = __lasx_xvmaxi_h(src1_l, 0);
@@ -1559,23 +1760,31 @@ static void avc_wgt_8x8_lasx(uint8_t *src, ptrdiff_t stride, int32_t log2_denom,
     src1_l = __lasx_xvssrlrn_bu_h(src1_l, denom);
     src1_h = __lasx_xvssrlrn_bu_h(src1_h, denom);
 
-    LASX_PCKEV_D_2(src0_h, src0_l, src1_h, src1_l, src0, src1);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, src, stride);
+    DUP2_ARG2(__lasx_xvpickev_d, src0_h, src0_l, src1_h, src1_l, src0, src1);
+    __lasx_xvstelm_d(src0, src, 0, 0);
+    __lasx_xvstelm_d(src0, src + stride, 0, 1);
+    __lasx_xvstelm_d(src0, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, src + stride_3x, 0, 3);
     src += stride_4x;
-    LASX_ST_D_4(src1, 0, 1, 2, 3, src, stride);
+    __lasx_xvstelm_d(src1, src, 0, 0);
+    __lasx_xvstelm_d(src1, src + stride, 0, 1);
+    __lasx_xvstelm_d(src1, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, src + stride_3x, 0, 3);
 }
 
 static void avc_wgt_8x16_lasx(uint8_t *src, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t offset_in)
 {
-    uint32_t offset_val;
     __m256i src0, src1, src2, src3;
     __m256i src0_h, src0_l, src1_h, src1_l, src2_h, src2_l, src3_h, src3_l;
     __m256i tmp0, tmp1, tmp2, tmp3, denom, offset, wgt;
     __m256i zero = __lasx_xvldi(0);
-    ptrdiff_t stride_4x = stride << 2;
+    uint32_t offset_val;
     uint8_t* src_tmp = src;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_val = (unsigned) offset_in << log2_denom;
 
@@ -1583,24 +1792,30 @@ static void avc_wgt_8x16_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src1);
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src1 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
     src_tmp += stride_4x;
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src2);
-    LASX_LD_4(src_tmp, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_D_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src3);
-
-    LASX_ILVLH_B_4(zero, src0, zero, src1, zero, src2, zero, src3,
-                   src0_h, src0_l, src1_h, src1_l, src2_h, src2_l, src3_h, src3_l);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src2 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, stride, src_tmp, stride_2x,
+              src_tmp, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src3 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+
+    DUP4_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+              src0_l, src1_l, src2_l, src3_l);
+    DUP4_ARG2(__lasx_xvilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+              src0_h, src1_h, src2_h, src3_h);
     src0_l = __lasx_xvmul_h(wgt, src0_l);
     src0_h = __lasx_xvmul_h(wgt, src0_h);
     src1_l = __lasx_xvmul_h(wgt, src1_l);
@@ -1610,10 +1825,10 @@ static void avc_wgt_8x16_lasx(uint8_t *src, ptrdiff_t stride,
     src3_l = __lasx_xvmul_h(wgt, src3_l);
     src3_h = __lasx_xvmul_h(wgt, src3_h);
 
-    LASX_SADD_H_4(src0_l, offset, src0_h, offset, src1_l, offset,
-                  src1_h, offset, src0_l, src0_h, src1_l, src1_h);
-    LASX_SADD_H_4(src2_l, offset, src2_h, offset, src3_l, offset,
-                  src3_h, offset, src2_l, src2_h, src3_l, src3_h);
+    DUP4_ARG2(__lasx_xvsadd_h, src0_l, offset, src0_h, offset, src1_l, offset,
+              src1_h, offset, src0_l, src0_h, src1_l, src1_h);
+    DUP4_ARG2(__lasx_xvsadd_h, src2_l, offset, src2_h, offset, src3_l, offset,
+              src3_h, offset, src2_l, src2_h, src3_l, src3_h);
 
     src0_l = __lasx_xvmaxi_h(src0_l, 0);
     src0_h = __lasx_xvmaxi_h(src0_h, 0);
@@ -1631,16 +1846,28 @@ static void avc_wgt_8x16_lasx(uint8_t *src, ptrdiff_t stride,
     src2_h = __lasx_xvssrlrn_bu_h(src2_h, denom);
     src3_l = __lasx_xvssrlrn_bu_h(src3_l, denom);
     src3_h = __lasx_xvssrlrn_bu_h(src3_h, denom);
-    LASX_PCKEV_D_4(src0_h, src0_l, src1_h, src1_l, src2_h, src2_l, src3_h, src3_l,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvpickev_d, src0_h, src0_l, src1_h, src1_l, src2_h, src2_l,
+              src3_h, src3_l, src0, src1, src2, src3);
 
-    LASX_ST_D_4(src0, 0, 1, 2, 3, src, stride);
+    __lasx_xvstelm_d(src0, src, 0, 0);
+    __lasx_xvstelm_d(src0, src + stride, 0, 1);
+    __lasx_xvstelm_d(src0, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, src + stride_3x, 0, 3);
     src += stride_4x;
-    LASX_ST_D_4(src1, 0, 1, 2, 3, src, stride);
+    __lasx_xvstelm_d(src1, src, 0, 0);
+    __lasx_xvstelm_d(src1, src + stride, 0, 1);
+    __lasx_xvstelm_d(src1, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, src + stride_3x, 0, 3);
     src += stride_4x;
-    LASX_ST_D_4(src2, 0, 1, 2, 3, src, stride);
+    __lasx_xvstelm_d(src2, src, 0, 0);
+    __lasx_xvstelm_d(src2, src + stride, 0, 1);
+    __lasx_xvstelm_d(src2, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src2, src + stride_3x, 0, 3);
     src += stride_4x;
-    LASX_ST_D_4(src3, 0, 1, 2, 3, src, stride);
+    __lasx_xvstelm_d(src3, src, 0, 0);
+    __lasx_xvstelm_d(src3, src + stride, 0, 1);
+    __lasx_xvstelm_d(src3, src + stride_2x, 0, 2);
+    __lasx_xvstelm_d(src3, src + stride_3x, 0, 3);
 }
 
 void ff_weight_h264_pixels8_8_lasx(uint8_t *src, ptrdiff_t stride,
@@ -1670,23 +1897,26 @@ static void avc_wgt_4x2_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_2(src, stride, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, src0);
-    LASX_ILVL_B_128SV(zero, src0, src0);
+    DUP2_ARG2(__lasx_xvldx, src, 0, src, stride, tmp0, tmp1);
+    src0 = __lasx_xvilvl_w(tmp1, tmp0);
+    src0 = __lasx_xvilvl_b(zero, src0);
     src0 = __lasx_xvmul_h(wgt, src0);
     src0 = __lasx_xvsadd_h(src0, offset);
     src0 = __lasx_xvmaxi_h(src0, 0);
     src0 = __lasx_xvssrlrn_bu_h(src0, denom);
-    LASX_ST_W_2(src0, 0, 1, src, stride);
+    __lasx_xvstelm_w(src0, src, 0, 0);
+    __lasx_xvstelm_w(src0, src + stride, 0, 1);
 }
 
 static void avc_wgt_4x4_lasx(uint8_t *src, ptrdiff_t stride,
                              int32_t log2_denom, int32_t weight_src,
                              int32_t offset_in)
 {
-    uint32_t offset_val;
     __m256i wgt;
     __m256i src0, tmp0, tmp1, tmp2, tmp3, denom, offset;
+    uint32_t offset_val;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_val = (unsigned) offset_in << log2_denom;
 
@@ -1694,25 +1924,32 @@ static void avc_wgt_4x4_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_4(src, stride, tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
-    LASX_ILVL_W_128SV(tmp1, tmp0, src0);
-    LASX_UNPCK_L_HU_BU(src0, src0);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp0, tmp1);
+    src0 = __lasx_xvilvl_w(tmp1, tmp0);
+    src0 = __lasx_vext2xv_hu_bu(src0);
     src0 = __lasx_xvmul_h(wgt, src0);
     src0 = __lasx_xvsadd_h(src0, offset);
     src0 = __lasx_xvmaxi_h(src0, 0);
     src0 = __lasx_xvssrlrn_bu_h(src0, denom);
-    LASX_ST_W_4(src0, 0, 1, 4, 5, src, stride);
+    __lasx_xvstelm_w(src0, src, 0, 0);
+    __lasx_xvstelm_w(src0, src + stride, 0, 1);
+    __lasx_xvstelm_w(src0, src + stride_2x, 0, 4);
+    __lasx_xvstelm_w(src0, src + stride_3x, 0, 5);
 }
 
 static void avc_wgt_4x8_lasx(uint8_t *src, ptrdiff_t stride,
                              int32_t log2_denom, int32_t weight_src,
                              int32_t offset_in)
 {
-    uint32_t offset_val;
     __m256i src0, src0_h, src0_l;
     __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, denom, offset;
     __m256i wgt, zero = __lasx_xvldi(0);
+    uint32_t offset_val;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     offset_val = (unsigned) offset_in << log2_denom;
 
@@ -1720,12 +1957,18 @@ static void avc_wgt_4x8_lasx(uint8_t *src, ptrdiff_t stride,
     offset = __lasx_xvreplgr2vr_h(offset_val);
     denom  = __lasx_xvreplgr2vr_h(log2_denom);
 
-    LASX_LD_8(src, stride, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_ILVL_W_4_128SV(tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5,
-                        tmp0, tmp1, tmp2, tmp3);
-    LASX_ILVL_W_2_128SV(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LASX_PCKEV_Q(tmp1, tmp0, src0);
-    LASX_ILVLH_B(zero, src0, src0_h, src0_l);
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp0, tmp1, tmp2, tmp3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, stride, src, stride_2x,
+              src, stride_3x, tmp4, tmp5, tmp6, tmp7);
+    src -= stride_4x;
+    DUP4_ARG2(__lasx_xvilvl_w, tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7,
+              tmp5, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lasx_xvilvl_w, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    src0 = __lasx_xvpermi_q(tmp1, tmp0, 0x20);
+    src0_l = __lasx_xvilvl_b(zero, src0);
+    src0_h = __lasx_xvilvh_b(zero, src0);
     src0_l = __lasx_xvmul_h(wgt, src0_l);
     src0_h = __lasx_xvmul_h(wgt, src0_h);
     src0_l = __lasx_xvsadd_h(src0_l, offset);
@@ -1734,9 +1977,15 @@ static void avc_wgt_4x8_lasx(uint8_t *src, ptrdiff_t stride,
     src0_h = __lasx_xvmaxi_h(src0_h, 0);
     src0_l = __lasx_xvssrlrn_bu_h(src0_l, denom);
     src0_h = __lasx_xvssrlrn_bu_h(src0_h, denom);
-    LASX_ST_W_4(src0_l, 0, 1, 4, 5, src, stride);
-    src += (stride << 2);
-    LASX_ST_W_4(src0_h, 0, 1, 4, 5, src, stride);
+    __lasx_xvstelm_w(src0_l, src, 0, 0);
+    __lasx_xvstelm_w(src0_l, src + stride, 0, 1);
+    __lasx_xvstelm_w(src0_h, src + stride_2x, 0, 0);
+    __lasx_xvstelm_w(src0_h, src + stride_3x, 0, 1);
+    src += stride_4x;
+    __lasx_xvstelm_w(src0_l, src, 0, 4);
+    __lasx_xvstelm_w(src0_l, src + stride, 0, 5);
+    __lasx_xvstelm_w(src0_h, src + stride_2x, 0, 4);
+    __lasx_xvstelm_w(src0_h, src + stride_3x, 0, 5);
 }
 
 void ff_weight_h264_pixels4_8_lasx(uint8_t *src, ptrdiff_t stride,
diff --git a/libavcodec/loongarch/h264idct_lasx.c b/libavcodec/loongarch/h264idct_lasx.c
index a4acb2d404..88dc56ef2c 100644
--- a/libavcodec/loongarch/h264idct_lasx.c
+++ b/libavcodec/loongarch/h264idct_lasx.c
@@ -22,39 +22,10 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "h264dsp_lasx.h"
 #include "libavcodec/bit_depth_template.c"
 
-#define BUTTERFLY_4_H(tmp0_m, tmp1_m, tmp2_m, tmp3_m, out0, out1, out2, out3) \
-{                                                                             \
-     out0 = __lasx_xvadd_h(tmp0_m, tmp3_m);                                   \
-     out1 = __lasx_xvadd_h(tmp1_m, tmp2_m);                                   \
-     out2 = __lasx_xvsub_h(tmp1_m, tmp2_m);                                   \
-     out3 = __lasx_xvsub_h(tmp0_m, tmp3_m);                                   \
- }
-
-#define BUTTERFLY_4_W(tmp0_m, tmp1_m, tmp2_m, tmp3_m, out0, out1, out2, out3) \
-{                                                                             \
-     out0 = __lasx_xvadd_w(tmp0_m, tmp3_m);                                   \
-     out1 = __lasx_xvadd_w(tmp1_m, tmp2_m);                                   \
-     out2 = __lasx_xvsub_w(tmp1_m, tmp2_m);                                   \
-     out3 = __lasx_xvsub_w(tmp0_m, tmp3_m);                                   \
- }
-
-#define BUTTERFLY_8_H(in0, in1, in2, in3, in4, in5, in6, in7,          \
-                      out0, out1, out2, out3, out4, out5, out6, out7)  \
-{                                                                      \
-    out0 = __lasx_xvadd_h(in0, in7);                                   \
-    out1 = __lasx_xvadd_h(in1, in6);                                   \
-    out2 = __lasx_xvadd_h(in2, in5);                                   \
-    out3 = __lasx_xvadd_h(in3, in4);                                   \
-    out4 = __lasx_xvsub_h(in3, in4);                                   \
-    out5 = __lasx_xvsub_h(in2, in5);                                   \
-    out6 = __lasx_xvsub_h(in1, in6);                                   \
-    out7 = __lasx_xvsub_h(in0, in7);                                   \
-}
-
 #define AVC_ITRANS_H(in0, in1, in2, in3, out0, out1, out2, out3)     \
 {                                                                    \
    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                           \
@@ -66,8 +37,8 @@
     tmp3_m = __lasx_xvsrai_h(in3, 1);                                \
     tmp3_m = __lasx_xvadd_h(in1, tmp3_m);                            \
                                                                      \
-    BUTTERFLY_4_H(tmp0_m, tmp1_m, tmp2_m, tmp3_m,                    \
-                  out0, out1, out2, out3);                           \
+    LASX_BUTTERFLY_4_H(tmp0_m, tmp1_m, tmp2_m, tmp3_m,               \
+                       out0, out1, out2, out3);                      \
 }
 
 void ff_h264_idct_add_lasx(uint8_t *dst, int16_t *src, int32_t dst_stride)
@@ -76,27 +47,34 @@ void ff_h264_idct_add_lasx(uint8_t *dst, int16_t *src, int32_t dst_stride)
     __m256i dst0_m, dst1_m;
     __m256i hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3;
     __m256i inp0_m, inp1_m, res0_m, src1, src3;
-    __m256i src0 = LASX_LD(src);
-    __m256i src2 = LASX_LD(src + 8);
+    __m256i src0 = __lasx_xvld(src, 0);
+    __m256i src2 = __lasx_xvld(src, 16);
     __m256i zero = __lasx_xvldi(0);
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
 
-    LASX_ST(zero, src);
-    LASX_ILVH_D_2_128SV(src0, src0, src2, src2, src1, src3);
+    __lasx_xvst(zero, src, 0);
+    DUP2_ARG2(__lasx_xvilvh_d, src0, src0, src2, src2, src1, src3);
     AVC_ITRANS_H(src0, src1, src2, src3, hres0, hres1, hres2, hres3);
-    LASX_TRANSPOSE4x4_H_128SV(hres0, hres1, hres2, hres3,
-                              hres0, hres1, hres2, hres3);
+    LASX_TRANSPOSE4x4_H(hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3);
     AVC_ITRANS_H(hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3);
-    LASX_LD_4(dst, dst_stride, src0_m, src1_m, src2_m, src3_m);
-    LASX_ILVL_D_2_128SV(vres1, vres0, vres3, vres2, inp0_m, inp1_m);
-    LASX_PCKEV_Q(inp1_m, inp0_m, inp0_m);
-    LASX_SRARI_H(inp0_m, inp0_m, 6);
-    LASX_ILVL_W_2_128SV(src1_m, src0_m, src3_m, src2_m, dst0_m, dst1_m);
-    LASX_ILVL_D_128SV(dst1_m, dst0_m, dst0_m);
-    LASX_UNPCK_L_HU_BU(dst0_m, res0_m);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, src0_m, src1_m, src2_m, src3_m);
+    DUP4_ARG2(__lasx_xvld, dst, 0, dst + dst_stride, 0, dst + dst_stride_2x,
+              0, dst + dst_stride_3x, 0, src0_m, src1_m, src2_m, src3_m);
+    DUP2_ARG2(__lasx_xvilvl_d, vres1, vres0, vres3, vres2, inp0_m, inp1_m);
+    inp0_m = __lasx_xvpermi_q(inp1_m, inp0_m, 0x20);
+    inp0_m = __lasx_xvsrari_h(inp0_m, 6);
+    DUP2_ARG2(__lasx_xvilvl_w, src1_m, src0_m, src3_m, src2_m, dst0_m, dst1_m);
+    dst0_m = __lasx_xvilvl_d(dst1_m, dst0_m);
+    res0_m = __lasx_vext2xv_hu_bu(dst0_m);
     res0_m = __lasx_xvadd_h(res0_m, inp0_m);
-    LASX_CLIP_H_0_255(res0_m, res0_m);
+    res0_m = __lasx_xvclip255_h(res0_m);
     dst0_m = __lasx_xvpickev_b(res0_m, res0_m);
-    LASX_ST_W_4(dst0_m, 0, 1, 4, 5, dst, dst_stride);
+    __lasx_xvstelm_w(dst0_m, dst, 0, 0);
+    __lasx_xvstelm_w(dst0_m, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_w(dst0_m, dst + dst_stride_2x, 0, 4);
+    __lasx_xvstelm_w(dst0_m, dst + dst_stride_3x, 0, 5);
 }
 
 void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
@@ -108,10 +86,17 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     __m256i res0, res1, res2, res3, res4, res5, res6, res7;
     __m256i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     __m256i zero = __lasx_xvldi(0);
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
 
     src[0] += 32;
-    LASX_LD_8(src, 8, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_ST_4(zero, zero, zero, zero, src, 16);
+    DUP4_ARG2(__lasx_xvld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 64, src, 80, src, 96, src, 112, src4, src5, src6, src7);
+    __lasx_xvst(zero, src, 0);
+    __lasx_xvst(zero, src, 32);
+    __lasx_xvst(zero, src, 64);
+    __lasx_xvst(zero, src, 96);
 
     vec0 = __lasx_xvadd_h(src0, src4);
     vec1 = __lasx_xvsub_h(src0, src4);
@@ -120,7 +105,7 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     vec3 = __lasx_xvsrai_h(src6, 1);
     vec3 = __lasx_xvadd_h(src2, vec3);
 
-    BUTTERFLY_4_H(vec0, vec1, vec2, vec3, tmp0, tmp1, tmp2, tmp3);
+    LASX_BUTTERFLY_4_H(vec0, vec1, vec2, vec3, tmp0, tmp1, tmp2, tmp3);
 
     vec0 = __lasx_xvsrai_h(src7, 1);
     vec0 = __lasx_xvsub_h(src5, vec0);
@@ -151,13 +136,13 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     tmp7 = __lasx_xvsrai_h(vec0, 2);
     tmp7 = __lasx_xvsub_h(vec3, tmp7);
 
-    BUTTERFLY_8_H(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
-                  res0, res1, res2, res3, res4, res5, res6, res7);
-    LASX_TRANSPOSE8x8_H_128SV(res0, res1, res2, res3, res4, res5, res6, res7,
-                              res0, res1, res2, res3, res4, res5, res6, res7);
+    LASX_BUTTERFLY_8_H(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                       res0, res1, res2, res3, res4, res5, res6, res7);
+    LASX_TRANSPOSE8x8_H(res0, res1, res2, res3, res4, res5, res6, res7,
+                        res0, res1, res2, res3, res4, res5, res6, res7);
 
-    LASX_UNPCK_L_W_H_8(res0, res1, res2, res3, res4, res5, res6, res7,
-                       tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG1(__lasx_vext2xv_w_h, res0, res1, res2, res3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG1(__lasx_vext2xv_w_h, res4, res5, res6, res7, tmp4, tmp5, tmp6, tmp7);
     vec0 = __lasx_xvadd_w(tmp0, tmp4);
     vec1 = __lasx_xvsub_w(tmp0, tmp4);
 
@@ -200,52 +185,77 @@ void ff_h264_idct8_addblk_lasx(uint8_t *dst, int16_t *src,
     tmp7 = __lasx_xvsrai_w(vec0, 2);
     tmp7 = __lasx_xvsub_w(vec3, tmp7);
 
-    BUTTERFLY_4_W(tmp0, tmp2, tmp5, tmp7, res0, res1, res6, res7);
-    BUTTERFLY_4_W(tmp4, tmp6, tmp1, tmp3, res2, res3, res4, res5);
-
-    LASX_SRAI_W_8(res0, res1, res2, res3, res4, res5, res6, res7,
-                  res0, res1, res2, res3, res4, res5, res6, res7, 6);
-    LASX_PCKEV_H_4(res1, res0, res3, res2, res5, res4, res7, res6,
-                   res0, res1, res2, res3);
-
-    LASX_LD_8(dst, dst_stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
-    LASX_ILVL_B_8_128SV(zero, dst0, zero, dst1, zero, dst2, zero, dst3,
-                        zero, dst4, zero, dst5, zero, dst6, zero, dst7,
-                        dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
-    LASX_PCKEV_Q_4(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                   dst0, dst1, dst2, dst3);
+    LASX_BUTTERFLY_4_W(tmp0, tmp2, tmp5, tmp7, res0, res1, res6, res7);
+    LASX_BUTTERFLY_4_W(tmp4, tmp6, tmp1, tmp3, res2, res3, res4, res5);
+
+    DUP4_ARG2(__lasx_xvsrai_w, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1, res2, res3);
+    DUP4_ARG2(__lasx_xvsrai_w, res4, 6, res5, 6, res6, 6, res7, 6, res4, res5, res6, res7);
+    DUP4_ARG2(__lasx_xvpickev_h, res1, res0, res3, res2, res5, res4, res7, res6,
+              res0, res1, res2, res3);
+    DUP4_ARG2(__lasx_xvpermi_d, res0, 0xd8, res1, 0xd8, res2, 0xd8, res3, 0xd8,
+              res0, res1, res2, res3);
+
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, dst0, dst1, dst2, dst3);
+    dst += dst_stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, dst4, dst5, dst6, dst7);
+    dst -= dst_stride_4x;
+    DUP4_ARG2(__lasx_xvilvl_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lasx_xvilvl_b, zero, dst4, zero, dst5, zero, dst6, zero, dst7,
+              dst4, dst5, dst6, dst7);
+    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5, dst4, 0x20,
+              dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
     res0 = __lasx_xvadd_h(res0, dst0);
     res1 = __lasx_xvadd_h(res1, dst1);
     res2 = __lasx_xvadd_h(res2, dst2);
     res3 = __lasx_xvadd_h(res3, dst3);
-    LASX_CLIP_H_0_255_4(res0, res1, res2, res3, res0, res1, res2, res3);
-    LASX_PCKEV_B_2_128SV(res1, res0, res3, res2, res0, res1);
-    LASX_ST_D_4(res0, 0, 2, 1, 3, dst, dst_stride);
-    LASX_ST_D_4(res1, 0, 2, 1, 3, dst + 4 * dst_stride, dst_stride);
+    DUP4_ARG1(__lasx_xvclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
+    DUP2_ARG2(__lasx_xvpickev_b, res1, res0, res3, res2, res0, res1);
+    __lasx_xvstelm_d(res0, dst, 0, 0);
+    __lasx_xvstelm_d(res0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(res0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(res0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(res1, dst, 0, 0);
+    __lasx_xvstelm_d(res1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(res1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(res1, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_h264_idct4x4_addblk_dc_lasx(uint8_t *dst, int16_t *src,
                                     int32_t dst_stride)
 {
     const int16_t dc = (src[0] + 32) >> 6;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     __m256i pred, out;
     __m256i src0, src1, src2, src3;
     __m256i input_dc = __lasx_xvreplgr2vr_h(dc);
 
     src[0] = 0;
-    LASX_LD_4(dst, dst_stride, src0, src1, src2, src3);
-    LASX_ILVL_W_2_128SV(src1, src0, src3, src2, src0, src1);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, src0, src1, src2, src3);
+    DUP2_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src0, src1);
+
     pred = __lasx_xvpermi_q(src0, src1, 0x02);
-    LASX_ADDW_H_H_BU_128SV(input_dc, pred, pred);
-    LASX_CLIP_H_0_255(pred, pred);
+    pred = __lasx_xvaddw_h_h_bu(input_dc, pred);
+    pred = __lasx_xvclip255_h(pred);
     out = __lasx_xvpickev_b(pred, pred);
-    LASX_ST_W_4(out, 0, 1, 4, 5, dst, dst_stride);
+    __lasx_xvstelm_w(out, dst, 0, 0);
+    __lasx_xvstelm_w(out, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_w(out, dst + dst_stride_2x, 0, 4);
+    __lasx_xvstelm_w(out, dst + dst_stride_3x, 0, 5);
 }
 
 void ff_h264_idct8_dc_addblk_lasx(uint8_t *dst, int16_t *src,
                                   int32_t dst_stride)
 {
     int32_t dc_val;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     __m256i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     __m256i dc;
 
@@ -254,19 +264,31 @@ void ff_h264_idct8_dc_addblk_lasx(uint8_t *dst, int16_t *src,
 
     src[0] = 0;
 
-    LASX_LD_8(dst, dst_stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
-    LASX_UNPCK_L_HU_BU_8(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7,
-                         dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
-    LASX_PCKEV_Q_4(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                   dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, dst0, dst1, dst2, dst3);
+    dst += dst_stride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dst_stride, dst, dst_stride_2x,
+              dst, dst_stride_3x, dst4, dst5, dst6, dst7);
+    dst -= dst_stride_4x;
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst0, dst1, dst2, dst3, dst0, dst1, dst2, dst3);
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, dst4, dst5, dst6, dst7, dst4, dst5, dst6, dst7);
+    DUP4_ARG3(__lasx_xvpermi_q, dst1, dst0, 0x20, dst3, dst2, 0x20, dst5, dst4, 0x20,
+              dst7, dst6, 0x20, dst0, dst1, dst2, dst3);
     dst0 = __lasx_xvadd_h(dst0, dc);
     dst1 = __lasx_xvadd_h(dst1, dc);
     dst2 = __lasx_xvadd_h(dst2, dc);
     dst3 = __lasx_xvadd_h(dst3, dc);
-    LASX_CLIP_H_0_255_4(dst0, dst1, dst2, dst3, dst0, dst1, dst2, dst3);
-    LASX_PCKEV_B_2_128SV(dst1, dst0, dst3, dst2, dst0, dst1);
-    LASX_ST_D_4(dst0, 0, 2, 1, 3, dst, dst_stride);
-    LASX_ST_D_4(dst1, 0, 2, 1, 3, dst + 4 * dst_stride, dst_stride);
+    DUP4_ARG1(__lasx_xvclip255_h, dst0, dst1, dst2, dst3, dst0, dst1, dst2, dst3);
+    DUP2_ARG2(__lasx_xvpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(dst0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(dst0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(dst1, dst, 0, 0);
+    __lasx_xvstelm_d(dst1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(dst1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(dst1, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_h264_idct_add16_lasx(uint8_t *dst,
@@ -424,22 +446,23 @@ void ff_h264_deq_idct_luma_dc_lasx(int16_t *dst, int16_t *src,
     __m256i vres0, vres1, vres2, vres3;
     __m256i de_q_vec = __lasx_xvreplgr2vr_w(de_qval);
 
-    LASX_LD_4(src, 4, src0, src1, src2, src3);
-    LASX_TRANSPOSE4x4_H_128SV(src0, src1, src2, src3, tmp0, tmp1, tmp2, tmp3);
-    BUTTERFLY_4_H(tmp0, tmp2, tmp3, tmp1, vec0, vec3, vec2, vec1);
-    BUTTERFLY_4_H(vec0, vec1, vec2, vec3, hres0, hres3, hres2, hres1);
-    LASX_TRANSPOSE4x4_H_128SV(hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3);
-    BUTTERFLY_4_H(hres0, hres1, hres3, hres2, vec0, vec3, vec2, vec1);
-    BUTTERFLY_4_H(vec0, vec1, vec2, vec3, vres0, vres1, vres2, vres3);
-    LASX_UNPCK_L_W_H_4(vres0, vres1, vres2, vres3, vres0, vres1, vres2, vres3);
-    LASX_PCKEV_Q_2(vres1, vres0, vres3, vres2, vres0, vres1);
+    DUP4_ARG2(__lasx_xvld, src, 0, src, 8, src, 16, src, 24, src0, src1, src2, src3);
+    LASX_TRANSPOSE4x4_H(src0, src1, src2, src3, tmp0, tmp1, tmp2, tmp3);
+    LASX_BUTTERFLY_4_H(tmp0, tmp2, tmp3, tmp1, vec0, vec3, vec2, vec1);
+    LASX_BUTTERFLY_4_H(vec0, vec1, vec2, vec3, hres0, hres3, hres2, hres1);
+    LASX_TRANSPOSE4x4_H(hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3);
+    LASX_BUTTERFLY_4_H(hres0, hres1, hres3, hres2, vec0, vec3, vec2, vec1);
+    LASX_BUTTERFLY_4_H(vec0, vec1, vec2, vec3, vres0, vres1, vres2, vres3);
+    DUP4_ARG1(__lasx_vext2xv_w_h, vres0, vres1, vres2, vres3, vres0, vres1, vres2, vres3);
+    DUP2_ARG3(__lasx_xvpermi_q, vres1, vres0, 0x20, vres3, vres2, 0x20, vres0, vres1);
 
     vres0 = __lasx_xvmul_w(vres0, de_q_vec);
     vres1 = __lasx_xvmul_w(vres1, de_q_vec);
 
     vres0 = __lasx_xvsrari_w(vres0, 8);
     vres1 = __lasx_xvsrari_w(vres1, 8);
-    LASX_PCKEV_H(vres1, vres0, vec0);
+    vec0 = __lasx_xvpickev_h(vres1, vres0);
+    vec0 = __lasx_xvpermi_d(vec0, 0xd8);
     __lasx_xvstelm_h(vec0, dst + 0  * DC_DEST_STRIDE, 0, 0);
     __lasx_xvstelm_h(vec0, dst + 2  * DC_DEST_STRIDE, 0, 1);
     __lasx_xvstelm_h(vec0, dst + 8  * DC_DEST_STRIDE, 0, 2);
diff --git a/libavcodec/loongarch/h264qpel_lasx.c b/libavcodec/loongarch/h264qpel_lasx.c
index 9bf1f60a4f..833cb07027 100644
--- a/libavcodec/loongarch/h264qpel_lasx.c
+++ b/libavcodec/loongarch/h264qpel_lasx.c
@@ -22,7 +22,7 @@
  */
 
 #include "h264qpel_lasx.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "libavutil/attributes.h"
 
 static const uint8_t luma_mask_arr[16 * 6] __attribute__((aligned(0x40))) = {
@@ -43,9 +43,9 @@ static const uint8_t luma_mask_arr[16 * 6] __attribute__((aligned(0x40))) = {
     tmp0_m = __lasx_xvshuf_b(in1, in0, mask0);             \
     out0_m = __lasx_xvhaddw_h_b(tmp0_m, tmp0_m);           \
     tmp0_m = __lasx_xvshuf_b(in1, in0, mask1);             \
-    LASX_DP2ADD_H_B(out0_m, minus5b, tmp0_m, out0_m);      \
+    out0_m = __lasx_xvdp2add_h_b(out0_m, minus5b, tmp0_m); \
     tmp0_m = __lasx_xvshuf_b(in1, in0, mask2);             \
-    LASX_DP2ADD_H_B(out0_m, plus20b, tmp0_m, out0_m);      \
+    out0_m = __lasx_xvdp2add_h_b(out0_m, plus20b, tmp0_m); \
                                                            \
     out0_m;                                                \
 } )
@@ -54,9 +54,9 @@ static const uint8_t luma_mask_arr[16 * 6] __attribute__((aligned(0x40))) = {
 ( {                                                            \
     __m256i out0_m;                                            \
                                                                \
-    LASX_DP2_H_B(in0, coeff0, out0_m);                         \
-    LASX_DP2ADD_H_B(out0_m, in1, coeff1, out0_m);              \
-    LASX_DP2ADD_H_B(out0_m, in2, coeff2, out0_m);              \
+    out0_m = __lasx_xvdp2_h_b(in0, coeff0);                    \
+    DUP2_ARG3(__lasx_xvdp2add_h_b, out0_m, in1, coeff1, out0_m,\
+              in2, coeff2, out0_m, out0_m);                    \
                                                                \
     out0_m;                                                    \
 } )
@@ -70,9 +70,9 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
     const int16_t filt_const1 = 0x1414;
     const int16_t filt_const2 = 0x1fb;
     uint32_t loop_cnt;
-    int32_t stride_x2 = stride << 1;
-    int32_t stride_x3 = stride_x2 + stride;
-    int32_t stride_x4 = stride << 2;
+    int32_t stride_2x = stride << 1;
+    int32_t stride_3x = stride_2x + stride;
+    int32_t stride_4x = stride << 2;
     __m256i tmp0, tmp1;
     __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
     __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
@@ -88,23 +88,27 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
     filt1 = __lasx_xvreplgr2vr_h(filt_const1);
     filt2 = __lasx_xvreplgr2vr_h(filt_const2);
 
-    mask0 = LASX_LD(luma_mask_arr);
-    LASX_LD_2(luma_mask_arr + 32, 32, mask1, mask2);
-    src_vt0 = LASX_LD(src_y);
-    LASX_LD_4(src_y + stride, stride, src_vt1, src_vt2, src_vt3, src_vt4);
-    src_y += stride_x4 + stride;
+    mask0 = __lasx_xvld(luma_mask_arr, 0);
+    DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
+    src_vt0 = __lasx_xvld(src_y, 0);
+    DUP4_ARG2(__lasx_xvld, src_y + stride, 0, src_y + stride_2x, 0, src_y + stride_3x, 0,
+              src_y + stride_4x, 0, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x + stride;
 
-    LASX_XORI_B_128(src_vt0);
-    LASX_XORI_B_4_128(src_vt1, src_vt2, src_vt3, src_vt4);
+    src_vt0 = __lasx_xvxori_b(src_vt0, 128);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128, src_vt4, 128,
+              src_vt1, src_vt2, src_vt3, src_vt4);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LASX_LD_4(src_x, stride, src_hz0, src_hz1, src_hz2, src_hz3);
-        src_x  += stride_x4;
+        DUP4_ARG2(__lasx_xvld, src_x, 0, src_x + stride, 0, src_x + stride_2x, 0,
+                  src_x + stride_3x, 0, src_hz0, src_hz1, src_hz2, src_hz3);
+        src_x  += stride_4x;
         src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
         src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
         src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
         src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
-        LASX_XORI_B_4_128(src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128, src_hz3,
+                  128, src_hz0, src_hz1, src_hz2, src_hz3);
 
         hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
         hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
@@ -113,16 +117,27 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
         hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
         hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
 
-        LASX_LD_4(src_y, stride, src_vt5, src_vt6, src_vt7, src_vt8);
-        src_y += stride_x4;
-
-        LASX_XORI_B_4_128(src_vt5, src_vt6, src_vt7, src_vt8);
-        LASX_ILVL_B_4(src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                      src_vt4, src_vt3, src_vt10_h, src_vt21_h, src_vt32_h,
-                      src_vt43_h);
-        LASX_ILVL_B_4(src_vt5, src_vt4, src_vt6, src_vt5, src_vt7, src_vt6,
-                      src_vt8, src_vt7, src_vt54_h, src_vt65_h, src_vt76_h,
-                      src_vt87_h);
+        DUP4_ARG2(__lasx_xvld, src_y, 0, src_y + stride, 0, src_y + stride_2x, 0,
+                  src_y + stride_3x, 0, src_vt5, src_vt6, src_vt7, src_vt8);
+        src_y += stride_4x;
+
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128, src_vt8,
+                  128, src_vt5, src_vt6, src_vt7, src_vt8);
+
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5, 0x02,
+                  src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02, src_vt0, src_vt1,
+                  src_vt2, src_vt3);
+        src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
+                  src_vt87_h, src_vt3, src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
+                  src_vt87_h, src_vt3, src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1, 0x02,
+                  src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02, src_vt10_h, src_vt21_h,
+                  src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1, 0x13,
+                  src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13, src_vt54_h, src_vt65_h,
+                  src_vt76_h, src_vt87_h);
         vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0,
                                  filt1, filt2);
         vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0,
@@ -134,29 +149,34 @@ void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(const uint8_t *src_x,
         vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
         vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
 
-        LASX_ADDWL_H_B_2_128SV(hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
-        LASX_ADDWH_H_B_2_128SV(hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
         tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
         tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
 
-        LASX_XORI_B_2_128(tmp0, tmp1);
-
-        LASX_LD_4(dst, stride, out0, out1, out2, out3);
-        LASX_ILVL_D_2(out1, out0, out3, out2, out0, out1);
+        DUP2_ARG2(__lasx_xvxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP4_ARG2(__lasx_xvldx, dst, 0, dst, stride, dst, stride_2x, dst, stride_3x,
+                  out0, out1, out2, out3);
+        out0 = __lasx_xvpermi_q(out0, out2, 0x02);
+        out1 = __lasx_xvpermi_q(out1, out3, 0x02);
+        out2 = __lasx_xvilvl_d(out1, out0);
+        out3 = __lasx_xvilvh_d(out1, out0);
+        out0 = __lasx_xvpermi_q(out2, out3, 0x02);
+        out1 = __lasx_xvpermi_q(out2, out3, 0x13);
         tmp0 = __lasx_xvavgr_bu(out0, tmp0);
         tmp1 = __lasx_xvavgr_bu(out1, tmp1);
 
-        *(int64_t*)dst               = __lasx_xvpickve2gr_d(tmp0, 0);
-        *(int64_t*)(dst + stride)    = __lasx_xvpickve2gr_d(tmp0, 1);
-        *(int64_t*)(dst + stride_x2) = __lasx_xvpickve2gr_d(tmp1, 0);
-        *(int64_t*)(dst + stride_x3) = __lasx_xvpickve2gr_d(tmp1, 1);
+        __lasx_xvstelm_d(tmp0, dst, 0, 0);
+        __lasx_xvstelm_d(tmp0, dst + stride, 0, 1);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 0, 0);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 0, 1);
 
-        *(int64_t*)(dst + 8)             = __lasx_xvpickve2gr_d(tmp0, 2);
-        *(int64_t*)(dst + 8 + stride)    = __lasx_xvpickve2gr_d(tmp0, 3);
-        *(int64_t*)(dst + 8 + stride_x2) = __lasx_xvpickve2gr_d(tmp1, 2);
-        *(int64_t*)(dst + 8 + stride_x3) = __lasx_xvpickve2gr_d(tmp1, 3);
+        __lasx_xvstelm_d(tmp0, dst, 8, 2);
+        __lasx_xvstelm_d(tmp0, dst + stride, 8, 3);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 8, 2);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 8, 3);
 
-        dst    += stride_x4;
+        dst    += stride_4x;
         src_vt0 = src_vt4;
         src_vt1 = src_vt5;
         src_vt2 = src_vt6;
@@ -173,9 +193,9 @@ avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *
     const int16_t filt_const1 = 0x1414;
     const int16_t filt_const2 = 0x1fb;
     uint32_t loop_cnt;
-    int32_t stride_x2 = stride << 1;
-    int32_t stride_x3 = stride_x2 + stride;
-    int32_t stride_x4 = stride << 2;
+    int32_t stride_2x = stride << 1;
+    int32_t stride_3x = stride_2x + stride;
+    int32_t stride_4x = stride << 2;
     __m256i tmp0, tmp1;
     __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
     __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
@@ -191,23 +211,27 @@ avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *
     filt1 = __lasx_xvreplgr2vr_h(filt_const1);
     filt2 = __lasx_xvreplgr2vr_h(filt_const2);
 
-    mask0 = LASX_LD(luma_mask_arr);
-    LASX_LD_2(luma_mask_arr + 32, 32, mask1, mask2);
-    src_vt0 = LASX_LD(src_y);
-    LASX_LD_4(src_y + stride, stride, src_vt1, src_vt2, src_vt3, src_vt4);
-    src_y += stride_x4 + stride;
+    mask0 = __lasx_xvld(luma_mask_arr, 0);
+    DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
+    src_vt0 = __lasx_xvld(src_y, 0);
+    DUP4_ARG2(__lasx_xvld, src_y + stride, 0, src_y + stride_2x, 0, src_y + stride_3x,
+              0, src_y + stride_4x, 0, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x + stride;
 
-    LASX_XORI_B_128(src_vt0);
-    LASX_XORI_B_4_128(src_vt1, src_vt2, src_vt3, src_vt4);
+    src_vt0 = __lasx_xvxori_b(src_vt0, 128);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128, src_vt4, 128,
+              src_vt1, src_vt2, src_vt3, src_vt4);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LASX_LD_4(src_x, stride, src_hz0, src_hz1, src_hz2, src_hz3);
-        src_x  += stride_x4;
+        DUP4_ARG2(__lasx_xvld, src_x, 0, src_x + stride, 0, src_x + stride_2x, 0,
+                  src_x + stride_3x, 0, src_hz0, src_hz1, src_hz2, src_hz3);
+        src_x  += stride_4x;
         src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
         src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
         src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
         src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
-        LASX_XORI_B_4_128(src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128, src_hz3,
+                  128, src_hz0, src_hz1, src_hz2, src_hz3);
 
         hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
         hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
@@ -216,44 +240,51 @@ avc_luma_hv_qrt_16x16_lasx(const uint8_t *src_x, const uint8_t *src_y, uint8_t *
         hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
         hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
 
-        LASX_LD_4(src_y, stride, src_vt5, src_vt6, src_vt7, src_vt8);
-        src_y += stride_x4;
-
-        LASX_XORI_B_4_128(src_vt5, src_vt6, src_vt7, src_vt8);
-        LASX_ILVL_B_4(src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
-                      src_vt4, src_vt3, src_vt10_h, src_vt21_h, src_vt32_h,
-                      src_vt43_h);
-        LASX_ILVL_B_4(src_vt5, src_vt4, src_vt6, src_vt5, src_vt7, src_vt6,
-                      src_vt8, src_vt7, src_vt54_h, src_vt65_h, src_vt76_h,
-                      src_vt87_h);
-        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0,
-                                 filt1, filt2);
-        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0,
-                                 filt1, filt2);
-        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h, filt0,
-                                 filt1, filt2);
-        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h, filt0,
-                                 filt1, filt2);
+        DUP4_ARG2(__lasx_xvld, src_y, 0, src_y + stride, 0, src_y + stride_2x, 0,
+                  src_y + stride_3x, 0, src_vt5, src_vt6, src_vt7, src_vt8);
+        src_y += stride_4x;
+
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128, src_vt8,
+                  128, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5, 0x02,
+                  src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02, src_vt0, src_vt1,
+                  src_vt2, src_vt3);
+        src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
+                  src_vt87_h, src_vt3, src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1, src_vt3, src_vt2,
+                  src_vt87_h, src_vt3, src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1, 0x02,
+                  src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02, src_vt10_h, src_vt21_h,
+                  src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1, 0x13,
+                  src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13, src_vt54_h, src_vt65_h,
+                  src_vt76_h, src_vt87_h);
+
+        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0, filt1, filt2);
+        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0, filt1, filt2);
+        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h, filt0, filt1, filt2);
+        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h, filt0, filt1, filt2);
         vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
         vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
 
-        LASX_ADDWL_H_B_2_128SV(hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
-        LASX_ADDWH_H_B_2_128SV(hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2, out1, out3);
         tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
         tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
 
-        LASX_XORI_B_2_128(tmp0, tmp1);
-        *(int64_t*)dst               = __lasx_xvpickve2gr_d(tmp0, 0);
-        *(int64_t*)(dst + stride)    = __lasx_xvpickve2gr_d(tmp0, 1);
-        *(int64_t*)(dst + stride_x2) = __lasx_xvpickve2gr_d(tmp1, 0);
-        *(int64_t*)(dst + stride_x3) = __lasx_xvpickve2gr_d(tmp1, 1);
+        DUP2_ARG2(__lasx_xvxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lasx_xvstelm_d(tmp0, dst, 0, 0);
+        __lasx_xvstelm_d(tmp0, dst + stride, 0, 1);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 0, 0);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 0, 1);
 
-        *(int64_t*)(dst + 8)             = __lasx_xvpickve2gr_d(tmp0, 2);
-        *(int64_t*)(dst + 8 + stride)    = __lasx_xvpickve2gr_d(tmp0, 3);
-        *(int64_t*)(dst + 8 + stride_x2) = __lasx_xvpickve2gr_d(tmp1, 2);
-        *(int64_t*)(dst + 8 + stride_x3) = __lasx_xvpickve2gr_d(tmp1, 3);
+        __lasx_xvstelm_d(tmp0, dst, 8, 2);
+        __lasx_xvstelm_d(tmp0, dst + stride, 8, 3);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 8, 2);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 8, 3);
 
-        dst    += stride_x4;
+        dst    += stride_4x;
         src_vt0 = src_vt4;
         src_vt1 = src_vt5;
         src_vt2 = src_vt6;
@@ -1028,25 +1059,25 @@ avg_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
     );
 }
 
-#define QPEL8_H_LOWPASS(out_v)                                          \
-    src00 = LASX_LD(src - 2);                                           \
-    src += srcStride;                                                   \
-    src10 = LASX_LD(src - 2);                                           \
-    src += srcStride;                                                   \
-    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                       \
-    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);              \
-    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);              \
-    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);              \
-    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);              \
-    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);              \
-    LASX_ADDWL_H_BU_2_128SV(src02, src03, src01, src04, src02, src01);  \
-    LASX_ADDWL_H_BU_128SV(src00, src05, src00);                         \
-    src02 = __lasx_xvmul_h(src02, h_20);                                \
-    src01 = __lasx_xvmul_h(src01, h_5);                                 \
-    src02 = __lasx_xvssub_h(src02, src01);                              \
-    src02 = __lasx_xvsadd_h(src02, src00);                              \
-    src02 = __lasx_xvsadd_h(src02, h_16);                               \
-    out_v = __lasx_xvssrani_bu_h(src02, src02, 5);                      \
+#define QPEL8_H_LOWPASS(out_v)                                               \
+    src00 = __lasx_xvld(src, - 2);                                           \
+    src += srcStride;                                                        \
+    src10 = __lasx_xvld(src, - 2);                                           \
+    src += srcStride;                                                        \
+    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                            \
+    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);                   \
+    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);                   \
+    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);                   \
+    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);                   \
+    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);                   \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, src02, src03, src01, src04, src02, src01);\
+    src00 = __lasx_xvaddwl_h_bu(src00, src05);                               \
+    src02 = __lasx_xvmul_h(src02, h_20);                                     \
+    src01 = __lasx_xvmul_h(src01, h_5);                                      \
+    src02 = __lasx_xvssub_h(src02, src01);                                   \
+    src02 = __lasx_xvsadd_h(src02, src00);                                   \
+    src02 = __lasx_xvsadd_h(src02, h_16);                                    \
+    out_v = __lasx_xvssrani_bu_h(src02, src02, 5);                           \
 
 static av_always_inline void
 put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
@@ -1074,13 +1105,17 @@ put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     QPEL8_H_LOWPASS(out1)
     QPEL8_H_LOWPASS(out2)
     QPEL8_H_LOWPASS(out3)
-    LASX_ST_D_2(out0, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + dstStride, 0, 2);
     dst += dstStride_2x;
-    LASX_ST_D_2(out1, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + dstStride, 0, 2);
     dst += dstStride_2x;
-    LASX_ST_D_2(out2, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(out2, dst, 0, 0);
+    __lasx_xvstelm_d(out2, dst + dstStride, 0, 2);
     dst += dstStride_2x;
-    LASX_ST_D_2(out3, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(out3, dst, 0, 0);
+    __lasx_xvstelm_d(out3, dst + dstStride, 0, 2);
 }
 
 #define QPEL8_V_LOWPASS(src0, src1, src2, src3, src4, src5, src6,       \
@@ -1092,8 +1127,8 @@ put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     tmp3 = __lasx_xvpermi_q(src3, src4, 0x02);                          \
     tmp4 = __lasx_xvpermi_q(src4, src5, 0x02);                          \
     tmp5 = __lasx_xvpermi_q(src5, src6, 0x02);                          \
-    LASX_ADDWL_H_BU_2_128SV(tmp2, tmp3, tmp1, tmp4, tmp2, tmp1);  \
-    LASX_ADDWL_H_BU_128SV(tmp0, tmp5, tmp0);                         \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, tmp2, tmp3, tmp1, tmp4, tmp2, tmp1); \
+    tmp0 = __lasx_xvaddwl_h_bu(tmp0, tmp5);                             \
     tmp2 = __lasx_xvmul_h(tmp2, h_20);                                  \
     tmp1 = __lasx_xvmul_h(tmp1, h_5);                                   \
     tmp2 = __lasx_xvssub_h(tmp2, tmp1);                                 \
@@ -1107,6 +1142,9 @@ put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
                               int srcStride)
 {
     int srcStride_2x = srcStride << 1;
+    int dstStride_2x = dstStride << 1;
+    int srcStride_4x = srcStride << 2;
+    int srcStride_3x = srcStride_2x + srcStride;
     __m256i src00, src01, src02, src03, src04, src05, src06;
     __m256i src07, src08, src09, src10, src11, src12;
     __m256i tmp00, tmp01, tmp02, tmp03, tmp04, tmp05;
@@ -1120,29 +1158,36 @@ put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     h_5  = __lasx_xvreplve0_h(h_5);
     h_16 = __lasx_xvreplve0_h(h_16);
 
-    LASX_LD_2(src - srcStride_2x, srcStride, src00, src01);
-    LASX_LD_8(src, srcStride, src02, src03, src04, src05,
-              src06, src07, src08, src09);
-    src += srcStride << 3;
-    LASX_LD_2(src, srcStride, src10, src11);
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0, src00, src01);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
+              src + srcStride_3x, 0, src02, src03, src04, src05);
+    src += srcStride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
+              src + srcStride_3x, 0, src06, src07, src08, src09);
+    src += srcStride_4x;
+    DUP2_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src10, src11);
     src += srcStride_2x;
-    src12 = LASX_LD(src);
+    src12 = __lasx_xvld(src, 0);
 
     QPEL8_V_LOWPASS(src00, src01, src02, src03, src04, src05, src06,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
-    LASX_ST_D_2(tmp02, 0, 2, dst, dstStride);
-    dst += dstStride << 1;
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src02, src03, src04, src05, src06, src07, src08,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
-    LASX_ST_D_2(tmp02, 0, 2, dst, dstStride);
-    dst += dstStride << 1;
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src04, src05, src06, src07, src08, src09, src10,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
-    LASX_ST_D_2(tmp02, 0, 2, dst, dstStride);
-    dst += dstStride << 1;
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
     QPEL8_V_LOWPASS(src06, src07, src08, src09, src10, src11, src12,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
-    LASX_ST_D_2(tmp02, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
 }
 
 static av_always_inline void
@@ -1150,6 +1195,11 @@ avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
                               int srcStride)
 {
     int srcStride_2x = srcStride << 1;
+    int srcStride_4x = srcStride << 2;
+    int dstStride_2x = dstStride << 1;
+    int dstStride_4x = dstStride << 2;
+    int srcStride_3x = srcStride_2x + srcStride;
+    int dstStride_3x = dstStride_2x + dstStride;
     __m256i src00, src01, src02, src03, src04, src05, src06;
     __m256i src07, src08, src09, src10, src11, src12;
     __m256i tmp00, tmp01, tmp02, tmp03, tmp04, tmp05, tmp06, tmp07, tmp08, tmp09;
@@ -1163,16 +1213,24 @@ avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     h_5  = __lasx_xvreplve0_h(h_5);
     h_16 = __lasx_xvreplve0_h(h_16);
 
-    LASX_LD_2(src - srcStride_2x, srcStride, src00, src01);
-    LASX_LD_8(src, srcStride, src02, src03, src04, src05,
-              src06, src07, src08, src09);
-    src += srcStride << 3;
-    LASX_LD_2(src, srcStride, src10, src11);
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0, src00, src01);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
+              src + srcStride_3x, 0, src02, src03, src04, src05);
+    src += srcStride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src + srcStride_2x, 0,
+              src + srcStride_3x, 0, src06, src07, src08, src09);
+    src += srcStride_4x;
+    DUP2_ARG2(__lasx_xvld, src, 0, src + srcStride, 0, src10, src11);
     src += srcStride_2x;
-    src12 = LASX_LD(src);
+    src12 = __lasx_xvld(src, 0);
 
-    LASX_LD_8(dst, dstStride, tmp06, tmp07, tmp02, tmp03,
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
+              tmp06, tmp07, tmp02, tmp03);
+    dst += dstStride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
               tmp04, tmp05, tmp00, tmp01);
+    dst -= dstStride_4x;
+
     tmp06 = __lasx_xvpermi_q(tmp06, tmp07, 0x02);
     tmp07 = __lasx_xvpermi_q(tmp02, tmp03, 0x02);
     tmp08 = __lasx_xvpermi_q(tmp04, tmp05, 0x02);
@@ -1181,67 +1239,71 @@ avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     QPEL8_V_LOWPASS(src00, src01, src02, src03, src04, src05, src06,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp06 = __lasx_xvavgr_bu(tmp06, tmp02);
-    LASX_ST_D_2(tmp06, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp06, dst, 0, 0);
+    __lasx_xvstelm_d(tmp06, dst + dstStride, 0, 2);
     dst += dstStride << 1;
     QPEL8_V_LOWPASS(src02, src03, src04, src05, src06, src07, src08,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp07 = __lasx_xvavgr_bu(tmp07, tmp02);
-    LASX_ST_D_2(tmp07, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp07, dst, 0, 0);
+    __lasx_xvstelm_d(tmp07, dst + dstStride, 0, 2);
     dst += dstStride << 1;
     QPEL8_V_LOWPASS(src04, src05, src06, src07, src08, src09, src10,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp08 = __lasx_xvavgr_bu(tmp08, tmp02);
-    LASX_ST_D_2(tmp08, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp08, dst, 0, 0);
+    __lasx_xvstelm_d(tmp08, dst + dstStride, 0, 2);
     dst += dstStride << 1;
     QPEL8_V_LOWPASS(src06, src07, src08, src09, src10, src11, src12,
                     tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
     tmp09 = __lasx_xvavgr_bu(tmp09, tmp02);
-    LASX_ST_D_2(tmp09, 0, 2, dst, dstStride);
-}
-
-#define QPEL8_HV_LOWPASS_H(tmp)                                         \
-{                                                                       \
-    src00 = LASX_LD(src - 2);                                           \
-    src += srcStride;                                                   \
-    src10 = LASX_LD(src - 2);                                           \
-    src += srcStride;                                                   \
-    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                       \
-    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);              \
-    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);              \
-    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);              \
-    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);              \
-    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);              \
-    LASX_ADDWL_H_BU_2_128SV(src02, src03, src01, src04, src02, src01);  \
-    LASX_ADDWL_H_BU_128SV(src00, src05, src00);                         \
-    src02 = __lasx_xvmul_h(src02, h_20);                                \
-    src01 = __lasx_xvmul_h(src01, h_5);                                 \
-    src02 = __lasx_xvssub_h(src02, src01);                              \
-    tmp  = __lasx_xvsadd_h(src02, src00);                               \
-}
-
-#define QPEL8_HV_LOWPASS_V(src0, src1, src2, src3,                     \
-                           src4, src5, temp0, temp1,                   \
-                           temp2, temp3, temp4, temp5,                 \
-                           out)                                        \
-{                                                                      \
-    LASX_ADDWL_W_H_2_128SV(src2, src3, src1, src4, temp0, temp2);      \
-    LASX_ADDWH_W_H_2_128SV(src2, src3, src1, src4, temp1, temp3);      \
-    LASX_ADDWL_W_H_128SV(src0, src5, temp4);                           \
-    LASX_ADDWH_W_H_128SV(src0, src5, temp5);                           \
-    temp0 = __lasx_xvmul_w(temp0, w_20);                               \
-    temp1 = __lasx_xvmul_w(temp1, w_20);                               \
-    temp2 = __lasx_xvmul_w(temp2, w_5);                                \
-    temp3 = __lasx_xvmul_w(temp3, w_5);                                \
-    temp0 = __lasx_xvssub_w(temp0, temp2);                             \
-    temp1 = __lasx_xvssub_w(temp1, temp3);                             \
-    temp0 = __lasx_xvsadd_w(temp0, temp4);                             \
-    temp1 = __lasx_xvsadd_w(temp1, temp5);                             \
-    temp0 = __lasx_xvsadd_w(temp0, w_512);                             \
-    temp1 = __lasx_xvsadd_w(temp1, w_512);                             \
-    temp0 = __lasx_xvssrani_hu_w(temp0, temp0, 10);                    \
-    temp1 = __lasx_xvssrani_hu_w(temp1, temp1, 10);                    \
-    temp0 = __lasx_xvpackev_d(temp1, temp0);                           \
-    out   = __lasx_xvssrani_bu_h(temp0, temp0, 0);                     \
+    __lasx_xvstelm_d(tmp09, dst, 0, 0);
+    __lasx_xvstelm_d(tmp09, dst + dstStride, 0, 2);
+}
+
+#define QPEL8_HV_LOWPASS_H(tmp)                                              \
+{                                                                            \
+    src00 = __lasx_xvld(src, -2);                                            \
+    src += srcStride;                                                        \
+    src10 = __lasx_xvld(src, -2);                                            \
+    src += srcStride;                                                        \
+    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                            \
+    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);                   \
+    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);                   \
+    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);                   \
+    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);                   \
+    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);                   \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, src02, src03, src01, src04, src02, src01);\
+    src00 = __lasx_xvaddwl_h_bu(src00, src05);                               \
+    src02 = __lasx_xvmul_h(src02, h_20);                                     \
+    src01 = __lasx_xvmul_h(src01, h_5);                                      \
+    src02 = __lasx_xvssub_h(src02, src01);                                   \
+    tmp  = __lasx_xvsadd_h(src02, src00);                                    \
+}
+
+#define QPEL8_HV_LOWPASS_V(src0, src1, src2, src3,                       \
+                           src4, src5, temp0, temp1,                     \
+                           temp2, temp3, temp4, temp5,                   \
+                           out)                                          \
+{                                                                        \
+    DUP2_ARG2(__lasx_xvaddwl_w_h, src2, src3, src1, src4, temp0, temp2); \
+    DUP2_ARG2(__lasx_xvaddwh_w_h, src2, src3, src1, src4, temp1, temp3); \
+    temp4 = __lasx_xvaddwl_w_h(src0, src5);                              \
+    temp5 = __lasx_xvaddwh_w_h(src0, src5);                              \
+    temp0 = __lasx_xvmul_w(temp0, w_20);                                 \
+    temp1 = __lasx_xvmul_w(temp1, w_20);                                 \
+    temp2 = __lasx_xvmul_w(temp2, w_5);                                  \
+    temp3 = __lasx_xvmul_w(temp3, w_5);                                  \
+    temp0 = __lasx_xvssub_w(temp0, temp2);                               \
+    temp1 = __lasx_xvssub_w(temp1, temp3);                               \
+    temp0 = __lasx_xvsadd_w(temp0, temp4);                               \
+    temp1 = __lasx_xvsadd_w(temp1, temp5);                               \
+    temp0 = __lasx_xvsadd_w(temp0, w_512);                               \
+    temp1 = __lasx_xvsadd_w(temp1, w_512);                               \
+    temp0 = __lasx_xvssrani_hu_w(temp0, temp0, 10);                      \
+    temp1 = __lasx_xvssrani_hu_w(temp1, temp1, 10);                      \
+    temp0 = __lasx_xvpackev_d(temp1, temp0);                             \
+    out   = __lasx_xvssrani_bu_h(temp0, temp0, 0);                       \
 }
 
 static av_always_inline void
@@ -1291,13 +1353,17 @@ put_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
                        src02, src03, src04, src05, tmp4)
     QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
                        src02, src03, src04, src05, tmp6)
-    LASX_ST_D_2(tmp0, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp0, dst, 0, 0);
+    __lasx_xvstelm_d(tmp0, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp2, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp2, dst, 0, 0);
+    __lasx_xvstelm_d(tmp2, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp4, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp4, dst, 0, 0);
+    __lasx_xvstelm_d(tmp4, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp6, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp6, dst, 0, 0);
+    __lasx_xvstelm_d(tmp6, dst + dstStride, 0, 2);
 }
 
 static av_always_inline void
@@ -1306,7 +1372,7 @@ avg_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
 {
     int dstStride_2x = dstStride << 1;
     int dstStride_4x = dstStride << 2;
-    int dstStride_6x = dstStride_2x + dstStride_4x;
+    int dstStride_3x = dstStride_2x + dstStride;
     __m256i src00, src01, src02, src03, src04, src05, src10;
     __m256i dst00, dst01, dst0, dst1, dst2, dst3;
     __m256i out0, out1, out2, out3;
@@ -1329,10 +1395,12 @@ avg_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     QPEL8_H_LOWPASS(out1)
     QPEL8_H_LOWPASS(out2)
     QPEL8_H_LOWPASS(out3)
-    LASX_LD_2(dst, dstStride, src00, src01);
-    LASX_LD_2(dst + dstStride_2x, dstStride, src02, src03);
-    LASX_LD_2(dst + dstStride_4x, dstStride, src04, src05);
-    LASX_LD_2(dst + dstStride_6x, dstStride, dst00, dst01);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
+              src00, src01, src02, src03);
+    dst += dstStride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
+              src04, src05, dst00, dst01);
+    dst -= dstStride_4x;
     dst0 = __lasx_xvpermi_q(src00, src01, 0x02);
     dst1 = __lasx_xvpermi_q(src02, src03, 0x02);
     dst2 = __lasx_xvpermi_q(src04, src05, 0x02);
@@ -1341,10 +1409,15 @@ avg_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
     dst1 = __lasx_xvavgr_bu(dst1, out1);
     dst2 = __lasx_xvavgr_bu(dst2, out2);
     dst3 = __lasx_xvavgr_bu(dst3, out3);
-    LASX_ST_D_2(dst0, 0, 2, dst, dstStride);
-    LASX_ST_D_2(dst1, 0, 2, dst + dstStride_2x, dstStride);
-    LASX_ST_D_2(dst2, 0, 2, dst + dstStride_4x, dstStride);
-    LASX_ST_D_2(dst3, 0, 2, dst + dstStride_6x, dstStride);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + dstStride, 0, 2);
+    __lasx_xvstelm_d(dst1, dst + dstStride_2x, 0, 0);
+    __lasx_xvstelm_d(dst1, dst + dstStride_3x, 0, 2);
+    dst += dstStride_4x;
+    __lasx_xvstelm_d(dst2, dst, 0, 0);
+    __lasx_xvstelm_d(dst2, dst + dstStride, 0, 2);
+    __lasx_xvstelm_d(dst3, dst + dstStride_2x, 0, 0);
+    __lasx_xvstelm_d(dst3, dst + dstStride_3x, 0, 2);
 }
 
 static av_always_inline void
@@ -1364,6 +1437,9 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
     __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
     __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
+    ptrdiff_t dstStride_2x = dstStride << 1;
+    ptrdiff_t dstStride_4x = dstStride << 2;
+    ptrdiff_t dstStride_3x = dstStride_2x + dstStride;
 
     h_20 = __lasx_xvreplve0_h(w_20);
     h_5  = __lasx_xvreplve0_h(w_5);
@@ -1395,7 +1471,12 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
                        src02, src03, src04, src05, tmp6)
 
-    LASX_LD_8(dst, dstStride, src00, src01, src02, src03, src04, src05, tmp8, tmp9);
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
+              src00, src01, src02, src03);
+    dst += dstStride_4x;
+    DUP4_ARG2(__lasx_xvldx, dst, 0, dst, dstStride, dst, dstStride_2x, dst, dstStride_3x,
+              src04, src05, tmp8, tmp9);
+    dst -= dstStride_4x;
     tmp1 = __lasx_xvpermi_q(src00, src01, 0x02);
     tmp3 = __lasx_xvpermi_q(src02, src03, 0x02);
     tmp5 = __lasx_xvpermi_q(src04, src05, 0x02);
@@ -1404,13 +1485,17 @@ avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
     tmp2 = __lasx_xvavgr_bu(tmp2, tmp3);
     tmp4 = __lasx_xvavgr_bu(tmp4, tmp5);
     tmp6 = __lasx_xvavgr_bu(tmp6, tmp7);
-    LASX_ST_D_2(tmp0, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp0, dst, 0, 0);
+    __lasx_xvstelm_d(tmp0, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp2, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp2, dst, 0, 0);
+    __lasx_xvstelm_d(tmp2, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp4, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp4, dst, 0, 0);
+    __lasx_xvstelm_d(tmp4, dst + dstStride, 0, 2);
     dst += dstStride << 1;
-    LASX_ST_D_2(tmp6, 0, 2, dst, dstStride);
+    __lasx_xvstelm_d(tmp6, dst, 0, 0);
+    __lasx_xvstelm_d(tmp6, dst + dstStride, 0, 2);
 }
 
 static av_always_inline void
diff --git a/libavcodec/loongarch/hevc_idct_lsx.c b/libavcodec/loongarch/hevc_idct_lsx.c
index 2d166e5790..c73193d1f9 100644
--- a/libavcodec/loongarch/hevc_idct_lsx.c
+++ b/libavcodec/loongarch/hevc_idct_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 
 static const int16_t gt8x8_cnst[16] __attribute__ ((aligned (64))) = {
@@ -71,12 +71,12 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     __m128i cnst83 = __lsx_vldi(0x0853);                      \
     __m128i cnst36 = __lsx_vldi(0x0824);                      \
                                                               \
-    vec0 = __lsx_dp2_w_h(in_r0, cnst64);                      \
-    vec1 = __lsx_dp2_w_h(in_l0, cnst83);                      \
-    vec2 = __lsx_dp2_w_h(in_r1, cnst64);                      \
-    vec3 = __lsx_dp2_w_h(in_l1, cnst36);                      \
-    vec4 = __lsx_dp2_w_h(in_l0, cnst36);                      \
-    vec5 = __lsx_dp2_w_h(in_l1, cnst83);                      \
+    vec0 = __lsx_vdp2_w_h(in_r0, cnst64);                     \
+    vec1 = __lsx_vdp2_w_h(in_l0, cnst83);                     \
+    vec2 = __lsx_vdp2_w_h(in_r1, cnst64);                     \
+    vec3 = __lsx_vdp2_w_h(in_l1, cnst36);                     \
+    vec4 = __lsx_vdp2_w_h(in_l0, cnst36);                     \
+    vec5 = __lsx_vdp2_w_h(in_l1, cnst83);                     \
                                                               \
     sum0 = __lsx_vadd_w(vec0, vec2);                          \
     sum1 = __lsx_vsub_w(vec0, vec2);                          \
@@ -107,27 +107,27 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     __m128i sum0_r, sum1_r, sum2_r, sum3_r;                              \
     __m128i sum0_l, sum1_l, sum2_l, sum3_l;                              \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in4, in0, in6, in2, in5, in1, in3, in7, \
-                  src0_r, src1_r, src2_r, src3_r);                       \
-    LSX_DUP4_ARG2(__lsx_vilvh_h, in4, in0, in6, in2, in5, in1, in3, in7, \
-                  src0_l, src1_l, src2_l, src3_l);                       \
+    DUP4_ARG2(__lsx_vilvl_h, in4, in0, in6, in2, in5, in1, in3, in7,     \
+              src0_r, src1_r, src2_r, src3_r);                           \
+    DUP4_ARG2(__lsx_vilvh_h, in4, in0, in6, in2, in5, in1, in3, in7,     \
+              src0_l, src1_l, src2_l, src3_l);                           \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 8,      \
-                  filter, 12, filter0, filter1, filter2, filter3);       \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src0_r, filter0, src0_l, filter0,       \
-                  src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,   \
-                  temp1_r, temp1_l);                                     \
+    DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 8,          \
+              filter, 12, filter0, filter1, filter2, filter3);           \
+    DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+              src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,       \
+              temp1_r, temp1_l);                                         \
                                                                          \
-    BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,    \
-                  sum1_l, sum1_r);                                       \
+    LSX_BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,\
+                      sum1_l, sum1_r);                                   \
     sum2_r = sum1_r;                                                     \
     sum2_l = sum1_l;                                                     \
     sum3_r = sum0_r;                                                     \
     sum3_l = sum0_l;                                                     \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src2_r, filter2, src2_l, filter2,       \
-                  src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,   \
-                  temp3_r, temp3_l);                                     \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter2, src2_l, filter2,          \
+              src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,       \
+              temp3_r, temp3_l);                                         \
     temp2_r = __lsx_vadd_w(temp2_r, temp3_r);                            \
     temp2_l = __lsx_vadd_w(temp2_l, temp3_l);                            \
     sum0_r  = __lsx_vadd_w(sum0_r, temp2_r);                             \
@@ -138,9 +138,9 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     in0 = __lsx_vssrarni_h_w(sum0_l, sum0_r, shift);                     \
     in7 = __lsx_vssrarni_h_w(sum3_l, sum3_r, shift);                     \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src2_r, filter3, src2_l, filter3,       \
-                  src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,   \
-                  temp5_r, temp5_l);                                     \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter3, src2_l, filter3,          \
+              src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,       \
+              temp5_r, temp5_l);                                         \
     temp4_r = __lsx_vsub_w(temp4_r, temp5_r);                            \
     temp4_l = __lsx_vsub_w(temp4_l, temp5_l);                            \
     sum1_r  = __lsx_vadd_w(sum1_r, temp4_r);                             \
@@ -151,22 +151,22 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     in3 = __lsx_vssrarni_h_w(sum1_l, sum1_r, shift);                     \
     in4 = __lsx_vssrarni_h_w(sum2_l, sum2_r, shift);                     \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_vldrepl_w, filter, 16, filter, 20, filter, 24,   \
-                  filter, 28, filter0, filter1, filter2, filter3);       \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src0_r, filter0, src0_l, filter0,       \
-                  src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,   \
-                  temp1_r, temp1_l);                                     \
+    DUP4_ARG2(__lsx_vldrepl_w, filter, 16, filter, 20, filter, 24,       \
+              filter, 28, filter0, filter1, filter2, filter3);           \
+    DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+              src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,       \
+              temp1_r, temp1_l);                                         \
                                                                          \
-    BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,    \
-                  sum1_l, sum1_r);                                       \
+    LSX_BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,\
+                      sum1_l, sum1_r);                                   \
     sum2_r = sum1_r;                                                     \
     sum2_l = sum1_l;                                                     \
     sum3_r = sum0_r;                                                     \
     sum3_l = sum0_l;                                                     \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src2_r, filter2, src2_l, filter2,       \
-                  src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,   \
-                  temp3_r, temp3_l);                                     \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter2, src2_l, filter2,          \
+              src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,       \
+              temp3_r, temp3_l);                                         \
     temp2_r = __lsx_vadd_w(temp2_r, temp3_r);                            \
     temp2_l = __lsx_vadd_w(temp2_l, temp3_l);                            \
     sum0_r  = __lsx_vadd_w(sum0_r, temp2_r);                             \
@@ -177,9 +177,9 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
     in1 = __lsx_vssrarni_h_w(sum0_l, sum0_r, shift);                     \
     in6 = __lsx_vssrarni_h_w(sum3_l, sum3_r, shift);                     \
                                                                          \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, src2_r, filter3, src2_l, filter3,       \
-                  src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,   \
-                  temp5_r, temp5_l);                                     \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter3, src2_l, filter3,          \
+              src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,       \
+              temp5_r, temp5_l);                                         \
     temp4_r = __lsx_vsub_w(temp4_r, temp5_r);                            \
     temp4_l = __lsx_vsub_w(temp4_l, temp5_l);                            \
     sum1_r  = __lsx_vsub_w(sum1_r, temp4_r);                             \
@@ -209,31 +209,31 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
                                                                              \
     for (j = 0; j < 4; j++)                                                  \
     {                                                                        \
-        LSX_DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 16,     \
+        DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 16,         \
                   filter, 20, filter0, filter1, filter2, filter3);           \
-        LSX_DUP4_ARG2(__lsx_dp2_w_h, src0_r, filter0, src0_l, filter0,       \
-                      src4_r, filter2, src4_l, filter2,  sum0_r, sum0_l,     \
-                      sum2_r, sum2_l);                                       \
-        LSX_DUP2_ARG2(__lsx_dp2_w_h, src7_r, filter2, src7_l, filter2,       \
-                      sum3_r, sum3_l);                                       \
-        LSX_DUP4_ARG3(__lsx_dp2add_w_h, sum0_r, src1_r, filter1, sum0_l,     \
-                      src1_l, filter1, sum2_r, src5_r, filter3, sum2_l,      \
-                      src5_l, filter3, sum0_r, sum0_l, sum2_r, sum2_l);      \
-        LSX_DUP2_ARG3(__lsx_dp2add_w_h, sum3_r, src6_r, filter3, sum3_l,     \
-                      src6_l, filter3, sum3_r, sum3_l);                      \
+        DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+                  src4_r, filter2, src4_l, filter2,  sum0_r, sum0_l,         \
+                  sum2_r, sum2_l);                                           \
+        DUP2_ARG2(__lsx_vdp2_w_h, src7_r, filter2, src7_l, filter2,          \
+                  sum3_r, sum3_l);                                           \
+        DUP4_ARG3(__lsx_vdp2add_w_h, sum0_r, src1_r, filter1, sum0_l,        \
+                  src1_l, filter1, sum2_r, src5_r, filter3, sum2_l,          \
+                  src5_l, filter3, sum0_r, sum0_l, sum2_r, sum2_l);          \
+        DUP2_ARG3(__lsx_vdp2add_w_h, sum3_r, src6_r, filter3, sum3_l,        \
+                  src6_l, filter3, sum3_r, sum3_l);                          \
                                                                              \
         sum1_r = sum0_r;                                                     \
         sum1_l = sum0_l;                                                     \
                                                                              \
-        LSX_DUP4_ARG2(__lsx_vldrepl_w, filter, 8, filter, 12, filter, 24,    \
+        DUP4_ARG2(__lsx_vldrepl_w, filter, 8, filter, 12, filter, 24,        \
                   filter, 28, filter0, filter1, filter2, filter3);           \
         filter += 16;                                                        \
-        LSX_DUP2_ARG2(__lsx_dp2_w_h, src2_r, filter0, src2_l, filter0,       \
-                      temp0_r, temp0_l);                                     \
-        LSX_DUP2_ARG3(__lsx_dp2add_w_h, sum2_r, src6_r, filter2, sum2_l,     \
-                      src6_l, filter2, sum2_r, sum2_l);                      \
-        LSX_DUP2_ARG2(__lsx_dp2_w_h, src5_r, filter2, src5_l, filter2,       \
-                      temp1_r, temp1_l);                                     \
+        DUP2_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,          \
+                  temp0_r, temp0_l);                                         \
+        DUP2_ARG3(__lsx_vdp2add_w_h, sum2_r, src6_r, filter2, sum2_l,        \
+                  src6_l, filter2, sum2_r, sum2_l);                          \
+        DUP2_ARG2(__lsx_vdp2_w_h, src5_r, filter2, src5_l, filter2,          \
+                  temp1_r, temp1_l);                                         \
                                                                              \
         sum0_r = __lsx_vadd_w(sum0_r, temp0_r);                              \
         sum0_l = __lsx_vadd_w(sum0_l, temp0_l);                              \
@@ -242,26 +242,26 @@ static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
         sum3_r = __lsx_vsub_w(temp1_r, sum3_r);                              \
         sum3_l = __lsx_vsub_w(temp1_l, sum3_l);                              \
                                                                              \
-        LSX_DUP2_ARG2(__lsx_dp2_w_h, src3_r, filter1, src3_l, filter1,       \
-                      temp0_r, temp0_l);                                     \
-        LSX_DUP4_ARG3(__lsx_dp2add_w_h, sum2_r, src7_r, filter3, sum2_l,     \
-                      src7_l, filter3, sum3_r, src4_r, filter3, sum3_l,      \
-                      src4_l, filter3, sum2_r, sum2_l, sum3_r, sum3_l);      \
+        DUP2_ARG2(__lsx_vdp2_w_h, src3_r, filter1, src3_l, filter1,          \
+                  temp0_r, temp0_l);                                         \
+        DUP4_ARG3(__lsx_vdp2add_w_h, sum2_r, src7_r, filter3, sum2_l,        \
+                  src7_l, filter3, sum3_r, src4_r, filter3, sum3_l,          \
+                  src4_l, filter3, sum2_r, sum2_l, sum3_r, sum3_l);          \
                                                                              \
         sum0_r = __lsx_vadd_w(sum0_r, temp0_r);                              \
         sum0_l = __lsx_vadd_w(sum0_l, temp0_l);                              \
         sum1_r = __lsx_vsub_w(sum1_r, temp0_r);                              \
         sum1_l = __lsx_vsub_w(sum1_l, temp0_l);                              \
                                                                              \
-        BUTTERFLY_4_W(sum0_r, sum0_l, sum2_l, sum2_r, res0_r, res0_l,        \
-                      res1_l, res1_r);                                       \
+        LSX_BUTTERFLY_4_W(sum0_r, sum0_l, sum2_l, sum2_r, res0_r, res0_l,    \
+                          res1_l, res1_r);                                   \
         dst0 = __lsx_vssrarni_h_w(res0_l, res0_r, shift);                    \
         dst1 = __lsx_vssrarni_h_w(res1_l, res1_r, shift);                    \
         __lsx_vst(dst0, buf_ptr, 0);                                         \
         __lsx_vst(dst1, (buf_ptr + ((15 - (j * 2)) << 4)), 0);               \
                                                                              \
-        BUTTERFLY_4_W(sum1_r, sum1_l, sum3_l, sum3_r, res0_r, res0_l,        \
-                      res1_l, res1_r);                                       \
+        LSX_BUTTERFLY_4_W(sum1_r, sum1_l, sum3_l, sum3_r, res0_r, res0_l,    \
+                          res1_l, res1_r);                                   \
                                                                              \
         dst0 = __lsx_vssrarni_h_w(res0_l, res0_r, shift);                    \
         dst1 = __lsx_vssrarni_h_w(res1_l, res1_r, shift);                    \
@@ -341,7 +341,7 @@ static void hevc_idct_4x4_lsx(int16_t *coeffs)
     in_l1 = __lsx_vilvh_h(zero, in1);
 
     HEVC_IDCT4x4_COL(in_r0, in_l0, in_r1, in_l1, sum0, sum1, sum2, sum3, 7);
-    TRANSPOSE4x4_W(sum0, sum1, sum2, sum3, in_r0, in_l0, in_r1, in_l1);
+    LSX_TRANSPOSE4x4_W(sum0, sum1, sum2, sum3, in_r0, in_l0, in_r1, in_l1);
     HEVC_IDCT4x4_COL(in_r0, in_l0, in_r1, in_l1, sum0, sum1, sum2, sum3, 12);
 
     /* Pack and transpose */
@@ -361,16 +361,16 @@ static void hevc_idct_8x8_lsx(int16_t *coeffs)
     const int16_t *filter = &gt8x8_cnst[0];
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
-    LSX_DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 16, coeffs, 32,
-                  coeffs, 48, in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld, coeffs, 64, coeffs, 80, coeffs, 96,
-                  coeffs, 112, in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 16, coeffs, 32,
+              coeffs, 48, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, coeffs, 64, coeffs, 80, coeffs, 96,
+              coeffs, 112, in4, in5, in6, in7);
     HEVC_IDCT8x8_COL(in0, in1, in2, in3, in4, in5, in6, in7, 7);
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   in0, in1, in2, in3, in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
     HEVC_IDCT8x8_COL(in0, in1, in2, in3, in4, in5, in6, in7, 12);
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   in0, in1, in2, in3, in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
 
     __lsx_vst(in0, coeffs,0);
     __lsx_vst(in1, coeffs,16);
@@ -396,23 +396,23 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __m128i src0_l, src1_l, src2_l, src3_l, src4_l, src5_l, src6_l, src7_l;
 
     for (i = 2; i--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
-                      in4, in5, in6, in7);
-        LSX_DUP4_ARG2(__lsx_vld, src, 256, src, 288, src, 320, src, 352,
-                      in8, in9, in10, in11);
-        LSX_DUP4_ARG2(__lsx_vld, src, 384, src, 416, src, 448, src, 480,
-                      in12, in13, in14, in15);
-
-        LSX_DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
-                      src0_r, src1_r, src2_r, src3_r);
-        LSX_DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
-                      src4_r, src5_r, src6_r, src7_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
-                      src0_l, src1_l, src2_l, src3_l);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
-                      src4_l, src5_l, src6_l, src7_l);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+                  in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src, 256, src, 288, src, 320, src, 352,
+                  in8, in9, in10, in11);
+        DUP4_ARG2(__lsx_vld, src, 384, src, 416, src, 448, src, 480,
+                  in12, in13, in14, in15);
+
+        DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_r, src1_r, src2_r, src3_r);
+        DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_r, src5_r, src6_r, src7_r);
+        DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_l, src1_l, src2_l, src3_l);
+        DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_l, src5_l, src6_l, src7_l);
 
         HEVC_IDCT16x16_COL(src0_r, src1_r, src2_r, src3_r, src4_r, src5_r,
                            src6_r, src7_r, src0_l, src1_l, src2_l, src3_l,
@@ -428,26 +428,26 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     filter = &gt16x16_cnst[0];
 
     for (i = 2; i--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                      in0, in8, in1, in9);
-        LSX_DUP4_ARG2(__lsx_vld, src, 64, src, 80, src, 96, src, 112,
-                      in2, in10, in3, in11);
-        LSX_DUP4_ARG2(__lsx_vld, src, 128, src, 144, src, 160, src, 176,
-                      in4, in12, in5, in13);
-        LSX_DUP4_ARG2(__lsx_vld, src, 192, src, 208, src, 224, src, 240,
-                      in6, in14, in7, in15);
-        TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                       in0, in1, in2, in3, in4, in5, in6, in7);
-        TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
-                       in8, in9, in10, in11, in12, in13, in14, in15);
-        LSX_DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
-                      src0_r, src1_r, src2_r, src3_r);
-        LSX_DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
-                      src4_r, src5_r, src6_r, src7_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
-                      src0_l, src1_l, src2_l, src3_l);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
-                      src4_l, src5_l, src6_l, src7_l);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  in0, in8, in1, in9);
+        DUP4_ARG2(__lsx_vld, src, 64, src, 80, src, 96, src, 112,
+                  in2, in10, in3, in11);
+        DUP4_ARG2(__lsx_vld, src, 128, src, 144, src, 160, src, 176,
+                  in4, in12, in5, in13);
+        DUP4_ARG2(__lsx_vld, src, 192, src, 208, src, 224, src, 240,
+                  in6, in14, in7, in15);
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
+        LSX_TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
+                           in8, in9, in10, in11, in12, in13, in14, in15);
+        DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_r, src1_r, src2_r, src3_r);
+        DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_r, src5_r, src6_r, src7_r);
+        DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_l, src1_l, src2_l, src3_l);
+        DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_l, src5_l, src6_l, src7_l);
         HEVC_IDCT16x16_COL(src0_r, src1_r, src2_r, src3_r, src4_r, src5_r,
                            src6_r, src7_r, src0_l, src1_l, src2_l, src3_l,
                            src4_l, src5_l, src6_l, src7_l, 12);
@@ -457,12 +457,12 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
         filter = &gt16x16_cnst[0];
     }
 
-    LSX_DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 32, coeffs, 64, coeffs, 96,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld, coeffs, 128, coeffs, 160, coeffs, 192, coeffs, 224,
-                  in4, in5, in6, in7);
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 32, coeffs, 64, coeffs, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, coeffs, 128, coeffs, 160, coeffs, 192, coeffs, 224,
+              in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     __lsx_vst(vec0, coeffs, 0);
     __lsx_vst(vec1, coeffs, 32);
     __lsx_vst(vec2, coeffs, 64);
@@ -473,17 +473,13 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __lsx_vst(vec7, coeffs, 224);
 
     src = coeffs + 8;
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
-                  in4, in5, in6, in7);
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     src = coeffs + 128;
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
-                  in8, in9, in10, in11);
-    LSX_DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
-                  in12, in13, in14, in15);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in8, in9, in10, in11);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in12, in13, in14, in15);
 
     __lsx_vst(vec0, src, 0);
     __lsx_vst(vec1, src, 32);
@@ -493,8 +489,8 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __lsx_vst(vec5, src, 160);
     __lsx_vst(vec6, src, 192);
     __lsx_vst(vec7, src, 224);
-    TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
-                   vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    LSX_TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     src = coeffs + 8;
     __lsx_vst(vec0, src, 0);
     __lsx_vst(vec1, src, 32);
@@ -506,12 +502,10 @@ static void hevc_idct_16x16_lsx(int16_t *coeffs)
     __lsx_vst(vec7, src, 224);
 
     src = coeffs + 136;
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
-                  in4, in5, in6, in7);
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224, in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
     __lsx_vst(vec0, src, 0);
     __lsx_vst(vec1, src, 32);
     __lsx_vst(vec2, src, 64);
@@ -567,30 +561,30 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
     in5 = __lsx_vld(src3 + buf_pitch_8x, 0);
     in6 = __lsx_vld(src3 + buf_pitch_16x, 0);
     in7 = __lsx_vld(src3 + buf_pitch_16x + buf_pitch_8x, 0);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in6, in4, in7, in5,
-                  src0_r, src1_r, src2_r, src3_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in6, in4, in7, in5,
-                  src0_l, src1_l, src2_l, src3_l);
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in6, in4, in7, in5,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in6, in4, in7, in5,
+              src0_l, src1_l, src2_l, src3_l);
 
     /* loop for all columns of constants */
     for (i = 0; i < 2; i++) {
         /* processing single column of constants */
         filter0 = __lsx_vldrepl_w(filter_ptr2, 0);
         filter1 = __lsx_vldrepl_w(filter_ptr2, 4);
-        sum0_r = __lsx_dp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_dp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
         __lsx_vst(sum0_r, (tmp_buf_ptr + 2 * i * 8), 0);
         __lsx_vst(sum0_l, (tmp_buf_ptr + 2 * i * 8), 16);
 
         /* processing single column of constants */
         filter0 = __lsx_vldrepl_w(filter_ptr2, 8);
         filter1 = __lsx_vldrepl_w(filter_ptr2, 12);
-        sum0_r = __lsx_dp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_dp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
         __lsx_vst(sum0_r, (tmp_buf_ptr + (2 * i + 1) * 8), 0);
         __lsx_vst(sum0_l, (tmp_buf_ptr + (2 * i + 1) * 8), 16);
 
@@ -604,9 +598,8 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
         filter0 = __lsx_vldrepl_w(filter_ptr3, 0);
         filter1 = __lsx_vldrepl_w(filter_ptr3, 4);
 
-        LSX_DUP4_ARG2(__lsx_dp2_w_h, src2_r, filter0, src2_l, filter0,
-                      src3_r, filter1, src3_l, filter1, sum0_r, sum0_l,
-                      tmp1_r, tmp1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
+                  src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
         sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
         sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
         sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
@@ -635,10 +628,10 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
     in5 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_4x, 0);
     in6 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_8x, 0);
     in7 = __lsx_vld(src1 + buf_pitch_16x + buf_pitch_8x + buf_pitch_4x, 0);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src0_r, src1_r, src2_r, src3_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src0_l, src1_l, src2_l, src3_l);
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_l, src1_l, src2_l, src3_l);
 
     /* loop for all columns of constants */
     for (i = 0; i < 8; i++) {
@@ -647,14 +640,14 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
         filter1 = __lsx_vldrepl_w(filter_ptr1, 4);
         filter2 = __lsx_vldrepl_w(filter_ptr1, 8);
         filter3 = __lsx_vldrepl_w(filter_ptr1, 12);
-        sum0_r = __lsx_dp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_dp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src1_l, filter1);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src2_r, filter2);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src2_l, filter2);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src3_r, filter3);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src3_l, filter3);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src2_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src2_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src3_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src3_l, filter3);
 
         tmp0_r = __lsx_vld(tmp_buf_ptr + (i << 3), 0);
         tmp0_l = __lsx_vld(tmp_buf_ptr + (i << 3), 16);
@@ -690,10 +683,10 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
     in6 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x, 0);
     in7 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x + buf_pitch_2x, 0);
     src0 += 16 * buf_pitch;
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src0_r, src1_r, src2_r, src3_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src0_l, src1_l, src2_l, src3_l);
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_l, src1_l, src2_l, src3_l);
     in0 = __lsx_vld(src0, 0);
     // TODO: Use vldx once gcc fixed.
     //in1 = __lsx_vldx(src0, buf_pitch_4x);
@@ -710,10 +703,10 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
     in5 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_2x, 0);
     in6 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x, 0);
     in7 = __lsx_vld(src0 + buf_pitch_8x + buf_pitch_4x + buf_pitch_2x, 0);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src4_r, src5_r, src6_r, src7_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
-                  src4_l, src5_l, src6_l, src7_l);
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src4_r, src5_r, src6_r, src7_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src4_l, src5_l, src6_l, src7_l);
 
     /* loop for all columns of filter constants */
     for (i = 0; i < 16; i++) {
@@ -722,14 +715,14 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
         filter1 = __lsx_vldrepl_w(filter_ptr0, 4);
         filter2 = __lsx_vldrepl_w(filter_ptr0, 8);
         filter3 = __lsx_vldrepl_w(filter_ptr0, 12);
-        sum0_r = __lsx_dp2_w_h(src0_r, filter0);
-        sum0_l = __lsx_dp2_w_h(src0_l, filter0);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src1_r, filter1);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src1_l, filter1);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src2_r, filter2);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src2_l, filter2);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src3_r, filter3);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src3_l, filter3);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src2_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src2_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src3_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src3_l, filter3);
         tmp1_r = sum0_r;
         tmp1_l = sum0_l;
 
@@ -737,14 +730,14 @@ static void hevc_idct_8x32_column_lsx(int16_t *coeffs, uint8_t buf_pitch,
         filter1 = __lsx_vldrepl_w(filter_ptr0, 20);
         filter2 = __lsx_vldrepl_w(filter_ptr0, 24);
         filter3 = __lsx_vldrepl_w(filter_ptr0, 28);
-        sum0_r = __lsx_dp2_w_h(src4_r, filter0);
-        sum0_l = __lsx_dp2_w_h(src4_l, filter0);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src5_r, filter1);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src5_l, filter1);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src6_r, filter2);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src6_l, filter2);
-        sum0_r = __lsx_dp2add_w_h(sum0_r, src7_r, filter3);
-        sum0_l = __lsx_dp2add_w_h(sum0_l, src7_l, filter3);
+        sum0_r = __lsx_vdp2_w_h(src4_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src4_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src5_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src5_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src6_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src6_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src7_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src7_l, filter3);
         sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
         sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
 
@@ -776,13 +769,13 @@ static void hevc_idct_transpose_32x8_to_8x32(int16_t *coeffs, int16_t *tmp_buf)
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (i = 0; i < 4; i++) {
-        LSX_DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 64, coeffs, 128,
-                      coeffs, 192, in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, coeffs, 256, coeffs, 320, coeffs, 384,
-                      coeffs, 448, in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 64, coeffs, 128,
+                  coeffs, 192, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, coeffs, 256, coeffs, 320, coeffs, 384,
+                  coeffs, 448, in4, in5, in6, in7);
         coeffs += 8;
-        TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                       in0, in1, in2, in3, in4, in5, in6, in7);
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
         __lsx_vst(in0, tmp_buf, 0);
         __lsx_vst(in1, tmp_buf, 16);
         __lsx_vst(in2, tmp_buf, 32);
@@ -801,13 +794,13 @@ static void hevc_idct_transpose_8x32_to_32x8(int16_t *tmp_buf, int16_t *coeffs)
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (i = 0; i < 4; i++) {
-        LSX_DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 16, tmp_buf, 32,
-                      tmp_buf, 48, in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, tmp_buf, 64, tmp_buf, 80, tmp_buf, 96,
-                      tmp_buf, 112, in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 16, tmp_buf, 32,
+                  tmp_buf, 48, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, tmp_buf, 64, tmp_buf, 80, tmp_buf, 96,
+                  tmp_buf, 112, in4, in5, in6, in7);
         tmp_buf += 64;
-        TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                       in0, in1, in2, in3, in4, in5, in6, in7);
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
         __lsx_vst(in0, coeffs, 0);
         __lsx_vst(in1, coeffs, 64);
         __lsx_vst(in2, coeffs, 128);
diff --git a/libavcodec/loongarch/hevc_lpf_sao_lsx.c b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
index 01b22085ed..556e45e1ff 100644
--- a/libavcodec/loongarch/hevc_lpf_sao_lsx.c
+++ b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 
 static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
@@ -69,20 +69,19 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
     q_is_pcm0 = q_is_pcm[0];
     q_is_pcm4 = q_is_pcm[1];
 
-    LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
+    DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
     p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
     p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
     d0030 = (d00 + d30) >= beta;
     d0434 = (d04 + d34) >= beta;
-    LSX_DUP2_ARG1(__lsx_vreplgr2vr_w, d0030, d0434, cmp0, cmp1);
+    DUP2_ARG1(__lsx_vreplgr2vr_w, d0030, d0434, cmp0, cmp1);
     cmp3 = __lsx_vpackev_w(cmp1, cmp0);
     cmp3 = __lsx_vseqi_w(cmp3, 0);
 
     if ((!p_is_pcm0 || !p_is_pcm4 || !q_is_pcm0 || !q_is_pcm4) &&
         (!d0030 || !d0434)) {
-        LSX_DUP4_ARG2(__lsx_vld, p3, 0, p2, 0, p1, 0, p0, 0, p3_src, p2_src, p1_src,
-                      p0_src);
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
+        DUP4_ARG2(__lsx_vld, p3, 0, p2, 0, p1, 0, p0, 0, p3_src, p2_src, p1_src, p0_src);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
 
@@ -93,24 +92,23 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
         tc4 = tc[1];
         tc254 = (((tc4 << 2) + tc4 + 1) >> 1);
 
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_h, tc0, tc4, cmp0, cmp1);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
-                      p0_src, p3_src, p2_src, p1_src, p0_src);
-        LSX_DUP4_ARG2(__lsx_vld, q0, 0, q1, 0, q2, 0, q3, 0, q0_src, q1_src, q2_src,
-                      q3_src);
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc0, tc4, cmp0, cmp1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
+                  p0_src, p3_src, p2_src, p1_src, p0_src);
+        DUP4_ARG2(__lsx_vld, q0, 0, q1, 0, q2, 0, q3, 0, q0_src, q1_src, q2_src, q3_src);
         flag0 = abs(p3[0] - p0[0]) + abs(q3[0] - q0[0]) < beta30 && abs(p0[0] - q0[0])
                 < tc250;
         flag0 = flag0 && (abs(p3[3] - p0[3]) + abs(q3[3] - q0[3]) < beta30 &&
                 abs(p0[3] - q0[3]) < tc250 && (d00 << 1) < beta20 && (d30 << 1) < beta20);
         tc_pos = __lsx_vpackev_d(cmp1, cmp0);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero,
-                      q3_src, q0_src, q1_src, q2_src, q3_src);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero, q3_src,
+                  q0_src, q1_src, q2_src, q3_src);
 
         flag1 = abs(p3[4] - p0[4]) + abs(q3[4] - q0[4]) < beta30 && abs(p0[4] - q0[4])
                 < tc254;
         flag1 = flag1 && (abs(p3[7] - p0[7]) + abs(q3[7] - q0[7]) < beta30 &&
                 abs(p0[7] - q0[7]) < tc254 && (d04 << 1) < beta20 && (d34 << 1) < beta20);
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_w, flag0, flag1, cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_w, flag0, flag1, cmp0, cmp1);
         cmp2 = __lsx_vpackev_w(cmp1, cmp0);
         cmp2 = __lsx_vseqi_w(cmp2, 0);
 
@@ -120,71 +118,71 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst0 = __lsx_vadd_h(temp2, p2_src);
 
             temp1 = __lsx_vadd_h(temp0, p2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, p1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst1 = __lsx_vadd_h(temp2, p1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                          p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst5 = __lsx_vadd_h(temp2, q2_src);
 
             temp1 = __lsx_vadd_h(temp0, q2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, q1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst4 = __lsx_vadd_h(temp2, q1_src);
 
             temp0 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp0, p1_src, temp1, q2_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp0, p1_src, temp1, q2_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                          q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
 
             /* pack results to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
             dst2 = __lsx_vpickev_b(dst5, dst4);
 
             /* pack src to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, q1_src);
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
 
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -197,9 +195,9 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
         } else if (flag0 == flag1) { /* weak only */
             /* weak filter */
             tc_neg = __lsx_vneg_h(tc_pos);
-            LSX_DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                          diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
+                      diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
             temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
@@ -207,56 +205,53 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
 
-            delta0 = __lsx_clip_h(delta0, tc_neg, tc_pos);
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
             temp2 = __lsx_vadd_h(delta0, p0_src);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp0 = __lsx_vbitsel_v(temp2, p0_src, __lsx_vnor_v(p_is_pcm_vec,
                                     p_is_pcm_vec));
             temp2 = __lsx_vsub_h(q0_src, delta0);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp2 = __lsx_vbitsel_v(temp2, q0_src, __lsx_vnor_v(q_is_pcm_vec,
                                     q_is_pcm_vec));
-            LSX_DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
-                          q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
+            DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
+                      q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
 
             tmp = (beta + (beta >> 1)) >> 3;
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
-                          cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp, cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             cmp0 = __lsx_vseqi_d(cmp0, 0);
             p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, cmp0);
 
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
-                          cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp, cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             cmp0 = __lsx_vseqi_d(cmp0, 0);
             q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, cmp0);
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
-            LSX_DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_clip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
-                          delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
-            LSX_DUP2_ARG1(__lsx_clamp255_h, delta1, delta2, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                          q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
+                      q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
-                          abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src,
-                          abs_delta0, dst1, dst2, dst3, dst4);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
+                      abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src, abs_delta0,
+                      dst1, dst2, dst3, dst4);
             /* pack results to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst2, dst1, dst4, dst3, dst0, dst1);
+            DUP2_ARG2(__lsx_vpickev_b, dst2, dst1, dst4, dst3, dst0, dst1);
             /* pack src to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, p0_src, p1_src, q1_src, q0_src, dst2, dst3);
+            DUP2_ARG2(__lsx_vpickev_b, p0_src, p1_src, q1_src, q0_src, dst2, dst3);
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, dst2, cmp3, dst1, dst3, cmp3, dst0,
-                          dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst2, cmp3, dst1, dst3, cmp3, dst0, dst1);
 
             p2 += stride;
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -270,63 +265,63 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst0 = __lsx_vadd_h(temp2, p2_src);
 
             temp1 = __lsx_vadd_h(temp0, p2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, p1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst1 = __lsx_vadd_h(temp2, p1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                          p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1,  q2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1,  q2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst5 = __lsx_vadd_h(temp2, q2_src);
 
             temp1 = __lsx_vadd_h(temp0, q2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, q1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst4 = __lsx_vadd_h(temp2, q1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                          q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
 
             /* pack strong results to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
             dst2 = __lsx_vpickev_b(dst5, dst4);
             /* strong filter ends */
 
@@ -334,9 +329,9 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                          diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
+                      diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
             temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
@@ -344,59 +339,57 @@ static void hevc_loopfilter_luma_hor_lsx(uint8_t *src, int32_t stride,
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
 
-            delta0 = __lsx_clip_h(delta0, tc_neg, tc_pos);
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
             temp2 = __lsx_vadd_h(delta0, p0_src);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
 
             temp2 = __lsx_vsub_h(q0_src, delta0);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
 
             tmp = (beta + (beta >> 1)) >> 3;
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
-                          cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp, cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
-                          cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp, cmp0, cmp1);
             cmp0 = __lsx_vpackev_d(cmp1, cmp0);
             q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
 
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
-            LSX_DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_clip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
-                          delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
-            LSX_DUP2_ARG1(__lsx_clamp255_h, delta1, delta2, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                          q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
+                      q_is_pcm_vec, delta1, delta2);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
-                          abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src,
-                          abs_delta0, delta1, delta2, temp0, temp2);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
+                      abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src,
+                      abs_delta0, delta1, delta2, temp0, temp2);
             /* weak filter ends */
 
             /* pack weak results to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, delta1, p2_src, temp2, temp0, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, delta1, p2_src, temp2, temp0, dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, delta2);
 
             /* select between weak or strong */
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp2, dst1, dst4, cmp2, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp2, dst1, dst4, cmp2, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp2);
 
             /* pack src to 8 bit */
-            LSX_DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src, dst3, dst4);
             dst5 = __lsx_vpickev_b(q2_src, q1_src);
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
 
             __lsx_vstelm_d(dst0, p2, 0, 0);
@@ -455,28 +448,28 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
     p_is_pcm4 = p_is_pcm[1];
     q_is_pcm4 = q_is_pcm[1];
 
-    LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
+    DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
     p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
     p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
 
     d0030 = (d00 + d30) >= beta;
     d0434 = (d04 + d34) >= beta;
 
-    LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, d0030, d0434, cmp0, cmp1);
+    DUP2_ARG1(__lsx_vreplgr2vr_d, d0030, d0434, cmp0, cmp1);
     cmp3 = __lsx_vpackev_d(cmp1, cmp0);
     cmp3 = __lsx_vseqi_d(cmp3, 0);
 
     if ((!p_is_pcm0 || !p_is_pcm4 || !q_is_pcm0 || !q_is_pcm4) &&
         (!d0030 || !d0434)) {
         src -= 4;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
-                      src + stride_3x, 0, p3_src, p2_src, p1_src, p0_src);
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, p3_src, p2_src, p1_src, p0_src);
         src += stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
-                      src + stride_3x, 0, q0_src, q1_src, q2_src, q3_src);
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, q0_src, q1_src, q2_src, q3_src);
         src -= stride_4x;
 
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
 
@@ -486,11 +479,11 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
         tc250 = (((tc0 << 2) + tc0 + 1) >> 1);
         tc4 = tc[1];
         tc254 = (((tc4 << 2) + tc4 + 1) >> 1);
-        LSX_DUP2_ARG1( __lsx_vreplgr2vr_h, tc0 << 1, tc4 << 1, cmp0, cmp1);
+        DUP2_ARG1( __lsx_vreplgr2vr_h, tc0 << 1, tc4 << 1, cmp0, cmp1);
         tc_pos = __lsx_vpackev_d(cmp1, cmp0);
-        TRANSPOSE8x8_B(p3_src, p2_src, p1_src, p0_src, q0_src, q1_src,
-                       q2_src, q3_src, p3_src, p2_src, p1_src, p0_src,
-                       q0_src, q1_src, q2_src, q3_src);
+        LSX_TRANSPOSE8x8_B(p3_src, p2_src, p1_src, p0_src, q0_src, q1_src,
+                           q2_src, q3_src, p3_src, p2_src, p1_src, p0_src,
+                           q0_src, q1_src, q2_src, q3_src);
 
         flag0 = abs(p3[-4] - p3[-1]) + abs(p3[3] - p3[0]) < beta30 &&
                 abs(p3[-1] - p3[0]) < tc250;
@@ -498,16 +491,16 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
                 abs(p2[-1] - p2[0]) < tc250 && (d00 << 1) < beta20 &&
                 (d30 << 1) < beta20);
         cmp0 = __lsx_vreplgr2vr_d(flag0);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
-                      p0_src, p3_src, p2_src, p1_src, p0_src);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
+                  p0_src, p3_src, p2_src, p1_src, p0_src);
 
         flag1 = abs(p1[-4] - p1[-1]) + abs(p1[3] - p1[0]) < beta30 &&
                 abs(p1[-1] - p1[0]) < tc254;
         flag1 = flag1 && (abs(p0[-4] - p0[-1]) + abs(p0[3] - p0[0]) < beta30 &&
                 abs(p0[-1] - p0[0]) < tc254 && (d04 << 1) < beta20 &&
                 (d34 << 1) < beta20);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero,
-                      q3_src, q0_src, q1_src, q2_src, q3_src);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero,
+                  q3_src, q0_src, q1_src, q2_src, q3_src);
 
         cmp1 = __lsx_vreplgr2vr_d(flag1);
         cmp2 = __lsx_vpackev_d(cmp1, cmp0);
@@ -517,60 +510,60 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             /* strong filter */
             tc_neg = __lsx_vneg_h(tc_pos);
             /* p part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
 
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst0 = __lsx_vadd_h(temp2, p2_src);
 
             temp1 = __lsx_vadd_h(temp0, p2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, p1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst1 = __lsx_vadd_h(temp2, p1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                          p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst5 = __lsx_vadd_h(temp2, q2_src);
 
             temp1 = __lsx_vadd_h(temp0, q2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, q1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst4 = __lsx_vadd_h(temp2, q1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                          q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
             /* strong filter ends */
         } else if (flag0 == flag1) { /* weak only */
@@ -578,9 +571,9 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                          diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
+                      diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
             temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3), __lsx_vslli_h(tc_pos, 1));
@@ -588,52 +581,52 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
 
-            delta0 = __lsx_clip_h(delta0, tc_neg, tc_pos);
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
             temp2 = __lsx_vadd_h(delta0, p0_src);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
             temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
 
             temp2 = __lsx_vsub_h(q0_src, delta0);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
             temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
 
             tmp = ((beta + (beta >> 1)) >> 3);
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
-                          !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
+                      !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
             p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
             p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
 
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
-                          (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
+                      (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
             q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
             q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
-            LSX_DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_clip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
-                          delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
-            LSX_DUP2_ARG1(__lsx_clamp255_h, delta1, delta2, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                          q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
+                      q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
-                          abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src,
-                          abs_delta0, dst0, dst1, dst2, dst3);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0, p0_src,
+                      abs_delta0, temp2, q0_src, abs_delta0, delta2, q1_src,
+                      abs_delta0, dst0, dst1, dst2, dst3);
             /* weak filter ends */
 
             cmp3 = __lsx_vnor_v(cmp3, cmp3);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, dst0, p1_src, cmp3, dst1, p0_src, cmp3, dst2,
-                          q0_src, cmp3, dst3, q1_src, cmp3, dst0, dst1, dst2, dst3);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst0, dst1);
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p1_src, cmp3, dst1, p0_src, cmp3, dst2,
+                      q0_src, cmp3, dst3, q1_src, cmp3, dst0, dst1, dst2, dst3);
+            DUP2_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst0, dst1);
 
             /* transpose */
             dst4 = __lsx_vilvl_b(dst1, dst0);
@@ -657,60 +650,60 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_neg = __lsx_vneg_h(tc_pos);
 
             /* p part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src, temp0, temp0);
 
             temp1 = __lsx_vadd_h(p3_src, p2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst0 = __lsx_vadd_h(temp2, p2_src);
 
             temp1 = __lsx_vadd_h(temp0, p2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, p1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst1 = __lsx_vadd_h(temp2, p1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, p0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst2 = __lsx_vadd_h(temp2, p0_src);
 
             p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
-                          p_is_pcm_vec, dst0, dst1);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
             dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
 
             /* q part */
-            LSX_DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
             temp1 = __lsx_vadd_h(q3_src, q2_src);
             temp1 = __lsx_vslli_h(temp1, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q2_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst5 = __lsx_vadd_h(temp2, q2_src);
 
             temp1 = __lsx_vadd_h(temp0, q2_src);
             temp1 = __lsx_vsrari_h(temp1, 2);
             temp2 = __lsx_vsub_h(temp1, q1_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst4 = __lsx_vadd_h(temp2, q1_src);
 
             temp1 = __lsx_vslli_h(temp0, 1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
             temp1 = __lsx_vsrari_h(temp1, 3);
             temp2 = __lsx_vsub_h(temp1, q0_src);
-            temp2 = __lsx_clip_h(temp2, tc_neg, tc_pos);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
             dst3 = __lsx_vadd_h(temp2, q0_src);
 
             q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
-                          q_is_pcm_vec, dst3, dst4);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
             dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
             /* strong filter ends */
 
@@ -718,9 +711,9 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
-            LSX_DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
-                          diff1, 1), diff1, diff0, diff1);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src, diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0, __lsx_vslli_h(
+                      diff1, 1), diff1, diff0, diff1);
             delta0 = __lsx_vsub_h(diff0, diff1);
             delta0 = __lsx_vsrari_h(delta0, 4);
 
@@ -728,66 +721,65 @@ static void hevc_loopfilter_luma_ver_lsx(uint8_t *src, int32_t stride,
             abs_delta0 = __lsx_vadda_h(delta0, zero);
             abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            delta0 = __lsx_clip_h(delta0, tc_neg, tc_pos);
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
             temp2 = __lsx_vadd_h(delta0, p0_src);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
             temp2 = __lsx_vsub_h(q0_src, delta0);
-            temp2 = __lsx_clamp255_h(temp2);
+            temp2 = __lsx_vclip255_h(temp2);
             temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
 
             tmp = (beta + (beta >> 1)) >> 3;
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
-                          !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
+                      !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
             p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
             p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
 
-            LSX_DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
-                          (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
+            DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
+                      (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
             q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
             q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
             tc_pos = __lsx_vsrai_h(tc_pos, 1);
             tc_neg = __lsx_vneg_h(tc_pos);
 
-            LSX_DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src, delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src, delta1, delta2);
             delta1 = __lsx_vadd_h(delta1, delta0);
             delta2 = __lsx_vsub_h(delta2, delta0);
-            LSX_DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_clip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
-                          delta1, delta2);
-            LSX_DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
-            LSX_DUP2_ARG1(__lsx_clamp255_h, delta1, delta2, delta1, delta2);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
-                          q_is_pcm_vec, delta1, delta2);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg, tc_pos,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2, delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2, q1_src,
+                      q_is_pcm_vec, delta1, delta2);
 
             abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
-                          abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src,
-                          abs_delta0, delta1, delta2, temp0, temp2);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2, q1_src,
+                      abs_delta0, temp0, p0_src, abs_delta0, temp2, q0_src, abs_delta0,
+                      delta1, delta2, temp0, temp2);
             /* weak filter ends*/
 
             /* select between weak or strong */
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp2, dst1, delta1, cmp2, dst2,
-                          temp0, cmp2, dst3, temp2, cmp2, dst0, dst1, dst2, dst3);
-            LSX_DUP2_ARG3(__lsx_vbitsel_v, dst4, delta2, cmp2, dst5, q2_src, cmp2, dst4,
-                          dst5);
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp2, dst1, delta1, cmp2, dst2,
+                      temp0, cmp2, dst3, temp2, cmp2, dst0, dst1, dst2, dst3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst4, delta2, cmp2, dst5, q2_src, cmp2, dst4, dst5);
         }
 
         cmp3 = __lsx_vnor_v(cmp3, cmp3);
-        LSX_DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp3, dst1, p1_src, cmp3, dst2,
-                      p0_src, cmp3, dst3, q0_src, cmp3, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, dst4, q1_src, cmp3, dst5, q2_src, cmp3, dst4, dst5);
+        DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp3, dst1, p1_src, cmp3, dst2,
+                  p0_src, cmp3, dst3, q0_src, cmp3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vbitsel_v, dst4, q1_src, cmp3, dst5, q2_src, cmp3, dst4, dst5);
 
         /* pack results to 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst4, dst4, dst5, dst5,
-                      dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst4, dst4, dst5, dst5,
+                  dst0, dst1, dst2, dst3);
 
         /* transpose */
-        LSX_DUP2_ARG2(__lsx_vilvl_b, dst1, dst0, dst3, dst2, dst4, dst6);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, dst1, dst0, dst3, dst2, dst5, dst7);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst7, dst6, dst0, dst2);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst7, dst6, dst1, dst3);
+        DUP2_ARG2(__lsx_vilvl_b, dst1, dst0, dst3, dst2, dst4, dst6);
+        DUP2_ARG2(__lsx_vilvh_b, dst1, dst0, dst3, dst2, dst5, dst7);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst7, dst6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst7, dst6, dst1, dst3);
 
         src += 1;
         __lsx_vstelm_w(dst0, src, 0, 0);
@@ -834,39 +826,36 @@ static void hevc_loopfilter_chroma_hor_lsx(uint8_t *src, int32_t stride,
     __m128i temp0, temp1, delta;
 
     if (!(tc[0] <= 0) || !(tc[1] <= 0)) {
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
         tc_pos = __lsx_vpackev_d(cmp1, cmp0);
         tc_neg = __lsx_vneg_h(tc_pos);
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
         p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
 
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
 
-        LSX_DUP4_ARG2(__lsx_vld, p1_ptr, 0, p0_ptr, 0, q0_ptr, 0, q1_ptr, 0, p1,
-                      p0, q0, q1);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1,
-                      p0, q0, q1);
-        LSX_DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
+        DUP4_ARG2(__lsx_vld, p1_ptr, 0, p0_ptr, 0, q0_ptr, 0, q1_ptr, 0, p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1, p0, q0, q1);
+        DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
         temp0 = __lsx_vslli_h(temp0, 2);
         temp0 = __lsx_vadd_h(temp0, temp1);
         delta = __lsx_vsrari_h(temp0, 3);
-        delta = __lsx_clip_h(delta, tc_neg, tc_pos);
+        delta = __lsx_vclip_h(delta, tc_neg, tc_pos);
         temp0 = __lsx_vadd_h(p0, delta);
-        temp0 = __lsx_clamp255_h(temp0);
+        temp0 = __lsx_vclip255_h(temp0);
         p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
         temp0 = __lsx_vbitsel_v(temp0, p0, p_is_pcm_vec);
 
         temp1 = __lsx_vsub_h(q0, delta);
-        temp1 = __lsx_clamp255_h(temp1);
+        temp1 = __lsx_vclip255_h(temp1);
         q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
         temp1 = __lsx_vbitsel_v(temp1, q0, q_is_pcm_vec);
 
         tc_pos = __lsx_vslei_d(tc_pos, 0);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
-                      temp0, temp1);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos, temp0, temp1);
         temp0 = __lsx_vpickev_b(temp1, temp0);
         __lsx_vstelm_d(temp0, p0_ptr, 0, 0);
         __lsx_vstelm_d(temp0, p0_ptr + stride, 0, 1);
@@ -888,46 +877,44 @@ static void hevc_loopfilter_chroma_ver_lsx(uint8_t *src, int32_t stride,
     __m128i temp0, temp1, delta;
 
     if (!(tc[0] <= 0) || !(tc[1] <= 0)) {
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
         tc_pos = __lsx_vpackev_d(cmp1, cmp0);
         tc_neg = __lsx_vneg_h(tc_pos);
 
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
         p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
-        LSX_DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
         q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
         q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
 
         src -= 2;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
-                      src + stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, src0, src1, src2, src3);
         src += stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
-                      src + stride_3x, 0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, src4, src5, src6, src7);
         src -= stride_4x;
-        TRANSPOSE8x4_B(src0, src1, src2, src3, src4, src5, src6, src7,
-                       p1, p0, q0, q1);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1, p0,
-                      q0, q1);
+        LSX_TRANSPOSE8x4_B(src0, src1, src2, src3, src4, src5, src6, src7,
+                           p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1, p1, p0, q0, q1);
 
-        LSX_DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
+        DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
         temp0 = __lsx_vslli_h(temp0, 2);
         temp0 = __lsx_vadd_h(temp0, temp1);
         delta = __lsx_vsrari_h(temp0, 3);
-        delta = __lsx_clip_h(delta, tc_neg, tc_pos);
+        delta = __lsx_vclip_h(delta, tc_neg, tc_pos);
 
         temp0 = __lsx_vadd_h(p0, delta);
         temp1 = __lsx_vsub_h(q0, delta);
-        LSX_DUP2_ARG1(__lsx_clamp255_h, temp0, temp1, temp0, temp1);
-        LSX_DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
-                      q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, p_is_pcm_vec, temp1, q0,
-                      q_is_pcm_vec, temp0, temp1);
+        DUP2_ARG1(__lsx_vclip255_h, temp0, temp1, temp0, temp1);
+        DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec, q_is_pcm_vec,
+                  p_is_pcm_vec, q_is_pcm_vec);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, p_is_pcm_vec, temp1, q0, q_is_pcm_vec,
+                  temp0, temp1);
 
         tc_pos = __lsx_vslei_d(tc_pos, 0);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
-                      temp0, temp1);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos, temp0, temp1);
         temp0 = __lsx_vpackev_b(temp1, temp0);
 
         src += 1;
@@ -966,7 +953,7 @@ static void hevc_sao_edge_filter_0degree_4width_lsx(uint8_t *dst,
     src -= 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
@@ -974,24 +961,24 @@ static void hevc_sao_edge_filter_0degree_4width_lsx(uint8_t *dst,
         src0 = __lsx_vshuf_b(zero, src_minus10, shuf1);
         src_plus10 = __lsx_vshuf_b(zero, src_minus10, shuf2);
 
-        LSX_DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                      cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                      cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                      const1, cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+                  cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+                  cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11, diff_minus10, diff_minus11);
 
         offset = __lsx_vadd_b(diff_minus10, diff_minus11);
         offset = __lsx_vaddi_bu(offset, 2);
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                      offset, offset, offset);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         src0 = __lsx_vxori_b(src0, 128);
         dst0 = __lsx_vsadd_b(src0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1005,21 +992,21 @@ static void hevc_sao_edge_filter_0degree_4width_lsx(uint8_t *dst,
     src0 = __lsx_vshuf_b(zero, src_minus10, shuf1);
     src_plus10 = __lsx_vshuf_b(zero, src_minus10, shuf2);
 
-    LSX_DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                      const1, cmp_minus11, diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
 
     offset = __lsx_vadd_b(diff_minus10, diff_minus11);
     offset = __lsx_vaddi_bu(offset, 2);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     src0 = __lsx_vxori_b(src0, 128);
     dst0 = __lsx_vsadd_b(src0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1050,36 +1037,36 @@ static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
     src -= 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11,
-                      shuf1, src0, src1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
-                      shuf2, src_plus10, src_plus11);
-        LSX_DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
-                      src_minus10, src_plus10);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11, shuf1,
+                  src0, src1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
+                  src_plus10, src_plus11);
+        DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
+                  src_minus10, src_plus10);
         src0 = __lsx_vpickev_d(src1, src0);
 
-        LSX_DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                      cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                      cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                      const1, cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+                  cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+                  cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11, diff_minus10, diff_minus11);
 
         offset = __lsx_vadd_b(diff_minus10, diff_minus11);
         offset = __lsx_vaddi_bu(offset, 2);
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                      offset, offset, offset);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         src0 = __lsx_vxori_b(src0, 128);
         dst0 = __lsx_vsadd_b(src0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1089,29 +1076,29 @@ static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
         dst += dst_stride_2x;
     }
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11, shuf1,
-                  src0, src1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-                  src_plus10, src_plus11);
-    LSX_DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
-                  src_minus10, src_plus10);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11, shuf1,
+              src0, src1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
+              src_plus10, src_plus11);
+    DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11, src_plus10,
+              src_minus10, src_plus10);
     src0 =  __lsx_vpickev_d(src1, src0);
 
-    LSX_DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
-                  cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                      const1, cmp_minus11, diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
 
     offset = __lsx_vadd_b(diff_minus10, diff_minus11);
     offset = __lsx_vaddi_bu(offset, 2);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     src0 = __lsx_vxori_b(src0, 128);
     dst0 = __lsx_vsadd_b(src0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1157,79 +1144,78 @@ static void hevc_sao_edge_filter_0degree_16multiple_lsx(uint8_t *dst,
 
     for (; height; height -= 4) {
         src_minus1 = src - 1;
-        LSX_DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
-                      src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
-                      src_minus10, src_minus11, src_minus12, src_minus13);
+        DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
+                  src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
+                  src_minus10, src_minus11, src_minus12, src_minus13);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus1 += 16;
             dst_ptr = dst + v_cnt;
-            LSX_DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
-                          src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
-                          src10, src11, src12, src13);
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf1, src11, src_minus11,
-                          shuf1, src12, src_minus12, shuf1, src13, src_minus13, shuf1,
-                          src_zero0, src_zero1, src_zero2, src_zero3);
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf2, src11, src_minus11,
-                          shuf2, src12, src_minus12, shuf2, src13, src_minus13, shuf2,
-                          src_plus10, src_plus11, src_plus12, src_plus13);
-
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
-                          diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                          cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
-                          diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                          cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-
-            LSX_DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
-                          diff_plus11, diff_minus12, diff_plus12, diff_minus13,
-                          diff_plus13, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                          2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                          sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                          sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                          sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                          sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            LSX_DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                          src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
-            LSX_DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                          offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                          dst0, dst1, dst2, dst3);
-            LSX_DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                          dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vld, src_minus1, 0, src_minus1 + src_stride, 0,
+                      src_minus1 + src_stride_2x, 0, src_minus1 + src_stride_3x, 0,
+                      src10, src11, src12, src13);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf1, src11, src_minus11,
+                      shuf1, src12, src_minus12, shuf1, src13, src_minus13, shuf1,
+                      src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf2, src11, src_minus11,
+                      shuf2, src12, src_minus12, shuf2, src13, src_minus13, shuf2,
+                      src_plus10, src_plus11, src_plus12, src_plus13);
+
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11, diff_plus11,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
+                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
+                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
+                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
+                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
+                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
+                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
+                      dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+                      dst0, dst1, dst2, dst3);
 
             src_minus10 = src10;
             src_minus11 = src11;
@@ -1268,33 +1254,33 @@ static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
     sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
 
     /* load in advance */
-    LSX_DUP4_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src + src_stride, 0,
-                  src + src_stride_2x, 0, src_minus10, src_minus11, src10, src11);
+    DUP4_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src + src_stride, 0,
+              src + src_stride_2x, 0, src_minus10, src_minus11, src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
-                      src11, src_minus11, src10, src10, src_minus10, src_zero0,
-                      src_minus11, src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                      offset, offset, offset);
+        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+                  src11, src_minus11, src10, src10, src_minus10, src_zero0,
+                  src_minus11, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
 
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
@@ -1303,36 +1289,33 @@ static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0,
-                      src10, src11);
+        DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
         __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-                  src_minus11, src10, src10, src_minus10, src_zero0, src_minus11,
-                  src_zero1);
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
+              src_minus11, src10, src10, src_minus10, src_zero0, src_minus11, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1361,33 +1344,33 @@ static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
     sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
-    LSX_DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
+    DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src += src_stride_2x;
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-                      src_minus11, src10, src10, src_minus10, src_zero0, src_minus11,
-                      src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                      offset, offset, offset);
+        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
+                  src_minus11, src10, src10, src_minus10, src_zero0, src_minus11,
+                  src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
 
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
@@ -1396,36 +1379,33 @@ static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0,
-                      src10, src11);
+        DUP2_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0, src10, src11);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
-                  src_minus11, src10, src10, src_minus10, src_zero0, src_minus11,
-                  src_zero1);
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11, src11,
+              src_minus11, src10, src10, src_minus10, src_zero0, src_minus11, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11, diff_minus11,
+              offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 =  __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1469,68 +1449,67 @@ static void hevc_sao_edge_filter_90degree_16multiple_lsx(uint8_t *dst,
         src = src_orig + v_cnt;
         dst = dst_orig + v_cnt;
 
-        LSX_DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
 
         for (h_cnt = (height >> 2); h_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0,
-                          src + src_stride_3x, 0, src + src_stride_4x, 0,
-                          src10, src11, src12, src13);
-
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_minus11, src_minus10, src_minus11, src10,
-                          src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
-                          cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vseq_b, src11, src10, src11, src12, src12, src11, src12,
-                          src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_minus11, src_minus10, src_minus11, src10,
-                          src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
-                          cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src11, src10, src11, src12, src12, src11, src12,
-                          src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
-                          diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                          cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
-                          diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                          cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-
-            LSX_DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
-                          diff_plus11, diff_minus12, diff_plus12, diff_minus13,
-                          diff_plus13, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                          2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                          sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                          sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                          sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                          sao_offset, offset_mask3, offset_mask3, offset_mask3);
+            DUP4_ARG2(__lsx_vld, src + src_stride, 0, src + src_stride_2x, 0,
+                      src + src_stride_3x, 0, src + src_stride_4x, 0,
+                      src10, src11, src12, src13);
+
+            DUP4_ARG2(__lsx_vseq_b, src_minus11, src_minus10, src_minus11, src10,
+                      src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
+                      cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src11, src10, src11, src12, src12, src11, src12,
+                      src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_minus11, src_minus10, src_minus11, src10,
+                      src10, src_minus11, src10, src11, cmp_minus10, cmp_plus10,
+                      cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src11, src10, src11, src12, src12, src11, src12,
+                      src13, cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
+                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
+                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
+                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
+                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
+                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
 
             src_minus10 = src12;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src_minus11, 128, src10, 128, src11, 128,
-                          src12, 128, src_minus11, src10, src11, src12);
-            LSX_DUP4_ARG2(__lsx_vsadd_b, src_minus11, offset_mask0, src10, offset_mask1,
-                          src11, offset_mask2, src12, offset_mask3, dst0, dst1, dst2,
-                          dst3);
-            LSX_DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                          dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, src_minus11, 128, src10, 128, src11, 128,
+                      src12, 128, src_minus11, src10, src11, src12);
+            DUP4_ARG2(__lsx_vsadd_b, src_minus11, offset_mask0, src10, offset_mask1,
+                      src11, offset_mask2, src12, offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+                      dst0, dst1, dst2, dst3);
             src_minus11 = src13;
 
             __lsx_vst(dst0, dst, 0);
@@ -1568,42 +1547,41 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
-                  src_minus11);
-    LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                  src10, src11);
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+              src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
-                      src_plus0, src_plus1);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
-                      src_minus10, src_minus11);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                     offset, offset, offset);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+                  src_plus0, src_plus1);
+
+        DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
+                  src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1612,42 +1590,42 @@ static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x,
-                      0, src10, src11);
+        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+                  src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
         __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus0,
-                  src_plus1);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
-                  src_minus10, src_minus11);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-                  src_zero1);
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus0,
+              src_plus1);
+
+    DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
+              src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1681,42 +1659,42 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
-                  src_minus11);
-    LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                  src10, src11);
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
+              src_minus11);
+    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+              src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
-                      src_plus10, src_plus11);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
-                      src_minus10, src_minus11);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                     offset, offset, offset);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+                  src_plus10, src_plus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
+                  src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1725,41 +1703,41 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                      src10, src11)
+        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+                  src10, src11)
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus10,
-                  src_plus11);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
-                  src_minus10, src_minus11);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-                  src_zero1);
-
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2, src_plus10,
+              src_plus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
+              src_zero1);
+
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -1768,8 +1746,8 @@ static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
     src_minus11 = src11;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                  src10, src11);
+    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+              src10, src11);
 
     __lsx_vstelm_d(dst0, dst, 0, 0);
     __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
@@ -1814,82 +1792,81 @@ static void hevc_sao_edge_filter_45degree_16multiple_lsx(uint8_t *dst,
     for (; height; height -= 4) {
         src_orig = src - 1;
         dst_orig = dst;
-        LSX_DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
-                      src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
-                      src_minus11, src_minus12, src_minus13, src_minus14);
+        DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
+                  src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
+                  src_minus11, src_minus12, src_minus13, src_minus14);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus10 = __lsx_vld(src_orig - src_stride, 0);
-            LSX_DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
-                          src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
-                          src10, src11, src12, src13);
+            DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
+                      src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
+                      src10, src11, src12, src13);
             src_plus13 = __lsx_vld(src + v_cnt + src_stride_4x, 1);
             src_orig += 16;
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_minus12,
-                          shuf1, src12, src_minus13, shuf1, src13, src_minus14, shuf1,
-                          src_zero0, src_zero1, src_zero2, src_zero3);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src11, src_minus12, shuf2, src12, src_minus13,
-                          shuf2, src_plus10, src_plus11);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_minus12,
+                      shuf1, src12, src_minus13, shuf1, src13, src_minus14, shuf1,
+                      src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_minus12, shuf2, src12, src_minus13,
+                      shuf2, src_plus10, src_plus11);
             src_plus12 = __lsx_vshuf_b(src13, src_minus14, shuf2);
 
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
-                          diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                          cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
-                          diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                          cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-
-            LSX_DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
-                          diff_plus11, diff_minus12, diff_plus12, diff_minus13,
-                          diff_plus13, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                          2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                          sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                          sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                          sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                          sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            LSX_DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                          src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
-            LSX_DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                          offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                          dst0, dst1, dst2, dst3);
-            LSX_DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                          dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11, diff_plus11,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
+                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
+                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
+                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
+                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
+                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
+                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
+                      dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+                      dst0, dst1, dst2, dst3);
 
             src_minus11 = src10;
             src_minus12 = src11;
@@ -1933,42 +1910,41 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
-                  src_minus11);
-    LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                  src10, src11);
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+              src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
-                      shuf2, src_minus10, src_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                      src_minus11);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                     offset, offset, offset);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+                  shuf2, src_minus10, src_minus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
+                  src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -1977,42 +1953,42 @@ static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                      src10, src11);
+        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+                  src10, src11);
 
         __lsx_vstelm_w(dst0, dst, 0, 0);
         __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-                  src_minus10, src_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                 src_minus11);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-                  src_zero1);
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
+              src_minus10, src_minus11);
+
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
+              src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
+              src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -2048,42 +2024,41 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
     src_orig = src - 1;
 
     /* load in advance */
-    LSX_DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
-                  src_minus11);
-    LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                  src10, src11);
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+              src10, src11);
 
     for (height -= 2; height; height -= 2) {
         src_orig += src_stride_2x;
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
-                      shuf2, src_minus10, src_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                      src_minus11);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
-                      src_zero0, src_zero1);
-        LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      diff_minus10, diff_minus11);
-        LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                      cmp_minus10, cmp_minus11);
-        LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                       const1, cmp_minus11,  diff_minus10, diff_minus11);
-
-        LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                      diff_minus11, offset_mask0, offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                      offset_mask1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                      offset, dst0);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                     offset, offset, offset);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+                  src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+                  shuf2, src_minus10, src_minus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
+                  src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+                  const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+                  offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+                  offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+                  offset, offset, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
         dst0 = __lsx_vsadd_b(dst0, offset);
         dst0 = __lsx_vxori_b(dst0, 128);
@@ -2092,42 +2067,42 @@ static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
         src_minus11 = src11;
 
         /* load in advance */
-        LSX_DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
-                      src10, src11);
+        DUP2_ARG2(__lsx_vld, src_orig + src_stride, 0, src_orig + src_stride_2x, 0,
+                  src10, src11);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
         dst += dst_stride_2x;
     }
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
-                  src_zero0, src_zero1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
-                  src_minus10, src_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
-                  src_minus11);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
-                  src_zero1);
-    LSX_DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  diff_minus10, diff_minus11);
-    LSX_DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
-                  cmp_minus10, cmp_minus11);
-    LSX_DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
-                  const1, cmp_minus11, diff_minus10, diff_minus11);
-
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
-                  diff_minus11, offset_mask0, offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
-                  offset_mask1);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
-                  offset, dst0);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
-                  offset, offset, offset);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11, shuf2,
+              src_minus10, src_minus11);
+
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11, src_minus10,
+              src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
+              src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1, src_zero0,
+              offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
     dst0 = __lsx_vsadd_b(dst0, offset);
     dst0 = __lsx_vxori_b(dst0, 128);
@@ -2173,82 +2148,82 @@ static void hevc_sao_edge_filter_135degree_16multiple_lsx(uint8_t *dst,
         src_orig = src - 1;
         dst_orig = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
-                      src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
-                      src_minus11, src_plus10, src_plus11, src_plus12);
+        DUP4_ARG2(__lsx_vld, src_orig, 0, src_orig + src_stride, 0,
+                  src_orig + src_stride_2x, 0, src_orig + src_stride_3x, 0,
+                  src_minus11, src_plus10, src_plus11, src_plus12);
 
         for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
             src_minus10 = __lsx_vld(src_orig - src_stride, 2);
-            LSX_DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
-                          src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
-                          src10, src11, src12, src13);
+            DUP4_ARG2(__lsx_vld, src_orig, 16, src_orig + src_stride, 16,
+                      src_orig + src_stride_2x, 16, src_orig + src_stride_3x, 16,
+                      src10, src11, src12, src13);
             src_plus13 = __lsx_vld(src_orig + src_stride_4x, 0);
             src_orig += 16;
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_plus10,
-                          shuf1, src12, src_plus11, shuf1, src13, src_plus12, shuf1,
-                          src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11, src_plus10,
+                      shuf1, src12, src_plus11, shuf1, src13, src_plus12, shuf1,
+                      src_zero0, src_zero1, src_zero2, src_zero3);
             src_minus11 = __lsx_vshuf_b(src10, src_minus11, shuf2);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src11, src_plus10, shuf2, src12, src_plus11,
-                          shuf2, src_minus12, src_minus13);
-
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
-                          src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
-                          src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
-                          cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
-                          cmp_plus10, cmp_minus11, cmp_plus11);
-            LSX_DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
-                          cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
-                          cmp_plus12, cmp_minus13, cmp_plus13);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
-                          diff_plus10, const1, cmp_plus10, diff_minus11, const1,
-                          cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
-                          diff_plus10, diff_minus11, diff_plus11);
-            LSX_DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
-                          diff_plus12, const1, cmp_plus12, diff_minus13, const1,
-                          cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
-                          diff_plus12, diff_minus13, diff_plus13);
-
-            LSX_DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
-                          diff_plus11, diff_minus12, diff_plus12, diff_minus13,
-                          diff_plus13, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-            LSX_DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
-                          2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
-                          offset_mask3);
-
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
-                          sao_offset, offset_mask0, offset_mask0, offset_mask0);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
-                           sao_offset, offset_mask1, offset_mask1, offset_mask1);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
-                          sao_offset, offset_mask2, offset_mask2, offset_mask2);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
-                          sao_offset, offset_mask3, offset_mask3, offset_mask3);
-
-            LSX_DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
-                          src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
-            LSX_DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
-                          offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
-                          dst0, dst1, dst2, dst3);
-            LSX_DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
-                          dst0, dst1, dst2, dst3);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_plus10, shuf2, src12, src_plus11,
+                      shuf2, src_minus12, src_minus13);
+
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0, src_plus10,
+                      src_zero1, src_minus11, src_zero1, src_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2, src_plus12,
+                      src_zero3, src_minus13, src_zero3, src_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10, cmp_plus10,
+                      cmp_minus11, cmp_minus11, cmp_plus11, cmp_plus11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12, cmp_plus12,
+                      cmp_minus13, cmp_minus13, cmp_plus13, cmp_plus13, cmp_minus12,
+                      cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11, diff_minus10,
+                      diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13, diff_minus12,
+                      diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2, offset_mask2,
+                      2, offset_mask3, 2, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0, sao_offset,
+                      sao_offset, offset_mask0, offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1, sao_offset,
+                      sao_offset, offset_mask1, offset_mask1, offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2, sao_offset,
+                      sao_offset, offset_mask2, offset_mask2, offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3, sao_offset,
+                      sao_offset, offset_mask3, offset_mask3, offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2, 128,
+                      src_zero3, 128, src_zero0, src_zero1, src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3, offset_mask3,
+                      dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3, 128,
+                      dst0, dst1, dst2, dst3);
 
             src_minus11 = src10;
             src_plus10 = src11;
diff --git a/libavcodec/loongarch/hevc_macros_lsx.h b/libavcodec/loongarch/hevc_macros_lsx.h
index 580157a8cf..809efc289c 100644
--- a/libavcodec/loongarch/hevc_macros_lsx.h
+++ b/libavcodec/loongarch/hevc_macros_lsx.h
@@ -28,10 +28,10 @@ static inline __m128i __lsx_hevc_filt_8tap_h(__m128i in0, __m128i in1, __m128i i
 {
     __m128i out_m;
 
-    out_m = __lsx_dp2_h_b(in0, filt0);
-    out_m = __lsx_dp2add_h_b(out_m, in1, filt1);
-    out_m = __lsx_dp2add_h_b(out_m, in2, filt2);
-    out_m = __lsx_dp2add_h_b(out_m, in3, filt3);
+    out_m = __lsx_vdp2_h_b(in0, filt0);
+    out_m = __lsx_vdp2add_h_b(out_m, in1, filt1);
+    out_m = __lsx_vdp2add_h_b(out_m, in2, filt2);
+    out_m = __lsx_vdp2add_h_b(out_m, in3, filt3);
     return out_m;
 }
 
@@ -41,10 +41,10 @@ static inline __m128i __lsx_hevc_filt_8tap_w(__m128i in0, __m128i in1, __m128i i
 {
     __m128i out_m;
 
-    out_m = __lsx_dp2_w_h(in0, filt0);
-    out_m = __lsx_dp2add_w_h(out_m, in1, filt1);
-    out_m = __lsx_dp2add_w_h(out_m, in2, filt2);
-    out_m = __lsx_dp2add_w_h(out_m, in3, filt3);
+    out_m = __lsx_vdp2_w_h(in0, filt0);
+    out_m = __lsx_vdp2add_w_h(out_m, in1, filt1);
+    out_m = __lsx_vdp2add_w_h(out_m, in2, filt2);
+    out_m = __lsx_vdp2add_w_h(out_m, in3, filt3);
     return out_m;
 }
 
@@ -53,8 +53,8 @@ static inline __m128i __lsx_hevc_filt_4tap_h(__m128i in0, __m128i in1, __m128i f
 {
     __m128i out_m;
 
-    out_m = __lsx_dp2_h_b(in0, filt0);
-    out_m = __lsx_dp2add_h_b(out_m, in1, filt1);
+    out_m = __lsx_vdp2_h_b(in0, filt0);
+    out_m = __lsx_vdp2add_h_b(out_m, in1, filt1);
     return out_m;
 }
 
@@ -63,8 +63,8 @@ static inline __m128i __lsx_hevc_filt_4tap_w(__m128i in0, __m128i in1, __m128i f
 {
     __m128i out_m;
 
-    out_m = __lsx_dp2_w_h(in0, filt0);
-    out_m = __lsx_dp2add_w_h(out_m, in1, filt1);
+    out_m = __lsx_vdp2_w_h(in0, filt0);
+    out_m = __lsx_vdp2add_w_h(out_m, in1, filt1);
     return out_m;
 }
 
diff --git a/libavcodec/loongarch/hevc_mc_bi_lsx.c b/libavcodec/loongarch/hevc_mc_bi_lsx.c
index 22dfad90c1..00ee34fff8 100644
--- a/libavcodec/loongarch/hevc_mc_bi_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_bi_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 #include "hevc_macros_lsx.h"
 
@@ -37,8 +37,8 @@ void __lsx_hevc_bi_rnd_clip2(__m128i in0, __m128i in1, __m128i vec0, __m128i vec
     *out1 = __lsx_vsadd_h(vec1, in1);
     *out0 = __lsx_vsrari_h(*out0, 7);
     *out1 = __lsx_vsrari_h(*out1, 7);
-    *out0 = __lsx_clamp255_h(*out0);
-    *out1 = __lsx_clamp255_h(*out1);
+    *out0 = __lsx_vclip255_h(*out0);
+    *out1 = __lsx_vclip255_h(*out1);
 }
 
 static av_always_inline
@@ -78,10 +78,10 @@ static void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr,
     if (2 == height) {
         tp0 = *(uint32_t *)src0_ptr;
         tp1 = *(uint32_t *)(src0_ptr + src_stride);
-        LSX_DUP2_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, src0);
+        DUP2_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, src0);
         tpd0 = *(uint64_t *)src1_ptr;
         tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
-        LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in0, in0);
+        DUP2_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in0, in0);
 
         dst0 = __lsx_vilvl_b(zero, src0);
         dst0 = __lsx_vslli_h(dst0, 6);
@@ -95,17 +95,17 @@ static void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr,
         tp1 = *(uint32_t *)(src0_ptr + src_stride);
         tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
         tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2, src0,
-                      tp3, 3, src0, src0, src0, src0);
+        DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2, src0,
+                  tp3, 3, src0, src0, src0, src0);
         tpd0 = *(uint64_t *)src1_ptr;
         tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
         tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
         tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0, in1,
-                      tpd3, 1, in0, in0, in1, in1);
+        DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0, in1,
+                  tpd3, 1, in0, in0, in1, in1);
         dst0 = __lsx_vilvl_b(zero, src0);
         dst1 = __lsx_vilvh_b(zero, src0);
-        LSX_DUP2_ARG2(__lsx_vslli_h, dst0, 6, dst1, 6, dst0, dst1);
+        DUP2_ARG2(__lsx_vslli_h, dst0, 6, dst1, 6, dst0, dst1);
         __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
         dst0 = __lsx_vpickev_b(dst1, dst0);
         __lsx_vstelm_w(dst0, dst, 0, 0);
@@ -118,37 +118,37 @@ static void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr,
             tp1 = *(uint32_t *)(src0_ptr + src_stride);
             tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
             tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2,
-                          src0, tp3, 3, src0, src0, src0, src0);
+            DUP4_ARG3(__lsx_vinsgr2vr_w, src0, tp0, 0, src0, tp1, 1, src0, tp2, 2,
+                      src0, tp3, 3, src0, src0, src0, src0);
             src0_ptr += src_stride_4x;
             tp0 = *(uint32_t *)src0_ptr;
             tp1 = *(uint32_t *)(src0_ptr + src_stride);
             tp2 = *(uint32_t *)(src0_ptr + src_stride_2x);
             tp3 = *(uint32_t *)(src0_ptr + src_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_w, src1, tp0, 0, src1, tp1, 1, src1, tp2, 2,
-                          src1, tp3, 3, src1, src1, src1, src1);
+            DUP4_ARG3(__lsx_vinsgr2vr_w, src1, tp0, 0, src1, tp1, 1, src1, tp2, 2,
+                      src1, tp3, 3, src1, src1, src1, src1);
             src0_ptr += src_stride_4x;
             tpd0 = *(uint64_t *)src1_ptr;
             tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
             tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
             tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0,
-                          in1, tpd3, 1, in0, in0, in1, in1);
+            DUP4_ARG3(__lsx_vinsgr2vr_d, in0, tpd0, 0, in0, tpd1, 1, in1, tpd2, 0,
+                      in1, tpd3, 1, in0, in0, in1, in1);
             src1_ptr += src2_stride_4x;
             tpd0 = *(uint64_t *)src1_ptr;
             tpd1 = *(uint64_t *)(src1_ptr + src2_stride);
             tpd2 = *(uint64_t *)(src1_ptr + src2_stride_2x);
             tpd3 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, in2, tpd0, 0, in2, tpd1, 1, in3, tpd2, 0,
-                          in3, tpd3, 1, in2, in2, in3, in3);
+            DUP4_ARG3(__lsx_vinsgr2vr_d, in2, tpd0, 0, in2, tpd1, 1, in3, tpd2, 0,
+                      in3, tpd3, 1, in2, in2, in3, in3);
             src1_ptr += src2_stride_4x;
-            LSX_DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-            LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-            LSX_DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
+            DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
+            DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
+            DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
 
             __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                     &dst0, &dst1, &dst2, &dst3);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
             __lsx_vstelm_w(dst0, dst, 0, 0);
             __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
             __lsx_vstelm_w(dst0, dst + dst_stride_2x, 0, 2);
@@ -193,36 +193,36 @@ static void hevc_bi_copy_6w_lsx(uint8_t *src0_ptr,
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
         tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
         tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                      tp3, 1, src0, src0, src1, src1);
+        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
+                  tp3, 1, src0, src0, src1, src1);
         src0_ptr += src_stride_4x;
         tp0 = *(uint64_t *)src0_ptr;
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
         tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
         tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0, src3,
-                      tp3, 1, src2, src2, src3, src3);
+        DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0, src3,
+                  tp3, 1, src2, src2, src3, src3);
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
         src1_ptr += src2_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in4, in5, in6, in7);
         src1_ptr += src2_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0, dst2, dst4, dst6);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst1, dst3, dst5, dst7);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                      dst5, dst7);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
+                  dst5, dst7);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
                                 &dst4, &dst5, &dst6, &dst7);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
         __lsx_vstelm_w(out0, dst, 0, 0);
         __lsx_vstelm_w(out0, dst + dst_stride, 0, 2);
         __lsx_vstelm_h(out0, dst, 4, 2);
@@ -271,8 +271,8 @@ static void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr,
     if (2 == height) {
         tp0 = *(uint64_t *)src0_ptr;
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src0, src0);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+        DUP2_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src0, src0);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
         dst0 = __lsx_vsllwil_hu_bu(src0, 6);
         dst1 = __lsx_vilvh_b(zero, src0);
         dst1 = __lsx_vslli_h(dst1, 6);
@@ -285,17 +285,17 @@ static void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr,
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
         tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
         tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                      tp3, 1, src0, src0, src1, src1);
-        LSX_DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-        LSX_DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0, in0,
-                      in1, in2, in3);
+        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
+                  tp3, 1, src0, src0, src1, src1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
+        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0, in0,
+                  in1, in2, in3);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -305,28 +305,28 @@ static void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr,
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
         tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
         tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-        LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
-                      tp3, 1, src0, src0, src1, src1);
+        DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0, src1,
+                  tp3, 1, src0, src0, src1, src1);
         src0_ptr += src_stride_4x;
         tp0 = *(uint64_t *)src0_ptr;
         tp1 = *(uint64_t *)(src0_ptr + src_stride);
-        LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src2, src2);
+        DUP2_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src2, src2);
 
-        LSX_DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
-        LSX_DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
+        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
         dst4 = __lsx_vsllwil_hu_bu(src2, 6);
         dst5 = __lsx_vilvh_b(zero, src2);
         dst5 = __lsx_vslli_h(dst5, 6);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
         src1_ptr += src2_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
         out2 = __lsx_vpickev_b(dst5, dst4);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
@@ -342,36 +342,36 @@ static void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr,
             tp1 = *(uint64_t *)(src0_ptr + src_stride);
             tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
             tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0,
-                          src1, tp3, 1, src0, src0, src1, src1);
+            DUP4_ARG3(__lsx_vinsgr2vr_d, src0, tp0, 0, src0, tp1, 1, src1, tp2, 0,
+                      src1, tp3, 1, src0, src0, src1, src1);
             src0_ptr += src_stride_4x;
             tp0 = *(uint64_t *)src0_ptr;
             tp1 = *(uint64_t *)(src0_ptr + src_stride);
             tp2 = *(uint64_t *)(src0_ptr + src_stride_2x);
             tp3 = *(uint64_t *)(src0_ptr + src_stride_3x);
-            LSX_DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0,
-                          src3, tp3, 1, src2, src2, src3, src3);
+            DUP4_ARG3(__lsx_vinsgr2vr_d, src2, tp0, 0, src2, tp1, 1, src3, tp2, 0,
+                      src3, tp3, 1, src2, src2, src3, src3);
             src0_ptr += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6, dst0,
-                          dst2, dst4, dst6);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                          dst1, dst3, dst5, dst7);
-            LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1,
-                          dst3, dst5, dst7);
-            LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                          src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                          in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6, dst0,
+                      dst2, dst4, dst6);
+            DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                      dst1, dst3, dst5, dst7);
+            DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1,
+                      dst3, dst5, dst7);
+            DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                      in0, in1, in2, in3);
             src1_ptr += src2_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                          src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                          in4, in5, in6, in7);
+            DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                      in4, in5, in6, in7);
             src1_ptr += src2_stride_4x;
             __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2,
                                     dst3, &dst0, &dst1, &dst2, &dst3);
             __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6,
                                     dst7, &dst4, &dst5, &dst6, &dst7);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+            DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
             __lsx_vstelm_d(out0, dst, 0, 0);
             __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
             __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -410,27 +410,27 @@ static void hevc_bi_copy_12w_lsx(uint8_t *src0_ptr,
     __m128i dst0, dst1, dst2, dst3, dst4, dst5;
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
+                  src0, src1, src2, src3);
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                      src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
+                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
+                  in4, in5, in6, in7);
 
         src1_ptr += src2_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0, dst1, dst2, dst3)
-        LSX_DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst4, dst5)
+        DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst1, dst2, dst3)
+        DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst4, dst5)
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
         out2 = __lsx_vpickev_b(dst5, dst4);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
@@ -469,30 +469,30 @@ static void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr,
     __m128i zero = {0};
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
+                  src0, src1, src2, src3);
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                      src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
+                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
+                  in4, in5, in6, in7);
         src1_ptr += src2_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0_r, dst1_r, dst2_r, dst3_r)
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst0_l, dst1_l, dst2_l, dst3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
-                      dst1_l, dst2_l, dst3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r)
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst0_l, dst1_l, dst2_l, dst3_l);
+        DUP4_ARG2(__lsx_vslli_h, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
+                  dst1_l, dst2_l, dst3_l);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in4, in5, dst0_r, dst1_r, dst0_l,
                                 dst1_l, &dst0_r, &dst1_r, &dst0_l, &dst1_l);
         __lsx_hevc_bi_rnd_clip4(in2, in3, in6, in7, dst2_r, dst3_r, dst2_l,
                                 dst3_l, &dst2_r, &dst3_r, &dst2_l, &dst3_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, out2, out3);
         __lsx_vst(out0, dst, 0);
         __lsx_vstx(out1, dst, dst_stride);
         __lsx_vstx(out2, dst, dst_stride_2x);
@@ -525,32 +525,32 @@ static void hevc_bi_copy_24w_lsx(uint8_t *src0_ptr,
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10, dst11;
 
     for (loop_cnt = 8; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                      in0, in1, in4, in5);
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
-                      src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
-                      in2, in3, in6, in7);
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, in0, 6, in1, 6, in2, 6, in3, 6,
-                      dst0, dst2, dst4, dst5)
-
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, in0, zero, in1, zero, in4, zero, in5,
-                      dst1, dst3, dst7, dst9);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst7, 6, dst9, 6,
-                      dst1, dst3, dst7, dst9);
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, in4, 6, in5, 6, in6, 6, in7, 6,
-                      dst6, dst8, dst10, dst11)
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
+                  in0, in1, in4, in5);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
+                  src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
+                  in2, in3, in6, in7);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, in0, 6, in1, 6, in2, 6, in3, 6,
+                  dst0, dst2, dst4, dst5)
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, in0, zero, in1, zero, in4, zero, in5,
+                  dst1, dst3, dst7, dst9);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst7, 6, dst9, 6,
+                  dst1, dst3, dst7, dst9);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, in4, 6, in5, 6, in6, 6, in7, 6,
+                  dst6, dst8, dst10, dst11)
 
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                      src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                      in4, in5, in6, in7);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32,
-                      src1_ptr + src2_stride_2x, 32, src1_ptr + src2_stride_3x, 32,
-                      in8, in9, in10, in11);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
+                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
+                  in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32,
+                  src1_ptr + src2_stride_2x, 32, src1_ptr + src2_stride_3x, 32,
+                  in8, in9, in10, in11);
         src1_ptr += src2_stride_4x;
 
         __lsx_hevc_bi_rnd_clip4(in0, in4, in1, in5, dst0, dst1, dst2, dst3,
@@ -559,9 +559,9 @@ static void hevc_bi_copy_24w_lsx(uint8_t *src0_ptr,
                                 &dst4, &dst5, &dst6, &dst7);
         __lsx_hevc_bi_rnd_clip4(in3, in7, in10, in11, dst8, dst9, dst10, dst11,
                                 &dst8, &dst9, &dst10, &dst11);
-        LSX_DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      out0, out1, out2, out3);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
+        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  out0, out1, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
         __lsx_vst(out0, dst, 0);
         __lsx_vstelm_d(out2, dst, 16, 0);
         __lsx_vstx(out1, dst, dst_stride);
@@ -590,29 +590,29 @@ static void hevc_bi_copy_32w_lsx(uint8_t *src0_ptr,
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src0_ptr += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src3);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src3);
         src0_ptr += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
+                  in0, in1, in2, in3);
         src1_ptr += src2_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
+                  in4, in5, in6, in7);
         src1_ptr += src2_stride;
 
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0, dst2, dst4, dst6)
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst1, dst3, dst5, dst7);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                      dst5, dst7);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6)
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
+                  dst5, dst7);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
                                 &dst4, &dst5, &dst6, &dst7);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
         dst += dst_stride;
@@ -638,31 +638,31 @@ static void hevc_bi_copy_48w_lsx(uint8_t *src0_ptr,
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10, dst11;
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src2 = __lsx_vld(src0_ptr, 32);
         src0_ptr += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src3, src4);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src3, src4);
         src5 = __lsx_vld(src0_ptr, 32);
         src0_ptr += src_stride;
 
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                      in0, in1, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
+                  in0, in1, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
         src1_ptr += src2_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                      in6, in7, in8, in9);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in10, in11);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
+                  in6, in7, in8, in9);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in10, in11);
         src1_ptr += src2_stride;
 
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0, dst2, dst4, dst6);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst1, dst3, dst5, dst7);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                      dst5, dst7);
-        LSX_DUP2_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, dst8, dst10);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, dst9, dst11);
-        LSX_DUP2_ARG2(__lsx_vslli_h, dst9, 6, dst11, 6, dst9, dst11);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
+                  dst5, dst7);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, dst8, dst10);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, dst9, dst11);
+        DUP2_ARG2(__lsx_vslli_h, dst9, 6, dst11, 6, dst9, dst11);
 
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
@@ -671,9 +671,9 @@ static void hevc_bi_copy_48w_lsx(uint8_t *src0_ptr,
                                 &dst4, &dst5, &dst6, &dst7);
         __lsx_hevc_bi_rnd_clip4(in8, in9, in10, in11, dst8, dst9, dst10, dst11,
                                 &dst8, &dst9, &dst10, &dst11);
-        LSX_DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      out0, out1, out2, out3);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
+        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  out0, out1, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst9, dst8, dst11, dst10, out4, out5);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
         __lsx_vst(out2, dst, 32);
@@ -701,28 +701,28 @@ static void hevc_bi_copy_64w_lsx(uint8_t *src0_ptr,
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0_ptr, 32, src0_ptr, 48,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0_ptr, 32, src0_ptr, 48,
+                  src0, src1, src2, src3);
         src0_ptr += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr, 112,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr, 48,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr, 112,
+                  in4, in5, in6, in7);
         src1_ptr += src2_stride;
 
-        LSX_DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
-                      dst0, dst2, dst4, dst6);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      dst1, dst3, dst5, dst7);
-        LSX_DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
-                      dst5, dst7);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
+                  dst5, dst7);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
                                 &dst4, &dst5, &dst6, &dst7);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, out0, out1);
+        DUP2_ARG2(__lsx_vpickev_b, dst5, dst4, dst7, dst6, out2, out3);
 
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
@@ -754,45 +754,44 @@ static void hevc_hz_bi_8t_16w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src0, src1);
         src0_ptr += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src2, src3);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src2, src3);
         src0_ptr += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
         src1_ptr += src2_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
         src1_ptr += src2_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3, &dst0, &dst1,
                                 &dst2, &dst3);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
         __lsx_vst(dst0, dst, 0);
         __lsx_vstx(dst1, dst, dst_stride);
         dst += dst_stride_2x;
@@ -820,34 +819,34 @@ static void hevc_hz_bi_8t_24w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src0_ptr += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
         in2 = __lsx_vld(src1_ptr, 32);
         src1_ptr += src2_stride;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                      mask0, src0, src0, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, dst0, vec3, filt1, dst0, dst1, dst2, dst0);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask5, src1, src1, mask1, src0, src0,
-                      mask2, src1, src0, mask6, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst1, vec0, filt1, dst2, vec1, filt1, dst0, vec2,
-                      filt2, dst1, vec3, filt2, dst1, dst2, dst0, dst1);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask2, src0, src0, mask3, src1, src0,
-                      mask7, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst2, vec0, filt2, dst0, vec1, filt3, dst1,
-                      vec2, filt3, dst2, vec3, filt3, dst2, dst0, dst1, dst2);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
+                  mask0, src0, src0, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, dst0, vec3, filt1, dst0, dst1, dst2, dst0);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask5, src1, src1, mask1, src0, src0,
+                  mask2, src1, src0, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst1, vec0, filt1, dst2, vec1, filt1, dst0, vec2,
+                  filt2, dst1, vec3, filt2, dst1, dst2, dst0, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask2, src0, src0, mask3, src1, src0,
+                  mask7, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst2, vec0, filt2, dst0, vec1, filt3, dst1,
+                  vec2, filt3, dst2, vec3, filt3, dst2, dst0, dst1, dst2);
 
         __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
         tmp0 = __lsx_vpickev_b(dst1, dst0);
@@ -883,46 +882,45 @@ static void hevc_hz_bi_8t_32w_lsx(uint8_t *src0_ptr,
     src0_ptr -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src2 = __lsx_vld(src0_ptr, 24);
         src0_ptr += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                      48, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
+                  48, in0, in1, in2, in3);
         src1_ptr += src2_stride;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2, vec2,
-                      filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2, vec2,
-                      filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2, vec2,
+                  filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2, vec2,
+                  filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst, 16);
         dst += dst_stride;
@@ -951,56 +949,55 @@ static void hevc_hz_bi_8t_48w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
 
     for (loop_cnt = 64; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 40, src2, src3);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 40, src2, src3);
         src0_ptr += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                      48, in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                      mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                      mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                      mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                      mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
+                  48, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
+                  mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
+                  mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
+                  mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
+                  mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
         __lsx_hevc_bi_rnd_clip2(in0, in1, dst0, dst1, &dst0, &dst1);
         __lsx_hevc_bi_rnd_clip2(in2, in3, dst2, dst3, &dst2, &dst3);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst, 16);
 
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, in4, in5);
         src1_ptr += src2_stride;
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, src2, src2,
-                      mask1, src3, src3, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      dst4, vec2, filt1, dst5, vec3, filt1, dst4, dst5, dst4, dst5);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, src2, src2,
-                      mask3, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst4, vec0, filt2, dst5, vec1, filt2, dst4,
-                      vec2, filt3, dst5, vec3, filt3, dst4, dst5, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, src2, src2,
+                  mask1, src3, src3, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  dst4, vec2, filt1, dst5, vec3, filt1, dst4, dst5, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, src2, src2,
+                  mask3, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst4, vec0, filt2, dst5, vec1, filt2, dst4,
+                  vec2, filt3, dst5, vec3, filt3, dst4, dst5, dst4, dst5);
 
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst4, dst5, &dst4, &dst5);
         tmp2 = __lsx_vpickev_b(dst5, dst4);
@@ -1031,46 +1028,45 @@ static void hevc_hz_bi_8t_64w_lsx(uint8_t *src0_ptr,
     src0_ptr -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src2 = __lsx_vld(src0_ptr, 24);
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 48, src3, src4);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 32, src0_ptr, 48, src3, src4);
         src5 = __lsx_vld(src0_ptr, 56);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
-                      48, in0, in1, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
+                  48, in0, in1, in2, in3);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst, 16);
 
@@ -1078,32 +1074,31 @@ static void hevc_hz_bi_8t_64w_lsx(uint8_t *src0_ptr,
         src1 = src4;
         src2 = src5;
 
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr,
-                      112, in0, in1, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 64, src1_ptr, 80, src1_ptr, 96, src1_ptr,
+                  112, in0, in1, in2, in3);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask2, src2, src2, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask3, src2, src2, mask3, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 32);
         __lsx_vst(tmp1, dst, 48);
         src1_ptr += src2_stride;
@@ -1144,55 +1139,54 @@ void hevc_vt_bi_8t_8w_lsx(uint8_t *src0_ptr,
     src0_ptr -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0, src1,
-                  src2, src3);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
     src0_ptr += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src0_ptr + src_stride_2x, 0);
     src0_ptr += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 =  __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                  src10_r, src32_r, src54_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
+                  src7, src8, src9, src10);
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
         src1_ptr += src2_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                      dst0_r, dst0_r, dst0_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                      dst1_r, dst1_r, dst1_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                      filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                      dst2_r, dst2_r, dst2_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                      filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                      dst3_r, dst3_r, dst3_r);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
+                  dst0_r, dst0_r, dst0_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
+                  dst1_r, dst1_r, dst1_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
+                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
+                  dst2_r, dst2_r, dst2_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
+                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
+                  dst3_r, dst3_r, dst3_r);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst2_r, dst3_r,
                                 &dst0_r, &dst1_r, &dst2_r, &dst3_r);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(dst1_r, dst + dst_stride_2x, 0, 0);
@@ -1245,64 +1239,64 @@ void hevc_vt_bi_8t_16multx2mult_lsx(uint8_t *src0_ptr,
     src0_ptr -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
     for (cnt = (width >> 4); cnt--;) {
         src0_ptr_tmp = src0_ptr;
         src1_ptr_tmp = src1_ptr;
         dst_tmp = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                      src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
-                      0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
+                  src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
+                  0, src0, src1, src2, src3);
         src0_ptr_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
         src0_ptr_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_r, src32_r, src54_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_l, src32_l, src54_l, src21_l);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 1); loop_cnt--;) {
-            LSX_DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                          src7, src8);
+            DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
+                      src7, src8);
             src0_ptr_tmp += src_stride_2x;
-            LSX_DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                          in0, in1);
-            LSX_DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 16, src1_ptr_tmp + src2_stride,
-                          16, in2, in3);
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
+                      in0, in1);
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 16, src1_ptr_tmp + src2_stride,
+                      16, in2, in3);
             src1_ptr_tmp += src2_stride_2x;
-            LSX_DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
-
-            LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
-
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                          filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                          dst0_r, dst0_r, dst0_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                          filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                          dst1_r, dst1_r, dst1_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                          filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3, dst0_l,
-                          dst0_l, dst0_l, dst0_l);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                          filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3, dst1_l,
-                          dst1_l, dst1_l, dst1_l);
+            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
+
+            DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+            DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
+                      dst0_r, dst0_r, dst0_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
+                      dst1_r, dst1_r, dst1_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                      filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3, dst0_l,
+                      dst0_l, dst0_l, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                      filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3, dst1_l,
+                      dst1_l, dst1_l, dst1_l);
 
             __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                     &dst0_r, &dst1_r, &dst0_l, &dst1_l);
 
-            LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+            DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
             __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
@@ -1434,15 +1428,15 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
     src0_ptr -= src_stride_3x + 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-                  filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
+              filt0, filt1, filt2, filt3);
     filt_h3 = __lsx_vld(filter_y, 0);
     filt_h3 = __lsx_vsllwil_h_b(filt_h3, 0);
 
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filt_h3, 0, filt_h3, 1, filt_h3, 2, filt_h3, 3,
-                  filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP4_ARG2(__lsx_vreplvei_w, filt_h3, 0, filt_h3, 1, filt_h3, 2, filt_h3, 3,
+              filt_h0, filt_h1, filt_h2, filt_h3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (cnt = width >> 3; cnt--;) {
@@ -1450,27 +1444,27 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
         dst_tmp = dst;
         src1_ptr_tmp = src1_ptr;
 
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                      src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x, 0,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
+                  src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x, 0,
+                  src0, src1, src2, src3);
         src0_ptr_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
         src0_ptr_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
         dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
                                       filt3);
         dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
@@ -1480,12 +1474,12 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
         dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1,
                                       filt2, filt3);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                      mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                      mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                      mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
         dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
                                       filt3);
         dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
@@ -1501,14 +1495,14 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
             in0 = __lsx_vld(src1_ptr_tmp, 0);
             src1_ptr_tmp += src2_stride;
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
-                          src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
             dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
                                           filt2, filt3);
-            LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                          dst10_r, dst32_r, dst54_r, dst76_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                          dst10_l, dst32_l, dst54_l, dst76_l);
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                      dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                      dst10_l, dst32_l, dst54_l, dst76_l);
 
             dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
@@ -1518,7 +1512,7 @@ void hevc_hv_bi_8t_8multx1mult_lsx(uint8_t *src0_ptr,
             dst0_l = __lsx_vsrli_w(dst0_l, 6);
 
             tmp = __lsx_vpickev_h(dst0_l, dst0_r);
-            LSX_DUP2_ARG2(__lsx_vsadd_h, tmp, in0, tmp, const_vec, tmp, tmp);
+            DUP2_ARG2(__lsx_vsadd_h, tmp, in0, tmp, const_vec, tmp, tmp);
             tmp = __lsx_vmaxi_h(tmp, 0);
             out = __lsx_vssrlrni_bu_h(tmp, tmp, 7);
             __lsx_vstelm_d(out, dst_tmp, 0, 0);
@@ -1662,85 +1656,82 @@ static void hevc_hz_bi_4t_24w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= 1;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 10);
 
     dst_tmp = dst + 16;
     src1_ptr_tmp = src1_ptr + 16;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
-                      src0, src2, src4, src6);
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
-                      src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
-                      src1, src3, src5, src7);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0,
+                  src0, src2, src4, src6);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16,
+                  src0_ptr + src_stride_2x, 16, src0_ptr + src_stride_3x, 16,
+                  src1, src3, src5, src7);
         src0_ptr += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in2, in4, in6);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                      src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                      in1, in3, in5, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in2, in4, in6);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
+                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
+                  in1, in3, in5, in7);
         src1_ptr += src2_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                      src5, src6, src7);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src2, src2,
-                      mask0, src3, src2, mask2, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src2, src2,
-                      mask1, src3, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src4, mask2, src6, src6,
-                      mask0, src7, src6, mask2, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst4, dst5, dst6,
-                      dst7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src4, mask3, src6, src6,
-                      mask1, src7, src6, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst4, vec0, filt1, dst5, vec1, filt1, dst6,
-                      vec2, filt1, dst7, vec3, filt1, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
+                  src5, src6, src7);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src2, src2,
+                  mask0, src3, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src2, src2,
+                  mask1, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src4, mask2, src6, src6,
+                  mask0, src7, src6, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst4, dst5, dst6, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src4, mask3, src6, src6,
+                  mask1, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst4, vec0, filt1, dst5, vec1, filt1, dst6,
+                  vec2, filt1, dst7, vec3, filt1, dst4, dst5, dst6, dst7);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
         __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
                                 &dst4, &dst5, &dst6, &dst7);
 
-        LSX_DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  dst0, dst1, dst2, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vstx(dst1, dst, dst_stride);
         __lsx_vstx(dst2, dst, dst_stride_2x);
         __lsx_vstx(dst3, dst, dst_stride_3x);
         dst += dst_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                      src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
+                  src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
         src1_ptr_tmp += src2_stride_4x;
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src3, src3, mask0, src5, src5,
-                      mask0, src7, src7, mask0, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask1, src3, src3, mask1, src5, src5,
-                      mask1, src7, src7, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src3, src3, mask0, src5, src5,
+                  mask0, src7, src7, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask1, src3, src3, mask1, src5, src5,
+                  mask1, src7, src7, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
         __lsx_vstelm_d(dst0, dst_tmp, 0, 0);
         __lsx_vstelm_d(dst0, dst_tmp + dst_stride, 0, 1);
         __lsx_vstelm_d(dst1, dst_tmp + dst_stride_2x, 0, 0);
@@ -1771,33 +1762,32 @@ static void hevc_hz_bi_4t_32w_lsx(uint8_t *src0_ptr,
     src0_ptr -= 1;
 
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 10);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
         src2 = __lsx_vld(src0_ptr, 24);
         src0_ptr += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32, src1_ptr,
                       48, in0, in1, in2, in3);
         src1_ptr += src2_stride;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src1, src1,
-                      mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src1, src1,
-                      mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2, vec2,
-                      filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src1, src1,
+                  mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src1, src1,
+                  mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2, vec2,
+                  filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                 &dst0, &dst1, &dst2, &dst3);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         dst += dst_stride;
@@ -1833,54 +1823,54 @@ static void hevc_vt_bi_4t_12w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
     src0_ptr += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
     src2110 = __lsx_vilvl_d(src21_l, src10_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src6);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src6);
         src0_ptr += src_stride_2x;
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
-                      src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
-                      in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16,
+                  src1_ptr + src2_stride_2x, 16, src1_ptr + src2_stride_3x, 16,
+                  in4, in5, in6, in7);
         src1_ptr += src2_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                      src4, src5, src6);
+        DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
+                  src4, src5, src6);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
         src4332 = __lsx_vilvl_d(src43_l, src32_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src6, src5, src54_r, src65_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src54_l, src65_l);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src6, src5, src54_r, src65_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src54_l, src65_l);
         src6554 = __lsx_vilvl_d(src65_l, src54_l);
 
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, const_vec, src21_r, filt0, dst1_r,  src43_r, filt1, dst0_r,
-                      dst0_r, dst1_r, dst1_r );
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
-                      filt1, const_vec, src32_r, filt0, dst2_r, src54_r, filt1, dst0_l,
-                      dst0_l, dst2_r, dst2_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                      filt1, const_vec, src4332, filt0, dst1_l, src6554, filt1, dst3_r,
-                      dst3_r, dst1_l, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, const_vec, src21_r, filt0, dst1_r,  src43_r, filt1, dst0_r,
+                  dst0_r, dst1_r, dst1_r );
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
+                  filt1, const_vec, src32_r, filt0, dst2_r, src54_r, filt1, dst0_l,
+                  dst0_l, dst2_r, dst2_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
+                  filt1, const_vec, src4332, filt0, dst1_l, src6554, filt1, dst3_r,
+                  dst3_r, dst1_l, dst1_l);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst2_r, dst3_r,
                                 &dst0_r, &dst1_r, &dst2_r, &dst3_r);
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst0_l, dst1_l, &dst0_l, &dst1_l);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         dst0_l = __lsx_vpickev_b(dst1_l, dst0_l);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
@@ -1923,63 +1913,63 @@ static void hevc_vt_bi_4t_16w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
     src0_ptr += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
         src1_ptr += src2_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, dst1_l);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                  filt1, dst1_l, dst1_l);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                 &dst0_r, &dst1_r, &dst0_l, &dst1_l);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst1_r, dst + dst_stride, 0);
         dst += dst_stride_2x;
 
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
         src1_ptr += src2_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                      filt1, dst1_l, dst1_l);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
+                  filt1, dst1_l, dst1_l);
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                 &dst0_r, &dst1_r, &dst0_l, &dst1_l);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
         dst += dst_stride_2x;
@@ -2013,55 +2003,55 @@ static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     /* 16width */
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
     /* 8width */
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src0_ptr + src_stride_2x, 16);
     src0_ptr += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         /* 16width */
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src1_ptr, 0,
-                      src1_ptr + src2_stride, 0, src3, src4, in0, in1);
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, src1_ptr,
-                      32, src1_ptr + src2_stride, 32, in2, in3, in4, in5);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src1_ptr, 0,
+                  src1_ptr + src2_stride, 0, src3, src4, in0, in1);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, src1_ptr,
+                  32, src1_ptr + src2_stride, 32, in2, in3, in4, in5);
         src1_ptr += src2_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
         /* 8width */
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
         /* 16width */
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, const_vec, src10_l, filt0, dst0_l, src32_l, filt1, dst0_r,
-                      dst0_r, dst0_l, dst0_l);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, const_vec, src21_l, filt0, dst1_l, src43_l, filt1, dst1_r,
-                      dst1_r, dst1_l, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, const_vec, src10_l, filt0, dst0_l, src32_l, filt1, dst0_r,
+                  dst0_r, dst0_l, dst0_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, const_vec, src21_l, filt0, dst1_l, src43_l, filt1, dst1_r,
+                  dst1_r, dst1_l, dst1_l);
         /* 8width */
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                      filt1, const_vec, src87_r, filt0, dst3_r, src109_r, filt1, dst2_r,
-                      dst2_r, dst3_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
+                  filt1, const_vec, src87_r, filt0, dst3_r, src109_r, filt1, dst2_r,
+                  dst2_r, dst3_r, dst3_r);
         /* 16width */
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                 &dst0_r, &dst1_r, &dst0_l, &dst1_l);
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst2_r, dst3_r, &dst2_r, &dst3_r);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         dst2_r = __lsx_vpickev_b(dst3_r, dst2_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
@@ -2070,36 +2060,36 @@ static void hevc_vt_bi_4t_24w_lsx(uint8_t *src0_ptr,
         dst += dst_stride_2x;
 
         /* 16width */
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
         src1_ptr += src2_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
         /* 8width */
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src11, src8);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src11, src8);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
         /* 16width */
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                      filt1, const_vec, src32_l, filt0, dst0_l, src10_l, filt1, dst0_r,
-                      dst0_r, dst0_l, dst0_l);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                      filt1, const_vec, src43_l, filt0, dst1_l, src21_l, filt1, dst1_r,
-                      dst1_r, dst1_l, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
+                  filt1, const_vec, src32_l, filt0, dst0_l, src10_l, filt1, dst0_r,
+                  dst0_r, dst0_l, dst0_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
+                  filt1, const_vec, src43_l, filt0, dst1_l, src21_l, filt1, dst1_r,
+                  dst1_r, dst1_l, dst1_l);
         /* 8width */
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                      filt1, const_vec, src109_r, filt0, dst3_r, src87_r, filt1, dst2_r,
-                      dst2_r, dst3_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
+                  filt1, const_vec, src109_r, filt0, dst3_r, src87_r, filt1, dst2_r,
+                  dst2_r, dst3_r, dst3_r);
 
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                 &dst0_r, &dst1_r, &dst0_l, &dst1_l);
         __lsx_hevc_bi_rnd_clip2(in4, in5, dst2_r, dst3_r, &dst2_r, &dst3_r);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         dst2_r = __lsx_vpickev_b(dst3_r, dst2_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vstx(dst1_r, dst, dst_stride);
@@ -2138,45 +2128,45 @@ static void hevc_vt_bi_4t_32w_lsx(uint8_t *src0_ptr,
     src0_ptr -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     /* 16width */
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     /* next 16width */
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src0_ptr + src_stride_2x, 16);
     src0_ptr += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         /* 16width */
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
-        LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 48, src1_ptr + src2_stride, 48, in6, in7);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 16, src1_ptr + src2_stride, 16, in2, in3);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 32, src1_ptr + src2_stride, 32, in4, in5);
+        DUP2_ARG2(__lsx_vld, src1_ptr, 48, src1_ptr + src2_stride, 48, in6, in7);
         src1_ptr += src2_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
         /* 16width */
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, dst1_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                  filt1, dst1_l, dst1_l);
         /* 16width */
         __lsx_hevc_bi_rnd_clip4(in0, in1, in2, in3, dst0_r, dst1_r, dst0_l, dst1_l,
                                 &dst0_r, &dst1_r, &dst0_l, &dst1_l);
@@ -2187,31 +2177,31 @@ static void hevc_vt_bi_4t_32w_lsx(uint8_t *src0_ptr,
         src21_l = src43_l;
         src2 = src4;
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst1_r, dst + dst_stride, 0);
         dst += dst_stride_2x;
 
         /* next 16width */
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 16, src0_ptr + src_stride, 16, src9, src10);
         src0_ptr += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
         /* next 16width */
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                      filt1, dst2_r, dst2_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
-                      filt1, dst2_l, dst2_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                      filt1, dst3_r, dst3_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
-                      filt1, dst3_l, dst3_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
+                  filt1, dst2_r, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
+                  filt1, dst2_l, dst2_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
+                  filt1, dst3_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
+                  filt1, dst3_l, dst3_l);
         /* next 16width */
         __lsx_hevc_bi_rnd_clip4(in4, in5, in6, in7, dst2_r, dst3_r, dst2_l, dst3_l,
                                 &dst2_r, &dst3_r, &dst2_l, &dst3_l);
 
-        LSX_DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_b, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
         __lsx_vst(dst2_r, dst_tmp, 0);
         __lsx_vstx(dst3_r, dst_tmp, dst_stride);
         dst_tmp += dst_stride_2x;
@@ -2258,41 +2248,40 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src0_ptr -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filt_h1 = __lsx_vld(filter_y, 0);
     filt_h1 = __lsx_vsllwil_h_b(filt_h1, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filt_h1, 0, filt_h1, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filt_h1, 0, filt_h1, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src0_ptr + src_stride_2x, 0);
     src0_ptr += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
     dsth0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dsth1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
     dsth2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, tmp0, tmp2);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, tmp1, tmp3);
+    DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, tmp0, tmp2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, tmp1, tmp3);
 
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src3, src4,
-                  src5, src6);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
-                  src5, src6);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src3, src4, src5, src6);
+    DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
+              src5, src6);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
     dsth3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dsth4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -2300,79 +2289,77 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
     dsth6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
     src0_ptr += src_stride_4x;
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src3, src4,
-                  src5, src6);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
-                  src5, src6);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src3, src4, src5, src6);
+    DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3, src4,
+              src5, src6);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
     dsth7 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dsth8 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
     dsth9 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
     dsth10 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, tmp4, tmp6);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, tmp5, tmp7);
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth5, dsth4, dsth6, dsth5, dsth0, dsth2);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth5, dsth4, dsth6, dsth5, dsth1, dsth3);
+    DUP2_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, tmp5, tmp7);
+    DUP2_ARG2(__lsx_vilvl_h, dsth5, dsth4, dsth6, dsth5, dsth0, dsth2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth5, dsth4, dsth6, dsth5, dsth1, dsth3);
     dst0_r = __lsx_hevc_filt_4tap_w(tmp0, tmp4, filt_h0, filt_h1);
     dst1_r = __lsx_hevc_filt_4tap_w(tmp2, tmp6, filt_h0, filt_h1);
     dst2_r = __lsx_hevc_filt_4tap_w(tmp4, dsth0, filt_h0, filt_h1);
     dst3_r = __lsx_hevc_filt_4tap_w(tmp6, dsth2, filt_h0, filt_h1);
-    LSX_DUP2_ARG2(__lsx_vpickev_d, tmp3, tmp1, tmp7, tmp5, tmp0, tmp8);
+    DUP2_ARG2(__lsx_vpickev_d, tmp3, tmp1, tmp7, tmp5, tmp0, tmp8);
     dst0_l = __lsx_hevc_filt_4tap_w(tmp0, tmp8, filt_h0, filt_h1);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth7, dsth6, dsth8, dsth7, tmp0, tmp2);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth7, dsth6, dsth8, dsth7, tmp1, tmp3);
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth9, dsth8, dsth10, dsth9, tmp4, tmp6);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth9, dsth8, dsth10, dsth9, tmp5, tmp7);
+    DUP2_ARG2(__lsx_vilvl_h, dsth7, dsth6, dsth8, dsth7, tmp0, tmp2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth7, dsth6, dsth8, dsth7, tmp1, tmp3);
+    DUP2_ARG2(__lsx_vilvl_h, dsth9, dsth8, dsth10, dsth9, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_h, dsth9, dsth8, dsth10, dsth9, tmp5, tmp7);
     dst4_r = __lsx_hevc_filt_4tap_w(dsth0, tmp0, filt_h0, filt_h1);
     dst5_r = __lsx_hevc_filt_4tap_w(dsth2, tmp2, filt_h0, filt_h1);
     dst6_r = __lsx_hevc_filt_4tap_w(tmp0, tmp4, filt_h0, filt_h1);
     dst7_r = __lsx_hevc_filt_4tap_w(tmp2, tmp6, filt_h0, filt_h1);
-    LSX_DUP2_ARG2(__lsx_vpickev_d, dsth3, dsth1, tmp3, tmp1, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpickev_d, dsth3, dsth1, tmp3, tmp1, tmp0, tmp1);
     tmp2 = __lsx_vpickev_d(tmp7, tmp5);
 
     dst1_l = __lsx_hevc_filt_4tap_w(tmp8, tmp0, filt_h0, filt_h1);
     dst2_l = __lsx_hevc_filt_4tap_w(tmp0, tmp1, filt_h0, filt_h1);
     dst3_l = __lsx_hevc_filt_4tap_w(tmp1, tmp2, filt_h0, filt_h1);
 
-    LSX_DUP4_ARG2(__lsx_vsrai_d, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6, dst0_r,
-                  dst1_r, dst2_r, dst3_r);
-    LSX_DUP4_ARG2(__lsx_vsrai_d, dst4_r, 6, dst5_r, 6, dst6_r, 6, dst7_r, 6, dst4_r,
-                  dst5_r, dst6_r, dst7_r);
-    LSX_DUP4_ARG2(__lsx_vsrai_d, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
-                  dst1_l, dst2_l, dst3_l);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
+    DUP4_ARG2(__lsx_vsrai_d, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6, dst0_r,
+              dst1_r, dst2_r, dst3_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst4_r, 6, dst5_r, 6, dst6_r, 6, dst7_r, 6, dst4_r,
+              dst5_r, dst6_r, dst7_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6, dst0_l,
+              dst1_l, dst2_l, dst3_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpickev_h, dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpickev_h, dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
 
     tp0 = *(uint64_t *)src1_ptr;
     tp1 = *(uint64_t *)(src1_ptr + src2_stride);
-    LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth0, tp1, 1, dsth0, dsth0);
+    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth0, tp1, 1, dsth0, dsth0);
     tp0 = *(uint64_t *)(src1_ptr + src2_stride_2x);
     tp1 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-    LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth1, tp1, 1, dsth1, dsth1);
+    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth1, tp1, 1, dsth1, dsth1);
     src1_ptr += src2_stride_4x;
     tp0 = *(uint64_t *)src1_ptr;
     tp1 = *(uint64_t *)(src1_ptr + src2_stride);
-    LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth2, tp1, 1, dsth2, dsth2);
+    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth2, tp1, 1, dsth2, dsth2);
     tp0 = *(uint64_t *)(src1_ptr + src2_stride_2x);
     tp1 = *(uint64_t *)(src1_ptr + src2_stride_3x);
-    LSX_DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth3, tp1, 1, dsth3, dsth3);
+    DUP2_ARG3(__lsx_vinsgr2vr_d, zero, tp0, 0, dsth3, tp1, 1, dsth3, dsth3);
 
-    LSX_DUP4_ARG2(__lsx_vsadd_h, dsth0, const_vec, dsth1, const_vec, dsth2, const_vec,
-                  dsth3, const_vec, dsth0, dsth1, dsth2, dsth3);
-    LSX_DUP4_ARG2(__lsx_vsadd_h, dsth0, tmp0, dsth1, tmp1, dsth2, tmp2, dsth3, tmp3,
-                  tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2,
-                  tmp3);
-    LSX_DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+    DUP4_ARG2(__lsx_vsadd_h, dsth0, const_vec, dsth1, const_vec, dsth2, const_vec,
+              dsth3, const_vec, dsth0, dsth1, dsth2, dsth3);
+    DUP4_ARG2(__lsx_vsadd_h, dsth0, tmp0, dsth1, tmp1, dsth2, tmp2, dsth3, tmp3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
 
     __lsx_vstelm_w(out0, dst, 0, 0);
     __lsx_vstelm_w(out0, dst + dst_stride, 0, 1);
@@ -2390,18 +2377,18 @@ static void hevc_hv_bi_4t_6w_lsx(uint8_t *src0_ptr,
     tpw1 = *(uint32_t *)(src1_ptr + 4 + src2_stride);
     tpw2 = *(uint32_t *)(src1_ptr + 4 + src2_stride_2x);
     tpw3 = *(uint32_t *)(src1_ptr + 4 + src2_stride_3x);
-    LSX_DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth4, tpw1, 1, dsth4, tpw2, 2,
-                  dsth4, tpw3, 3, dsth4, dsth4, dsth4, dsth4);
+    DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth4, tpw1, 1, dsth4, tpw2, 2,
+              dsth4, tpw3, 3, dsth4, dsth4, dsth4, dsth4);
     src1_ptr += src2_stride_4x;
     tpw0 = *(uint32_t *)(src1_ptr + 4);
     tpw1 = *(uint32_t *)(src1_ptr + 4 + src2_stride);
     tpw2 = *(uint32_t *)(src1_ptr + 4 + src2_stride_2x);
     tpw3 = *(uint32_t *)(src1_ptr + 4 + src2_stride_3x);
-    LSX_DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth5, tpw1, 1, dsth5, tpw2, 2,
-                  dsth5, tpw3, 3, dsth5, dsth5, dsth5, dsth5);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, dsth4, const_vec, dsth5, const_vec, dsth4, dsth5);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, dsth4, tmp4, dsth5, tmp5, tmp4, tmp5);
-    LSX_DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 7, tmp4, tmp5);
+    DUP4_ARG3(__lsx_vinsgr2vr_w, zero, tpw0, 0, dsth5, tpw1, 1, dsth5, tpw2, 2,
+              dsth5, tpw3, 3, dsth5, dsth5, dsth5, dsth5);
+    DUP2_ARG2(__lsx_vsadd_h, dsth4, const_vec, dsth5, const_vec, dsth4, dsth5);
+    DUP2_ARG2(__lsx_vsadd_h, dsth4, tmp4, dsth5, tmp5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 7, tmp4, tmp5);
     out0 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
 
     __lsx_vstelm_h(out0, dst, 4, 0);
@@ -2445,31 +2432,30 @@ void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
     __m128i in0, in1;
 
     src0_ptr -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0, src1,
-                  src2, src3);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
     src4 = __lsx_vld(src0_ptr + src_stride_4x, 0);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
     src4 = __lsx_vxori_b(src4, 128);
 
-    LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in0, in1);
+    DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+    DUP2_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in0, in1);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
 
     dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -2477,19 +2463,19 @@ void hevc_hv_bi_4t_8x2_lsx(uint8_t *src0_ptr,
     dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
     dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
     dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
     dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-                  dst0_l, dst1_r, dst1_l);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp0, tmp1);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
+              dst0_l, dst1_r, dst1_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp0, tmp1);
     out = __lsx_vssrlrni_bu_h(tmp1, tmp0, 7);
     __lsx_vstelm_d(out, dst, 0, 0);
     __lsx_vstelm_d(out, dst + dst_stride, 0, 1);
@@ -2526,61 +2512,61 @@ void hevc_hv_bi_4t_8multx4_lsx(uint8_t *src0_ptr,
     __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
 
     src0_ptr -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width8mult; cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                      src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0,
-                      src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
+                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0,
+                  src1, src2, src3);
         src0_ptr += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src0_ptr + src_stride_2x, 0);
         src0_ptr += (8 - src_stride_4x);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
-        LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                      src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
-                      in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+                  src1_ptr + src2_stride_2x, 0, src1_ptr + src2_stride_3x, 0,
+                  in0, in1, in2, in3);
         src1_ptr += 8;
-        LSX_DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec,
-                      in3, const_vec, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec,
+                  in3, const_vec, in0, in1, in2, in3);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
         dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
         dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
         dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
         dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2591,17 +2577,17 @@ void hevc_hv_bi_4t_8multx4_lsx(uint8_t *src0_ptr,
         dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
         dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
-                      dst0_r, dst0_l, dst1_r, dst1_l);
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
-                      dst2_r, dst2_l, dst3_r, dst3_l);
-        LSX_DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
-                      dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
-                      tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1,
-                      tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+                  dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1,
+                  tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -2650,48 +2636,46 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     __m128i dst76_r, dst76_l, dst87_r, dst87_l;
 
     src0_ptr -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src0, src1,
-                  src2, src3);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src0, src1, src2, src3);
     src0_ptr += src_stride_4x;
-    LSX_DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0,
-                  src0_ptr + src_stride_2x, 0, src0_ptr + src_stride_3x, 0, src4, src5,
-                  src6, src7);
+    DUP4_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr + src_stride, 0, src0_ptr + src_stride_2x,
+              0, src0_ptr + src_stride_3x, 0, src4, src5, src6, src7);
     src8 = __lsx_vld(src0_ptr + src_stride_4x, 0);
 
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4, src5,
-                  src6, src7)
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4, src5,
+              src6, src7)
     src8 = __lsx_vxori_b(src8, 128);
 
-    LSX_DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
-                  src1_ptr + src2_stride_2x, 0,  src1_ptr + src2_stride_3x, 0, in0, in1,
-                  in2, in3);
+    DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0,
+              src1_ptr + src2_stride_2x, 0,  src1_ptr + src2_stride_3x, 0, in0, in1,
+              in2, in3);
     src1_ptr += src2_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
-    LSX_DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec, in3,
-                  const_vec, in0, in1, in2, in3);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, in4, const_vec, in5, const_vec, in4, in5);
-
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec10, vec11);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec12, vec13);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, vec14, vec15);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
+    DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in4, in5);
+    DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2, const_vec, in3,
+              const_vec, in0, in1, in2, in3);
+    DUP2_ARG2(__lsx_vsadd_h, in4, const_vec, in5, const_vec, in4, in5);
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec10, vec11);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec12, vec13);
+    DUP2_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
 
     dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -2703,14 +2687,14 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     dst7 = __lsx_hevc_filt_4tap_h(vec14, vec15, filt0, filt1);
     dst8 = __lsx_hevc_filt_4tap_h(vec16, vec17, filt0, filt1);
 
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_r, dst21_r, dst32_r, dst43_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_l, dst21_l, dst32_l, dst43_l);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_r, dst65_r, dst76_r, dst87_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_l, dst65_l, dst76_l, dst87_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
 
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2725,22 +2709,21 @@ void hevc_hv_bi_4t_8x6_lsx(uint8_t *src0_ptr,
     dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
     dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
 
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-                  dst0_l, dst1_r, dst1_l);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
-                  dst2_l, dst3_r, dst3_l);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
-                  dst4_l, dst5_r, dst5_l);
-    LSX_DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
-                  dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, tmp4, tmp5);
-    LSX_DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1,
-                  tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vsadd_h, in4, tmp4, in5, tmp5, tmp4, tmp5);
-    LSX_DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2,
-                  tmp3);
-    LSX_DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 0, tmp4, tmp5);
-    LSX_DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
+              dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
+              dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
+              dst4_l, dst5_r, dst5_l);
+    DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+              dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, tmp4, tmp5);
+    DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1,
+              tmp2, tmp3);
+    DUP2_ARG2(__lsx_vsadd_h, in4, tmp4, in5, tmp5, tmp4, tmp5);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 0, tmp4, tmp5);
+    DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
     out2 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
     __lsx_vstelm_d(out0, dst, 0, 0);
     __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
@@ -2794,12 +2777,12 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
 
     src0_ptr -= (src_stride + 1);
 
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
 
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
@@ -2809,52 +2792,52 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
         dst_tmp = dst;
         src1_ptr_tmp = src1_ptr;
 
-        LSX_DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0, src0, src1);
         src2 = __lsx_vld(src0_ptr_tmp + src_stride_2x, 0);
         src0_ptr_tmp += src_stride_3x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
         dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = height >> 2; loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
-                          src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
-                          0, src3, src4, src5, src6);
+            DUP4_ARG2(__lsx_vld, src0_ptr_tmp, 0, src0_ptr_tmp + src_stride, 0,
+                      src0_ptr_tmp + src_stride_2x, 0, src0_ptr_tmp + src_stride_3x,
+                      0, src3, src4, src5, src6);
             src0_ptr_tmp += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
-                          src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x,
-                          0, in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp + src2_stride, 0,
+                      src1_ptr_tmp + src2_stride_2x, 0, src1_ptr_tmp + src2_stride_3x,
+                      0, in0, in1, in2, in3);
             src1_ptr_tmp += src2_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                          src3, src4, src5, src6);
+            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                      src3, src4, src5, src6);
 
-            LSX_DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2,
-                          const_vec, in3, const_vec, in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vsadd_h, in0, const_vec, in1, const_vec, in2,
+                      const_vec, in3, const_vec, in0, in1, in2, in3);
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
-                          src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
-                          src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+            DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                      src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                      src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
 
             dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
             dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
             dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
             dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-            LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-            LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
             dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
             dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2865,17 +2848,17 @@ void hevc_hv_bi_4t_8multx4mult_lsx(uint8_t *src0_ptr,
             dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
             dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-            LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
-                          dst0_r, dst0_l, dst1_r, dst1_l);
-            LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
-                          dst2_r, dst2_l, dst3_r, dst3_l);
-            LSX_DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
-                          dst2_r, dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
-            LSX_DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
-                          tmp0, tmp1, tmp2, tmp3);
-            LSX_DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0,
-                          tmp1, tmp2, tmp3);
-            LSX_DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+            DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                      dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                      dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                      dst2_r, dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+                      tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0,
+                      tmp1, tmp2, tmp3);
+            DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
             __lsx_vstelm_d(out0, dst_tmp, 0, 0);
             __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
             __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
diff --git a/libavcodec/loongarch/hevc_mc_uni_lsx.c b/libavcodec/loongarch/hevc_mc_uni_lsx.c
index bd481c7ff1..9976502f18 100644
--- a/libavcodec/loongarch/hevc_mc_uni_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_uni_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 #include "hevc_macros_lsx.h"
 
@@ -48,40 +48,38 @@ void common_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
     src -= 3;
 
     /* rearranging filter */
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 8, src, 16, src, 24, src0, src1, src2,
-                      src3);
-        LSX_DUP4_ARG2(__lsx_vld, src, 32, src, 40, src, 48, src, 56, src4, src5, src6,
-                      src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 8, src, 16, src, 24, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 32, src, 40, src, 48, src, 56, src4, src5, src6, src7);
         src += src_stride;
 
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                      src5, src6, src7);
-
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3, filt0,
-                      res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
-                      vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
-                      vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
-                      vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
+                  src5, src6, src7);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3, filt0,
+                  res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
+                  vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
+                  vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
+                  vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
 
         out = __lsx_vssrarni_b_h(res1, res0, 6);
         out = __lsx_vxori_b(out, 128);
@@ -90,27 +88,27 @@ void common_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
         out = __lsx_vxori_b(out, 128);
         __lsx_vst(out, dst, 16);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src5, mask0, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src7, src7, mask0, vec2, vec3);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3,
-                      filt0, res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask2, src5, src5, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask2, src7, src7, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
-                      vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src5, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask1, src7, src7, mask1, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
-                      vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask3, src5, src5, mask3, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask3, src7, src7, mask3, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
-                      vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
-
-        LSX_DUP4_ARG2(__lsx_vsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1,
-                      res2, res3);
-        LSX_DUP4_ARG2(__lsx_vsat_h, res0, 7, res1, 7, res2, 7, res3, 7, res0, res1,
-                      res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src5, mask0, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src7, src7, mask0, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_b, vec0, filt0, vec1, filt0, vec2, filt0, vec3,
+                  filt0, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask2, src5, src5, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask2, src7, src7, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec0, filt2, res1, vec1, filt2, res2,
+                  vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src5, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask1, src7, src7, mask1, vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt1, res1, vec5, filt1, res2,
+                  vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask3, src5, src5, mask3, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask3, src7, src7, mask3, vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_b, res0, vec4, filt3, res1, vec5, filt3, res2,
+                  vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+
+        DUP4_ARG2(__lsx_vsrari_h, res0, 6, res1, 6, res2, 6, res3, 6, res0, res1,
+                  res2, res3);
+        DUP4_ARG2(__lsx_vsat_h, res0, 7, res1, 7, res2, 7, res3, 7, res0, res1,
+                  res2, res3);
         out = __lsx_vpickev_b(res1, res0);
         out = __lsx_vxori_b(out, 128);
         __lsx_vst(out, dst, 32);
@@ -141,43 +139,43 @@ void common_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
     __m128i out0_r, out1_r, out2_r, out3_r;
 
     src -= src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                 src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, src10_r,
-                  src32_r, src54_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, src10_r,
+              src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10);
         src += src_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, src10_r, filt0, src21_r, filt0, src32_r, filt0,
-                      src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, out0_r, src32_r, filt1, out1_r, src43_r, filt1,
-                      out2_r, src54_r, filt1, out3_r, src65_r, filt1, out0_r, out1_r,
-                      out2_r, out3_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, out0_r, src54_r, filt2, out1_r, src65_r, filt2,
-                      out2_r, src76_r, filt2, out3_r, src87_r, filt2, out0_r, out1_r,
-                      out2_r, out3_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, out0_r, src76_r, filt3, out1_r, src87_r, filt3,
-                      out2_r, src98_r, filt3, out3_r, src109_r, filt3, out0_r, out1_r,
-                      out2_r, out3_r);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vdp2_h_b, src10_r, filt0, src21_r, filt0, src32_r, filt0,
+                  src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src32_r, filt1, out1_r, src43_r, filt1,
+                  out2_r, src54_r, filt1, out3_r, src65_r, filt1, out0_r, out1_r,
+                  out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src54_r, filt2, out1_r, src65_r, filt2,
+                  out2_r, src76_r, filt2, out3_r, src87_r, filt2, out0_r, out1_r,
+                  out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, out0_r, src76_r, filt3, out1_r, src87_r, filt3,
+                  out2_r, src98_r, filt3, out3_r, src109_r, filt3, out0_r, out1_r,
+                  out2_r, out3_r);
 
         tmp0 = __lsx_vssrarni_b_h(out1_r, out0_r, 6);
         tmp0 = __lsx_vxori_b(tmp0, 128);
@@ -224,42 +222,42 @@ void common_vt_8t_16w_mult_lsx(uint8_t *src, int32_t src_stride,
     __m128i out0_r, out1_r, out2_r, out3_r, out0_l, out1_l, out2_l, out3_l;
 
     src -= src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
     for (cnt = (width >> 4); cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
-                      src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
+                  src1, src2, src3);
         src_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
         src_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_r, src32_r, src54_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_l, src32_l, src54_l, src21_l);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                          src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src7,
-                          src8, src9, src10);
-            LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                          src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src7,
+                      src8, src9, src10);
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                      src7, src8, src9, src10);
             src_tmp += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
-                          src9, src76_r, src87_r, src98_r, src109_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
-                          src9, src76_l, src87_l, src98_l, src109_l);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                      src9, src76_r, src87_r, src98_r, src109_r);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
+                      src9, src76_l, src87_l, src98_l, src109_l);
             out0_r = __lsx_hevc_filt_8tap_h(src10_r, src32_r, src54_r, src76_r,
                                             filt0, filt1, filt2, filt3);
             out1_r = __lsx_hevc_filt_8tap_h(src21_r, src43_r, src65_r, src87_r,
@@ -276,10 +274,10 @@ void common_vt_8t_16w_mult_lsx(uint8_t *src, int32_t src_stride,
                                             filt0, filt1, filt2, filt3);
             out3_l = __lsx_hevc_filt_8tap_h(src43_l, src65_l, src87_l, src109_l,
                                             filt0, filt1, filt2, filt3);
-            LSX_DUP4_ARG3(__lsx_vssrarni_b_h, out0_l, out0_r, 6, out1_l, out1_r, 6,
-                          out2_l, out2_r, 6, out3_l, out3_r, 6, tmp0, tmp1, tmp2, tmp3);
-            LSX_DUP4_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp2, 128, tmp3, 128,
-                          tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG3(__lsx_vssrarni_b_h, out0_l, out0_r, 6, out1_l, out1_r, 6,
+                      out2_l, out2_r, 6, out3_l, out3_r, 6, tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp2, 128, tmp3, 128,
+                      tmp0, tmp1, tmp2, tmp3);
             __lsx_vst(tmp0, dst_tmp, 0);
             __lsx_vstx(tmp1, dst_tmp, dst_stride);
             __lsx_vstx(tmp2, dst_tmp, dst_stride_2x);
@@ -374,71 +372,71 @@ void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= (src_stride_3x + 3);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-                  filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
+              filt0, filt1, filt2, filt3);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
-                  filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (cnt = width >> 3; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
-                      src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
+                  src1, src2, src3);
         src_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
         dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
         dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2, filt3);
         dst2 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2, filt3);
         dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1, filt2, filt3);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                      mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                      mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                      mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
         dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
         dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2, filt3);
         dst6 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2, filt3);
 
         for (loop_cnt = height >> 1; loop_cnt--;) {
-            LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
-            LSX_DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
+            DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
+            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
             src_tmp += src_stride_2x;
 
-            LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                          dst10_r, dst32_r, dst54_r, dst21_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                          dst10_l, dst32_l, dst54_l, dst21_l);
-            LSX_DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
+                      dst10_r, dst32_r, dst54_r, dst21_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
+                      dst10_l, dst32_l, dst54_l, dst21_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
-                          src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
             dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3);
 
             dst76_r = __lsx_vilvl_h(dst7, dst6);
@@ -447,10 +445,10 @@ void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
                                             filt_h1, filt_h2, filt_h3);
             dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l, filt_h0,
                                             filt_h1, filt_h2, filt_h3);
-            LSX_DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
-                          src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
+                      src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
             dst8 =  __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
                                            filt2, filt3);
 
@@ -460,9 +458,9 @@ void hevc_hv_uni_8t_8multx2mult_lsx(uint8_t *src,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
             dst1_l = __lsx_hevc_filt_8tap_w(dst21_l, dst43_l, dst65_l, dst87_l,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
-            LSX_DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
-                          dst0, dst1);
+            DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
+            DUP2_ARG3(__lsx_vssrarni_b_h, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+                      dst0, dst1);
             out = __lsx_vpickev_b(dst1, dst0);
             out = __lsx_vxori_b(out, 128);
             __lsx_vstelm_d(out, dst_tmp, 0, 0);
@@ -571,36 +569,36 @@ void common_vt_4t_24w_lsx(uint8_t *src, int32_t src_stride,
     __m128i out, out0_r, out1_r, out2_r, out3_r, out0_l, out1_l;
 
     src -= src_stride;
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     /* 16 width */
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     /* 8 width */
-    LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src + src_stride_2x, 16);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = 8; loop_cnt--;) {
         /* 16 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
         /* 8 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
 
         /* 16 width */
         out0_r = __lsx_hevc_filt_4tap_h(src10_r, src32_r, filt0, filt1);
@@ -616,9 +614,9 @@ void common_vt_4t_24w_lsx(uint8_t *src, int32_t src_stride,
         out = __lsx_vssrarni_b_h(out0_l, out0_r, 6);
         out = __lsx_vxori_b(out, 128);
         __lsx_vst(out, dst, 0);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out2_r, out2_r, 6, out3_r, out3_r, 6,
-                      out2_r, out3_r);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out2_r, 128, out3_r, 128, out2_r, out3_r);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out2_r, out2_r, 6, out3_r, out3_r, 6,
+                  out2_r, out3_r);
+        DUP2_ARG2(__lsx_vxori_b, out2_r, 128, out3_r, 128, out2_r, out3_r);
         __lsx_vstelm_d(out2_r, dst, 16, 0);
         dst += dst_stride;
         out = __lsx_vssrarni_b_h(out1_l, out1_r, 6);
@@ -628,16 +626,16 @@ void common_vt_4t_24w_lsx(uint8_t *src, int32_t src_stride,
         dst += dst_stride;
 
         /* 16 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
         /* 8 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
 
         /* 16 width */
         out0_r = __lsx_hevc_filt_4tap_h(src32_r, src10_r, filt0, filt1);
@@ -688,33 +686,33 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
     __m128i out;
 
     src -= src_stride;
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
     /* 16 width */
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     /* next 16 width */
-    LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src + src_stride_2x, 16);
     src += src_stride_3x;
 
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         /* 16 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
         /* 16 width */
         out0_r = __lsx_hevc_filt_4tap_h(src10_r, src32_r, filt0, filt1);
@@ -737,11 +735,11 @@ void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
         src2 = src4;
 
         /* next 16 width */
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
 
         /* next 16 width */
         out2_r = __lsx_hevc_filt_4tap_h(src76_r, src98_r, filt0, filt1);
@@ -792,41 +790,40 @@ void hevc_hv_uni_4t_8x2_lsx(uint8_t *src,
     __m128i out0_r, out1_r;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src4 = __lsx_vld(src + src_stride_4x, 0);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
     src4 = __lsx_vxori_b(src4, 128);
 
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
-                  mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
-                  mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
 
     dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
     dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
     dst3 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
     dst4 = __lsx_hevc_filt_4tap_h(vec8, vec9, filt0, filt1);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_r, dst21_r, dst32_r, dst43_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
     dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
     dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    LSX_DUP2_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
-                  out0_r, out1_r);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, out0_r, out1_r);
     out = __lsx_vssrarni_b_h(out1_r, out0_r, 6);
     out = __lsx_vxori_b(out, 128);
     __lsx_vstelm_d(out, dst, 0, 0);
@@ -859,51 +856,51 @@ void hevc_hv_uni_4t_8multx4_lsx(uint8_t *src,
     __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
 
     for (cnt = width8mult; cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src + src_stride_2x, 0);
         src += (8 - src_stride_4x);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
         dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
         dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
         dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-        LSX_DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                      dst32_r, dst43_r, dst54_r, dst65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                      dst32_l, dst43_l, dst54_l, dst65_l);
+        DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
+                  dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
+                  dst32_l, dst43_l, dst54_l, dst65_l);
 
         dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -914,10 +911,10 @@ void hevc_hv_uni_4t_8multx4_lsx(uint8_t *src,
         dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
         dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
-                      dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
+                  dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
@@ -958,34 +955,34 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
     __m128i out0_r, out1_r, out2_r, out3_r, out4_r, out5_r;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src4 = __lsx_vld(src + src_stride_4x, 0);
     src += (src_stride_4x + src_stride);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src5, src6, src7, src8);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3)
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src5, src6, src7, src8);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3)
     src4 = __lsx_vxori_b(src4, 128);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5, src6,
-                  src7, src8);
-
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
-                  mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
-                  mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
-                  mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
-                  mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
+    DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5, src6,
+              src7, src8);
+
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
+              mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
+              mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
 
     dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -997,14 +994,14 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
     dst7 = __lsx_hevc_filt_4tap_h(vec14, vec15, filt0, filt1);
     dst8 = __lsx_hevc_filt_4tap_h(vec16, vec17, filt0, filt1);
 
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_r, dst21_r, dst32_r, dst43_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_l, dst21_l, dst32_l, dst43_l);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_r, dst65_r, dst76_r, dst87_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_l, dst65_l, dst76_l, dst87_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
 
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -1019,13 +1016,11 @@ void hevc_hv_uni_4t_8x6_lsx(uint8_t *src,
     dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
     dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
 
-    LSX_DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
-                  dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r, out2_r, out3_r);
-    LSX_DUP2_ARG3(__lsx_vsrani_h_w, dst4_l, dst4_r, 6, dst5_l, dst5_r, 6,
-                  out4_r, out5_r);
-    LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
-                  out0, out1);
-    LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1)
+    DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
+              dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r, out2_r, out3_r);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst4_l, dst4_r, 6, dst5_l, dst5_r, 6, out4_r, out5_r);
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1)
     out2 = __lsx_vssrarni_b_h(out5_r, out4_r, 6);
     out2 = __lsx_vxori_b(out2, 128);
 
@@ -1073,58 +1068,58 @@ void hevc_hv_uni_4t_8multx4mult_lsx(uint8_t *src,
     __m128i out0_r, out1_r, out2_r, out3_r;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
     mask1 = __lsx_vaddi_bu(mask0, 2);
 
     for (cnt = width8mult; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
         src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
 
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
         dst0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dst2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                          src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src3,
-                          src4, src5, src6);
+            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
+                      src3, src4, src5, src6);
             src_tmp += src_stride_4x;
 
-            LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                          src3, src4, src5, src6);
+            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                      src3, src4, src5, src6);
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
-                          src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
-                          src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+            DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                      src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                      src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
 
             dst3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
             dst4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
             dst5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
             dst6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-            LSX_DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                          dst32_r, dst43_r, dst54_r, dst65_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
-                          dst32_l, dst43_l, dst54_l, dst65_l);
+            DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
+                      dst32_r, dst43_r, dst54_r, dst65_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6, dst5,
+                      dst32_l, dst43_l, dst54_l, dst65_l);
 
             dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
             dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -1135,12 +1130,12 @@ void hevc_hv_uni_4t_8multx4mult_lsx(uint8_t *src,
             dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
             dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-            LSX_DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
-                          dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r,
-                          out2_r, out3_r);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
-                          out0, out1);
-            LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+            DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+                      dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r,
+                      out2_r, out3_r);
+            DUP2_ARG3(__lsx_vssrarni_b_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
+                      out0, out1);
+            DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
             __lsx_vstelm_d(out0, dst_tmp, 0, 0);
             __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
             __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
@@ -1213,11 +1208,11 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
@@ -1225,46 +1220,46 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     src_tmp = src;
     dst_tmp = dst;
 
-    LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
     src_tmp += src_stride_3x;
 
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
     dsth0 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dsth1 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
     dsth2 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, dst10_l, dst21_l);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
+                  src3, src4, src5, src6);
         src_tmp += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                      src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                  src3, src4, src5, src6);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4, src4,
-                      mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6, src6,
-                      mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4, src4,
+                  mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6, src6,
+                  mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
 
         dsth3 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dsth4 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
         dsth5 = __lsx_hevc_filt_4tap_h(vec4, vec5, filt0, filt1);
         dsth6 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
-        LSX_DUP4_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
-                      dsth5, dst32_r, dst43_r, dst54_r, dst65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
-                      dsth5, dst32_l, dst43_l, dst54_l, dst65_l);
+        DUP4_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
+                  dsth5, dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4, dsth6,
+                  dsth5, dst32_l, dst43_l, dst54_l, dst65_l);
 
         dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -1275,10 +1270,10 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
         dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
         dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
-                      dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
+                  dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
 
         __lsx_vstelm_d(out0, dst_tmp, 0, 0);
         __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
@@ -1299,13 +1294,13 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
     mask3 = __lsx_vaddi_bu(mask2, 2);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
 
     dst10 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
     dst21 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -1315,20 +1310,20 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
     dst22 = __lsx_vreplvei_d(dst21, 1);
 
     for (loop_cnt = 2; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src3, src4, src5, src6);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                      src4, src5, src6)
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10)
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, src8, src4,
-                      mask2, src8, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, src10, src6,
-                      mask2, src10, src6, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
+                  src4, src5, src6)
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10)
+        DUP4_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, src8, src4,
+                  mask2, src8, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, src10, src6,
+                  mask2, src10, src6, mask3, vec4, vec5, vec6, vec7);
 
         dst73 = __lsx_hevc_filt_4tap_h(vec0, vec1, filt0, filt1);
         dst84 = __lsx_hevc_filt_4tap_h(vec2, vec3, filt0, filt1);
@@ -1336,8 +1331,8 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
         dst106 = __lsx_hevc_filt_4tap_h(vec6, vec7, filt0, filt1);
 
         dst32_r = __lsx_vilvl_h(dst73, dst22);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst84, dst73, dst95, dst84, dst43_r, dst54_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst84, dst73, dst95, dst84, dst43_r, dst54_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
         dst65_r = __lsx_vilvl_h(dst106, dst95);
         dst109_r = __lsx_vilvh_h(dst106, dst95);
         dst22 = __lsx_vreplvei_d(dst73, 1);
@@ -1352,10 +1347,10 @@ void hevc_hv_uni_4t_12w_lsx(uint8_t *src,
         dst6 = __lsx_hevc_filt_4tap_w(dst76_r, dst98_r, filt_h0, filt_h1);
         dst7 = __lsx_hevc_filt_4tap_w(dst87_r, dst109_r, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG3(__lsx_vsrani_h_w, dst1, dst0, 6, dst3, dst2, 6, dst5, dst4, 6,
-                      dst7, dst6, 6, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP4_ARG3(__lsx_vsrani_h_w, dst1, dst0, 6, dst3, dst2, 6, dst5, dst4, 6,
+                  dst7, dst6, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
 
         __lsx_vstelm_w(out0, dst, 0, 0);
         __lsx_vstelm_w(out0, dst + dst_stride, 0, 1);
diff --git a/libavcodec/loongarch/hevc_mc_uniw_lsx.c b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
index 619ef95960..541d814438 100644
--- a/libavcodec/loongarch/hevc_mc_uniw_lsx.c
+++ b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 #include "hevc_macros_lsx.h"
 
@@ -78,39 +78,38 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
     denom_vec = __lsx_vsrar_w(const_128, denom_vec);
     offset_vec = __lsx_vadd_w(denom_vec, offset_vec);
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
-                  filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x, 6,
+              filt0, filt1, filt2, filt3);
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
-                  filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (cnt = width >> 3; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src0,
-                      src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
+                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
         src_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                       mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
         dst0 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
                                       filt3);
         dst1 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
@@ -119,12 +118,12 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
                                       filt3);
         dst3 = __lsx_hevc_filt_8tap_h(vec12, vec13, vec14, vec15, filt0, filt1,
                                       filt2, filt3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                      mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                      mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                      mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
         dst4 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
                                       filt3);
         dst5 = __lsx_hevc_filt_8tap_h(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
@@ -132,19 +131,19 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
         dst6 = __lsx_hevc_filt_8tap_h(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
                                       filt3);
 
-        LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                      dst10_r, dst32_r, dst54_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                      dst10_l, dst32_l, dst54_l, dst21_l);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
+                  dst10_r, dst32_r, dst54_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
+                  dst10_l, dst32_l, dst54_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
 
         for (loop_cnt = height >> 1; loop_cnt--;) {
-            LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
+            DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src7, src8);
             src_tmp += src_stride_2x;
-            LSX_DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
-                          src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            DUP2_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src7, src8);
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
             dst7 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
                                           filt2, filt3);
             dst76_r = __lsx_vilvl_h(dst7, dst6);
@@ -153,11 +152,11 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
             dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
-            LSX_DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
 
             /* row 8 */
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
-                          src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
+                      src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
             dst8 = __lsx_hevc_filt_8tap_h(vec0, vec1, vec2, vec3, filt0, filt1,
                                           filt2, filt3);
 
@@ -167,22 +166,18 @@ void hevc_hv_uniwgt_8t_8multx2mult_lsx(uint8_t *src,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
             dst1_l = __lsx_hevc_filt_8tap_w(dst21_l, dst43_l, dst65_l, dst87_l,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
-            LSX_DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
 
-            LSX_DUP2_ARG2(__lsx_vmul_w, dst0_r, weight_vec, dst0_l, weight_vec, dst0_r,
-                          dst0_l);
-            LSX_DUP2_ARG2(__lsx_vmul_w, dst1_r, weight_vec, dst1_l, weight_vec, dst1_r,
-                          dst1_l);
-            LSX_DUP4_ARG2(__lsx_vsrar_w, dst0_r, rnd_vec, dst1_r, rnd_vec, dst0_l,
-                          rnd_vec, dst1_l, rnd_vec, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP2_ARG2(__lsx_vmul_w, dst0_r, weight_vec, dst0_l, weight_vec, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vmul_w, dst1_r, weight_vec, dst1_l, weight_vec, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrar_w, dst0_r, rnd_vec, dst1_r, rnd_vec, dst0_l,
+                      rnd_vec, dst1_l, rnd_vec, dst0_r, dst1_r, dst0_l, dst1_l);
 
-            LSX_DUP2_ARG2(__lsx_vadd_w, dst0_r, offset_vec, dst0_l, offset_vec, dst0_r,
-                          dst0_l);
-            LSX_DUP2_ARG2(__lsx_vadd_w, dst1_r, offset_vec, dst1_l, offset_vec, dst1_r,
-                          dst1_l);
-            LSX_DUP4_ARG1(__lsx_clamp255_w, dst0_r, dst1_r, dst0_l, dst1_l, dst0_r,
-                          dst1_r, dst0_l, dst1_l);
-            LSX_DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+            DUP2_ARG2(__lsx_vadd_w, dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vadd_w, dst1_r, offset_vec, dst1_l, offset_vec, dst1_r, dst1_l);
+            DUP4_ARG1(__lsx_vclip255_w, dst0_r, dst1_r, dst0_l, dst1_l, dst0_r,
+                      dst1_r, dst0_l, dst1_l);
+            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
             dst0_r = __lsx_vpickev_b(dst1_r, dst0_r);
 
             __lsx_vstelm_d(dst0_r, dst_tmp, 0, 0);
diff --git a/libavcodec/loongarch/hevcdsp_lsx.c b/libavcodec/loongarch/hevcdsp_lsx.c
index 9e2d9e29b4..e4779c77a6 100644
--- a/libavcodec/loongarch/hevcdsp_lsx.c
+++ b/libavcodec/loongarch/hevcdsp_lsx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "hevcdsp_lsx.h"
 #include "hevc_macros_lsx.h"
 
@@ -46,7 +46,7 @@ static void hevc_copy_4w_lsx(uint8_t *src, int32_t src_stride,
         __m128i src0, src1;
         __m128i in0;
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
         src0 = __lsx_vilvl_w(src1, src0);
         in0 = __lsx_vilvl_b(zero, src0);
         in0 = __lsx_vslli_h(in0, 6);
@@ -56,11 +56,11 @@ static void hevc_copy_4w_lsx(uint8_t *src, int32_t src_stride,
         __m128i src0, src1, src2, src3;
         __m128i in0, in1;
 
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
+        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
         __lsx_vstelm_d(in0, dst, 0, 0);
         __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(in1, dst + dst_stride_2x, 0, 0);
@@ -70,20 +70,18 @@ static void hevc_copy_4w_lsx(uint8_t *src, int32_t src_stride,
         __m128i in0, in1, in2, in3;
         uint32_t loop_cnt;
         for (loop_cnt = (height >> 3); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
-                          src + src_stride_2x, 0, src + src_stride_3x, 0,
-                          src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
             src += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,src + src_stride_2x,
-                          0, src + src_stride_3x, 0, src4, src5, src6, src7);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,src + src_stride_2x,
+                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
             src += src_stride_4x;
 
-            LSX_DUP4_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src5, src4, src7, src6,
-                          src0, src1, src2, src3);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                          in0, in1, in2, in3);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1,
-                          in2, in3);
+            DUP4_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src5, src4, src7, src6,
+                      src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                      in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
 
             __lsx_vstelm_d(in0, dst, 0, 0);
             __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
@@ -117,29 +115,29 @@ static void hevc_copy_6w_lsx(uint8_t *src, int32_t src_stride,
     __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
         src += src_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in4, in5, in6, in7);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
 
-        LSX_DUP4_ARG2(__lsx_vpickve2gr_du, in0, 0, in1, 0, in2, 0, in3, 0, out0_m,
-                      out1_m, out2_m, out3_m);
-        LSX_DUP4_ARG2(__lsx_vpickve2gr_du, in4, 0, in5, 0, in6, 0, in7, 0, out4_m,
-                      out5_m, out6_m, out7_m);
+        DUP4_ARG2(__lsx_vpickve2gr_du, in0, 0, in1, 0, in2, 0, in3, 0, out0_m,
+                  out1_m, out2_m, out3_m);
+        DUP4_ARG2(__lsx_vpickve2gr_du, in4, 0, in5, 0, in6, 0, in7, 0, out4_m,
+                  out5_m, out6_m, out7_m);
 
-        LSX_DUP4_ARG2(__lsx_vpickve2gr_wu, in0, 2, in1, 2, in2, 2, in3, 2, out8_m,
-                      out9_m, out10_m, out11_m);
-        LSX_DUP4_ARG2(__lsx_vpickve2gr_wu, in4, 2, in5, 2, in6, 2, in7, 2, out12_m,
-                      out13_m, out14_m, out15_m);
+        DUP4_ARG2(__lsx_vpickve2gr_wu, in0, 2, in1, 2, in2, 2, in3, 2, out8_m,
+                  out9_m, out10_m, out11_m);
+        DUP4_ARG2(__lsx_vpickve2gr_wu, in4, 2, in5, 2, in6, 2, in7, 2, out12_m,
+                  out13_m, out14_m, out15_m);
 
         *(uint64_t *)dst = out0_m;
         *(uint32_t *)(dst + 4) = out8_m;
@@ -184,20 +182,20 @@ static void hevc_copy_8w_lsx(uint8_t *src, int32_t src_stride,
         __m128i src0, src1;
         __m128i in0, in1;
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
+        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
         __lsx_vst(in0, dst, 0);
         __lsx_vst(in1, dst + dst_stride, 0);
     } else if (4 == height) {
         __m128i src0, src1, src2, src3;
         __m128i in0, in1, in2, in3;
 
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0, in1, in2, in3);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
         __lsx_vst(in0, dst, 0);
         __lsx_vst(in1, dst + dst_stride, 0);
         __lsx_vst(in2, dst + dst_stride_2x, 0);
@@ -206,15 +204,15 @@ static void hevc_copy_8w_lsx(uint8_t *src, int32_t src_stride,
         __m128i src0, src1, src2, src3, src4, src5;
         __m128i in0, in1, in2, in3, in4, in5;
 
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3, in0,
-                      in1, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4, in5);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-        LSX_DUP2_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in4, in5);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3, in0,
+                  in1, in2, in3);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4, in5);
+        DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
+        DUP2_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in4, in5);
         __lsx_vst(in0, dst, 0);
         __lsx_vst(in1, dst + dst_stride, 0);
         __lsx_vst(in2, dst + dst_stride_2x, 0);
@@ -227,21 +225,19 @@ static void hevc_copy_8w_lsx(uint8_t *src, int32_t src_stride,
         __m128i in0, in1, in2, in3, in4, in5, in6, in7;
 
         for (loop_cnt = (height >> 3); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
-                          src + src_stride_2x, 0, src + src_stride_3x, 0, src0,
-                          src1, src2, src3);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
             src += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
-                          src + src_stride_2x, 0, src + src_stride_3x, 0, src4,
-                          src5, src6, src7);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
             src += src_stride_4x;
 
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                          in0, in1, in2, in3);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                          in4, in5, in6, in7);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
+            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                      in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                      in4, in5, in6, in7);
+            DUP4_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in2, 6, in3, 6, in0, in1, in2, in3);
+            DUP4_ARG2(__lsx_vslli_h, in4, 6, in5, 6, in6, 6, in7, 6, in4, in5, in6, in7);
             __lsx_vst(in0, dst, 0);
             __lsx_vst(in1, dst + dst_stride, 0);
             __lsx_vst(in2, dst + dst_stride_2x, 0);
@@ -272,20 +268,20 @@ static void hevc_copy_12w_lsx(uint8_t *src, int32_t src_stride,
     __m128i in0, in1, in0_r, in1_r, in2_r, in3_r;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
         src += src_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                       in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
+        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -296,13 +292,13 @@ static void hevc_copy_12w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vstelm_d(in1, dst + dst_stride_3x, 16, 1);
         dst += dst_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_w, src5, src4, src7, src6, src0, src1);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
-        LSX_DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP2_ARG2(__lsx_vilvh_w, src5, src4, src7, src6, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, in0, in1);
+        DUP2_ARG2(__lsx_vslli_h, in0, 6, in1, 6, in0, in1);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -332,17 +328,17 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
         __m128i in0_r, in1_r, in2_r, in3_r;
         __m128i in0_l, in1_l, in2_l, in3_l;
 
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -357,23 +353,23 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
         __m128i in0_r, in1_r, in2_r, in3_r;
         __m128i in0_l, in1_l, in2_l, in3_l;
 
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src8, src9, src10, src11);
-
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src8, src9, src10, src11);
+
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -384,14 +380,14 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst + dst_stride_3x, 16);
         dst += dst_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -402,14 +398,14 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst + dst_stride_3x, 16);
         dst += dst_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src8, zero, src9, zero, src10, zero, src11,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src8, zero, src9, zero, src10, zero, src11,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src8, zero, src9, zero, src10, zero, src11,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src8, zero, src9, zero, src10, zero, src11,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -424,22 +420,20 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
         __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
         for (loop_cnt = (height >> 3); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
-                          src + src_stride_2x, 0, src + src_stride_3x, 0, src0,
-                          src1, src2, src3);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                      src + src_stride_3x, 0, src0, src1, src2, src3);
             src += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
-                          src + src_stride_2x, 0, src + src_stride_3x, 0, src4,
-                          src5, src6, src7);
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                      src + src_stride_3x, 0, src4, src5, src6, src7);
             src += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                          in0_r, in1_r, in2_r, in3_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                          in0_l, in1_l, in2_l, in3_l);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                          in1_r, in2_r, in3_r);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                          in1_l, in2_l, in3_l);
+            DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                      in0_r, in1_r, in2_r, in3_r);
+            DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                      in0_l, in1_l, in2_l, in3_l);
+            DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                      in1_r, in2_r, in3_r);
+            DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                      in1_l, in2_l, in3_l);
             __lsx_vst(in0_r, dst, 0);
             __lsx_vst(in1_r, dst + dst_stride, 0);
             __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -450,14 +444,14 @@ static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
             __lsx_vst(in3_l, dst + dst_stride_3x, 16);
             dst += dst_stride_4x;
 
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                          in0_r, in1_r, in2_r, in3_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                          in0_l, in1_l, in2_l, in3_l);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                          in1_r, in2_r, in3_r);
-            LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                          in1_l, in2_l, in3_l);
+            DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                      in0_r, in1_r, in2_r, in3_r);
+            DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                      in0_l, in1_l, in2_l, in3_l);
+            DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                      in1_r, in2_r, in3_r);
+            DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                      in1_l, in2_l, in3_l);
             __lsx_vst(in0_r, dst, 0);
             __lsx_vst(in1_r, dst + dst_stride, 0);
             __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -487,19 +481,19 @@ static void hevc_copy_24w_lsx(uint8_t *src, int32_t src_stride,
     __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
-                      16, src + src_stride_3x, 16, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
+                  16, src + src_stride_3x, 16, src4, src5, src6, src7);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in1_r, dst + dst_stride, 0);
         __lsx_vst(in2_r, dst + dst_stride_2x, 0);
@@ -508,10 +502,10 @@ static void hevc_copy_24w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in1_l, dst + dst_stride, 16);
         __lsx_vst(in2_l, dst + dst_stride_2x, 16);
         __lsx_vst(in3_l, dst + dst_stride_3x, 16);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
         __lsx_vst(in0_r, dst, 32);
         __lsx_vst(in1_r, dst + dst_stride, 32);
         __lsx_vst(in2_r, dst + dst_stride_2x, 32);
@@ -533,20 +527,20 @@ static void hevc_copy_32w_lsx(uint8_t *src, int32_t src_stride,
     __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src2, src4, src6);
-        LSX_DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
-                      16, src + src_stride_3x, 16, src1, src3, src5, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src2, src4, src6);
+        DUP4_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src + src_stride_2x,
+                  16, src + src_stride_3x, 16, src1, src3, src5, src7);
         src += src_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -558,14 +552,14 @@ static void hevc_copy_32w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst, 48);
         dst += dst_stride;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -591,31 +585,31 @@ static void hevc_copy_48w_lsx(uint8_t *src, int32_t src_stride,
     __m128i in0_l, in1_l, in2_l, in3_l, in4_l, in5_l;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 32);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src3, src4);
         src5 = __lsx_vld(src, 32);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src6, src7);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src6, src7);
         src8 = __lsx_vld(src, 32);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src9, src10);
         src11 = __lsx_vld(src, 32);
         src += src_stride;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4_r, in5_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, in4_l, in5_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
-                      in5_r, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
+                  in5_r, in4_l, in5_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -631,18 +625,18 @@ static void hevc_copy_48w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in5_l, dst, 80);
         dst += dst_stride;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src6, zero, src7, zero, src8, zero, src9,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src6, zero, src7, zero, src8, zero, src9,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, zero, src10, zero, src11, in4_r, in5_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, zero, src10, zero, src11, in4_l, in5_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
-                      in5_r, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src6, zero, src7, zero, src8, zero, src9,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src6, zero, src7, zero, src8, zero, src9,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vilvl_b, zero, src10, zero, src11, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src10, zero, src11, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in4_r, 6, in5_r, 6, in4_l, 6, in5_l, 6, in4_r,
+                  in5_r, in4_l, in5_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -671,20 +665,18 @@ static void hevc_copy_64w_lsx(uint8_t *src, int32_t src_stride,
 
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2,
-                      src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6,
-                      src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6, src7);
         src += src_stride;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vilvl_b, zero, src0, zero, src1, zero, src2, zero, src3,
                       in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
                       in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
                       in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
                       in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
@@ -696,14 +688,14 @@ static void hevc_copy_64w_lsx(uint8_t *src, int32_t src_stride,
         __lsx_vst(in3_l, dst, 112);
         dst += dst_stride;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_r, in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
-                      in0_l, in1_l, in2_l, in3_l);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
-                      in1_r, in2_r, in3_r);
-        LSX_DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
-                      in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vslli_h, in0_r, 6, in1_r, 6, in2_r, 6, in3_r, 6, in0_r,
+                  in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
         __lsx_vst(in0_r, dst, 0);
         __lsx_vst(in0_l, dst, 16);
         __lsx_vst(in1_r, dst, 32);
@@ -737,39 +729,39 @@ static void hevc_hz_8t_4w_lsx(uint8_t *src, int32_t src_stride,
 
     src -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128,
-                      src4, src5, src6, src7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1, src0,
-                      mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                      vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask0, src3, src2, mask1, src3, src2,
-                      mask2, src3, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                      vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src4, mask0, src5, src4, mask1, src5, src4,
-                      mask2, src5, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                      vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src6, mask0, src7, src6, mask1, src7, src6,
-                      mask2, src7, src6, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                      vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128,
+                  src4, src5, src6, src7);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1, src0,
+                  mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask0, src3, src2, mask1, src3, src2,
+                  mask2, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
+                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src4, mask0, src5, src4, mask1, src5, src4,
+                  mask2, src5, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
+                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src6, mask0, src7, src6, mask1, src7, src6,
+                  mask2, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
+                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
 
         __lsx_vstelm_d(dst0, dst, 0, 0);
         __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
@@ -805,36 +797,35 @@ static void hevc_hz_8t_8w_lsx(uint8_t *src, int32_t src_stride,
 
     src -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                      vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                      vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                      vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        dst3 = const_vec;
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst3, vec0, filt0, dst3, vec1, filt1, dst3,
-                      vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
+                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
+                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
+                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst + dst_stride, 0);
@@ -863,53 +854,53 @@ static void hevc_hz_8t_12w_lsx(uint8_t *src, int32_t src_stride,
 
     src -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     mask4 = __lsx_vld(ff_hevc_mask_arr, 16);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
+    DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask4, 6);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src + src_stride_2x, 8,
-                      src + src_stride_3x, 8, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src + src_stride_2x, 8,
+                  src + src_stride_3x, 8, src4, src5, src6, src7);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
-                      src5, src6, src7);
-
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask4, src7, src6, mask4, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
-                      dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask5, src7, src6, mask5, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask6, src7, src6, mask6, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask7, src7, src6, mask7, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128, src4,
+                  src5, src6, src7);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask4, src7, src6, mask4, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
+                  dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask5, src7, src6, mask5, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask6, src7, src6, mask6, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask7, src7, src6, mask7, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst + dst_stride, 0);
@@ -943,36 +934,36 @@ static void hevc_hz_8t_16w_lsx(uint8_t *src, int32_t src_stride,
     src -= 3;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4,
-                  filter, 6, filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
-        LSX_DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
         src += src_stride_2x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
+                  dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst2, dst + dst_stride, 0);
@@ -996,49 +987,48 @@ static void hevc_hz_8t_24w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src2, src3);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src2, src3);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                      mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src3, mask0, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
-                      dst4, dst5);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                      mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask5, src3, src3, mask1, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                      mask2, src2, src2, mask2, vec0, vec1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask6, src3, src3, mask2, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                      mask3, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask7, src3, src3, mask3, vec4, vec5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
+                  mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src3, mask0, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+                  dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
+                  mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask5, src3, src3, mask1, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
+                  mask2, src2, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask6, src3, src3, mask2, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
+                  mask3, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask7, src3, src3, mask3, vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
@@ -1065,39 +1055,39 @@ static void hevc_hz_8t_32w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2,  filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2,  filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 24);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                      vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
-                      mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                      vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                      vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                      vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
+                  mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
+                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
+                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
+                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
 
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
@@ -1121,54 +1111,54 @@ static void hevc_hz_8t_48w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask0, 14);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 32);
         src3 = __lsx_vld(src, 40);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
-                      mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
-                      mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
-                      mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
-                      vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
-                      mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
-                      vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
+                  mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
+                  dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1, src1,
+                  mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1, src1,
+                  mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt2, dst1, vec1, filt2, dst2,
+                  vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1, src1,
+                  mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt3, dst1, vec1, filt3, dst2,
+                  vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         __lsx_vst(dst2, dst, 32);
         __lsx_vst(dst3, dst, 48);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
-                      dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0, vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, const_vec, vec5, filt0,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt1, dst5, vec5, filt1, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2, vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt2, dst5, vec5, filt2, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3, vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, dst4, vec4, filt3, dst5, vec5, filt3, dst4, dst5);
         __lsx_vst(dst4, dst, 64);
         __lsx_vst(dst5, dst, 80);
         dst += dst_stride;
@@ -1189,73 +1179,69 @@ static void hevc_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
     __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
 
     src -= 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
-                  mask3, mask4);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6)
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1, mask2,
+              mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6)
     mask7 = __lsx_vaddi_bu(mask0, 14);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16,  src, 32, src, 48, src0, src1, src2,
-                      src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16,  src, 32, src, 48, src0, src1, src2, src3);
         src4 = __lsx_vld(src, 56);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
         src4 = __lsx_vxori_b(src4, 128);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                      vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
         __lsx_vst(dst0, dst, 0);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
-                      mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
-                      vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1, src0,
+                  mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst1, vec1, filt1, dst1,
+                  vec2, filt2, dst1, vec3, filt3, dst1, dst1, dst1, dst1);
         __lsx_vst(dst1, dst, 16);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
-                      vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst2, vec1, filt1, dst2,
+                  vec2, filt2, dst2, vec3, filt3, dst2, dst2, dst2, dst2);
         __lsx_vst(dst2, dst, 32);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src1, mask4, src2, src1, mask5, src2, src1,
-                      mask6, src2, src1, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
-                      vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src1, mask4, src2, src1, mask5, src2, src1,
+                  mask6, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3,
+                  vec2, filt2, dst3, vec3, filt3, dst3, dst3, dst3, dst3);
         __lsx_vst(dst3, dst, 48);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
-                      vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
+                  vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
         __lsx_vst(dst4, dst, 64);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src2, mask5, src3, src2,
-                      mask6, src3, src2, mask7, vec0, vec1, vec2, vec3);
-        dst5 = const_vec;
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst5, vec0, filt0, dst5, vec1, filt1, dst5,
-                      vec2, filt2, dst5, vec3, filt3, dst5, dst5, dst5, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src2, mask5, src3, src2,
+                  mask6, src3, src2, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst5, vec1, filt1, dst5,
+                  vec2, filt2, dst5, vec3, filt3, dst5, dst5, dst5, dst5);
         __lsx_vst(dst5, dst, 80);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
-        dst6 = const_vec;
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst6, vec0, filt0, dst6, vec1, filt1, dst6,
-                      vec2, filt2, dst6, vec3, filt3, dst6, dst6, dst6, dst6);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst6, vec1, filt1, dst6,
+                  vec2, filt2, dst6, vec3, filt3, dst6, dst6, dst6, dst6);
         __lsx_vst(dst6, dst, 96);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                      mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        dst7 = const_vec;
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst7, vec0, filt0, dst7, vec1, filt1, dst7,
-                      vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1, dst7,
+                  vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
         __lsx_vst(dst7, dst, 112);
         dst += dst_stride;
     }
@@ -1286,53 +1272,53 @@ static void hevc_vt_8t_4w_lsx(uint8_t *src, int32_t src_stride,
     src -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                  src10_r, src32_r, src54_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_d, src21_r, src10_r, src43_r, src32_r, src2110, src4332);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP2_ARG2(__lsx_vilvl_d, src21_r, src10_r, src43_r, src32_r, src2110, src4332);
     src6554 = __lsx_vilvl_d(src65_r, src54_r);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src2110, 128, src4332, 128, src2110, src4332);
+    DUP2_ARG2(__lsx_vxori_b, src2110, 128, src4332, 128, src2110, src4332);
     src6554 = __lsx_vxori_b(src6554, 128);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,  src + src_stride_2x,
-                      0, src + src_stride_3x, 0, src11, src12, src13, src14);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0,  src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src11, src12, src13, src14);
         src += src_stride_4x;
 
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src11, src10, src12, src11, src13, src12, src14,
-                      src13, src1110_r, src1211_r, src1312_r, src1413_r);
-        LSX_DUP4_ARG2(__lsx_vilvl_d, src87_r, src76_r, src109_r, src98_r, src1211_r,
-                      src1110_r, src1413_r, src1312_r, src8776, src10998, src12111110,
-                      src14131312);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src8776, 128, src10998, 128, src12111110, 128,
-                      src14131312, 128, src8776, src10998, src12111110, src14131312);
-
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src2110, filt0, dst10, src4332, filt1,
-                      dst10, src6554, filt2, dst10, src8776, filt3, dst10, dst10, dst10,
-                      dst10);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src4332, filt0, dst32, src6554, filt1,
-                      dst32, src8776, filt2, dst32, src10998, filt3, dst32, dst32,
-                      dst32, dst32);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src6554, filt0, dst54, src8776, filt1,
-                      dst54, src10998, filt2, dst54, src12111110, filt3, dst54, dst54,
-                      dst54, dst54);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src8776, filt0, dst76, src10998,
-                      filt1, dst76, src12111110, filt2, dst76, src14131312, filt3, dst76,
-                      dst76, dst76, dst76);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vilvl_b, src11, src10, src12, src11, src13, src12, src14,
+                  src13, src1110_r, src1211_r, src1312_r, src1413_r);
+        DUP4_ARG2(__lsx_vilvl_d, src87_r, src76_r, src109_r, src98_r, src1211_r,
+                  src1110_r, src1413_r, src1312_r, src8776, src10998, src12111110,
+                  src14131312);
+        DUP4_ARG2(__lsx_vxori_b, src8776, 128, src10998, 128, src12111110, 128,
+                  src14131312, 128, src8776, src10998, src12111110, src14131312);
+
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst10, src4332, filt1,
+                  dst10, src6554, filt2, dst10, src8776, filt3, dst10, dst10, dst10,
+                  dst10);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src4332, filt0, dst32, src6554, filt1,
+                  dst32, src8776, filt2, dst32, src10998, filt3, dst32, dst32,
+                  dst32, dst32);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src6554, filt0, dst54, src8776, filt1,
+                  dst54, src10998, filt2, dst54, src12111110, filt3, dst54, dst54,
+                  dst54, dst54);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src8776, filt0, dst76, src10998,
+                  filt1, dst76, src12111110, filt2, dst76, src14131312, filt3, dst76,
+                  dst76, dst76, dst76);
 
         __lsx_vstelm_d(dst10, dst, 0, 0);
         __lsx_vstelm_d(dst10, dst + dst_stride, 0, 1);
@@ -1373,44 +1359,44 @@ static void hevc_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
     src -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4,
-                  filter, 6, filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                  src10_r, src32_r, src54_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                      dst0_r, dst0_r, dst0_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                      dst1_r, dst1_r, dst1_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                      filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                      dst2_r, dst2_r, dst2_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                      filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                      dst3_r, dst3_r, dst3_r);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
+                  dst0_r, dst0_r, dst0_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
+                  dst1_r, dst1_r, dst1_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
+                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
+                  dst2_r, dst2_r, dst2_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
+                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
+                  dst3_r, dst3_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst1_r, dst + dst_stride, 0);
@@ -1453,57 +1439,57 @@ static void hevc_vt_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     src -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                  src10_r, src32_r, src54_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                  src10_l, src32_l, src54_l, src21_l);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
-    LSX_DUP2_ARG2(__lsx_vilvl_d, src21_l, src10_l, src43_l, src32_l, src2110, src4332);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_l, src32_l, src54_l, src21_l);
+    DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+    DUP2_ARG2(__lsx_vilvl_d, src21_l, src10_l, src43_l, src32_l, src2110, src4332);
     src6554 = __lsx_vilvl_d(src65_l, src54_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src+ src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src+ src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_r, src87_r, src98_r, src109_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src76_l, src87_l, src98_l, src109_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_d, src87_l, src76_l, src109_l, src98_l, src8776, src10998);
-
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
-                      dst0_r, dst0_r, dst0_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
-                      dst1_r, dst1_r, dst1_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                      filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
-                      dst2_r, dst2_r, dst2_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                      filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
-                      dst3_r, dst3_r, dst3_r);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
-                      filt1, dst0_l, src6554, filt2, dst0_l, src8776, filt3, dst0_l,
-                      dst0_l, dst0_l, dst0_l);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src4332, filt0, dst1_l, src6554,
-                      filt1, dst1_l, src8776, filt2, dst1_l, src10998, filt3, dst1_l,
-                      dst1_l, dst1_l, dst1_l);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_l, src87_l, src98_l, src109_l);
+        DUP2_ARG2(__lsx_vilvl_d, src87_l, src76_l, src109_l, src98_l, src8776, src10998);
+
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3, dst0_r,
+                  dst0_r, dst0_r, dst0_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3, dst1_r,
+                  dst1_r, dst1_r, dst1_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
+                  filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3, dst2_r,
+                  dst2_r, dst2_r, dst2_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
+                  filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3, dst3_r,
+                  dst3_r, dst3_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src2110, filt0, dst0_l, src4332,
+                  filt1, dst0_l, src6554, filt2, dst0_l, src8776, filt3, dst0_l,
+                  dst0_l, dst0_l, dst0_l);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src4332, filt0, dst1_l, src6554,
+                  filt1, dst1_l, src8776, filt2, dst1_l, src10998, filt3, dst1_l,
+                  dst1_l, dst1_l, dst1_l);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst1_r, dst + dst_stride, 0);
@@ -1558,67 +1544,66 @@ static void hevc_vt_8t_16multx4mult_lsx(uint8_t *src,
     src -= src_stride_3x;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
-                  filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
 
     for (cnt = width >> 4; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
+                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
         src_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_r, src32_r, src54_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      src10_l, src32_l, src54_l, src21_l);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                          src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                          src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
+                      src7, src8, src9, src10);
             src_tmp += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                          src7, src8, src9, src10);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src76_r, src87_r, src98_r, src109_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src76_l, src87_l, src98_l, src109_l);
-
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                          filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3,
-                          dst0_r, dst0_r, dst0_r, dst0_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                          filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3,
-                          dst1_r, dst1_r, dst1_r, dst1_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
-                          filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3,
-                          dst2_r, dst2_r, dst2_r, dst2_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
-                          filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3,
-                          dst3_r, dst3_r, dst3_r, dst3_r);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                          filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3,
-                          dst0_l, dst0_l, dst0_l, dst0_l);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                          filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3,
-                          dst1_l, dst1_l, dst1_l, dst1_l);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src32_l, filt0, dst2_l, src54_l,
-                          filt1, dst2_l, src76_l, filt2, dst2_l, src98_l, filt3,
-                          dst2_l, dst2_l, dst2_l, dst2_l);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, src43_l, filt0, dst3_l, src65_l,
-                          filt1, dst3_l, src87_l, filt2, dst3_l, src109_l, filt3,
-                          dst3_l, dst3_l, dst3_l, dst3_l);
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                      src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src76_r, src87_r, src98_r, src109_r);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src76_l, src87_l, src98_l, src109_l);
+
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                      filt1, dst0_r, src54_r, filt2, dst0_r, src76_r, filt3,
+                      dst0_r, dst0_r, dst0_r, dst0_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                      filt1, dst1_r, src65_r, filt2, dst1_r, src87_r, filt3,
+                      dst1_r, dst1_r, dst1_r, dst1_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst2_r, src54_r,
+                      filt1, dst2_r, src76_r, filt2, dst2_r, src98_r, filt3,
+                      dst2_r, dst2_r, dst2_r, dst2_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst3_r, src65_r,
+                      filt1, dst3_r, src87_r, filt2, dst3_r, src109_r, filt3,
+                      dst3_r, dst3_r, dst3_r, dst3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                      filt1, dst0_l, src54_l, filt2, dst0_l, src76_l, filt3,
+                      dst0_l, dst0_l, dst0_l, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                      filt1, dst1_l, src65_l, filt2, dst1_l, src87_l, filt3,
+                      dst1_l, dst1_l, dst1_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst2_l, src54_l,
+                      filt1, dst2_l, src76_l, filt2, dst2_l, src98_l, filt3,
+                      dst2_l, dst2_l, dst2_l, dst2_l);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst3_l, src65_l,
+                      filt1, dst3_l, src87_l, filt2, dst3_l, src109_l, filt3,
+                      dst3_l, dst3_l, dst3_l, dst3_l);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
             __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
@@ -1719,69 +1704,69 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
     mask0 = __lsx_vld(ff_hevc_mask_arr, 16);
 
     src -= src_stride_3x + 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
-                  filter_x, 6, filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
 
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
-                  filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask0, src3, src0, mask1, src3, src0,
-                  mask2, src3, src0, mask3, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask0, src4, src1, mask1, src4, src1,
-                  mask2, src4, src1, mask3, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask0, src5, src2, mask1, src5, src2,
-                  mask2, src5, src2, mask3, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask0, src6, src3, mask1, src6, src3,
-                  mask2, src6, src3, mask3, vec12, vec13, vec14, vec15);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
-                  vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
-                  vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
-                  vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
-                  vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
+    DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask0, src3, src0, mask1, src3, src0,
+              mask2, src3, src0, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask0, src4, src1, mask1, src4, src1,
+              mask2, src4, src1, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask0, src5, src2, mask1, src5, src2,
+              mask2, src5, src2, mask3, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask0, src6, src3, mask1, src6, src3,
+              mask2, src6, src3, mask3, vec12, vec13, vec14, vec15);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
+              vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
+              vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
+              vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
+              vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
     dst32_r = __lsx_vilvl_h(dst63, dst52);
     dst65_r = __lsx_vilvh_h(dst63, dst52);
     dst66 = __lsx_vreplvei_d(dst63, 1);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask0, src9, src7, mask1, src9, src7,
-                      mask2, src9, src7, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask0, src10, src8, mask1, src10, src8,
-                      mask2, src10, src8, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask0, src9, src7, mask1, src9, src7,
+                  mask2, src9, src7, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask0, src10, src8, mask1, src10, src8,
+                  mask2, src10, src8, mask3, vec4, vec5, vec6, vec7);
 
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
-                      dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97, dst97);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1,
-                      dst108, vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108, dst108);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
+                  dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97, dst97);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1,
+                  dst108, vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108, dst108);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
         dst109_r = __lsx_vilvh_h(dst108, dst97);
         dst66 = __lsx_vreplvei_d(dst97, 1);
         dst98_r = __lsx_vilvl_h(dst66, dst108);
@@ -1794,9 +1779,9 @@ static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
                                         filt_h0, filt_h1, filt_h2, filt_h3);
         dst3_r = __lsx_hevc_filt_8tap_w(dst43_r, dst65_r, dst87_r, dst109_r,
                                         filt_h0, filt_h1, filt_h2, filt_h3);
-        LSX_DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
-                      dst0_r, dst1_r, dst2_r, dst3_r);
-        LSX_DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
+        DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(dst2_r, dst + dst_stride_2x, 0, 0);
@@ -1842,80 +1827,79 @@ static void hevc_hv_8t_8multx1mult_lsx(uint8_t *src,
     __m128i mask0 = {0x403030202010100, 0x807070606050504};
 
     src -= src_stride_3x + 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
-                  filter_x, 6, filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
 
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
-                  filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width >> 3; cnt--;) {
         src_tmp = src;
         dst_tmp = dst;
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src_tmp + src_stride_2x,
+                  0, src_tmp + src_stride_3x, 0, src0, src1, src2, src3);
         src_tmp += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
         /* row 0 row 1 row 2 row 3 */
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                      mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                      mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                      mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                      mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                      vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
-                      vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
-                      vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1,
-                      dst3, vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
+                  vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
+                  vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1,
+                  dst3, vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
 
         /* row 4 row 5 row 6 */
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                      mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                      mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                      mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst4, filt1, vec1, dst4,
-                      vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
-                      vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
-                      vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, filt1, vec1, dst4,
+                  vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
+                  vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
+                  vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
 
         for (loop_cnt = height; loop_cnt--;) {
             src7 = __lsx_vld(src_tmp, 0);
             src7 = __lsx_vxori_b(src7, 128);
             src_tmp += src_stride;
 
-            LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
-                          src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-            LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1,
-                          dst7, vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1,
+                      dst7, vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
 
-            LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                          dst10_r, dst32_r, dst54_r, dst76_r);
-            LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                          dst10_l, dst32_l, dst54_l, dst76_l);
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                      dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                      dst10_l, dst32_l, dst54_l, dst76_l);
 
             dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r,
                                             filt_h0, filt_h1, filt_h2, filt_h3);
@@ -1977,81 +1961,81 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     __m128i dst0_r, dst0_l, dst1_r, dst2_r, dst3_r;
 
     src -= src_stride_3x + 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
-                  filter_x, 6, filt0, filt1, filt2, filt3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
 
-    LSX_DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
-                  filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     src_tmp = src;
     dst_tmp = dst;
 
-    LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+              src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
+              src0, src1, src2, src3);
     src_tmp += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src_tmp + src_stride_2x, 0);
     src_tmp += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
-                  src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
     /* row 0 row 1 row 2 row 3 */
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
-                  mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
-                  mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
-                  mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
-                  mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
-                  vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
-                  vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
-                  vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1, dst3,
-                  vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+              mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+              mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+              mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+              mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0,
+              vec2, filt2, dst0, vec3, filt3, dst0, dst0, dst0, dst0);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst1, vec5, filt1, dst1,
+              vec6, filt2, dst1, vec7, filt3, dst1, dst1, dst1, dst1);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst2, vec9, filt1, dst2,
+              vec10, filt2, dst2, vec11, filt3, dst2, dst2, dst2, dst2);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst3, vec13, filt1, dst3,
+              vec14, filt2, dst3, vec15, filt3, dst3, dst3, dst3, dst3);
 
     /* row 4 row 5 row 6 */
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
-                  mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
-                  mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
-                  mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
-                  vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
-                  vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+              mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+              mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+              mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst4, vec1, filt1, dst4,
+              vec2, filt2, dst4, vec3, filt3, dst4, dst4, dst4, dst4);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5,
+              vec6, filt2, dst5, vec7, filt3, dst5, dst5, dst5, dst5);
     dst6 = const_vec;
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
-                  vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst6, vec9, filt1, dst6,
+              vec10, filt2, dst6, vec11, filt3, dst6, dst6, dst6, dst6);
 
     for (loop_cnt = height; loop_cnt--;) {
         src7 = __lsx_vld(src_tmp, 0);
         src7 = __lsx_vxori_b(src7, 128);
         src_tmp += src_stride;
 
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7, src7,
-                      mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1, dst7,
-                      vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
-        LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_r, dst32_r, dst54_r, dst76_r);
-        LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
-                      dst10_l, dst32_l, dst54_l, dst76_l);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7, src7,
+                  mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst7, vec1, filt1, dst7,
+                  vec2, filt2, dst7, vec3, filt3, dst7, dst7, dst7, dst7);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  dst10_r, dst32_r, dst54_r, dst76_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  dst10_l, dst32_l, dst54_l, dst76_l);
         dst0_r = __lsx_hevc_filt_8tap_w(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
                                         filt_h1, filt_h2, filt_h3);
         dst0_l = __lsx_hevc_filt_8tap_w(dst10_l, dst32_l, dst54_l, dst76_l, filt_h0,
@@ -2075,64 +2059,61 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
     dst += 8;
 
     mask4 = __lsx_vld(ff_hevc_mask_arr, 16);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
+    DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
     mask7 = __lsx_vaddi_bu(mask4, 6);
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src += src_stride_4x;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
     src6 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3)
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3)
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask4, src3, src0, mask5, src3, src0,
-                  mask6, src3, src0, mask7, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask4, src4, src1, mask5, src4, src1,
-                  mask6, src4, src1, mask7, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask4, src5, src2, mask5, src5, src2,
-                  mask6, src5, src2, mask7, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask4, src6, src3, mask5, src6, src3,
-                  mask6, src6, src3, mask7, vec12, vec13, vec14, vec15);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
-                  vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
-                  vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
-                  vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
-    dst63 = const_vec;
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
-                  vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
+    DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask4, src3, src0, mask5, src3, src0,
+              mask6, src3, src0, mask7, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask4, src4, src1, mask5, src4, src1,
+              mask6, src4, src1, mask7, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask4, src5, src2, mask5, src5, src2,
+              mask6, src5, src2, mask7, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask4, src6, src3, mask5, src6, src3,
+              mask6, src6, src3, mask7, vec12, vec13, vec14, vec15);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst30, vec1, filt1, dst30,
+              vec2, filt2, dst30, vec3, filt3, dst30, dst30, dst30, dst30);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst41, vec5, filt1, dst41,
+              vec6, filt2, dst41, vec7, filt3, dst41, dst41, dst41, dst41);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst52, vec9, filt1, dst52,
+              vec10, filt2, dst52, vec11, filt3, dst52, dst52, dst52, dst52);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst63, vec13, filt1, dst63,
+              vec14, filt2, dst63, vec15, filt3, dst63, dst63, dst63, dst63);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
     dst32_r = __lsx_vilvl_h(dst63, dst52);
     dst65_r = __lsx_vilvh_h(dst63, dst52);
 
     dst66 = __lsx_vreplvei_d(dst63, 1);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10);
-
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask4, src9, src7, mask5, src9, src7,
-                      mask6, src9, src7, mask7, vec0, vec1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask4, src10, src8, mask5, src10,
-                      src8, mask6, src10, src8, mask7, vec4, vec5, vec6, vec7);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
-                      dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97,
-                      dst97);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1,
-                      dst108, vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108,
-                      dst108);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10);
+
+        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask4, src9, src7, mask5, src9, src7,
+                  mask6, src9, src7, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask4, src10, src8, mask5, src10,
+                  src8, mask6, src10, src8, mask7, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst97, vec1, filt1,
+                  dst97, vec2, filt2, dst97, vec3, filt3, dst97, dst97, dst97, dst97);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst108, vec5, filt1, dst108,
+                  vec6, filt2, dst108, vec7, filt3, dst108, dst108, dst108, dst108);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
         dst109_r = __lsx_vilvh_h(dst108, dst97);
         dst66 = __lsx_vreplvei_d(dst97, 1);
         dst98_r = __lsx_vilvl_h(dst66, dst108);
@@ -2145,9 +2126,9 @@ static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
                                         filt_h1, filt_h2, filt_h3);
         dst3_r = __lsx_hevc_filt_8tap_w(dst43_r, dst65_r, dst87_r, dst109_r, filt_h0,
                                         filt_h1, filt_h2, filt_h3);
-        LSX_DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
-                      dst0_r, dst1_r, dst2_r, dst3_r);
-        LSX_DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
+        DUP4_ARG2(__lsx_vsrli_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
         __lsx_vstelm_d(dst0_r, dst, 0, 0);
         __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
         __lsx_vstelm_d(dst2_r, dst + dst_stride_2x, 0, 0);
@@ -2226,29 +2207,29 @@ static void hevc_hz_4t_32w_lsx(uint8_t *src,
     __m128i const_vec;
 
     src -= 1;
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 10);
 
     for (loop_cnt = height; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
         src2 = __lsx_vld(src, 24);
         src += src_stride;
 
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
-                      const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
-                      dst3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
-                      vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, const_vec, vec1, filt0,
+                  const_vec, vec2, filt0, const_vec, vec3, filt0, dst0, dst1, dst2,
+                  dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, dst0, vec0, filt1, dst1, vec1, filt1, dst2,
+                  vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         __lsx_vst(dst2, dst, 32);
@@ -2276,30 +2257,30 @@ static void hevc_vt_4t_16w_lsx(uint8_t *src,
 
     src -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, dst1_l);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                  filt1, dst1_l, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
         dst += dst_stride;
@@ -2307,19 +2288,19 @@ static void hevc_vt_4t_16w_lsx(uint8_t *src,
         __lsx_vst(dst1_l, dst, 16);
         dst += dst_stride;
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                      filt1, dst1_l, dst1_l);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
+                  filt1, dst1_l, dst1_l);
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
         dst += dst_stride;
@@ -2352,45 +2333,45 @@ static void hevc_vt_4t_24w_lsx(uint8_t *src,
 
     src -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src + src_stride_2x, 16);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
-                      filt1, dst1_l, dst1_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                      filt1, dst2_r, dst2_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                      filt1, dst3_r, dst3_r);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l, src43_l,
+                  filt1, dst1_l, dst1_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
+                  filt1, dst2_r, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
+                  filt1, dst3_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2401,28 +2382,28 @@ static void hevc_vt_4t_24w_lsx(uint8_t *src,
         __lsx_vst(dst3_r, dst, 32);
         dst += dst_stride;
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                      filt1, dst1_l, dst1_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                      filt1, dst2_r, dst2_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
-                      filt1, dst3_r, dst3_r);
+        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
+                  filt1, dst1_l, dst1_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
+                  filt1, dst2_r, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
+                  filt1, dst3_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2459,51 +2440,51 @@ static void hevc_vt_4t_32w_lsx(uint8_t *src,
 
     src -= src_stride;
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1)
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1)
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
+    DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src6, src7);
     src8 = __lsx_vld(src + src_stride_2x, 16);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
+    DUP2_ARG2(__lsx_vxori_b, src6, 128, src7, 128, src6, src7);
     src8 = __lsx_vxori_b(src8, 128);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src3, src4);
+        DUP2_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src3, src4);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src9, src10);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src21_l, filt0, dst1_l,src43_l,
-                      filt1, dst1_l, dst1_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
-                      filt1, dst2_r, dst2_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
-                      filt1, dst2_l, dst2_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
-                      filt1, dst3_r, dst3_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
-                      filt1, dst3_l, dst3_l);
+        DUP2_ARG2(__lsx_vxori_b, src9, 128, src10, 128, src9, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_r, filt0, dst0_r, src32_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src10_l, filt0, dst0_l, src32_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_r, filt0, dst1_r, src43_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src21_l, filt0, dst1_l,src43_l,
+                  filt1, dst1_l, dst1_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_r, filt0, dst2_r, src98_r,
+                  filt1, dst2_r, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src76_l, filt0, dst2_l, src98_l,
+                  filt1, dst2_l, dst2_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_r, filt0, dst3_r, src109_r,
+                  filt1, dst3_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src87_l, filt0, dst3_l, src109_l,
+                  filt1, dst3_l, dst3_l);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2516,33 +2497,33 @@ static void hevc_vt_4t_32w_lsx(uint8_t *src,
         __lsx_vst(dst3_l, dst, 48);
         dst += dst_stride;
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src5, src2);
+        DUP2_ARG2(__lsx_vxori_b, src5, 128, src2, 128, src5, src2);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
+        DUP2_ARG2(__lsx_vld, src, 16, src + src_stride, 16, src11, src8);
         src += src_stride_2x;
-        LSX_DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src11, src10, src8, src11, src76_l, src87_l);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
-                      filt1, dst0_r, dst0_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
-                      filt1, dst0_l, dst0_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
-                      filt1, dst1_r, dst1_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
-                      filt1, dst1_l, dst1_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
-                      filt1, dst2_r, dst2_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src98_l, filt0, dst2_l, src76_l,
-                      filt1, dst2_l, dst2_l);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
-                      filt1, dst3_r, dst3_r);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, src109_l, filt0, dst3_l, src87_l,
-                      filt1, dst3_l, dst3_l);
+        DUP2_ARG2(__lsx_vxori_b, src11, 128, src8, 128, src11, src8);
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+        DUP2_ARG2(__lsx_vilvh_b, src11, src10, src8, src11, src76_l, src87_l);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_r, filt0, dst0_r, src10_r,
+                  filt1, dst0_r, dst0_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src32_l, filt0, dst0_l, src10_l,
+                  filt1, dst0_l, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_r, filt0, dst1_r, src21_r,
+                  filt1, dst1_r, dst1_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src43_l, filt0, dst1_l, src21_l,
+                  filt1, dst1_l, dst1_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_r, filt0, dst2_r, src76_r,
+                  filt1, dst2_r, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src98_l, filt0, dst2_l, src76_l,
+                  filt1, dst2_l, dst2_l);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_r, filt0, dst3_r, src87_r,
+                  filt1, dst3_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, src109_l, filt0, dst3_l, src87_l,
+                  filt1, dst3_l, dst3_l);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst0_l, dst, 16);
@@ -2581,51 +2562,51 @@ static void hevc_hv_4t_8x2_lsx(uint8_t *src,
     __m128i dst10_l, dst32_l, dst21_l, dst43_l;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src4 = __lsx_vld(src + src_stride_4x, 0);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
     src4 = __lsx_vxori_b(src4, 128);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
-
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-                  dst0, dst0);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
-                  dst1, dst1);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-                  dst2, dst2);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec6, filt0, dst3, vec7, filt1,
-                  dst3, dst3);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
-                  dst4, dst4);
-
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
+              dst0, dst0);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
+              dst1, dst1);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
+              dst2, dst2);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst3, vec7, filt1,
+              dst3, dst3);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
+              dst4, dst4);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
 
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
     dst1_r = __lsx_hevc_filt_4tap_w(dst21_r, dst43_r, filt_h0, filt_h1);
     dst1_l = __lsx_hevc_filt_4tap_w(dst21_l, dst43_l, filt_h0, filt_h1);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-                  dst0_l, dst1_r, dst1_l);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
+              dst0_l, dst1_r, dst1_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
     __lsx_vst(dst0_r, dst, 0);
     __lsx_vst(dst1_r, dst + dst_stride, 0);
 }
@@ -2651,58 +2632,51 @@ static void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride,
     __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
     for (cnt = width8mult; cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src0, src1, src2, src3);
         src += src_stride_4x;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src4, src5);
         src6 = __lsx_vld(src + src_stride_2x, 0);
         src += (8 - src_stride_4x);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                      src1, src2, src3)
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3)
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-                      dst0, dst0);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
-                      dst1, dst1);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-                      dst2, dst2);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
-
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1,
-                      dst3, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1,
-                      dst4, dst4);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1,
-                      dst5, dst5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1,
-                      dst6, dst6);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1, dst4, dst4);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5, dst5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1, dst6, dst6);
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
         dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2713,12 +2687,12 @@ static void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride,
         dst2_l = __lsx_hevc_filt_4tap_w(dst32_l, dst54_l, filt_h0, filt_h1);
         dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
         dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
-                      dst0_r, dst0_l, dst1_r, dst1_l);
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
-                      dst2_r, dst2_l, dst3_r, dst3_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
-        LSX_DUP2_ARG2(__lsx_vpickev_h, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst2_l, dst2_r, dst3_l, dst3_r, dst2_r, dst3_r);
 
         __lsx_vst(dst0_r, dst, 0);
         __lsx_vst(dst1_r, dst + dst_stride, 0);
@@ -2756,57 +2730,56 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     __m128i dst76_r, dst76_l, dst87_r, dst87_l;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
 
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src0, src1, src2, src3);
     src4 = __lsx_vld(src + src_stride_4x, 0);
     src += (src_stride_4x + src_stride);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                  src + src_stride_3x, 0, src5, src6, src7, src8);
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+              src + src_stride_3x, 0, src5, src6, src7, src8);
 
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
-                  src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
     src4 = __lsx_vxori_b(src4, 128);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
-                  src6, src7, src8);
-
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
-                  mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,src3, src3,
-                  mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
-                  mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
-    LSX_DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
-                  mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
-
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-                  const_vec, vec2, filt0, dst1, vec3, filt1, dst0, dst0, dst1, dst1);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-                  const_vec, vec6, filt0, dst3, vec7, filt1, dst2, dst2, dst3, dst3);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
-                  const_vec, vec10, filt0, dst5, vec11, filt1, dst4, dst4, dst5, dst5);
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, const_vec, vec12, filt0, dst6, vec13, filt1,
-                  const_vec, vec14, filt0, dst7, vec15, filt1, dst6, dst6, dst7, dst7);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec16, filt0, dst8, vec17, filt1, dst8,
-                  dst8);
-
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_r, dst21_r, dst32_r, dst43_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
-                  dst10_l, dst21_l, dst32_l, dst43_l);
-    LSX_DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_r, dst65_r, dst76_r, dst87_r);
-    LSX_DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
-                  dst54_l, dst65_l, dst76_l, dst87_l);
+    DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
+              src6, src7, src8);
+
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
+              mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
+              mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
+
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
+              const_vec, vec2, filt0, dst1, vec3, filt1, dst0, dst0, dst1, dst1);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
+              const_vec, vec6, filt0, dst3, vec7, filt1, dst2, dst2, dst3, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec8, filt0, dst4, vec9, filt1,
+              const_vec, vec10, filt0, dst5, vec11, filt1, dst4, dst4, dst5, dst5);
+    DUP4_ARG3(__lsx_vdp2add_h_b, const_vec, vec12, filt0, dst6, vec13, filt1,
+              const_vec, vec14, filt0, dst7, vec15, filt1, dst6, dst6, dst7, dst7);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec16, filt0, dst8, vec17, filt1, dst8, dst8);
+
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
 
     dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2821,16 +2794,16 @@ static void hevc_hv_4t_8x6_lsx(uint8_t *src,
     dst5_r = __lsx_hevc_filt_4tap_w(dst65_r, dst87_r, filt_h0, filt_h1);
     dst5_l = __lsx_hevc_filt_4tap_w(dst65_l, dst87_l, filt_h0, filt_h1);
 
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
-                  dst0_l, dst1_r, dst1_l);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
-                  dst2_l, dst3_r, dst3_l);
-    LSX_DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
-                  dst4_l, dst5_r, dst5_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
+              dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
+              dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
+              dst4_l, dst5_r, dst5_l);
 
-    LSX_DUP4_ARG2(__lsx_vpickev_h,dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
-                  dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, dst4_r, dst5_r);
+    DUP4_ARG2(__lsx_vpickev_h,dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+              dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, dst4_r, dst5_r);
 
     __lsx_vst(dst0_r, dst, 0);
     __lsx_vst(dst1_r, dst + dst_stride, 0);
@@ -2874,11 +2847,11 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
     __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask1 = __lsx_vaddi_bu(mask0, 2);
     const_vec = __lsx_vreplgr2vr_h(8192); // 128 << 6
@@ -2887,53 +2860,50 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
         src_tmp = src;
         dst_tmp = dst;
 
-        LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
         src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
         src_tmp += src_stride_3x;
 
-        LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+        DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
         src2 = __lsx_vxori_b(src2, 128);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-                      dst0, dst0);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
-                      dst1, dst1);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-                      dst2, dst2);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
         for (loop_cnt = height >> 2; loop_cnt--;) {
-            LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                          src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
-                          src3, src4, src5, src6);
+            DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0,
+                      src3, src4, src5, src6);
             src_tmp += src_stride_4x;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
-                          src3, src4, src5, src6);
-
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-            LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
-
-            LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1,
-                          dst3, dst3);
-            LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1,
-                          dst4, dst4);
-            LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1,
-                          dst5, dst5);
-            LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1,
-                          dst6, dst6);
-
-            LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-            LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
-            LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+            DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                      src3, src4, src5, src6);
+
+            DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+            DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+            DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+            DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+
+            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1,
+                      dst3, dst3);
+            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1,
+                      dst4, dst4);
+            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1,
+                      dst5, dst5);
+            DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1,
+                      dst6, dst6);
+
+            DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
             dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
             dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -2944,13 +2914,13 @@ static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
             dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
             dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-            LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
-                          dst0_r, dst0_l, dst1_r, dst1_l);
-            LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
-                          dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                      dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                      dst2_r, dst2_l, dst3_r, dst3_l);
 
-            LSX_DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
-                          dst2_r, dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+            DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                      dst2_r, dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
 
             __lsx_vst(dst0_r, dst_tmp, 0);
             __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
@@ -3023,11 +2993,11 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
 
     src -= (src_stride + 1);
-    LSX_DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
 
     filter_vec = __lsx_vld(filter_y, 0);
     filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
-    LSX_DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
 
     mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
     mask1 = __lsx_vaddi_bu(mask0, 2);
@@ -3036,53 +3006,46 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
     src_tmp = src;
     dst_tmp = dst;
 
-    LSX_DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src_tmp + src_stride_2x, 0);
     src_tmp += src_stride_3x;
 
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
 
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
 
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1,
-                  dst0, dst0);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1,
-                  dst1, dst1);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1,
-                  dst2, dst2);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst0, vec1, filt1, dst0, dst0);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst1, vec3, filt1, dst1, dst1);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst2, vec5, filt1, dst2, dst2);
 
-    LSX_DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
-    LSX_DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
 
     for (loop_cnt = 4; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
-                      src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src3,
-                      src4, src5, src6);
+        DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                  src_tmp + src_stride_2x, 0, src_tmp + src_stride_3x, 0, src3,
+                  src4, src5, src6);
         src_tmp += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                      src4, src5, src6);
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
+                  src4, src5, src6);
 
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
 
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1,
-                      dst3, dst3);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1,
-                      dst4, dst4);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1,
-                      dst5, dst5);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1,
-                      dst6, dst6);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst3, vec1, filt1, dst3, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst4, vec3, filt1, dst4, dst4);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst5, vec5, filt1, dst5, dst5);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst6, vec7, filt1, dst6, dst6);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
 
         dst0_r = __lsx_hevc_filt_4tap_w(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = __lsx_hevc_filt_4tap_w(dst10_l, dst32_l, filt_h0, filt_h1);
@@ -3093,12 +3056,12 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
         dst3_r = __lsx_hevc_filt_4tap_w(dst43_r, dst65_r, filt_h0, filt_h1);
         dst3_l = __lsx_hevc_filt_4tap_w(dst43_l, dst65_l, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
-                      dst0_r, dst0_l, dst1_r, dst1_l);
-        LSX_DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
-                      dst2_r, dst2_l, dst3_r, dst3_l);
-        LSX_DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
-                      dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+                  dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
         __lsx_vst(dst0_r, dst_tmp, 0);
         __lsx_vst(dst1_r, dst_tmp + dst_stride, 0);
         __lsx_vst(dst2_r, dst_tmp + dst_stride_2x, 0);
@@ -3118,49 +3081,47 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
     mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
     mask3 = __lsx_vaddi_bu(mask2, 2);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src2 = __lsx_vld(src + src_stride_2x, 0);
     src += src_stride_3x;
-    LSX_DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
     src2 = __lsx_vxori_b(src2, 128);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst10, vec1, filt1,
-                  dst10, dst10);
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst21, vec3, filt1,
-                  dst21, dst21);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst10, vec1, filt1, dst10, dst10);
+    DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst21, vec3, filt1, dst21, dst21);
     dst10_r = __lsx_vilvl_h(dst21, dst10);
     dst21_r = __lsx_vilvh_h(dst21, dst10);
     dst22 = __lsx_vreplvei_d(dst21, 1);
 
     for (loop_cnt = 2; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src3, src4, src5, src6);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
-                      src + src_stride_3x, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride_2x, 0,
+                  src + src_stride_3x, 0, src7, src8, src9, src10);
         src += src_stride_4x;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
-                      src4, src5, src6);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
-                      src8, src9, src10);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, vec0, vec1);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src8, src4, mask2, src8, src4, mask3, vec2, vec3);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, vec4, vec5);
-        LSX_DUP2_ARG3(__lsx_vshuf_b, src10, src6, mask2, src10, src6, mask3, vec6, vec7);
-
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec0, filt0, dst73, vec1, filt1,
-                      dst73, dst73);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec2, filt0, dst84, vec3, filt1,
-                      dst84, dst84);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec4, filt0, dst95, vec5, filt1,
-                      dst95, dst95);
-        LSX_DUP2_ARG3(__lsx_dp2add_h_b, const_vec, vec6, filt0, dst106, vec7, filt1,
-                      dst106, dst106);
-
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst73, dst22, dst84, dst73, dst32_r, dst43_r);
-        LSX_DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
-        LSX_DUP2_ARG2(__lsx_vilvl_h, dst95, dst84, dst106, dst95, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128, src3,
+                  src4, src5, src6);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128, src7,
+                  src8, src9, src10);
+        DUP2_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src8, src4, mask2, src8, src4, mask3, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src10, src6, mask2, src10, src6, mask3, vec6, vec7);
+
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec0, filt0, dst73, vec1, filt1,
+                  dst73, dst73);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec2, filt0, dst84, vec3, filt1,
+                  dst84, dst84);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec4, filt0, dst95, vec5, filt1,
+                  dst95, dst95);
+        DUP2_ARG3(__lsx_vdp2add_h_b, const_vec, vec6, filt0, dst106, vec7, filt1,
+                  dst106, dst106);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst73, dst22, dst84, dst73, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst95, dst84, dst106, dst95, dst54_r, dst65_r);
         dst109_r = __lsx_vilvh_h(dst106, dst95);
         dst22 = __lsx_vreplvei_d(dst73, 1);
         dst76_r = __lsx_vilvl_h(dst22, dst106);
@@ -3174,12 +3135,10 @@ static void hevc_hv_4t_12w_lsx(uint8_t *src,
         tmp6 = __lsx_hevc_filt_4tap_w(dst76_r, dst98_r, filt_h0, filt_h1);
         tmp7 = __lsx_hevc_filt_4tap_w(dst87_r, dst109_r, filt_h0, filt_h1);
 
-        LSX_DUP4_ARG2(__lsx_vsrai_w, tmp0, 6, tmp1, 6, tmp2, 6, tmp3, 6, tmp0, tmp1,
-                      tmp2, tmp3);
-        LSX_DUP4_ARG2(__lsx_vsrai_w, tmp4, 6, tmp5, 6, tmp6, 6, tmp7, 6, tmp4, tmp5,
-                      tmp6, tmp7);
-        LSX_DUP4_ARG2(__lsx_vpickev_h, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
-                      tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsrai_w, tmp0, 6, tmp1, 6, tmp2, 6, tmp3, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsrai_w, tmp4, 6, tmp5, 6, tmp6, 6, tmp7, 6, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vpickev_h, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+                  tmp0, tmp1, tmp2, tmp3);
 
         __lsx_vstelm_d(tmp0, dst, 0, 0);
         __lsx_vstelm_d(tmp0, dst + dst_stride, 0, 1);
diff --git a/libavcodec/loongarch/hpeldsp_lasx.c b/libavcodec/loongarch/hpeldsp_lasx.c
index f88e1244a8..256c6e79d8 100644
--- a/libavcodec/loongarch/hpeldsp_lasx.c
+++ b/libavcodec/loongarch/hpeldsp_lasx.c
@@ -19,8 +19,8 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lasx.h"
-#include "libavcodec/loongarch/hpeldsp_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hpeldsp_lasx.h"
 
 static inline void
 put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
@@ -282,86 +282,100 @@ static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
                                             uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
-
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x -1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4, 0x20,
+              src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
 }
 
 static void common_hz_bil_no_rnd_8x16_lasx(const uint8_t *src,
@@ -369,46 +383,54 @@ static void common_hz_bil_no_rnd_8x16_lasx(const uint8_t *src,
                                            uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    int32_t src_stride_2x = src_stride << 1;
     int32_t src_stride_4x = src_stride << 2;
-
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x -1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
     dst += dst_stride;
 
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    src += src_stride_4x;
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 0);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src1, dst, 0, 2);
-    __lasx_xvstelm_d(src1, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
 }
 
 void ff_put_no_rnd_pixels16_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
@@ -427,20 +449,32 @@ static void common_vt_bil_no_rnd_16x16_lasx(const uint8_t *src,
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i src9, src10, src11, src12, src13, src14, src15, src16;
-    int32_t src_stride_8x = src_stride << 3;
-
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    src += src_stride_8x;
-    LASX_LD_8(src, src_stride, src8, src9, src10, src11, src12, src13, src14, src15);
-    src += src_stride_8x;
-    src16 = LASX_LD(src);
-
-    LASX_PCKEV_Q_8(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src5, src4, src6, src5, src7, src6, src8, src7,
-                   src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_PCKEV_Q_8(src9, src8, src10, src9, src11, src10, src12, src11,
-                   src13, src12, src14, src13, src15, src14, src16, src15,
-                   src8, src9, src10, src11, src12, src13, src14, src15);
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src8, src9, src10, src11);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src12, src13, src14, src15);
+    src += src_stride_4x;
+    src16 = __lasx_xvld(src, 0);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src9, src8, 0x20, src10, src9, 0x20, src11, src10,
+              0x20, src12, src11, 0x20, src8, src9, src10, src11);
+    DUP4_ARG3(__lasx_xvpermi_q, src13, src12, 0x20, src14, src13, 0x20, src15, src14,
+              0x20, src16, src15, 0x20, src12, src13, src14, src15);
     src0  = __lasx_xvavg_bu(src0, src1);
     src2  = __lasx_xvavg_bu(src2, src3);
     src4  = __lasx_xvavg_bu(src4, src5);
@@ -451,52 +485,52 @@ static void common_vt_bil_no_rnd_16x16_lasx(const uint8_t *src,
     src14 = __lasx_xvavg_bu(src14, src15);
 
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src2, dst, 0, 0);
-    __lasx_xvstelm_d(src2, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src2, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src2, dst, 0, 2);
-    __lasx_xvstelm_d(src2, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src2, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src4, dst, 0, 0);
-    __lasx_xvstelm_d(src4, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src4, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src4, dst, 0, 2);
-    __lasx_xvstelm_d(src4, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src4, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src6, dst, 0, 0);
-    __lasx_xvstelm_d(src6, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src6, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src6, dst, 0, 2);
-    __lasx_xvstelm_d(src6, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src6, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src8, dst, 0, 0);
-    __lasx_xvstelm_d(src8, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src8, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src8, dst, 0, 2);
-    __lasx_xvstelm_d(src8, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src8, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src10, dst, 0, 0);
-    __lasx_xvstelm_d(src10, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src10, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src10, dst, 0, 2);
-    __lasx_xvstelm_d(src10, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src10, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src12, dst, 0, 0);
-    __lasx_xvstelm_d(src12, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src12, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src12, dst, 0, 2);
-    __lasx_xvstelm_d(src12, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src12, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src14, dst, 0, 0);
-    __lasx_xvstelm_d(src14, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src14, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src14, dst, 0, 2);
-    __lasx_xvstelm_d(src14, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src14, dst, 8, 3);
 }
 
 static void common_vt_bil_no_rnd_8x16_lasx(const uint8_t *src,
@@ -504,43 +538,50 @@ static void common_vt_bil_no_rnd_8x16_lasx(const uint8_t *src,
                                            uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    int32_t src_stride_8x = src_stride << 3;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    src += src_stride_8x;
-    src8 = LASX_LD(src);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += src_stride_4x;
+    src8 = __lasx_xvld(src, 0);
 
-    LASX_PCKEV_Q_8(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src5, src4, src6, src5, src7, src6, src8, src7,
-                   src0, src1, src2, src3, src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
     src0  = __lasx_xvavg_bu(src0, src1);
     src2  = __lasx_xvavg_bu(src2, src3);
     src4  = __lasx_xvavg_bu(src4, src5);
     src6  = __lasx_xvavg_bu(src6, src7);
 
     __lasx_xvstelm_d(src0, dst, 0, 0);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src0, dst, 0, 2);
-    __lasx_xvstelm_d(src0, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src2, dst, 0, 0);
-    __lasx_xvstelm_d(src2, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src2, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src2, dst, 0, 2);
-    __lasx_xvstelm_d(src2, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src2, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src4, dst, 0, 0);
-    __lasx_xvstelm_d(src4, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src4, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src4, dst, 0, 2);
-    __lasx_xvstelm_d(src4, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src4, dst, 8, 3);
     dst += dst_stride;
     __lasx_xvstelm_d(src6, dst, 0, 0);
-    __lasx_xvstelm_d(src6, dst + 8, 0, 1);
+    __lasx_xvstelm_d(src6, dst, 8, 1);
     dst += dst_stride;
     __lasx_xvstelm_d(src6, dst, 0, 2);
-    __lasx_xvstelm_d(src6, dst + 8, 0, 3);
+    __lasx_xvstelm_d(src6, dst, 8, 3);
 }
 
 void ff_put_no_rnd_pixels16_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
@@ -560,31 +601,47 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
     __m256i src10, src11, src12, src13, src14, src15, src16, src17;
     __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
-    int32_t src_stride_8x = src_stride << 3;
-
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_LD_8((src + 1), src_stride,
-              src9, src10, src11, src12, src13, src14, src15, src16);
-    src += src_stride_8x;
-    src8 = LASX_LD(src);
-    src17 = LASX_LD(src + 1);
-
-    LASX_ILVL_H_8(src9, src0, src10, src1, src11, src2, src12, src3,
-                  src13, src4, src14, src5, src15, src6, src16, src7,
-                  src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_ILVL_H(src17, src8, src8);
-    src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-    src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-    src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-    src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-    src4 = __lasx_xvhaddw_hu_bu(src4, src4);
-    src5 = __lasx_xvhaddw_hu_bu(src5, src5);
-    src6 = __lasx_xvhaddw_hu_bu(src6, src6);
-    src7 = __lasx_xvhaddw_hu_bu(src7, src7);
-    src8 = __lasx_xvhaddw_hu_bu(src8, src8);
-    LASX_ADD_H_8(src0, src1, src1, src2, src2, src3, src3, src4,
-                 src4, src5, src5, src6, src6, src7, src7, src8,
-                 sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7);
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (1 - src_stride_4x);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src9, src10, src11, src12);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src13, src14, src15, src16);
+    src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
+              src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
+              src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
     sum2 = __lasx_xvaddi_hu(sum2, 1);
@@ -601,48 +658,70 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     sum5 = __lasx_xvsrai_h(sum5, 2);
     sum6 = __lasx_xvsrai_h(sum6, 2);
     sum7 = __lasx_xvsrai_h(sum7, 2);
-    LASX_PCKEV_B_4_128SV(sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
-                         sum0, sum1, sum2, sum3);
-    LASX_ST_D_2(sum0, 0, 2, dst, 8);
+    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+              sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum0, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
     dst += dst_stride;
 
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_LD_8((src + 1), src_stride,
-              src9, src10, src11, src12, src13, src14, src15, src16);
-    src += src_stride_8x;
-    src8 = LASX_LD(src);
-    src17 = LASX_LD(src + 1);
-
-    LASX_ILVL_H_8(src9, src0, src10, src1, src11, src2, src12, src3,
-                  src13, src4, src14, src5, src15, src6, src16, src7,
-                  src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_ILVL_H(src17, src8, src8);
-    src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-    src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-    src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-    src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-    src4 = __lasx_xvhaddw_hu_bu(src4, src4);
-    src5 = __lasx_xvhaddw_hu_bu(src5, src5);
-    src6 = __lasx_xvhaddw_hu_bu(src6, src6);
-    src7 = __lasx_xvhaddw_hu_bu(src7, src7);
-    src8 = __lasx_xvhaddw_hu_bu(src8, src8);
-    LASX_ADD_H_8(src0, src1, src1, src2, src2, src3, src3, src4,
-                 src4, src5, src5, src6, src6, src7, src7, src8,
-                 sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (1 - src_stride_4x);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src9, src10, src11, src12);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src13, src14, src15, src16);
+    src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
+              src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
+              src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
     sum2 = __lasx_xvaddi_hu(sum2, 1);
@@ -659,23 +738,32 @@ static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
     sum5 = __lasx_xvsrai_h(sum5, 2);
     sum6 = __lasx_xvsrai_h(sum6, 2);
     sum7 = __lasx_xvsrai_h(sum7, 2);
-    LASX_PCKEV_B_4_128SV(sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
-                         sum0, sum1, sum2, sum3);
-    LASX_ST_D_2(sum0, 0, 2, dst, 8);
+    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+              sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum0, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 1, 3, dst, 8);
 }
 
 static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
@@ -685,31 +773,47 @@ static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
     __m256i src10, src11, src12, src13, src14, src15, src16, src17;
     __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
-    int32_t src_stride_8x = src_stride << 3;
-
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_LD_8((src + 1), src_stride,
-              src9, src10, src11, src12, src13, src14, src15, src16);
-    src += src_stride_8x;
-    src8 = LASX_LD(src);
-    src17 = LASX_LD(src + 1);
-
-    LASX_ILVL_H_8(src9, src0, src10, src1, src11, src2, src12, src3,
-                  src13, src4, src14, src5, src15, src6, src16, src7,
-                  src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_ILVL_H(src17, src8, src8);
-    src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-    src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-    src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-    src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-    src4 = __lasx_xvhaddw_hu_bu(src4, src4);
-    src5 = __lasx_xvhaddw_hu_bu(src5, src5);
-    src6 = __lasx_xvhaddw_hu_bu(src6, src6);
-    src7 = __lasx_xvhaddw_hu_bu(src7, src7);
-    src8 = __lasx_xvhaddw_hu_bu(src8, src8);
-    LASX_ADD_H_8(src0, src1, src1, src2, src2, src3, src3, src4,
-                 src4, src5, src5, src6, src6, src7, src7, src8,
-                 sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7);
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (1 - src_stride_4x);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src9, src10, src11, src12);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src13, src14, src15, src16);
+    src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
+              src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
+              src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
     sum2 = __lasx_xvaddi_hu(sum2, 1);
@@ -726,23 +830,32 @@ static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
     sum5 = __lasx_xvsrai_h(sum5, 2);
     sum6 = __lasx_xvsrai_h(sum6, 2);
     sum7 = __lasx_xvsrai_h(sum7, 2);
-    LASX_PCKEV_B_4_128SV(sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
-                         sum0, sum1, sum2, sum3);
-    LASX_ST_D_2(sum0, 0, 2, dst, 8);
+    DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+              sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum0, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum1, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum2, 1, 3, dst, 8);
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 0, 2, dst, 8);
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
     dst += dst_stride;
-    LASX_ST_D_2(sum3, 1, 3, dst, 8);
 }
 
 void ff_put_no_rnd_pixels16_xy2_8_lasx(uint8_t *block,
@@ -761,34 +874,66 @@ static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_LD_8((src + 1), src_stride,
-              src8, src9, src10, src11, src12, src13, src14, src15);
-    LASX_PCKEV_D_4_128SV(src1, src0, src3, src2, src5, src4, src7, src6,
-                         src0, src1, src2, src3);
-    LASX_PCKEV_D_4_128SV(src9, src8, src11, src10, src13, src12, src15, src14,
-                         src4, src5, src6, src7);
-    LASX_PCKEV_Q_4(src1, src0, src3, src2, src5, src4, src7, src6,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (1 - src_stride_4x);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src8, src9, src10, src11);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src12, src13, src14, src15);
+
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvpickev_d, src9, src8, src11, src10, src13, src12, src15,
+              src14, src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src2);
     src1 = __lasx_xvavg_bu(src1, src3);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, dst, dst_stride);
-    dst += (dst_stride << 2);
-    LASX_ST_D_4(src1, 0, 1, 2, 3, dst, dst_stride);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src1, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, dst + dst_stride_3x, 0, 3);
 }
 
 static void common_hz_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
                                           uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src4, src5, src6, src7);
-    LASX_PCKEV_D_4_128SV(src1, src0, src3, src2, src5, src4, src7, src6,
-                         src0, src1, src2, src3);
-    LASX_PCKEV_Q_2(src1, src0, src3, src2, src0, src1);
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src1);
     src0 = __lasx_xvavg_bu(src0, src1);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, dst, dst_stride);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_put_no_rnd_pixels8_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
@@ -805,37 +950,62 @@ static void common_vt_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
                                           uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    src += (src_stride << 3);
-    src8 = LASX_LD(src);
-
-    LASX_PCKEV_D_4_128SV(src1, src0, src2, src1, src3, src2, src4, src3,
-                         src0, src1, src2, src3);
-    LASX_PCKEV_D_4_128SV(src5, src4, src6, src5, src7, src6, src8, src7,
-                         src4, src5, src6, src7);
-    LASX_PCKEV_Q_4(src2, src0, src3, src1, src6, src4, src7, src5,
-                   src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += src_stride_4x;
+    src8 = __lasx_xvld(src, 0);
+
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvpickev_d, src5, src4, src6, src5, src7, src6, src8, src7,
+              src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src2, src0, 0x20, src3, src1, 0x20, src6, src4,
+              0x20, src7, src5, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvavg_bu(src0, src1);
     src1 = __lasx_xvavg_bu(src2, src3);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, dst, dst_stride);
-    dst += (dst_stride << 2);
-    LASX_ST_D_4(src1, 0, 1, 2, 3, dst, dst_stride);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src1, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, dst + dst_stride_3x, 0, 3);
 }
 
 static void common_vt_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
                                           uint8_t *dst, int32_t dst_stride)
 {
     __m256i src0, src1, src2, src3, src4;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    src += (src_stride << 2);
-    src4 = LASX_LD(src);
-    LASX_PCKEV_D_4_128SV(src1, src0, src2, src1, src3, src2, src4, src3,
-                         src0, src1, src2, src3);
-    LASX_PCKEV_Q_2(src2, src0, src3, src1, src0, src1);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    src4 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src2, src0, 0x20, src3, src1, 0x20, src0, src1);
     src0 = __lasx_xvavg_bu(src0, src1);
-    LASX_ST_D_4(src0, 0, 1, 2, 3, dst, dst_stride);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_put_no_rnd_pixels8_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
@@ -854,21 +1024,36 @@ static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i src8, src9, src10, src11, src12, src13, src14, src15, src16, src17;
     __m256i sum0, sum1, sum2, sum3;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_LD_8((src + 1), src_stride,
-              src9, src10, src11, src12, src13, src14, src15, src16);
-    src += (src_stride << 3);
-    src8 = LASX_LD(src);
-    src17 = LASX_LD(src + 1);
-
-    LASX_ILVL_B_8_128SV(src9, src0, src10, src1, src11, src2, src12, src3,
-                        src13, src4, src14, src5, src15, src6, src16, src7,
-                        src0, src1, src2, src3, src4, src5, src6, src7);
-    LASX_ILVL_B_128SV(src17, src8, src8);
-    LASX_PCKEV_Q_8(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src5, src4, src6, src5, src7, src6, src8, src7,
-                   src0, src1, src2, src3, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src4, src5, src6, src7);
+    src += (1 - src_stride_4x);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src9, src10, src11, src12);
+    src += src_stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src13, src14, src15, src16);
+    src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+
+    DUP4_ARG2(__lasx_xvilvl_b, src9, src0, src10, src1, src11, src2, src12, src3,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvilvl_b, src13, src4, src14, src5, src15, src6, src16, src7,
+              src4, src5, src6, src7);
+    src8 = __lasx_xvilvl_b(src17, src8);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
     src0 = __lasx_xvhaddw_hu_bu(src0, src0);
     src1 = __lasx_xvhaddw_hu_bu(src1, src1);
     src2 = __lasx_xvhaddw_hu_bu(src2, src2);
@@ -877,8 +1062,8 @@ static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     src5 = __lasx_xvhaddw_hu_bu(src5, src5);
     src6 = __lasx_xvhaddw_hu_bu(src6, src6);
     src7 = __lasx_xvhaddw_hu_bu(src7, src7);
-    LASX_ADD_H_4(src0, src1, src2, src3, src4, src5, src6, src7,
-                 sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, src4, src5, src6, src7,
+              sum0, sum1, sum2, sum3);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
     sum2 = __lasx_xvaddi_hu(sum2, 1);
@@ -887,10 +1072,16 @@ static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride
     sum1 = __lasx_xvsrai_h(sum1, 2);
     sum2 = __lasx_xvsrai_h(sum2, 2);
     sum3 = __lasx_xvsrai_h(sum3, 2);
-    LASX_PCKEV_B_2_128SV(sum1, sum0, sum3, sum2, sum0, sum1);
-    LASX_ST_D_4(sum0, 0, 2, 1, 3, dst, dst_stride);
-    dst += (dst_stride << 2);
-    LASX_ST_D_4(sum1, 0, 2, 1, 3, dst, dst_stride);
+    DUP2_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum0, sum1);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum1, dst + dst_stride_3x, 0, 3);
 }
 
 static void common_hv_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
@@ -898,29 +1089,39 @@ static void common_hv_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride
 {
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i src8, src9, sum0, sum1;
-
-    LASX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LASX_LD_4((src + 1), src_stride, src5, src6, src7, src8);
-    src += (src_stride << 2);
-    src4 = LASX_LD(src);
-    src9 = LASX_LD(src + 1);
-
-    LASX_ILVL_B_4_128SV(src5, src0, src6, src1, src7, src2, src8, src3,
-                        src0, src1, src2, src3);
-    LASX_ILVL_B_128SV(src9, src4, src4);
-    LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src4, src3,
-                   src0, src1, src2, src3);
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src0, src1, src2, src3);
+    src += 1;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+              0, src + src_stride_3x, 0, src5, src6, src7, src8);
+    src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src4, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
+              src0, src1, src2, src3);
+    src4 = __lasx_xvilvl_b(src9, src4);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
     src0 = __lasx_xvhaddw_hu_bu(src0, src0);
     src1 = __lasx_xvhaddw_hu_bu(src1, src1);
     src2 = __lasx_xvhaddw_hu_bu(src2, src2);
     src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-    LASX_ADD_H_2(src0, src1, src2, src3, sum0, sum1);
+    DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
     sum0 = __lasx_xvaddi_hu(sum0, 1);
     sum1 = __lasx_xvaddi_hu(sum1, 1);
     sum0 = __lasx_xvsrai_h(sum0, 2);
     sum1 = __lasx_xvsrai_h(sum1, 2);
-    LASX_PCKEV_B_128SV(sum1, sum0, sum0);
-    LASX_ST_D_4(sum0, 0, 2, 1, 3, dst, dst_stride);
+    sum0 = __lasx_xvpickev_b(sum1, sum0);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_put_no_rnd_pixels8_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
@@ -937,56 +1138,81 @@ static void common_hv_bil_16w_lasx(const uint8_t *src, int32_t src_stride,
                                    uint8_t *dst, int32_t dst_stride,
                                    uint8_t height)
 {
-    uint8_t loop_cnt;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
     __m256i src10, src11, src12, src13, src14, src15, src16, src17;
     __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
-    int32_t src_stride_8x = (src_stride << 3);
+    uint8_t loop_cnt;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
-        LASX_LD_8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-        LASX_LD_8((src + 1), src_stride,
-                  src9, src10, src11, src12, src13, src14, src15, src16);
-        src += src_stride_8x;
-
-        src8 = LASX_LD(src);
-        src17 = LASX_LD(src + 1);
-
-        LASX_ILVL_H_8(src9, src0, src10, src1, src11, src2, src12, src3,
-                      src13, src4, src14, src5, src15, src6, src16, src7,
-                      src0, src1, src2, src3, src4, src5, src6, src7);
-        LASX_ILVL_H(src17, src8, src8);
-        src0 = __lasx_xvhaddw_hu_bu(src0, src0);
-        src1 = __lasx_xvhaddw_hu_bu(src1, src1);
-        src2 = __lasx_xvhaddw_hu_bu(src2, src2);
-        src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-        src4 = __lasx_xvhaddw_hu_bu(src4, src4);
-        src5 = __lasx_xvhaddw_hu_bu(src5, src5);
-        src6 = __lasx_xvhaddw_hu_bu(src6, src6);
-        src7 = __lasx_xvhaddw_hu_bu(src7, src7);
-        src8 = __lasx_xvhaddw_hu_bu(src8, src8);
-        LASX_ADD_H_8(src0, src1, src1, src2, src2, src3, src3, src4,
-                     src4, src5, src5, src6, src6, src7, src7, src8,
-                     sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7);
-        LASX_SRARI_H_4(sum0, sum1, sum2, sum3, sum0, sum1, sum2, sum3, 2);
-        LASX_SRARI_H_4(sum4, sum5, sum6, sum7, sum4, sum5, sum6, sum7, 2);
-        LASX_PCKEV_B_4_128SV(sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
-                             sum0, sum1, sum2, sum3);
-        LASX_ST_D_2(sum0, 0, 2, dst, 8);
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src0, src1, src2, src3);
+        src += src_stride_4x;
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src4, src5, src6, src7);
+        src += (1 - src_stride_4x);
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src9, src10, src11, src12);
+        src += src_stride_4x;
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src13, src14, src15, src16);
+        src += (src_stride_4x - 1);
+        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src8, src17);
+
+        DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
+                  src3, src7, 0x02, src0, src1, src2, src3);
+        DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14,
+                  0x02, src11, src15, 0x02, src4, src5, src6, src7);
+        DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+        DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+                  sum0, sum2, sum4, sum6);
+        DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+                  sum1, sum3, sum5, sum7);
+        src8 = __lasx_xvilvl_h(src9, src4);
+        src9 = __lasx_xvilvh_h(src9, src4);
+
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+                  sum3, sum3, src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+                  sum7, sum7, src4, src5, src6, src7);
+        DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+        DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+                  sum0, sum1, sum2, sum3);
+        DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+                  sum4, sum5, sum6, sum7);
+        DUP4_ARG2(__lasx_xvsrari_h, sum0, 2, sum1, 2, sum2, 2, sum3, 2, sum0, sum1,
+                  sum2, sum3);
+        DUP4_ARG2(__lasx_xvsrari_h, sum4, 2, sum5, 2, sum6, 2, sum7, 2, sum4, sum5,
+                  sum6, sum7);
+        DUP4_ARG2(__lasx_xvpickev_b, sum1, sum0, sum3, sum2, sum5, sum4, sum7, sum6,
+                  sum0, sum1, sum2, sum3);
+        __lasx_xvstelm_d(sum0, dst, 0, 0);
+        __lasx_xvstelm_d(sum0, dst, 8, 1);
         dst += dst_stride;
-        LASX_ST_D_2(sum0, 1, 3, dst, 8);
+        __lasx_xvstelm_d(sum1, dst, 0, 0);
+        __lasx_xvstelm_d(sum1, dst, 8, 1);
         dst += dst_stride;
-        LASX_ST_D_2(sum1, 0, 2, dst, 8);
+        __lasx_xvstelm_d(sum2, dst, 0, 0);
+        __lasx_xvstelm_d(sum2, dst, 8, 1);
         dst += dst_stride;
-        LASX_ST_D_2(sum1, 1, 3, dst, 8);
+        __lasx_xvstelm_d(sum3, dst, 0, 0);
+        __lasx_xvstelm_d(sum3, dst, 8, 1);
         dst += dst_stride;
-        LASX_ST_D_2(sum2, 0, 2, dst, 8);
+        __lasx_xvstelm_d(sum0, dst, 0, 2);
+        __lasx_xvstelm_d(sum0, dst, 8, 3);
         dst += dst_stride;
-        LASX_ST_D_2(sum2, 1, 3, dst, 8);
+        __lasx_xvstelm_d(sum1, dst, 0, 2);
+        __lasx_xvstelm_d(sum1, dst, 8, 3);
         dst += dst_stride;
-        LASX_ST_D_2(sum3, 0, 2, dst, 8);
+        __lasx_xvstelm_d(sum2, dst, 0, 2);
+        __lasx_xvstelm_d(sum2, dst, 8, 3);
         dst += dst_stride;
-        LASX_ST_D_2(sum3, 1, 3, dst, 8);
+        __lasx_xvstelm_d(sum3, dst, 0, 2);
+        __lasx_xvstelm_d(sum3, dst, 8, 3);
         dst += dst_stride;
     }
 }
@@ -1001,33 +1227,42 @@ static void common_hv_bil_8w_lasx(const uint8_t *src, int32_t src_stride,
                                   uint8_t *dst, int32_t dst_stride,
                                   uint8_t height)
 {
-    uint8_t loop_cnt;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i src8, src9, sum0, sum1;
-    int32_t src_stride_4x = (src_stride << 2);
-    int32_t dst_stride_4x = (dst_stride << 2);
+    uint8_t loop_cnt;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
 
-    src0 = LASX_LD(src);
-    src5 = LASX_LD(src + 1);
+    DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src0, src5);
     src += src_stride;
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LASX_LD_4(src, src_stride, src1, src2, src3, src4);
-        LASX_LD_4((src + 1), src_stride, src6, src7, src8, src9);
-        src += src_stride_4x;
-        LASX_ILVL_B_4_128SV(src5, src0, src6, src1, src7, src2, src8, src3,
-                            src0, src1, src2, src3);
-        LASX_ILVL_B_128SV(src9, src4, src5);
-        LASX_PCKEV_Q_4(src1, src0, src2, src1, src3, src2, src5, src3,
-                       src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src1, src2, src3, src4);
+        src += 1;
+        DUP4_ARG2(__lasx_xvld, src, 0, src + src_stride, 0, src + src_stride_2x,
+                  0, src + src_stride_3x, 0, src6, src7, src8, src9);
+        src += (src_stride_4x - 1);
+        DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
+                  src0, src1, src2, src3);
+        src5 = __lasx_xvilvl_b(src9, src4);
+        DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+                  0x20, src5, src3, 0x20, src0, src1, src2, src3);
         src0 = __lasx_xvhaddw_hu_bu(src0, src0);
         src1 = __lasx_xvhaddw_hu_bu(src1, src1);
         src2 = __lasx_xvhaddw_hu_bu(src2, src2);
         src3 = __lasx_xvhaddw_hu_bu(src3, src3);
-        LASX_ADD_H_2(src0, src1, src2, src3, sum0, sum1);
-        LASX_SRARI_H_2(sum0, sum1, sum0, sum1, 2);
-        LASX_PCKEV_B_128SV(sum1, sum0, sum0);
-        LASX_ST_D_4(sum0, 0, 2, 1, 3, dst, dst_stride);
+        DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
+        DUP2_ARG2(__lasx_xvsrari_h, sum0, 2, sum1, 2, sum0, sum1);
+        sum0 = __lasx_xvpickev_b(sum1, sum0);
+        __lasx_xvstelm_d(sum0, dst, 0, 0);
+        __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+        __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+        __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
         dst += dst_stride_4x;
         src0 = src4;
         src5 = src9;
diff --git a/libavcodec/loongarch/hpeldsp_lasx.h b/libavcodec/loongarch/hpeldsp_lasx.h
index 0477cb54ea..2e035eade8 100644
--- a/libavcodec/loongarch/hpeldsp_lasx.h
+++ b/libavcodec/loongarch/hpeldsp_lasx.h
@@ -22,6 +22,7 @@
 #ifndef AVCODEC_LOONGARCH_HPELDSP_LASX_H
 #define AVCODEC_LOONGARCH_HPELDSP_LASX_H
 
+#include <stdint.h>
 #include <stddef.h>
 #include "libavutil/attributes.h"
 
diff --git a/libavcodec/loongarch/idctdsp_lasx.c b/libavcodec/loongarch/idctdsp_lasx.c
index 78f15f17ee..dfcb9b1a4f 100644
--- a/libavcodec/loongarch/idctdsp_lasx.c
+++ b/libavcodec/loongarch/idctdsp_lasx.c
@@ -20,20 +20,29 @@
  */
 
 #include "idctdsp_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 static void put_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
                                     int32_t stride)
 {
     __m256i b0, b1, b2, b3;
     __m256i temp0, temp1;
+    int32_t stride_2x = stride << 1;
+    int32_t stride_4x = stride << 2;
+    int32_t stride_3x = stride_2x + stride;
 
-    LASX_LD_4(block, 16, b0, b1, b2, b3);
-    LASX_CLIP_H_0_255_4(b0, b1, b2, b3, b0, b1, b2, b3);
-    LASX_PCKEV_B_2_128SV(b1, b0, b3, b2, temp0, temp1);
-    LASX_ST_D_4(temp0, 0, 2, 1, 3, pixels, stride);
-    pixels += (stride << 2);
-    LASX_ST_D_4(temp1, 0, 2, 1, 3, pixels, stride);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
+    DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
+    DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
 
 static void put_signed_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
@@ -42,17 +51,26 @@ static void put_signed_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels
     __m256i b0, b1, b2, b3;
     __m256i temp0, temp1;
     __m256i const_128 = {0x0080008000800080, 0x0080008000800080, 0x0080008000800080, 0x0080008000800080};
+    int32_t stride_2x = stride << 1;
+    int32_t stride_4x = stride << 2;
+    int32_t stride_3x = stride_2x + stride;
 
-    LASX_LD_4(block, 16, b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
     b0 = __lasx_xvadd_h(b0, const_128);
     b1 = __lasx_xvadd_h(b1, const_128);
     b2 = __lasx_xvadd_h(b2, const_128);
     b3 = __lasx_xvadd_h(b3, const_128);
-    LASX_CLIP_H_0_255_4(b0, b1, b2, b3, b0, b1, b2, b3);
-    LASX_PCKEV_B_2_128SV(b1, b0, b3, b2, temp0, temp1);
-    LASX_ST_D_4(temp0, 0, 2, 1, 3, pixels, stride);
-    pixels += (stride << 2);
-    LASX_ST_D_4(temp1, 0, 2, 1, 3, pixels, stride);
+    DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
+    DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
 
 static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
@@ -62,8 +80,11 @@ static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
     __m256i p0, p1, p2, p3, p4, p5, p6, p7;
     __m256i temp0, temp1, temp2, temp3;
     uint8_t *pix = pixels;
+    int32_t stride_2x = stride << 1;
+    int32_t stride_4x = stride << 2;
+    int32_t stride_3x = stride_2x + stride;
 
-    LASX_LD_4(block, 16, b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, b0, b1, b2, b3);
     p0   = __lasx_xvldrepl_d(pix, 0);
     pix += stride;
     p1   = __lasx_xvldrepl_d(pix, 0);
@@ -83,15 +104,19 @@ static void add_pixels_clamped_lasx(const int16_t *block, uint8_t *pixels,
     temp1 = __lasx_xvpermi_q(p3, p2, 0x20);
     temp2 = __lasx_xvpermi_q(p5, p4, 0x20);
     temp3 = __lasx_xvpermi_q(p7, p6, 0x20);
-    LASX_ADDW_H_H_BU_128SV(b0, temp0, temp0);
-    LASX_ADDW_H_H_BU_128SV(b1, temp1, temp1);
-    LASX_ADDW_H_H_BU_128SV(b2, temp2, temp2);
-    LASX_ADDW_H_H_BU_128SV(b3, temp3, temp3);
-    LASX_CLIP_H_0_255_4(temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
-    LASX_PCKEV_B_2_128SV(temp1, temp0, temp3, temp2, temp0, temp1);
-    LASX_ST_D_4(temp0, 0, 2, 1, 3, pixels, stride);
-    pixels += (stride << 2);
-    LASX_ST_D_4(temp1, 0, 2, 1, 3, pixels, stride);
+    DUP4_ARG2(__lasx_xvaddw_h_h_bu, b0, temp0, b1, temp1, b2, temp2, b3, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_xvclip255_h, temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvpickev_b, temp1, temp0, temp3, temp2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
 }
 
 void ff_put_pixels_clamped_lasx(const int16_t *block,
diff --git a/libavcodec/loongarch/simple_idct_lasx.c b/libavcodec/loongarch/simple_idct_lasx.c
index 38e879cfa8..3caf83d846 100644
--- a/libavcodec/loongarch/simple_idct_lasx.c
+++ b/libavcodec/loongarch/simple_idct_lasx.c
@@ -19,7 +19,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "idctdsp_loongarch.h"
 
 #define LASX_TRANSPOSE4x16(in_0, in_1, in_2, in_3, out_0, out_1, out_2, out_3)  \
@@ -30,17 +30,18 @@
     temp_1 = __lasx_xvpermi_q(in_2, in_0, 0x31);                                \
     temp_2 = __lasx_xvpermi_q(in_3, in_1, 0x20);                                \
     temp_3 = __lasx_xvpermi_q(in_3, in_1, 0x31);                                \
-    LASX_ILVLH_H_128SV(temp_1, temp_0, temp_5, temp_4);                         \
-    LASX_ILVLH_H_128SV(temp_3, temp_2, temp_7, temp_6);                         \
-    LASX_ILVLH_W_128SV(temp_6, temp_4, out_1, out_0);                           \
-    LASX_ILVLH_W_128SV(temp_7, temp_5, out_3, out_2);                           \
+    DUP2_ARG2(__lasx_xvilvl_h, temp_1, temp_0, temp_3, temp_2, temp_4, temp_6); \
+    DUP2_ARG2(__lasx_xvilvh_h, temp_1, temp_0, temp_3, temp_2, temp_5, temp_7); \
+    DUP2_ARG2(__lasx_xvilvl_w, temp_6, temp_4, temp_7, temp_5, out_0, out_2);   \
+    DUP2_ARG2(__lasx_xvilvh_w, temp_6, temp_4, temp_7, temp_5, out_1, out_3);   \
 }
 
 #define LASX_IDCTROWCONDDC                                                      \
     const_val  = 16383 * ((1 << 19) / 16383);                                   \
     const_val1 = __lasx_xvinsgr2vr_w(const_val0, const_val, 0);                 \
     const_val1 = __lasx_xvreplve0_w(const_val1);                                \
-    LASX_LD_4(block, 16, in0, in1, in2, in3);                                   \
+    DUP4_ARG2(__lasx_xvld, block, 0, block + 16, 0, block + 32, 0, block + 48,  \
+              0, in0, in1, in2, in3);                                           \
     LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                 \
     a0 = __lasx_xvpermi_d(in0, 0xD8);                                           \
     a0 = __lasx_vext2xv_w_h(a0);                                                \
@@ -71,45 +72,45 @@
     w1    = __lasx_xvrepl128vei_h(w1, 1);                                       \
                                                                                 \
     /* part of FUNC6(idctRowCondDC) */                                          \
-    LASX_MADDWL_W_H_128SV(const_val0, in0, w4, temp0);                          \
-    LASX_MULWL_W_H_2_128SV(in1, w2, in1, w6, temp1, temp2);                     \
+    temp0 = __lasx_xvmaddwl_w_h(const_val0, in0, w4);                           \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);              \
     a0    = __lasx_xvadd_w(temp0, temp1);                                       \
     a1    = __lasx_xvadd_w(temp0, temp2);                                       \
     a2    = __lasx_xvsub_w(temp0, temp2);                                       \
     a3    = __lasx_xvsub_w(temp0, temp1);                                       \
                                                                                 \
-    LASX_ILVH_H_2_128SV(in1, in0, w3, w1, temp0, temp1);                        \
-    LASX_DP2_W_H(temp0, temp1, b0);                                             \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                 \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                        \
     temp1 = __lasx_xvneg_h(w7);                                                 \
-    LASX_ILVL_H_128SV(temp1, w3, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b1);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
     temp1 = __lasx_xvneg_h(w1);                                                 \
-    LASX_ILVL_H_128SV(temp1, w5, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b2);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                         \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
     temp1 = __lasx_xvneg_h(w5);                                                 \
-    LASX_ILVL_H_128SV(temp1, w7, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b3);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                         \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
                                                                                 \
     /* if (AV_RAN64A(row + 4)) */                                               \
-    LASX_ILVL_H_2_128SV(in3, in2, w6, w4, temp0, temp1);                        \
-    LASX_DP2ADD_W_H(a0, temp0, temp1, a0);                                      \
-    LASX_ILVL_H_128SV(w2, w4, temp1);                                           \
-    LASX_DP2SUB_W_H(a1, temp0, temp1, a1);                                      \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                 \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                 \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                            \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                 \
     temp1 = __lasx_xvneg_h(w4);                                                 \
-    LASX_ILVL_H_128SV(w2, temp1, temp2);                                        \
-    LASX_DP2ADD_W_H(a2, temp0, temp2, a2);                                      \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                         \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                 \
     temp1 = __lasx_xvneg_h(w6);                                                 \
-    LASX_ILVL_H_128SV(temp1, w4, temp2);                                        \
-    LASX_DP2ADD_W_H(a3, temp0, temp2, a3);                                      \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                         \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                 \
                                                                                 \
-    LASX_ILVH_H_2_128SV(in3, in2, w7, w5, temp0, temp1);                        \
-    LASX_DP2ADD_W_H(b0, temp0, temp1, b0);                                      \
-    LASX_ILVL_H_2_128SV(w5, w1, w3, w7, temp1, temp2);                          \
-    LASX_DP2SUB_W_H(b1, temp0, temp1, b1);                                      \
-    LASX_DP2ADD_W_H(b2, temp0, temp2, b2);                                      \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                 \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                 \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                   \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                 \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                 \
     temp1 = __lasx_xvneg_h(w1);                                                 \
-    LASX_ILVL_H_128SV(temp1, w3, temp2);                                        \
-    LASX_DP2ADD_W_H(b3, temp0, temp2, b3);                                      \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                 \
                                                                                 \
     temp0 = __lasx_xvadd_w(a0, b0);                                             \
     temp1 = __lasx_xvadd_w(a1, b1);                                             \
@@ -119,8 +120,9 @@
     a1    = __lasx_xvsub_w(a1, b1);                                             \
     a2    = __lasx_xvsub_w(a2, b2);                                             \
     a3    = __lasx_xvsub_w(a3, b3);                                             \
-    LASX_SRAI_W_8(temp0, temp1, temp2, temp3, a0, a1, a2, a3,                   \
-                  temp0, temp1, temp2, temp3, a0, a1, a2, a3, 11);              \
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 11, temp1, 11, temp2, 11, temp3, 11,      \
+              temp0, temp1, temp2, temp3);                                      \
+    DUP4_ARG2(__lasx_xvsrai_w, a0, 11, a1, 11, a2, 11, a3, 11, a0, a1, a2, a3); \
     in0   = __lasx_xvbitsel_v(temp0, temp, select_vec);                         \
     in1   = __lasx_xvbitsel_v(temp1, temp, select_vec);                         \
     in2   = __lasx_xvbitsel_v(temp2, temp, select_vec);                         \
@@ -142,45 +144,45 @@
 #define LASX_IDCTCOLS                                                           \
     /* part of FUNC6(idctSparaseCol) */                                         \
     LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                 \
-    LASX_MADDWL_W_H_128SV(const_val1, in0, w4, temp0);                          \
-    LASX_MULWL_W_H_2_128SV(in1, w2, in1, w6, temp1, temp2);                     \
+    temp0 = __lasx_xvmaddwl_w_h(const_val1, in0, w4);                           \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);              \
     a0    = __lasx_xvadd_w(temp0, temp1);                                       \
     a1    = __lasx_xvadd_w(temp0, temp2);                                       \
     a2    = __lasx_xvsub_w(temp0, temp2);                                       \
     a3    = __lasx_xvsub_w(temp0, temp1);                                       \
                                                                                 \
-    LASX_ILVH_H_2_128SV(in1, in0, w3, w1, temp0, temp1);                        \
-    LASX_DP2_W_H(temp0, temp1, b0);                                             \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                 \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                        \
     temp1 = __lasx_xvneg_h(w7);                                                 \
-    LASX_ILVL_H_128SV(temp1, w3, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b1);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
     temp1 = __lasx_xvneg_h(w1);                                                 \
-    LASX_ILVL_H_128SV(temp1, w5, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b2);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                         \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
     temp1 = __lasx_xvneg_h(w5);                                                 \
-    LASX_ILVL_H_128SV(temp1, w7, temp2);                                        \
-    LASX_DP2_W_H(temp0, temp2, b3);                                             \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                         \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                        \
                                                                                 \
     /* if (AV_RAN64A(row + 4)) */                                               \
-    LASX_ILVL_H_2_128SV(in3, in2, w6, w4, temp0, temp1);                        \
-    LASX_DP2ADD_W_H(a0, temp0, temp1, a0);                                      \
-    LASX_ILVL_H_128SV(w2, w4, temp1);                                           \
-    LASX_DP2SUB_W_H(a1, temp0, temp1, a1);                                      \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                 \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                 \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                            \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                 \
     temp1 = __lasx_xvneg_h(w4);                                                 \
-    LASX_ILVL_H_128SV(w2, temp1, temp2);                                        \
-    LASX_DP2ADD_W_H(a2, temp0, temp2, a2);                                      \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                         \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                 \
     temp1 = __lasx_xvneg_h(w6);                                                 \
-    LASX_ILVL_H_128SV(temp1, w4, temp2);                                        \
-    LASX_DP2ADD_W_H(a3, temp0, temp2, a3);                                      \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                         \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                 \
                                                                                 \
-    LASX_ILVH_H_2_128SV(in3, in2, w7, w5, temp0, temp1);                        \
-    LASX_DP2ADD_W_H(b0, temp0, temp1, b0);                                      \
-    LASX_ILVL_H_2_128SV(w5, w1, w3, w7, temp1, temp2);                          \
-    LASX_DP2SUB_W_H(b1, temp0, temp1, b1);                                      \
-    LASX_DP2ADD_W_H(b2, temp0, temp2, b2);                                      \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                 \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                 \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                   \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                 \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                 \
     temp1 = __lasx_xvneg_h(w1);                                                 \
-    LASX_ILVL_H_128SV(temp1, w3, temp2);                                        \
-    LASX_DP2ADD_W_H(b3, temp0, temp2, b3);                                      \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                         \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                 \
                                                                                 \
     temp0 = __lasx_xvadd_w(a0, b0);                                             \
     temp1 = __lasx_xvadd_w(a1, b1);                                             \
@@ -190,8 +192,9 @@
     a2    = __lasx_xvsub_w(a2, b2);                                             \
     a1    = __lasx_xvsub_w(a1, b1);                                             \
     a0    = __lasx_xvsub_w(a0, b0);                                             \
-    LASX_SRAI_W_8(temp0, temp1, temp2, temp3, a0, a1, a2, a3,                   \
-                  temp0, temp1, temp2, temp3, a0, a1, a2, a3, 20);              \
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 20, temp1, 20, temp2, 20, temp3, 20,      \
+              temp0, temp1, temp2, temp3);                                      \
+    DUP4_ARG2(__lasx_xvsrai_w, a0, 20, a1, 20, a2, 20, a3, 20, a0, a1, a2, a3); \
     in0   = __lasx_xvpickev_h(temp1, temp0);                                    \
     in1   = __lasx_xvpickev_h(temp3, temp2);                                    \
     in2   = __lasx_xvpickev_h(a2, a3);                                          \
@@ -216,13 +219,19 @@ static void simple_idct_lasx(int16_t *block)
     in1   = __lasx_xvpermi_d(in1, 0xD8);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
     in3   = __lasx_xvpermi_d(in3, 0xD8);
-    LASX_ST_4(in0, in1, in2, in3, block, 16);
+    __lasx_xvst(in0, block, 0);
+    __lasx_xvst(in1, block, 32);
+    __lasx_xvst(in2, block, 64);
+    __lasx_xvst(in3, block, 96);
 }
 
 static void simple_idct_put_lasx(uint8_t *dst, int32_t dst_stride,
                                  int16_t *block)
 {
     int32_t const_val = 1 << 10;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
     __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF, 0x4B42539F58C50000, 0x11A822A332493FFF};
     __m256i in0, in1, in2, in3;
     __m256i w2, w3, w4, w5, w6, w7;
@@ -238,11 +247,17 @@ static void simple_idct_put_lasx(uint8_t *dst, int32_t dst_stride,
     in1   = __lasx_xvpermi_d(in1, 0xD8);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
     in3   = __lasx_xvpermi_d(in3, 0xD8);
-    LASX_CLIP_H_0_255_4(in0, in1, in2, in3, in0, in1, in2, in3);
-    LASX_PCKEV_B_2_128SV(in1, in0, in3, in2, in0, in1);
-    LASX_ST_D_4(in0, 0, 2, 1, 3, dst, dst_stride);
-    dst += (dst_stride << 2);
-    LASX_ST_D_4(in1, 0, 2, 1, 3, dst, dst_stride);
+    DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
+    __lasx_xvstelm_d(in0, dst, 0, 0);
+    __lasx_xvstelm_d(in0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(in1, dst, 0, 0);
+    __lasx_xvstelm_d(in1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
 }
 
 static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
@@ -250,8 +265,12 @@ static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
 {
     int32_t const_val = 1 << 10;
     uint8_t *dst1 = dst;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
     __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF, 0x4B42539F58C50000, 0x11A822A332493FFF};
-    __m256i sh = {0x0003000200010000, 0x000B000A00090008, 0x0007000600050004, 0x000F000E000D000B};
+    __m256i sh = {0x0003000200010000, 0x000B000A00090008, 0x0007000600050004, 0x000F000E000D000C};
     __m256i in0, in1, in2, in3;
     __m256i w2, w3, w4, w5, w6, w7;
     __m256i a0, a1, a2, a3;
@@ -297,11 +316,17 @@ static void simple_idct_add_lasx(uint8_t *dst, int32_t dst_stride,
     in1   = __lasx_xvpermi_d(in1, 0xD8);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
     in3   = __lasx_xvpermi_d(in3, 0xD8);
-    LASX_CLIP_H_0_255_4(in0, in1, in2, in3, in0, in1, in2, in3);
-    LASX_PCKEV_B_2_128SV(in1, in0, in3, in2, in0, in1);
-    LASX_ST_D_4(in0, 0, 2, 1, 3, dst, dst_stride);
-    dst += (dst_stride << 2);
-    LASX_ST_D_4(in1, 0, 2, 1, 3, dst, dst_stride);
+    DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
+    __lasx_xvstelm_d(in0, dst, 0, 0);
+    __lasx_xvstelm_d(in0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(in1, dst, 0, 0);
+    __lasx_xvstelm_d(in1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
 }
 
 void ff_simple_idct_lasx(int16_t *block)
diff --git a/libavcodec/loongarch/vc1dsp_lasx.c b/libavcodec/loongarch/vc1dsp_lasx.c
index 216fcd23fa..4f4088fa1d 100644
--- a/libavcodec/loongarch/vc1dsp_lasx.c
+++ b/libavcodec/loongarch/vc1dsp_lasx.c
@@ -20,7 +20,7 @@
  */
 
 #include "vc1dsp_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
 {
@@ -41,33 +41,31 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     __m256i const_11 = {0xfff70004fff70004, 0xfff70004fff70004, 0xfff70004fff70004, 0xfff70004fff70004};
     __m256i const_12 = {0xfff0000ffff0000f, 0xfff0000ffff0000f, 0xfff0000ffff0000f, 0xfff0000ffff0000f};
 
-    LASX_LD_4(block, 16, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, in0, in1, in2, in3);
     in0 = __lasx_xvpermi_d(in0, 0xD8);
     in1 = __lasx_xvpermi_d(in1, 0xD8);
     in2 = __lasx_xvpermi_d(in2, 0xD8);
     in3 = __lasx_xvpermi_d(in3, 0xD8);
     /* first loops */
-    LASX_ILVL_H_2_128SV(in2, in0, in3, in1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_h, in2, in0, in3, in1, temp0, temp1);
     t2 = __lasx_xvreplgr2vr_w(con_4);
-    LASX_DP2ADD_W_H(t2, temp0, const_1, t1);
-    LASX_DP2ADD_W_H(t2, temp0, const_2, t2);
-    LASX_DP2_W_H(temp1, const_3, t3);
-    LASX_DP2_W_H(temp1, const_4, t4);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t2, temp0, const_1, t2, temp0, const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
 
     t5 = __lasx_xvadd_w(t1, t3);
     t6 = __lasx_xvadd_w(t2, t4);
     t7 = __lasx_xvsub_w(t2, t4);
     t8 = __lasx_xvsub_w(t1, t3);
 
-    LASX_ILVH_H_2_128SV(in1, in0, in3, in2, temp0, temp1);
-    LASX_DP2_W_H(const_5, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_6, t1);
-    LASX_DP2_W_H(const_7, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_8, t2);
-    LASX_DP2_W_H(const_9, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_10, t3);
-    LASX_DP2_W_H(const_11, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_12, t4);
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, in3, in2, temp0, temp1);
+    temp2 = __lasx_xvdp2_w_h(const_5, temp0);
+    t1 = __lasx_xvdp2add_w_h(temp2, temp1, const_6);
+    temp2 = __lasx_xvdp2_w_h(const_7, temp0);
+    t2 = __lasx_xvdp2add_w_h(temp2, temp1, const_8);
+    temp2 = __lasx_xvdp2_w_h(const_9, temp0);
+    t3 = __lasx_xvdp2add_w_h(temp2, temp1, const_10);
+    temp2 = __lasx_xvdp2_w_h(const_11, temp0);
+    t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
 
     temp0 = __lasx_xvadd_w(t1, t5);
     temp1 = __lasx_xvadd_w(t6, t2);
@@ -77,41 +75,40 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     in1   = __lasx_xvsub_w(t7, t3);
     in2   = __lasx_xvsub_w(t6, t2);
     in3   = __lasx_xvsub_w(t5, t1);
-    LASX_SRAI_W_8(temp0, temp1, temp2, temp3, in0, in1, in2, in3,
-                  temp0, temp1, temp2, temp3, in0, in1, in2, in3, 3);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 3, temp1, 3, temp2, 3, temp3, 3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in2, 3, in3, 3, in0, in1, in2, in3);
 
     /* second loops */
     temp0 = __lasx_xvpackev_h(temp1, temp0);
     temp1 = __lasx_xvpackev_h(temp3, temp2);
     temp2 = __lasx_xvpackev_h(in1, in0);
     temp3 = __lasx_xvpackev_h(in3, in2);
-    LASX_ILVL_W_2_128SV(temp1, temp0, temp3, temp2, t1, t3);
-    LASX_ILVH_W_2_128SV(temp1, temp0, temp3, temp2, t2, t4);
+    DUP2_ARG2(__lasx_xvilvl_w, temp1, temp0, temp3, temp2, t1, t3);
+    DUP2_ARG2(__lasx_xvilvh_w, temp1, temp0, temp3, temp2, t2, t4);
     in0   = __lasx_xvpermi_q(t3, t1, 0x20);
     in1   = __lasx_xvpermi_q(t3, t1, 0x31);
     in2   = __lasx_xvpermi_q(t4, t2, 0x20);
     in3   = __lasx_xvpermi_q(t4, t2, 0x31);
-    LASX_ILVL_H_2_128SV(in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in3, in2, temp0, temp1);
     t3    = __lasx_xvreplgr2vr_w(con_64);
-    LASX_DP2ADD_W_H(t3, temp0, const_1, t1);
-    LASX_DP2ADD_W_H(t3, temp0, const_2, t2);
-    LASX_DP2_W_H(temp1, const_3, t3);
-    LASX_DP2_W_H(temp1, const_4, t4);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t3, temp0, const_1, t3, temp0, const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
 
     t5    = __lasx_xvadd_w(t1, t3);
     t6    = __lasx_xvadd_w(t2, t4);
     t7    = __lasx_xvsub_w(t2, t4);
     t8    = __lasx_xvsub_w(t1, t3);
 
-    LASX_ILVH_H_2_128SV(in2, in0, in3, in1, temp0, temp1);
-    LASX_DP2_W_H(const_5, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_6, t1);
-    LASX_DP2_W_H(const_7, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_8, t2);
-    LASX_DP2_W_H(const_9, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_10, t3);
-    LASX_DP2_W_H(const_11, temp0, temp2);
-    LASX_DP2ADD_W_H(temp2, temp1, const_12, t4);
+    DUP2_ARG2(__lasx_xvilvh_h, in2, in0, in3, in1, temp0, temp1);
+    temp2 = __lasx_xvdp2_w_h(const_5, temp0);
+    t1 = __lasx_xvdp2add_w_h(temp2, temp1, const_6);
+    temp2 = __lasx_xvdp2_w_h(const_7, temp0);
+    t2 = __lasx_xvdp2add_w_h(temp2, temp1, const_8);
+    temp2 = __lasx_xvdp2_w_h(const_9, temp0);
+    t3 = __lasx_xvdp2add_w_h(temp2, temp1, const_10);
+    temp2 = __lasx_xvdp2_w_h(const_11, temp0);
+    t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
 
     temp0 = __lasx_xvadd_w(t5, t1);
     temp1 = __lasx_xvadd_w(t6, t2);
@@ -125,8 +122,9 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     in1   = __lasx_xvaddi_wu(in1, 1);
     in2   = __lasx_xvaddi_wu(in2, 1);
     in3   = __lasx_xvaddi_wu(in3, 1);
-    LASX_SRAI_W_8(temp0, temp1, temp2, temp3, in0, in1, in2, in3,
-                  temp0, temp1, temp2, temp3, in0, in1, in2, in3, 7);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsrai_w, in0, 7, in1, 7, in2, 7, in3, 7, in0, in1, in2, in3);
     t1 = __lasx_xvpickev_h(temp1, temp0);
     t2 = __lasx_xvpickev_h(temp3, temp2);
     t3 = __lasx_xvpickev_h(in1, in0);
@@ -135,7 +133,10 @@ void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
     in1 = __lasx_xvpermi_d(t2, 0xD8);
     in2 = __lasx_xvpermi_d(t3, 0xD8);
     in3 = __lasx_xvpermi_d(t4, 0xD8);
-    LASX_ST_4(in0, in1, in2, in3, block, 16);
+    __lasx_xvst(in0, block, 0);
+    __lasx_xvst(in1, block, 32);
+    __lasx_xvst(in2, block, 64);
+    __lasx_xvst(in3, block, 96);
 }
 
 void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
@@ -152,9 +153,9 @@ void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -163,9 +164,9 @@ void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -174,9 +175,9 @@ void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -185,9 +186,9 @@ void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -196,6 +197,8 @@ void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
 void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
     uint8_t *dst = dest;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i shift    = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
     __m256i const_64 = {0x0000004000000040, 0x0000004000000040, 0x0000004000000040, 0x0000004000000040};
     __m256i const_1  = {0x00060010000C000C, 0x00060010000C000C, 0x00060010000C000C, 0x00060010000C000C};
@@ -212,16 +215,15 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     __m256i in0, in1;
     __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
 
-    LASX_LD_2(block, 16, in0, in1);
+    DUP2_ARG2(__lasx_xvld, block, 0, block, 32, in0, in1);
     /* first loops */
     temp0 = __lasx_xvpermi_d(in0, 0xB1);
     temp1 = __lasx_xvpermi_d(in1, 0xB1);
-    LASX_ILVL_H_2_128SV(temp0, in0, temp1, in1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_h, temp0, in0, temp1, in1, temp0, temp1);
     temp2 = __lasx_xvpickev_w(temp1, temp0);
     temp3 = __lasx_xvpickod_w(temp1, temp0);
 
-    LASX_DP2_W_H(temp2, const_1, temp0);
-    LASX_DP2_W_H(temp2, const_2, temp1);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp2, const_1, temp2, const_2, temp0, temp1);
     t1    = __lasx_xvadd_w(temp0, const_7);
     t2    = __lasx_xvadd_w(temp1, const_7);
     temp0 = __lasx_xvpickev_w(t2, t1);
@@ -230,11 +232,8 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t4    = __lasx_xvsub_w(temp0, temp1);
     t4    = __lasx_xvpermi_d(t4, 0xB1);
 
-    LASX_DP4_D_H(temp3, const_3, t1);
-    LASX_DP4_D_H(temp3, const_4, t2);
-    LASX_DP4_D_H(temp3, const_5, temp0);
-    LASX_DP4_D_H(temp3, const_6, temp1);
-
+    DUP4_ARG2(__lasx_xvdp4_d_h, temp3, const_3, temp3, const_4, temp3, const_5, temp3,
+              const_6, t1, t2, temp0, temp1);
     temp2 = __lasx_xvpickev_w(t2, t1);
     temp3 = __lasx_xvpickev_w(temp1, temp0);
 
@@ -242,22 +241,21 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t2    = __lasx_xvadd_w(temp3, t4);
     temp0 = __lasx_xvsub_w(t4, temp3);
     temp1 = __lasx_xvsub_w(t3, temp2);
-    LASX_SRAI_W_4(t1, t2, temp0, temp1, t1, t2, t3, t4, 3);
+    DUP4_ARG2(__lasx_xvsrai_w, t1, 3, t2, 3, temp0, 3, temp1, 3, t1, t2, t3, t4);
     /* second loops */
     temp2 = __lasx_xvpickev_h(t2, t1);
     temp3 = __lasx_xvpickev_h(t4, t3);
     temp3 = __lasx_xvshuf4i_h(temp3, 0x4E);
     temp0 = __lasx_xvpermi_q(temp3, temp2, 0x20);
     temp1 = __lasx_xvpermi_q(temp3, temp2, 0x31);
-    LASX_DP2ADD_W_H(const_64, temp0, const_8, t1);
-    LASX_DP2ADD_W_H(const_64, temp0, const_9, t2);
-    LASX_DP2_W_H(temp1, const_10, t3);
-    LASX_DP2_W_H(temp1, const_11, t4);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, const_64, temp0, const_8, const_64, temp0,
+              const_9, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_10, temp1, const_11, t3, t4);
     temp0 = __lasx_xvadd_w(t1, t3);
     temp1 = __lasx_xvsub_w(t2, t4);
     temp2 = __lasx_xvadd_w(t2, t4);
     temp3 = __lasx_xvsub_w(t1, t3);
-    LASX_SRAI_W_4(temp0, temp1, temp2, temp3, t1, t2, t3, t4, 7);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7, t1, t2, t3, t4);
 
     temp0 = __lasx_xvldrepl_d(dst, 0);
     temp0 = __lasx_vext2xv_wu_bu(temp0);
@@ -274,11 +272,14 @@ void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     t2    = __lasx_xvadd_w(temp1, t2);
     t3    = __lasx_xvadd_w(temp2, t3);
     t4    = __lasx_xvadd_w(temp3, t4);
-    LASX_CLIP_W_0_255_4(t1, t2, t3, t4, t1, t2, t3, t4);
-    LASX_PCKEV_H_2_128SV(t2, t1, t4, t3, temp0, temp1);
+    DUP4_ARG1(__lasx_xvclip255_w, t1, t2, t3, t4, t1, t2, t3, t4);
+    DUP2_ARG2(__lasx_xvpickev_h, t2, t1, t4, t3, temp0, temp1);
     temp2 = __lasx_xvpickev_b(temp1, temp0);
     temp0 = __lasx_xvperm_w(temp2, shift);
-    LASX_ST_D_4(temp0, 0, 1, 2, 3, dest, stride);
+    __lasx_xvstelm_d(temp0, dest, 0, 0);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride_2x, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride_3x, 0, 3);
 }
 
 void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
@@ -295,9 +296,9 @@ void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -306,9 +307,9 @@ void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in0   = __lasx_xvldrepl_d(dest, 0);
     in1   = __lasx_xvldrepl_d(dst, 0);
     in0   = __lasx_xvpermi_q(in1, in0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_d(temp0, dest, 0, 0);
     __lasx_xvstelm_d(temp0, dst, 0, 2);
@@ -331,11 +332,11 @@ void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in1   = __lasx_xvldrepl_w(dst1, 0);
     in2   = __lasx_xvldrepl_w(dst2, 0);
     in3   = __lasx_xvldrepl_w(dst3, 0);
-    LASX_ILVL_W_2_128SV(in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
     in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
     __lasx_xvstelm_w(temp0, dst1, 0, 1);
@@ -351,11 +352,11 @@ void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in1   = __lasx_xvldrepl_w(dst1, 0);
     in2   = __lasx_xvldrepl_w(dst2, 0);
     in3   = __lasx_xvldrepl_w(dst3, 0);
-    LASX_ILVL_W_2_128SV(in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
     in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
     __lasx_xvstelm_w(temp0, dst1, 0, 1);
@@ -367,6 +368,9 @@ void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
 void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
     uint8_t *dst = dest;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i in0, in1, in2, in3;
     __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
 
@@ -385,7 +389,7 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     __m256i shift    = {0x0000000400000000, 0x0000000600000002, 0x0000000500000001, 0x0000000700000003};
 
     /* first loops */
-    LASX_LD_4(block, 16, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96, in0, in1, in2, in3);
     in0   = __lasx_xvilvl_d(in1, in0);
     in1   = __lasx_xvilvl_d(in3, in2);
     temp0 = __lasx_xvpickev_h(in1, in0);
@@ -393,16 +397,16 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp0 = __lasx_xvperm_w(temp0, shift);
     temp1 = __lasx_xvperm_w(temp1, shift);
 
-    LASX_DP2ADD_W_H(const_5, temp0, const_1, t1);
-    LASX_DP2ADD_W_H(const_5, temp0, const_2, t2);
-    LASX_DP2_W_H(temp1, const_3, t3);
-    LASX_DP2_W_H(temp1, const_4, t4);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, const_5, temp0, const_1, const_5, temp0,
+              const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
 
     temp0 = __lasx_xvadd_w(t1, t3);
     temp1 = __lasx_xvsub_w(t2, t4);
     temp2 = __lasx_xvadd_w(t2, t4);
     temp3 = __lasx_xvsub_w(t1, t3);
-    LASX_SRAI_W_4(temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3, 3);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 3, temp1, 3, temp2, 3, temp3, 3,
+              temp0, temp1, temp2, temp3);
 
     /* second loops */
     t1    = __lasx_xvpickev_w(temp1, temp0);
@@ -413,16 +417,14 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp1 = __lasx_xvpickev_h(t4, t3);
     temp2 = __lasx_xvpermi_q(t1, t1, 0x00);
     temp3 = __lasx_xvpermi_q(t1, t1, 0x11);
-    LASX_DP2ADD_W_H(const_6, temp2, const_7, t1);
-    LASX_DP2_W_H(temp3, const_8, t2);
+    t1 = __lasx_xvdp2add_w_h(const_6, temp2, const_7);
+    t2 = __lasx_xvdp2_w_h(temp3, const_8);
     t3    = __lasx_xvadd_w(t1, t2);
     t4    = __lasx_xvsub_w(t1, t2);
     t4    = __lasx_xvpermi_d(t4, 0x4E);
 
-    LASX_DP2_W_H(temp1, const_9, t1);
-    LASX_DP2_W_H(temp1, const_10, t2);
-    LASX_DP2_W_H(temp1, const_11, temp2);
-    LASX_DP2_W_H(temp1, const_12, temp3);
+    DUP4_ARG2(__lasx_xvdp2_w_h, temp1, const_9, temp1, const_10, temp1, const_11,
+              temp1, const_12, t1, t2, temp2, temp3);
 
     temp0 = __lasx_xvpermi_q(t2, t1, 0x20);
     temp1 = __lasx_xvpermi_q(t2, t1, 0x31);
@@ -436,7 +438,8 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp3 = __lasx_xvsub_w(t3, t1);
     temp2 = __lasx_xvaddi_wu(temp2, 1);
     temp3 = __lasx_xvaddi_wu(temp3, 1);
-    LASX_SRAI_W_4(temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3, 7);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+              temp0, temp1, temp2, temp3);
 
     const_1 = __lasx_xvldrepl_w(dst, 0);
     dst += stride;
@@ -454,8 +457,8 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     dst += stride;
     const_8 = __lasx_xvldrepl_w(dst, 0);
 
-    LASX_ILVL_W_4_128SV(const_2, const_1, const_4, const_3, const_5, const_6,
-                        const_7, const_8, const_1, const_2, const_3, const_4);
+    DUP4_ARG2(__lasx_xvilvl_w, const_2, const_1, const_4, const_3, const_5, const_6,
+              const_7, const_8, const_1, const_2, const_3, const_4);
     const_1 = __lasx_vext2xv_wu_bu(const_1);
     const_2 = __lasx_vext2xv_wu_bu(const_2);
     const_3 = __lasx_vext2xv_wu_bu(const_3);
@@ -465,10 +468,18 @@ void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp1   = __lasx_xvadd_w(temp1, const_2);
     temp2   = __lasx_xvadd_w(temp2, const_3);
     temp3   = __lasx_xvadd_w(temp3, const_4);
-    LASX_CLIP_W_0_255_4(temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
-    LASX_PCKEV_H_2_128SV(temp1, temp0, temp3, temp2, temp0, temp1);
+    DUP4_ARG1(__lasx_xvclip255_w, temp0, temp1, temp2, temp3, temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvpickev_h, temp1, temp0, temp3, temp2, temp0, temp1);
     temp0   = __lasx_xvpickev_b(temp1, temp0);
-    LASX_ST_W_8(temp0, 0, 4, 1, 5, 6, 2, 7, 3, dest, stride);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 4);
+    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 1);
+    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 5);
+    dest += stride_4x;
+    __lasx_xvstelm_w(temp0, dest, 0, 6);
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 2);
+    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 7);
+    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 3);
 }
 
 void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
@@ -488,11 +499,11 @@ void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
     in1   = __lasx_xvldrepl_w(dst1, 0);
     in2   = __lasx_xvldrepl_w(dst2, 0);
     in3   = __lasx_xvldrepl_w(dst3, 0);
-    LASX_ILVL_W_2_128SV(in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
     in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
-    LASX_ILVL_B_128SV(zero, in0, temp0);
+    temp0 = __lasx_xvilvl_b(zero, in0);
     temp0 = __lasx_xvadd_h(temp0, const_dc);
-    LASX_CLIP_H_0_255(temp0, in0);
+    in0 = __lasx_xvclip255_h(temp0);
     temp0 = __lasx_xvpickev_b(in0, in0);
     __lasx_xvstelm_w(temp0, dest, 0, 0);
     __lasx_xvstelm_w(temp0, dst1, 0, 1);
@@ -503,6 +514,8 @@ void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *bloc
 void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
 {
     uint8_t *dst = dest + stride;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
     __m256i in0, in1, in2, in3;
     __m256i temp0, temp1, temp2, temp3, t1, t2;
 
@@ -510,28 +523,27 @@ void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     __m256i const_2  = {0x000A0016000A0016, 0x0016FFF60016FFF6, 0x000A0016000A0016, 0x0016FFF60016FFF6};
     __m256i const_64 = {0x0000004000000040, 0x0000004000000040, 0x0000004000000040, 0x0000004000000040};
 
-    LASX_LD_2(block, 16, in0, in1);
+    DUP2_ARG2(__lasx_xvld, block, 0, block, 32, in0, in1);
     /* first loops */
     temp0 = __lasx_xvilvl_d(in1, in0);
     temp1 = __lasx_xvpickev_h(temp0, temp0);
     temp2 = __lasx_xvpickod_h(temp0, temp0);
-    LASX_DP2_W_H(temp1, const_1, t1);
-    LASX_DP2_W_H(temp2, const_2, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_1, temp2, const_2, t1, t2);
     t1    = __lasx_xvaddi_wu(t1, 4);
     in0   = __lasx_xvadd_w(t1, t2);
     in1   = __lasx_xvsub_w(t1, t2);
-    LASX_SRAI_W_2(in0, in1, in0, in1, 3);
+    DUP2_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in0, in1);
     /* second loops */
     temp0   = __lasx_xvpickev_h(in1, in0);
     temp1   = __lasx_xvpermi_q(temp0, temp0, 0x00);
     temp2   = __lasx_xvpermi_q(temp0, temp0, 0x11);
     const_1 = __lasx_xvpermi_d(const_1, 0xD8);
     const_2 = __lasx_xvpermi_d(const_2, 0xD8);
-    LASX_DP2ADD_W_H(const_64, temp1, const_1, t1);
-    LASX_DP2_W_H(temp2, const_2, t2);
+    t1 = __lasx_xvdp2add_w_h(const_64, temp1, const_1);
+    t2 = __lasx_xvdp2_w_h(temp2, const_2);
     in0     = __lasx_xvadd_w(t1, t2);
     in1     = __lasx_xvsub_w(t1, t2);
-    LASX_SRAI_W_2(in0, in1, in0, in1, 7);
+    DUP2_ARG2(__lasx_xvsrai_w, in0, 7, in1, 7, in0, in1);
     temp0   = __lasx_xvshuf4i_w(in0, 0x9C);
     temp1   = __lasx_xvshuf4i_w(in1, 0x9C);
 
@@ -547,10 +559,13 @@ void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
     temp3   = __lasx_vext2xv_wu_bu(temp3);
     temp0   = __lasx_xvadd_w(temp0, temp2);
     temp1   = __lasx_xvadd_w(temp1, temp3);
-    LASX_CLIP_W_0_255_2(temp0, temp1, temp0, temp1);
+    DUP2_ARG1(__lasx_xvclip255_w, temp0, temp1, temp0, temp1);
     temp1   = __lasx_xvpickev_h(temp1, temp0);
     temp0   = __lasx_xvpickev_b(temp1, temp1);
-    LASX_ST_W_4(temp0, 0, 5, 4, 1, dest, stride);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 5);
+    __lasx_xvstelm_w(temp0, dest + stride_2x, 0, 4);
+    __lasx_xvstelm_w(temp0, dest + stride_3x, 0, 1);
 }
 
 static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
@@ -569,65 +584,70 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     int shift = (shift_value[hmode] + shift_value[vmode]) >> 1;
     int r     = (1 << (shift - 1)) + rnd - 1;
     const uint8_t *para_v = para_value[vmode - 1];
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
 
     const_r  = __lasx_xvreplgr2vr_h(r);
     const_sh = __lasx_xvreplgr2vr_h(shift);
     src -= 1, src -= stride;
     const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
     const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
-    LASX_LD_4(src, stride, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0, src + stride_3x,
+              0, in0, in1, in2, in3);
     in0   = __lasx_xvpermi_d(in0, 0xD8);
     in1   = __lasx_xvpermi_d(in1, 0xD8);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
     in3   = __lasx_xvpermi_d(in3, 0xD8);
-    LASX_ILVL_B_2_128SV(in2, in1, in3, in0, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t0);
-    LASX_DP2SUB_H_BU(t0, temp1, const_para0_3, t0);
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+    t0 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t0 = __lasx_xvdp2sub_h_bu(t0, temp1, const_para0_3);
     src  += (stride << 2);
-    in0   = LASX_LD(src);
+    in0   = __lasx_xvld(src, 0);
     in0   = __lasx_xvpermi_d(in0, 0xD8);
-    LASX_ILVL_B_2_128SV(in3, in2, in0, in1, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t1);
-    LASX_DP2SUB_H_BU(t1, temp1, const_para0_3, t1);
+    DUP2_ARG2(__lasx_xvilvl_b, in3, in2, in0, in1, temp0, temp1);
+    t1 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t1 = __lasx_xvdp2sub_h_bu(t1, temp1, const_para0_3);
     src  += stride;
-    in1   = LASX_LD(src);
+    in1   = __lasx_xvld(src, 0);
     in1   = __lasx_xvpermi_d(in1, 0xD8);
-    LASX_ILVL_B_2_128SV(in0, in3, in1, in2, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t2);
-    LASX_DP2SUB_H_BU(t2, temp1, const_para0_3, t2);
+    DUP2_ARG2(__lasx_xvilvl_b, in0, in3, in1, in2, temp0, temp1);
+    t2 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t2 = __lasx_xvdp2sub_h_bu(t2, temp1, const_para0_3);
     src  += stride;
-    in2   = LASX_LD(src);
+    in2   = __lasx_xvld(src, 0);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
-    LASX_ILVL_B_2_128SV(in1, in0, in2, in3, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t3);
-    LASX_DP2SUB_H_BU(t3, temp1, const_para0_3, t3);
+    DUP2_ARG2(__lasx_xvilvl_b, in1, in0, in2, in3, temp0, temp1);
+    t3 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t3 = __lasx_xvdp2sub_h_bu(t3, temp1, const_para0_3);
     src  += stride;
-    in3   = LASX_LD(src);
+    in3   = __lasx_xvld(src, 0);
     in3   = __lasx_xvpermi_d(in3, 0xD8);
-    LASX_ILVL_B_2_128SV(in2, in1, in3, in0, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t4);
-    LASX_DP2SUB_H_BU(t4, temp1, const_para0_3, t4);
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+    t4 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t4 = __lasx_xvdp2sub_h_bu(t4, temp1, const_para0_3);
     src  += stride;
-    in0   = LASX_LD(src);
+    in0   = __lasx_xvld(src, 0);
     in0   = __lasx_xvpermi_d(in0, 0xD8);
-    LASX_ILVL_B_2_128SV(in3, in2, in0, in1, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t5);
-    LASX_DP2SUB_H_BU(t5, temp1, const_para0_3, t5);
+    DUP2_ARG2(__lasx_xvilvl_b, in3, in2, in0, in1, temp0, temp1);
+    t5 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t5 = __lasx_xvdp2sub_h_bu(t5, temp1, const_para0_3);
     src  += stride;
-    in1   = LASX_LD(src);
+    in1   = __lasx_xvld(src, 0);
     in1   = __lasx_xvpermi_d(in1, 0xD8);
-    LASX_ILVL_B_2_128SV(in0, in3, in1, in2, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t6);
-    LASX_DP2SUB_H_BU(t6, temp1, const_para0_3, t6);
+    DUP2_ARG2(__lasx_xvilvl_b, in0, in3, in1, in2, temp0, temp1);
+    t6 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t6 = __lasx_xvdp2sub_h_bu(t6, temp1, const_para0_3);
     src  += stride;
-    in2   = LASX_LD(src);
+    in2   = __lasx_xvld(src, 0);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
-    LASX_ILVL_B_2_128SV(in1, in0, in2, in3, temp0, temp1);
-    LASX_DP2_H_BU(temp0, const_para1_2, t7);
-    LASX_DP2SUB_H_BU(t7, temp1, const_para0_3, t7);
-    LASX_ADD_H_8(t0, const_r, t1, const_r, t2, const_r, t3, const_r,
-                 t4, const_r, t5, const_r, t6, const_r, t7, const_r,
-                 t0, t1, t2, t3, t4, t5, t6, t7);
+    DUP2_ARG2(__lasx_xvilvl_b, in1, in0, in2, in3, temp0, temp1);
+    t7 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t7 = __lasx_xvdp2sub_h_bu(t7, temp1, const_para0_3);
+    DUP4_ARG2(__lasx_xvadd_h, t0, const_r, t1, const_r, t2, const_r, t3, const_r,
+              t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvadd_h, t4, const_r, t5, const_r, t6, const_r, t7, const_r,
+              t4, t5, t6, t7);
     t0    = __lasx_xvsra_h(t0, const_sh);
     t1    = __lasx_xvsra_h(t1, const_sh);
     t2    = __lasx_xvsra_h(t2, const_sh);
@@ -636,8 +656,7 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     t5    = __lasx_xvsra_h(t5, const_sh);
     t6    = __lasx_xvsra_h(t6, const_sh);
     t7    = __lasx_xvsra_h(t7, const_sh);
-    LASX_TRANSPOSE8x8_H_128SV(t0, t1, t2, t3, t4, t5, t6, t7,
-                              t0, t1, t2, t3, t4, t5, t6, t7);
+    LASX_TRANSPOSE8x8_H(t0, t1, t2, t3, t4, t5, t6, t7, t0, t1, t2, t3, t4, t5, t6, t7);
     para_v  = para_value[hmode - 1];
     const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
     const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
@@ -656,30 +675,30 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     t5      = __lasx_xvpermi_d(t5, 0xD8);
     t6      = __lasx_xvpermi_d(t6, 0xD8);
     t7      = __lasx_xvpermi_d(t7, 0xD8);
-    LASX_ILVL_H_2_128SV(t2, t1, t3, t0, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t0);
-    LASX_DP2SUB_W_H(t0, temp1, const_para0_3, t0);
-    LASX_ILVL_H_2_128SV(t3, t2, t4, t1, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t1);
-    LASX_DP2SUB_W_H(t1, temp1, const_para0_3, t1);
-    LASX_ILVL_H_2_128SV(t4, t3, t5, t2, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t2);
-    LASX_DP2SUB_W_H(t2, temp1, const_para0_3, t2);
-    LASX_ILVL_H_2_128SV(t5, t4, t6, t3, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t3);
-    LASX_DP2SUB_W_H(t3, temp1, const_para0_3, t3);
-    LASX_ILVL_H_2_128SV(t6, t5, t7, t4, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t4);
-    LASX_DP2SUB_W_H(t4, temp1, const_para0_3, t4);
-    LASX_ILVL_H_2_128SV(t7, t6, in0, t5, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t5);
-    LASX_DP2SUB_W_H(t5, temp1, const_para0_3, t5);
-    LASX_ILVL_H_2_128SV(in0, t7, in1, t6, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t6);
-    LASX_DP2SUB_W_H(t6, temp1, const_para0_3, t6);
-    LASX_ILVL_H_2_128SV(in1, in0, in2, t7, temp0, temp1);
-    LASX_DP2_W_H(temp0, const_para1_2, t7);
-    LASX_DP2SUB_W_H(t7, temp1, const_para0_3, t7);
+    DUP2_ARG2(__lasx_xvilvl_h, t2, t1, t3, t0, temp0, temp1);
+    t0 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t0 = __lasx_xvdp2sub_w_h(t0, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t3, t2, t4, t1, temp0, temp1);
+    t1 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t1 = __lasx_xvdp2sub_w_h(t1, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t4, t3, t5, t2, temp0, temp1);
+    t2 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t2 = __lasx_xvdp2sub_w_h(t2, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t5, t4, t6, t3, temp0, temp1);
+    t3 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t3 = __lasx_xvdp2sub_w_h(t3, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t6, t5, t7, t4, temp0, temp1);
+    t4 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t4 = __lasx_xvdp2sub_w_h(t4, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t7, t6, in0, t5, temp0, temp1);
+    t5 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t5 = __lasx_xvdp2sub_w_h(t5, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, in0, t7, in1, t6, temp0, temp1);
+    t6 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t6 = __lasx_xvdp2sub_w_h(t6, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in2, t7, temp0, temp1);
+    t7 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t7 = __lasx_xvdp2sub_w_h(t7, temp1, const_para0_3);
     t0    = __lasx_xvadd_w(t0, const_r);
     t1    = __lasx_xvadd_w(t1, const_r);
     t2    = __lasx_xvadd_w(t2, const_r);
@@ -688,20 +707,25 @@ static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
     t5    = __lasx_xvadd_w(t5, const_r);
     t6    = __lasx_xvadd_w(t6, const_r);
     t7    = __lasx_xvadd_w(t7, const_r);
-    LASX_SRAI_W_8(t0, t1, t2, t3, t4, t5, t6, t7,
-                  t0, t1, t2, t3, t4, t5, t6, t7, 7);
+    DUP4_ARG2(__lasx_xvsrai_w, t0, 7, t1, 7, t2, 7, t3, 7, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvsrai_w, t4, 7, t5, 7, t6, 7, t7, 7, t4, t5, t6, t7);
     LASX_TRANSPOSE8x8_W(t0, t1, t2, t3, t4, t5, t6, t7,
                         t0, t1, t2, t3, t4, t5, t6, t7);
-    LASX_CLIP_W_0_255_4(t0, t1, t2, t3, t0, t1, t2, t3);
-    LASX_CLIP_W_0_255_4(t4, t5, t6, t7, t4, t5, t6, t7);
-    LASX_PCKEV_H_4_128SV(t1, t0, t3, t2, t5, t4, t7, t6,
-                         t0, t1, t2, t3);
-    LASX_PCKEV_B_2_128SV(t1, t0, t3, t2, t0, t1);
+    DUP4_ARG1(__lasx_xvclip255_w, t0, t1, t2, t3, t0, t1, t2, t3);
+    DUP4_ARG1(__lasx_xvclip255_w, t4, t5, t6, t7, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvpickev_h, t1, t0, t3, t2, t5, t4, t7, t6, t0, t1, t2, t3);
+    DUP2_ARG2(__lasx_xvpickev_b, t1, t0, t3, t2, t0, t1);
     t0 = __lasx_xvperm_w(t0, sh);
     t1 = __lasx_xvperm_w(t1, sh);
-    LASX_ST_D_4(t0, 0, 1, 2, 3, dst, stride);
-    dst += (stride << 2);
-    LASX_ST_D_4(t1, 0, 1, 2, 3, dst, stride);
+    __lasx_xvstelm_d(t0, dst, 0, 0);
+    __lasx_xvstelm_d(t0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(t0, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(t0, dst + stride_3x, 0, 3);
+    dst += stride_4x;
+    __lasx_xvstelm_d(t1, dst, 0, 0);
+    __lasx_xvstelm_d(t1, dst + stride, 0, 1);
+    __lasx_xvstelm_d(t1, dst + stride_2x, 0, 2);
+    __lasx_xvstelm_d(t1, dst + stride_3x, 0, 3);
 }
 
 #define PUT_VC1_MSPEL_MC_LASX(hmode, vmode)                                   \
@@ -753,12 +777,12 @@ void ff_put_no_rnd_vc1_chroma_mc8_lasx(uint8_t *dst /* align 8 */,
     C = __lasx_xvreplgr2vr_h(intC);
     D = __lasx_xvreplgr2vr_h(intD);
     for(i = 0; i < h; i++){
-        LASX_LD_2(src, 1, src00, src01);
+        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src00, src01);
         src += stride;
-        LASX_LD_2(src, 1, src10, src11);
+        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src10, src11);
 
-        LASX_UNPCK_L_HU_BU_4(src00, src01, src10, src11,
-                             src00, src01, src10, src11);
+        DUP4_ARG1(__lasx_vext2xv_hu_bu, src00, src01, src10, src11,
+                  src00, src01, src10, src11);
         src00 = __lasx_xvmul_h(src00, A);
         src01 = __lasx_xvmul_h(src01, B);
         src10 = __lasx_xvmul_h(src10, C);
@@ -768,8 +792,8 @@ void ff_put_no_rnd_vc1_chroma_mc8_lasx(uint8_t *dst /* align 8 */,
         src00 = __lasx_xvadd_h(src00, src10);
         src00 = __lasx_xvaddi_hu(src00, 28);
         src00 = __lasx_xvsrli_h(src00, 6);
-        LASX_PCKEV_B_128SV(src00, src00, src00);
-        LASX_ST_D(src00, 0, dst);
+        src00 = __lasx_xvpickev_b(src00, src00);
+        __lasx_xvstelm_d(src00, dst, 0, 0);
         dst += stride;
     }
 }
@@ -794,22 +818,23 @@ static void put_vc1_mspel_mc_v_lasx(uint8_t *dst, const uint8_t *src,
     const_para0_3 = __lasx_xvreplgr2vr_h(*para_v);
     const_para1_2 = __lasx_xvreplgr2vr_h(*(para_v + 1));
 
-    LASX_LD_2((src - stride), stride, in0, in1);
-    in2 = LASX_LD(src + stride);
+    DUP2_ARG2(__lasx_xvld, src - stride, 0, src, 0, in0, in1);
+    in2 = __lasx_xvld(src + stride, 0);
     in0   = __lasx_xvpermi_d(in0, 0xD8);
     in1   = __lasx_xvpermi_d(in1, 0xD8);
     in2   = __lasx_xvpermi_d(in2, 0xD8);
     for (; i < 16; i++) {
-        in3 = LASX_LD(src + stride_2x);
+        in3 = __lasx_xvld(src + stride_2x, 0);
         in3 = __lasx_xvpermi_d(in3, 0xD8);
-        LASX_ILVL_B_2_128SV(in2, in1, in3, in0, temp0, temp1);
-        LASX_DP2_H_BU(temp0, const_para1_2, t0);
-        LASX_DP2SUB_H_BU(t0, temp1, const_para0_3, t0);
-        LASX_ADD_H(t0, const_r, t0);
+        DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+        t0 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+        t0 = __lasx_xvdp2sub_h_bu(t0, temp1, const_para0_3);
+        t0 = __lasx_xvadd_h(t0, const_r);
         t0 = __lasx_xvsra_h(t0, const_sh);
-        LASX_CLIP_H_0_255(t0, t0);
-        LASX_PCKEV_B_128SV(t0, t0, t0);
-        LASX_ST_D_2(t0, 0, 2, dst, 8);
+        t0 = __lasx_xvclip255_h(t0);
+        t0 = __lasx_xvpickev_b(t0, t0);
+        __lasx_xvstelm_d(t0, dst, 0, 0);
+        __lasx_xvstelm_d(t0, dst, 8, 2);
         dst += stride;
         src += stride;
         in0 = in1;
@@ -830,14 +855,15 @@ PUT_VC1_MSPEL_MC_V_LASX(1);
 PUT_VC1_MSPEL_MC_V_LASX(2);
 PUT_VC1_MSPEL_MC_V_LASX(3);
 
-#define ROW_LASX(in0, in1, in2, in3, out0)                   \
-    LASX_ILVL_B_2_128SV(in2, in1, in3, in0, tmp0_m, tmp1_m); \
-    LASX_DP2_H_BU(tmp0_m, const_para1_2, out0);              \
-    LASX_DP2SUB_H_BU(out0, tmp1_m, const_para0_3, out0);     \
-    LASX_ADD_H(out0, const_r, out0);                         \
-    out0 = __lasx_xvsra_h(out0, const_sh);                   \
-    LASX_CLIP_H_0_255(out0, out0);                           \
-    LASX_PCKEV_B(out0, out0, out0);
+#define ROW_LASX(in0, in1, in2, in3, out0)                                \
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, tmp0_m, tmp1_m);       \
+    out0 = __lasx_xvdp2_h_bu(tmp0_m, const_para1_2);                      \
+    out0 = __lasx_xvdp2sub_h_bu(out0, tmp1_m, const_para0_3);             \
+    out0 = __lasx_xvadd_h(out0, const_r);                                 \
+    out0 = __lasx_xvsra_h(out0, const_sh);                                \
+    out0 = __lasx_xvclip255_h(out0);                                      \
+    out0 = __lasx_xvpickev_b(out0, out0);                                 \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                  \
 
 static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
                                     ptrdiff_t stride, int hmode, int rnd)
@@ -850,6 +876,9 @@ static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
     __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;
     __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;
     __m256i t0, t1, t2, t3, t4, t5, t6, t7;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
     static const uint16_t para_value[][2] = {{0x0304, 0x1235},
                                             {0x0101, 0x0909},
                                             {0x0403, 0x3512}};
@@ -864,33 +893,53 @@ static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
     const_para1_2 = __lasx_xvreplgr2vr_h(*(para_v + 1));
     src -= 1;
 
-    LASX_LD_8(src, stride, in0, in1, in2, in3, in4, in5, in6, in7);
-    src += stride << 3;
-    LASX_LD_8(src, stride, in8, in9, in10, in11, in12, in13, in14, in15);
-    LASX_ILVL_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,
-                        in10, in8, in11, in9, in14, in12, in15, in13,
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out0, out3, out2);
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out5, out4, out7, out6);
-
-    LASX_ILVH_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,
-                        in10, in8, in11, in9, in14, in12, in15, in13,
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out9, out8, out11, out10);
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m,
-                         out13, out12, out15, out14);
-    LASX_PCKOD_Q_2(out0, out0, out1, out1, out16, out17);
-    LASX_PCKOD_Q(out2, out2, out18);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
+              src + stride_3x, 0, in0, in1, in2, in3);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
+              src + stride_3x, 0, in4, in5, in6, in7);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
+              src + stride_3x, 0, in8, in9, in10, in11);
+    src += stride_4x;
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride_2x, 0,
+              src + stride_3x, 0, in12, in13, in14, in15);
+    src -= stride_4x;
+    DUP4_ARG2(__lasx_xvilvl_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvl_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out0, out2, out4, out6);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out1, out3, out5, out7);
+
+    DUP4_ARG2(__lasx_xvilvh_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvh_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out8, out10, out12, out14);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out9, out11, out13, out15);
+    DUP2_ARG3(__lasx_xvpermi_q, out0, out0, 0x31, out1, out1, 0x31, out16, out17);
+    out18 = __lasx_xvpermi_q(out2, out2, 0x31);
 
     out0  = __lasx_xvpermi_d(out0, 0xD8);
     out1  = __lasx_xvpermi_d(out1, 0xD8);
@@ -929,59 +978,86 @@ static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
     ROW_LASX(out14, out15, out16, out17, in14);
     ROW_LASX(out15, out16, out17, out18, in15);
 
-    LASX_ILVL_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,
-                        in10, in8, in11, in9, in14, in12, in15, in13,
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out0, out3, out2);
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out5, out4, out7, out6);
-
-    LASX_ILVH_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,
-                        in10, in8, in11, in9, in14, in12, in15, in13,
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out9, out8, out11, out10);
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m,
-                         out13, out12, out15, out14);
-    LASX_ST_D_2(out0, 0, 1, dst, 8);
+    DUP4_ARG2(__lasx_xvilvl_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvl_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out0, out2, out4, out6);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out1, out3, out5, out7);
+
+    DUP4_ARG2(__lasx_xvilvh_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvh_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out8, out10, out12, out14);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out9, out11, out13, out15);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out1, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out2, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out2, dst, 0, 0);
+    __lasx_xvstelm_d(out2, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out3, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out3, dst, 0, 0);
+    __lasx_xvstelm_d(out3, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out4, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out4, dst, 0, 0);
+    __lasx_xvstelm_d(out4, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out5, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out5, dst, 0, 0);
+    __lasx_xvstelm_d(out5, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out6, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out6, dst, 0, 0);
+    __lasx_xvstelm_d(out6, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out7, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out7, dst, 0, 0);
+    __lasx_xvstelm_d(out7, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out8, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out8, dst, 0, 0);
+    __lasx_xvstelm_d(out8, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out9, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out9, dst, 0, 0);
+    __lasx_xvstelm_d(out9, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out10, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out10, dst, 0, 0);
+    __lasx_xvstelm_d(out10, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out11, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out11, dst, 0, 0);
+    __lasx_xvstelm_d(out11, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out12, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out12, dst, 0, 0);
+    __lasx_xvstelm_d(out12, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out13, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out13, dst, 0, 0);
+    __lasx_xvstelm_d(out13, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out14, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out14, dst, 0, 0);
+    __lasx_xvstelm_d(out14, dst, 8, 1);
     dst += stride;
-    LASX_ST_D_2(out15, 0, 1, dst, 8);
+    __lasx_xvstelm_d(out15, dst, 0, 0);
+    __lasx_xvstelm_d(out15, dst, 8, 1);
 }
 
 #define PUT_VC1_MSPEL_MC_H_LASX(hmode)                                    \
diff --git a/libavcodec/loongarch/vp8_lpf_lsx.c b/libavcodec/loongarch/vp8_lpf_lsx.c
index d16d5dca4f..f0fc3f3a5b 100644
--- a/libavcodec/loongarch/vp8_lpf_lsx.c
+++ b/libavcodec/loongarch/vp8_lpf_lsx.c
@@ -21,7 +21,62 @@
 
 #include "libavcodec/vp8dsp.h"
 #include "vp8dsp_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+#define VP8_LPF_FILTER4_4W(p1_in_out, p0_in_out, q0_in_out, q1_in_out,  \
+                           mask_in, hev_in)                             \
+{                                                                       \
+    __m128i p1_m, p0_m, q0_m, q1_m, q0_sub_p0, filt_sign;               \
+    __m128i filt, filt1, filt2, cnst4b, cnst3b;                         \
+    __m128i q0_sub_p0_l, q0_sub_p0_h, filt_h, filt_l, cnst3h;           \
+                                                                        \
+    p1_m = __lsx_vxori_b(p1_in_out, 0x80);                              \
+    p0_m = __lsx_vxori_b(p0_in_out, 0x80);                              \
+    q0_m = __lsx_vxori_b(q0_in_out, 0x80);                              \
+    q1_m = __lsx_vxori_b(q1_in_out, 0x80);                              \
+    filt = __lsx_vssub_b(p1_m, q1_m);                                   \
+    filt = filt & hev_in;                                               \
+                                                                        \
+    q0_sub_p0 = __lsx_vsub_b(q0_m, p0_m);                               \
+    filt_sign = __lsx_vslti_b(filt, 0);                                 \
+                                                                        \
+    cnst3h = __lsx_vreplgr2vr_h(3);                                     \
+    q0_sub_p0_l = __lsx_vilvl_b(q0_sub_p0, q0_sub_p0);                  \
+    q0_sub_p0_l = __lsx_vdp2_h_b(q0_sub_p0_l, cnst3h);                  \
+    filt_l = __lsx_vilvl_b(filt_sign, filt);                            \
+    filt_l = __lsx_vadd_h(filt_l, q0_sub_p0_l);                         \
+    filt_l = __lsx_vsat_h(filt_l, 7);                                   \
+                                                                        \
+    q0_sub_p0_h = __lsx_vilvh_b(q0_sub_p0, q0_sub_p0);                  \
+    q0_sub_p0_h = __lsx_vdp2_h_b(q0_sub_p0_h, cnst3h);                  \
+    filt_h = __lsx_vilvh_b(filt_sign, filt);                            \
+    filt_h = __lsx_vadd_h(filt_h, q0_sub_p0_h);                         \
+    filt_h = __lsx_vsat_h(filt_h, 7);                                   \
+                                                                        \
+    filt = __lsx_vpickev_b(filt_h, filt_l);                             \
+    filt = filt & mask_in;                                              \
+    cnst4b = __lsx_vreplgr2vr_b(4);                                     \
+    filt1 = __lsx_vsadd_b(filt, cnst4b);                                \
+    filt1 = __lsx_vsrai_b(filt1, 3);                                    \
+                                                                        \
+    cnst3b = __lsx_vreplgr2vr_b(3);                                     \
+    filt2 = __lsx_vsadd_b(filt, cnst3b);                                \
+    filt2 = __lsx_vsrai_b(filt2, 3);                                    \
+                                                                        \
+    q0_m = __lsx_vssub_b(q0_m, filt1);                                  \
+    q0_in_out = __lsx_vxori_b(q0_m, 0x80);                              \
+    p0_m = __lsx_vsadd_b(p0_m, filt2);                                  \
+    p0_in_out = __lsx_vxori_b(p0_m, 0x80);                              \
+                                                                        \
+    filt = __lsx_vsrari_b(filt1, 1);                                    \
+    hev_in = __lsx_vxori_b(hev_in, 0xff);                               \
+    filt = filt & hev_in;                                               \
+                                                                        \
+    q1_m = __lsx_vssub_b(q1_m, filt);                                   \
+    q1_in_out = __lsx_vxori_b(q1_m, 0x80);                              \
+    p1_m = __lsx_vsadd_b(p1_m, filt);                                   \
+    p1_in_out = __lsx_vxori_b(p1_m, 0x80);                              \
+}
 
 #define VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev)             \
 {                                                                   \
@@ -172,11 +227,22 @@
 
 #define VP8_ST6x1_UB(in0, in0_idx, in1, in1_idx, pdst, stride)      \
 {                                                                   \
-                                                                    \
     __lsx_vstelm_w(in0, pdst, 0, in0_idx);                          \
     __lsx_vstelm_h(in1, pdst + stride, 0, in1_idx);                 \
 }
 
+#define ST_W4(in, idx0, idx1, idx2, idx3, pdst, stride)     \
+{                                                           \
+    __lsx_vstelm_w(in, pdst, 0, idx0);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx1);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx2);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx3);                      \
+    pdst += stride;                                         \
+}
+
 void ff_vp8_v_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride, int b_limit_in,
                                 int limit_in, int thresh_in)
 {
@@ -192,10 +258,10 @@ void ff_vp8_v_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride, int b_limit_in,
     thresh = __lsx_vreplgr2vr_b(thresh_in);
 
     /*load vector elements*/
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3, 0,
-                  q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3, 0,
+              q0, q1, q2, q3);
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
     VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
 
@@ -226,19 +292,19 @@ void ff_vp8_v_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
     limit = __lsx_vreplgr2vr_b(limit_in);
     thresh = __lsx_vreplgr2vr_b(thresh_in);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_u - stride4, 0, dst_u - stride3, 0, dst_u - stride2, 0,
-                  dst_u - stride, 0, p3_u, p2_u, p1_u, p0_u);
-    LSX_DUP4_ARG2(__lsx_vld, dst_u, 0, dst_u + stride, 0, dst_u + stride2, 0,
-                  dst_u + stride3, 0, q0_u, q1_u, q2_u, q3_u);
+    DUP4_ARG2(__lsx_vld, dst_u - stride4, 0, dst_u - stride3, 0, dst_u - stride2, 0,
+              dst_u - stride, 0, p3_u, p2_u, p1_u, p0_u);
+    DUP4_ARG2(__lsx_vld, dst_u, 0, dst_u + stride, 0, dst_u + stride2, 0,
+              dst_u + stride3, 0, q0_u, q1_u, q2_u, q3_u);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_v - stride4, 0, dst_v - stride3, 0, dst_v - stride2, 0,
-                  dst_v - stride, 0, p3_v, p2_v, p1_v, p0_v);
-    LSX_DUP4_ARG2(__lsx_vld, dst_v, 0, dst_v + stride, 0, dst_v + stride2, 0,
-                  dst_v + stride3, 0, q0_v, q1_v, q2_v, q3_v);
+    DUP4_ARG2(__lsx_vld, dst_v - stride4, 0, dst_v - stride3, 0, dst_v - stride2, 0,
+              dst_v - stride, 0, p3_v, p2_v, p1_v, p0_v);
+    DUP4_ARG2(__lsx_vld, dst_v, 0, dst_v + stride, 0, dst_v + stride2, 0,
+              dst_v + stride3, 0, q0_v, q1_v, q2_v, q3_v);
 
     /* rht 8 element of p3 are u pixel and left 8 element of p3 are v pixei */
-    LSX_DUP4_ARG2(__lsx_vilvl_d, p3_v, p3_u, p2_v, p2_u, p1_v, p1_u, p0_v, p0_u, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vilvl_d, q0_v, q0_u, q1_v, q1_u, q2_v, q2_u, q3_v, q3_u, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vilvl_d, p3_v, p3_u, p2_v, p2_u, p1_v, p1_u, p0_v, p0_u, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vilvl_d, q0_v, q0_u, q1_v, q1_u, q2_v, q2_u, q3_v, q3_u, q0, q1, q2, q3);
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
                  hev, mask, flat);
     VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
@@ -279,20 +345,20 @@ void ff_vp8_h_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride, int b_limit_in,
     thresh = __lsx_vreplgr2vr_b(thresh_in);
 
     temp_src = dst - 4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
-                  temp_src + stride3, 0, row0, row1, row2, row3);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row0, row1, row2, row3);
     temp_src += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0,
-                  temp_src + stride2, 0, temp_src + stride3, 0, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row4, row5, row6, row7);
 
     temp_src += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
-                  temp_src + stride3, 0, row8, row9, row10, row11);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row8, row9, row10, row11);
     temp_src += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0,
-                  temp_src + stride2, 0, temp_src + stride3, 0, row12, row13, row14, row15);
-    TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7, row8, row9, row10, row11,
-                    row12, row13, row14, row15, p3, p2, p1, p0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row12, row13, row14, row15);
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7, row8, row9, row10,
+                        row11, row12, row13, row14, row15, p3, p2, p1, p0, q0, q1, q2, q3);
 
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
     VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
@@ -366,22 +432,22 @@ void ff_vp8_h_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
     thresh = __lsx_vreplgr2vr_b(thresh_in);
 
     temp_src = dst_u - 4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
-                  temp_src + stride3, 0, row0, row1, row2, row3);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row0, row1, row2, row3);
     temp_src += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0,
-                  temp_src + stride2, 0, temp_src + stride3, 0, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row4, row5, row6, row7);
 
     temp_src = dst_v - 4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
-                  temp_src + stride3, 0, row8, row9, row10, row11);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row8, row9, row10, row11);
     temp_src += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0,
-                  temp_src + stride2, 0, temp_src + stride3, 0, row12, row13, row14, row15);
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row12, row13, row14, row15);
 
-    TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
-                    row8, row9, row10, row11, row12, row13, row14, row15,
-                    p3, p2, p1, p0, q0, q1, q2, q3);
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
+                        row8, row9, row10, row11, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
 
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
     VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
@@ -435,3 +501,91 @@ void ff_vp8_h_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
     dst_v += stride;
     VP8_ST6x1_UB(tmp7, 3, tmp5, 7, dst_v, 4);
 }
+
+void ff_vp8_v_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h)
+{
+    __m128i mask, hev, flat;
+    __m128i thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    /* load vector elements */
+    src -= stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, p3, p2, p1, p0);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, q0, q1, q2, q3);
+    thresh = __lsx_vreplgr2vr_b(h);
+    b_limit = __lsx_vreplgr2vr_b(e);
+    limit = __lsx_vreplgr2vr_b(i);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP8_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev);
+
+    __lsx_vst(p1, src - stride2, 0);
+    __lsx_vst(p0, src - stride,  0);
+    __lsx_vst(q0, src,           0);
+    __lsx_vst(q1, src + stride,  0);
+}
+
+void ff_vp8_h_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h)
+{
+    __m128i mask, hev, flat;
+    __m128i thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    src -= 4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp0, tmp1, tmp2, tmp3);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp4, tmp5, tmp6, tmp7);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp8, tmp9, tmp10, tmp11);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp12, tmp13, tmp14, tmp15);
+    src -= 3 * stride4;
+
+    LSX_TRANSPOSE16x8_B(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                        tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh = __lsx_vreplgr2vr_b(h);
+    b_limit = __lsx_vreplgr2vr_b(e);
+    limit = __lsx_vreplgr2vr_b(i);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP8_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev);
+
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp2 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp3 = __lsx_vilvh_h(tmp1, tmp0);
+
+    src += 2;
+    ST_W4(tmp2, 0, 1, 2, 3, src, stride);
+    ST_W4(tmp3, 0, 1, 2, 3, src, stride);
+
+    DUP2_ARG2(__lsx_vilvh_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp2 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp3 = __lsx_vilvh_h(tmp1, tmp0);
+
+    ST_W4(tmp2, 0, 1, 2, 3, src, stride);
+    ST_W4(tmp3, 0, 1, 2, 3, src, stride);
+    src -= 4 * stride4;
+}
diff --git a/libavcodec/loongarch/vp8_mc_lsx.c b/libavcodec/loongarch/vp8_mc_lsx.c
new file mode 100644
index 0000000000..80c4f87e80
--- /dev/null
+++ b/libavcodec/loongarch/vp8_mc_lsx.c
@@ -0,0 +1,951 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+#include "libavcodec/vp8dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "vp8dsp_loongarch.h"
+
+static const uint8_t mc_filt_mask_arr[16 * 3] = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20,
+    /* 4 width cases */
+    8, 9, 9, 10, 10, 11, 11, 12, 24, 25, 25, 26, 26, 27, 27, 28
+};
+
+static const int8_t subpel_filters_lsx[7][8] = {
+    {-6, 123, 12, -1, 0, 0, 0, 0},
+    {2, -11, 108, 36, -8, 1, 0, 0},     /* New 1/4 pel 6 tap filter */
+    {-9, 93, 50, -6, 0, 0, 0, 0},
+    {3, -16, 77, 77, -16, 3, 0, 0},     /* New 1/2 pel 6 tap filter */
+    {-6, 50, 93, -9, 0, 0, 0, 0},
+    {1, -8, 36, 108, -11, 2, 0, 0},     /* New 1/4 pel 6 tap filter */
+    {-1, 12, 123, -6, 0, 0, 0, 0},
+};
+
+#define DPADD_SH3_SH(in0, in1, in2, coeff0, coeff1, coeff2)         \
+( {                                                                 \
+    __m128i out0_m;                                                 \
+                                                                    \
+    out0_m = __lsx_vdp2_h_b(in0, coeff0);                           \
+    out0_m = __lsx_vdp2add_h_b(out0_m, in1, coeff1);                \
+    out0_m = __lsx_vdp2add_h_b(out0_m, in2, coeff2);                \
+                                                                    \
+    out0_m;                                                         \
+} )
+
+#define VSHF_B3_SB(in0, in1, in2, in3, in4, in5, mask0, mask1, mask2,  \
+                out0, out1, out2)                                      \
+{                                                                      \
+    DUP2_ARG3(__lsx_vshuf_b, in1, in0, mask0, in3, in2, mask1,         \
+              out0, out1);                                             \
+    out2 = __lsx_vshuf_b(in5, in4, mask2);                             \
+}
+
+#define HORIZ_6TAP_FILT(src0, src1, mask0, mask1, mask2,                 \
+                        filt_h0, filt_h1, filt_h2)                       \
+( {                                                                      \
+    __m128i vec0_m, vec1_m, vec2_m;                                      \
+    __m128i hz_out_m;                                                    \
+                                                                         \
+    VSHF_B3_SB(src0, src1, src0, src1, src0, src1, mask0, mask1, mask2,  \
+               vec0_m, vec1_m, vec2_m);                                  \
+    hz_out_m = DPADD_SH3_SH(vec0_m, vec1_m, vec2_m,                      \
+                            filt_h0, filt_h1, filt_h2);                  \
+                                                                         \
+    hz_out_m = __lsx_vsrari_h(hz_out_m, 7);                              \
+    hz_out_m = __lsx_vsat_h(hz_out_m, 7);                                \
+                                                                         \
+    hz_out_m;                                                            \
+} )
+
+#define HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3,                            \
+                                   mask0, mask1, mask2,                               \
+                                   filt0, filt1, filt2,                               \
+                                   out0, out1, out2, out3)                            \
+{                                                                                     \
+    __m128i vec0_m, vec1_m, vec2_m, vec3_m, vec4_m, vec5_m, vec6_m, vec7_m;           \
+                                                                                      \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,        \
+              mask0, src3, src3, mask0, vec0_m, vec1_m, vec2_m, vec3_m);              \
+    DUP4_ARG2(__lsx_vdp2_h_b, vec0_m, filt0, vec1_m, filt0, vec2_m, filt0,            \
+              vec3_m, filt0, out0, out1, out2, out3);                                 \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,        \
+              mask1, src3, src3, mask1, vec0_m, vec1_m, vec2_m, vec3_m);              \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,        \
+              mask2, src3, src3, mask2, vec4_m, vec5_m, vec6_m, vec7_m);              \
+    DUP4_ARG3(__lsx_vdp2add_h_b, out0, vec0_m, filt1, out1, vec1_m, filt1,            \
+              out2, vec2_m, filt1, out3, vec3_m, filt1, out0, out1, out2, out3);      \
+    DUP4_ARG3(__lsx_vdp2add_h_b, out0, vec4_m, filt2, out1, vec5_m, filt2,            \
+              out2, vec6_m, filt2, out3, vec7_m, filt2, out0, out1, out2, out3);      \
+}
+
+#define FILT_4TAP_DPADD_S_H(vec0, vec1, filt0, filt1)           \
+( {                                                             \
+    __m128i tmp0;                                               \
+                                                                \
+    tmp0 = __lsx_vdp2_h_b(vec0, filt0);                         \
+    tmp0 = __lsx_vdp2add_h_b(tmp0, vec1, filt1);                \
+                                                                \
+    tmp0;                                                       \
+} )
+
+#define HORIZ_4TAP_FILT(src0, src1, mask0, mask1, filt_h0, filt_h1)    \
+( {                                                                    \
+    __m128i vec0_m, vec1_m;                                            \
+    __m128i hz_out_m;                                                  \
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1,     \
+              vec0_m, vec1_m);                                         \
+    hz_out_m = FILT_4TAP_DPADD_S_H(vec0_m, vec1_m, filt_h0, filt_h1);  \
+                                                                       \
+    hz_out_m = __lsx_vsrari_h(hz_out_m, 7);                            \
+    hz_out_m = __lsx_vsat_h(hz_out_m, 7);                              \
+                                                                       \
+    hz_out_m;                                                          \
+} )
+
+void ff_put_vp8_epel8_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[mx - 1];
+    __m128i src0, src1, src2, src3, filt0, filt1, filt2;
+    __m128i mask0, mask1, mask2;
+    __m128i out0, out1, out2, out3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 2;
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src += src_stride4;
+    HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                               filt0, filt1, filt2, out0, out1, out2, out3);
+
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    __lsx_vstelm_d(out0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 1);
+    dst += dst_stride;
+
+    for (loop_cnt = (height >> 2) - 1; loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        src += src_stride4;
+        HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out0, out1, out2, out3);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel16_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[mx - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, filt0, filt1;
+    __m128i filt2, mask0, mask1, mask2;
+    __m128i out0, out1, out2, out3, out4, out5, out6, out7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 2;
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src0 ,src2, src4, src6);
+        DUP4_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src + src_stride2,
+                  8, src + src_stride3, 8, src1, src3, src5, src7);
+
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128,
+                  src4, src5, src6, src7);
+        src += src_stride4;
+
+        HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out0, out1, out2, out3);
+        HORIZ_6TAP_8WID_4VECS_FILT(src4, src5, src6, src7, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out4, out5, out6, out7);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(out1, dst, 0);
+        dst += dst_stride;
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out5, out4, 7, out7, out6, 7, out4, out5);
+        DUP2_ARG2(__lsx_vxori_b, out4, 128, out5, 128, out4, out5);
+        __lsx_vst(out4, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(out5, dst, 0);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel8_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src7, src8, src9, src10;
+    __m128i src10_l, src32_l, src76_l, src98_l, src21_l, src43_l, src87_l;
+    __m128i src109_l, filt0, filt1, filt2;
+    __m128i out0_l, out1_l, out2_l, out3_l;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride2;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src += src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src2, src1, src4,
+              src3, src10_l, src32_l, src21_l, src43_l);
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                  128, src7, src8, src9, src10);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vilvl_b, src7, src4, src8, src7, src9, src8, src10,
+                  src9, src76_l, src87_l, src98_l, src109_l);
+
+        out0_l = DPADD_SH3_SH(src10_l, src32_l, src76_l, filt0, filt1, filt2);
+        out1_l = DPADD_SH3_SH(src21_l, src43_l, src87_l, filt0, filt1, filt2);
+        out2_l = DPADD_SH3_SH(src32_l, src76_l, src98_l, filt0, filt1, filt2);
+        out3_l = DPADD_SH3_SH(src43_l, src87_l, src109_l, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1_l, out0_l, 7, out3_l, out2_l, 7,
+                  out0_l, out1_l);
+        DUP2_ARG2(__lsx_vxori_b, out0_l, 128, out1_l, 128, out0_l, out1_l);
+
+        __lsx_vstelm_d(out0_l, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0_l, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1_l, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1_l, dst, 0, 1);
+        dst += dst_stride;
+
+        src10_l = src76_l;
+        src32_l = src98_l;
+        src21_l = src87_l;
+        src43_l = src109_l;
+        src4 = src10;
+    }
+}
+
+void ff_put_vp8_epel16_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i src10_l, src32_l, src54_l, src76_l, src21_l, src43_l, src65_l, src87_l;
+    __m128i src10_h, src32_h, src54_h, src76_h, src21_h, src43_h, src65_h, src87_h;
+    __m128i filt0, filt1, filt2;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP4_ARG2(__lsx_vld, src - src_stride2, 0, src - src_stride, 0, src, 0,
+              src + src_stride, 0, src0, src1, src2, src3);
+    src4 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src4, src3, src2, src1,
+              src10_l, src32_l, src43_l, src21_l);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src4, src3, src2, src1,
+              src10_h, src32_h, src43_h, src21_h);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        DUP4_ARG2(__lsx_vilvl_b, src5, src4, src6, src5, src7, src6, src8, src7,
+                  src54_l, src65_l, src76_l, src87_l);
+        DUP4_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src7, src6, src8, src7,
+                  src54_h, src65_h, src76_h, src87_h);
+
+        tmp0 = DPADD_SH3_SH(src10_l, src32_l, src54_l, filt0, filt1, filt2);
+        tmp1 = DPADD_SH3_SH(src21_l, src43_l, src65_l, filt0, filt1, filt2);
+        tmp2 = DPADD_SH3_SH(src10_h, src32_h, src54_h, filt0, filt1, filt2);
+        tmp3 = DPADD_SH3_SH(src21_h, src43_h, src65_h, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        tmp0 = DPADD_SH3_SH(src32_l, src54_l, src76_l, filt0, filt1, filt2);
+        tmp1 = DPADD_SH3_SH(src43_l, src65_l, src87_l, filt0, filt1, filt2);
+        tmp2 = DPADD_SH3_SH(src32_h, src54_h, src76_h, filt0, filt1, filt2);
+        tmp3 = DPADD_SH3_SH(src43_h, src65_h, src87_h, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        src10_l = src54_l;
+        src32_l = src76_l;
+        src21_l = src65_l;
+        src43_l = src87_l;
+        src10_h = src54_h;
+        src32_h = src76_h;
+        src21_h = src65_h;
+        src43_h = src87_h;
+        src4 = src8;
+    }
+}
+
+void ff_put_vp8_epel8_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt_hz0, filt_hz1, filt_hz2;
+    __m128i mask0, mask1, mask2, filt_vt0, filt_vt1, filt_vt2;
+    __m128i hz_out0, hz_out1, hz_out2, hz_out3, hz_out4, hz_out5, hz_out6;
+    __m128i hz_out7, hz_out8, out0, out1, out2, out3, out4, out5, out6, out7;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (2 + src_stride2);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    filt_hz2 = __lsx_vldrepl_h(filter_horiz, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src +=  src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0 ,src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    hz_out0 = HORIZ_6TAP_FILT(src0, src0, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out1 = HORIZ_6TAP_FILT(src1, src1, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out2 = HORIZ_6TAP_FILT(src2, src2, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out3 = HORIZ_6TAP_FILT(src3, src3, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out4 = HORIZ_6TAP_FILT(src4, src4, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+    filt_vt2 = __lsx_vldrepl_h(filter_vert, 4);
+
+    DUP2_ARG2(__lsx_vpackev_b, hz_out1, hz_out0, hz_out3, hz_out2, out0, out1);
+    DUP2_ARG2(__lsx_vpackev_b, hz_out2, hz_out1, hz_out4, hz_out3, out3, out4);
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        hz_out5 = HORIZ_6TAP_FILT(src5, src5, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out2 = __lsx_vpackev_b(hz_out5, hz_out4);
+        tmp0 = DPADD_SH3_SH(out0, out1, out2,filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out6 = HORIZ_6TAP_FILT(src6, src6, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out5 = __lsx_vpackev_b(hz_out6, hz_out5);
+        tmp1 = DPADD_SH3_SH(out3, out4, out5, filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out7 = HORIZ_6TAP_FILT(src7, src7, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+
+        out7 = __lsx_vpackev_b(hz_out7, hz_out6);
+        tmp2 = DPADD_SH3_SH(out1, out2, out7, filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out8 = HORIZ_6TAP_FILT(src8, src8, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out6 = __lsx_vpackev_b(hz_out8, hz_out7);
+        tmp3 = DPADD_SH3_SH(out4, out5, out6, filt_vt0, filt_vt1, filt_vt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+
+        hz_out4 = hz_out8;
+        out0 = out2;
+        out1 = out7;
+        out3 = out5;
+        out4 = out6;
+    }
+}
+
+void ff_put_vp8_epel16_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h6v6_lsx(dst, dst_stride, src, src_stride, height, mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_epel8_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src7, src8, src9, src10;
+    __m128i src10_l, src72_l, src98_l, src21_l, src87_l, src109_l, filt0, filt1;
+    __m128i out0, out1, out2, out3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride;
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src7, src8, src9, src10);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src2, src8, src7, src9, src8, src10, src9,
+                  src72_l, src87_l, src98_l, src109_l);
+
+        out0 = FILT_4TAP_DPADD_S_H(src10_l, src72_l, filt0, filt1);
+        out1 = FILT_4TAP_DPADD_S_H(src21_l, src87_l, filt0, filt1);
+        out2 = FILT_4TAP_DPADD_S_H(src72_l, src98_l, filt0, filt1);
+        out3 = FILT_4TAP_DPADD_S_H(src87_l, src109_l, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        src10_l = src98_l;
+        src21_l = src109_l;
+        src2 = src10;
+    }
+}
+
+void ff_put_vp8_epel16_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i src10_l, src32_l, src54_l, src21_l, src43_l, src65_l, src10_h;
+    __m128i src32_h, src54_h, src21_h, src43_h, src65_h, filt0, filt1;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_h, src21_h);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src3, src4, src5, src6);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                  src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src5, src4, src6,
+                  src5, src32_l, src43_l, src54_l, src65_l);
+        DUP4_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src5, src4, src6,
+                  src5, src32_h, src43_h, src54_h, src65_h);
+
+        tmp0 = FILT_4TAP_DPADD_S_H(src10_l, src32_l, filt0, filt1);
+        tmp1 = FILT_4TAP_DPADD_S_H(src21_l, src43_l, filt0, filt1);
+        tmp2 = FILT_4TAP_DPADD_S_H(src10_h, src32_h, filt0, filt1);
+        tmp3 = FILT_4TAP_DPADD_S_H(src21_h, src43_h, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        tmp0 = FILT_4TAP_DPADD_S_H(src32_l, src54_l, filt0, filt1);
+        tmp1 = FILT_4TAP_DPADD_S_H(src43_l, src65_l, filt0, filt1);
+        tmp2 = FILT_4TAP_DPADD_S_H(src32_h, src54_h, filt0, filt1);
+        tmp3 = FILT_4TAP_DPADD_S_H(src43_h, src65_h, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        src10_l = src54_l;
+        src21_l = src65_l;
+        src10_h = src54_h;
+        src21_h = src65_h;
+        src2 = src6;
+    }
+}
+
+void ff_put_vp8_epel8_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i filt_hz0, filt_hz1, filt_hz2, mask0, mask1, mask2;
+    __m128i filt_vt0, filt_vt1, hz_out0, hz_out1, hz_out2, hz_out3;
+    __m128i tmp0, tmp1, tmp2, tmp3, vec0, vec1, vec2, vec3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (2 + src_stride);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    filt_hz2 = __lsx_vldrepl_h(filter_horiz, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    hz_out0 = HORIZ_6TAP_FILT(src0, src0, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out1 = HORIZ_6TAP_FILT(src1, src1, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out2 = HORIZ_6TAP_FILT(src2, src2, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    DUP2_ARG2(__lsx_vpackev_b, hz_out1, hz_out0, hz_out2, hz_out1, vec0, vec2);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src3, src4, src5, src6);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                  src3, src4, src5, src6);
+
+        hz_out3 = HORIZ_6TAP_FILT(src3, src3, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec1 = __lsx_vpackev_b(hz_out3, hz_out2);
+        tmp0 = FILT_4TAP_DPADD_S_H(vec0, vec1, filt_vt0, filt_vt1);
+
+        hz_out0 = HORIZ_6TAP_FILT(src4, src4, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec3 = __lsx_vpackev_b(hz_out0, hz_out3);
+        tmp1 = FILT_4TAP_DPADD_S_H(vec2, vec3, filt_vt0, filt_vt1);
+
+        hz_out1 = HORIZ_6TAP_FILT(src5, src5, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec0 = __lsx_vpackev_b(hz_out1, hz_out0);
+        tmp2 = FILT_4TAP_DPADD_S_H(vec1, vec0, filt_vt0, filt_vt1);
+
+        hz_out2 = HORIZ_6TAP_FILT(src6, src6, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        DUP2_ARG2(__lsx_vpackev_b, hz_out0, hz_out3, hz_out2, hz_out1, vec1, vec2);
+        tmp3 = FILT_4TAP_DPADD_S_H(vec1, vec2, filt_vt0, filt_vt1);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel16_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h6v4_lsx(dst, dst_stride, src, src_stride, height,
+                                  mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_epel8_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt_hz0, filt_hz1, mask0, mask1;
+    __m128i filt_vt0, filt_vt1, filt_vt2;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8;
+    __m128i out0, out1, out2, out3, out4, out5, out6, out7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (1 + src_stride2);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src += src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    tmp0 = HORIZ_4TAP_FILT(src0, src0, mask0, mask1, filt_hz0, filt_hz1);
+    tmp1 = HORIZ_4TAP_FILT(src1, src1, mask0, mask1, filt_hz0, filt_hz1);
+    tmp2 = HORIZ_4TAP_FILT(src2, src2, mask0, mask1, filt_hz0, filt_hz1);
+    tmp3 = HORIZ_4TAP_FILT(src3, src3, mask0, mask1, filt_hz0, filt_hz1);
+    tmp4 = HORIZ_4TAP_FILT(src4, src4, mask0, mask1, filt_hz0, filt_hz1);
+
+    DUP4_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp2, tmp1,
+              tmp4, tmp3, out0, out1, out3, out4);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+    filt_vt2 = __lsx_vldrepl_h(filter_vert, 4);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        tmp5 = HORIZ_4TAP_FILT(src5, src5, mask0, mask1, filt_hz0, filt_hz1);
+        out2 = __lsx_vpackev_b(tmp5, tmp4);
+        tmp0 = DPADD_SH3_SH(out0, out1, out2, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp6 = HORIZ_4TAP_FILT(src6, src6, mask0, mask1, filt_hz0, filt_hz1);
+        out5 = __lsx_vpackev_b(tmp6, tmp5);
+        tmp1 = DPADD_SH3_SH(out3, out4, out5, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp7 = HORIZ_4TAP_FILT(src7, src7, mask0, mask1, filt_hz0, filt_hz1);
+        out6 = __lsx_vpackev_b(tmp7, tmp6);
+        tmp2 = DPADD_SH3_SH(out1, out2, out6, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp8 = HORIZ_4TAP_FILT(src8, src8, mask0, mask1, filt_hz0, filt_hz1);
+        out7 = __lsx_vpackev_b(tmp8, tmp7);
+        tmp3 = DPADD_SH3_SH(out4, out5, out7, filt_vt0, filt_vt1, filt_vt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+
+        tmp4 = tmp8;
+        out0 = out2;
+        out1 = out6;
+        out3 = out5;
+        out4 = out7;
+    }
+}
+
+void ff_put_vp8_epel16_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h4v6_lsx(dst, dst_stride, src, src_stride, height,
+                                  mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_pixels8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                            uint8_t *src, ptrdiff_t src_stride,
+                            int height, int mx, int my)
+{
+    int32_t cnt;
+    __m128i src0, src1, src2, src3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    if (0 == height % 8) {
+        for (cnt = height >> 3; cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+        }
+    } else if( 0 == height % 4) {
+        for (cnt = (height >> 2); cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+        }
+    }
+}
+
+void ff_put_vp8_pixels16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    int32_t width = 16;
+    int32_t cnt, loop_cnt;
+    uint8_t *src_tmp, *dst_tmp;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    ptrdiff_t dst_stride2 = dst_stride << 1;
+    ptrdiff_t dst_stride3 = dst_stride2 + dst_stride;
+    ptrdiff_t dst_stride4 = dst_stride2 << 1;
+
+    if (0 == height % 8) {
+        for (cnt = (width >> 4); cnt--;) {
+            src_tmp = src;
+            dst_tmp = dst;
+            for (loop_cnt = (height >> 3); loop_cnt--;) {
+                DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                          src_tmp + src_stride2, 0, src_tmp + src_stride3, 0,
+                          src4, src5, src6, src7);
+                src_tmp += src_stride4;
+
+                __lsx_vst(src4, dst_tmp,               0);
+                __lsx_vst(src5, dst_tmp + dst_stride,  0);
+                __lsx_vst(src6, dst_tmp + dst_stride2, 0);
+                __lsx_vst(src7, dst_tmp + dst_stride3, 0);
+                dst_tmp += dst_stride4;
+
+                DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                          src_tmp + src_stride2, 0, src_tmp + src_stride3, 0,
+                          src4, src5, src6, src7);
+                src_tmp += src_stride4;
+
+                __lsx_vst(src4, dst_tmp,               0);
+                __lsx_vst(src5, dst_tmp + dst_stride,  0);
+                __lsx_vst(src6, dst_tmp + dst_stride2, 0);
+                __lsx_vst(src7, dst_tmp + dst_stride3, 0);
+                dst_tmp += dst_stride4;
+            }
+            src += 16;
+            dst += 16;
+        }
+    } else if (0 == height % 4) {
+        for (cnt = (height >> 2); cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += 4 * src_stride4;
+
+            __lsx_vst(src0, dst,               0);
+            __lsx_vst(src1, dst + dst_stride,  0);
+            __lsx_vst(src2, dst + dst_stride2, 0);
+            __lsx_vst(src3, dst + dst_stride3, 0);
+            dst += dst_stride4;
+       }
+    }
+}
diff --git a/libavcodec/loongarch/vp8dsp_init_loongarch.c b/libavcodec/loongarch/vp8dsp_init_loongarch.c
index 17a7c4065c..7f36fb90c1 100644
--- a/libavcodec/loongarch/vp8dsp_init_loongarch.c
+++ b/libavcodec/loongarch/vp8dsp_init_loongarch.c
@@ -28,14 +28,35 @@
 #include "libavcodec/vp8dsp.h"
 #include "vp8dsp_loongarch.h"
 
+#define VP8_MC_LOONGARCH_FUNC(IDX, SIZE)                                          \
+    dsp->put_vp8_epel_pixels_tab[IDX][0][2] = ff_put_vp8_epel##SIZE##_h6_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][1][0] = ff_put_vp8_epel##SIZE##_v4_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][1][2] = ff_put_vp8_epel##SIZE##_h6v4_lsx;   \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][0] = ff_put_vp8_epel##SIZE##_v6_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][1] = ff_put_vp8_epel##SIZE##_h4v6_lsx;   \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][2] = ff_put_vp8_epel##SIZE##_h6v6_lsx;
+
+#define VP8_MC_LOONGARCH_COPY(IDX, SIZE)                                          \
+    dsp->put_vp8_epel_pixels_tab[IDX][0][0] = ff_put_vp8_pixels##SIZE##_lsx;      \
+    dsp->put_vp8_bilinear_pixels_tab[IDX][0][0] = ff_put_vp8_pixels##SIZE##_lsx;
+
 av_cold void ff_vp8dsp_init_loongarch(VP8DSPContext *dsp)
 {
     int cpu_flags = av_get_cpu_flags();
 
     if (have_lsx(cpu_flags)) {
+        VP8_MC_LOONGARCH_FUNC(0, 16);
+        VP8_MC_LOONGARCH_FUNC(1, 8);
+
+        VP8_MC_LOONGARCH_COPY(0, 16);
+        VP8_MC_LOONGARCH_COPY(1, 8);
+
         dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_lsx;
         dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_lsx;
         dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_lsx;
         dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_lsx;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_lsx;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_lsx;
     }
 }
diff --git a/libavcodec/loongarch/vp8dsp_loongarch.h b/libavcodec/loongarch/vp8dsp_loongarch.h
index f73ab784cc..87e9509db9 100644
--- a/libavcodec/loongarch/vp8dsp_loongarch.h
+++ b/libavcodec/loongarch/vp8dsp_loongarch.h
@@ -24,7 +24,58 @@
 
 #include "libavcodec/vp8dsp.h"
 
+void ff_put_vp8_pixels8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                            uint8_t *src, ptrdiff_t src_stride,
+                            int h, int x, int y);
+void ff_put_vp8_pixels16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int x, int y);
+
+void ff_put_vp8_epel16_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+void ff_put_vp8_epel16_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+void ff_put_vp8_epel16_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+
+void ff_put_vp8_epel8_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+void ff_put_vp8_epel8_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+void ff_put_vp8_epel8_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+void ff_put_vp8_epel8_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+void ff_put_vp8_epel8_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+
+void ff_put_vp8_epel8_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+
 /* loop filter */
+void ff_vp8_v_loop_filter16_inner_lsx(uint8_t *dst, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h);
+void ff_vp8_h_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h);
+
 void ff_vp8_v_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride,
                                 int flim_e, int flim_i, int hev_thresh);
 void ff_vp8_h_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride,
diff --git a/libavcodec/loongarch/vp9_idct_lsx.c b/libavcodec/loongarch/vp9_idct_lsx.c
index b9412c0b5d..b977687237 100644
--- a/libavcodec/loongarch/vp9_idct_lsx.c
+++ b/libavcodec/loongarch/vp9_idct_lsx.c
@@ -20,7 +20,7 @@
  */
 
 #include "libavcodec/vp9dsp.h"
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "vp9dsp_loongarch.h"
 #include "libavutil/attributes.h"
 
@@ -77,15 +77,13 @@ static const int32_t sinpi_4_9 = 15212;
     s0_m = __lsx_vilvh_h(__lsx_vneg_h(reg1), reg0);                \
     s3_m = __lsx_vilvl_h(reg0, reg1);                              \
     s2_m = __lsx_vilvh_h(reg0, reg1);                              \
-    LSX_DUP2_ARG2(__lsx_dp2_w_h, s1_m, k0_m, s0_m, k0_m, s1_m,     \
-                  s0_m);                                           \
-    LSX_DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,        \
-                  s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);           \
+    DUP2_ARG2(__lsx_vdp2_w_h, s1_m, k0_m, s0_m, k0_m, s1_m, s0_m); \
+    DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,            \
+              s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);               \
     out0 = __lsx_vpickev_h(s0_m, s1_m);                            \
-    LSX_DUP2_ARG2(__lsx_dp2_w_h, s3_m, k0_m, s2_m, k0_m, s1_m,     \
-                  s0_m);                                           \
-    LSX_DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,        \
-                  s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);           \
+    DUP2_ARG2(__lsx_vdp2_w_h, s3_m, k0_m, s2_m, k0_m, s1_m, s0_m); \
+    DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,            \
+              s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);               \
     out1 = __lsx_vpickev_h(s0_m, s1_m);                            \
 }
 
@@ -100,34 +98,28 @@ static const int32_t sinpi_4_9 = 15212;
     out0_m;                               \
 } )
 
-#define VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3)   \
-{                                                                  \
-    uint8_t *dst_m = (uint8_t *) (dst);                            \
-    __m128i dst0_m, dst1_m, dst2_m, dst3_m;                        \
-    __m128i tmp0_m, tmp1_m;                                        \
-    __m128i res0_m, res1_m, res2_m, res3_m;                        \
-    __m128i zero_m = __lsx_vldi(0);                                \
-    LSX_DUP4_ARG2(__lsx_vld,                                       \
-                  dst_m, 0,                                        \
-                  dst_m + dst_stride, 0,                           \
-                  dst_m + 2 * dst_stride, 0,                       \
-                  dst_m + 3 * dst_stride, 0,                       \
-                  dst0_m, dst1_m, dst2_m, dst3_m);                 \
-    LSX_DUP4_ARG2(__lsx_vilvl_b,                                   \
-                  zero_m, dst0_m, zero_m, dst1_m, zero_m, dst2_m,  \
-                  zero_m, dst3_m, res0_m, res1_m, res2_m, res3_m); \
-    LSX_DUP4_ARG2(__lsx_vadd_h,                                    \
-                  res0_m, in0, res1_m, in1, res2_m, in2, res3_m,   \
-                  in3, res0_m, res1_m, res2_m, res3_m);            \
-    LSX_DUP4_ARG1(__lsx_clamp255_h,                                \
-                  res0_m, res1_m, res2_m, res3_m,                  \
-                  res0_m, res1_m, res2_m, res3_m);                 \
-    LSX_DUP2_ARG2(__lsx_vpickev_b,                                 \
-                  res1_m, res0_m, res3_m, res2_m, tmp0_m, tmp1_m); \
-    __lsx_vstelm_d(tmp0_m, dst_m, 0, 0);                           \
-    __lsx_vstelm_d(tmp0_m, dst_m + dst_stride, 0, 1);              \
-    __lsx_vstelm_d(tmp1_m, dst_m + 2 * dst_stride, 0, 0);          \
-    __lsx_vstelm_d(tmp1_m, dst_m + 3 * dst_stride, 0, 1);          \
+#define VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3)      \
+{                                                                     \
+    uint8_t *dst_m = (uint8_t *) (dst);                               \
+    __m128i dst0_m, dst1_m, dst2_m, dst3_m;                           \
+    __m128i tmp0_m, tmp1_m;                                           \
+    __m128i res0_m, res1_m, res2_m, res3_m;                           \
+    __m128i zero_m = __lsx_vldi(0);                                   \
+    DUP4_ARG2(__lsx_vld, dst_m, 0, dst_m + dst_stride, 0,             \
+              dst_m + 2 * dst_stride, 0, dst_m + 3 * dst_stride, 0,   \
+              dst0_m, dst1_m, dst2_m, dst3_m);                        \
+    DUP4_ARG2(__lsx_vilvl_b, zero_m, dst0_m, zero_m, dst1_m, zero_m,  \
+              dst2_m, zero_m, dst3_m, res0_m, res1_m, res2_m, res3_m);\
+    DUP4_ARG2(__lsx_vadd_h, res0_m, in0, res1_m, in1, res2_m, in2,    \
+              res3_m, in3, res0_m, res1_m, res2_m, res3_m);           \
+    DUP4_ARG1(__lsx_vclip255_h, res0_m, res1_m, res2_m, res3_m,       \
+              res0_m, res1_m, res2_m, res3_m);                        \
+    DUP2_ARG2(__lsx_vpickev_b, res1_m, res0_m, res3_m, res2_m,        \
+              tmp0_m, tmp1_m);                                        \
+    __lsx_vstelm_d(tmp0_m, dst_m, 0, 0);                              \
+    __lsx_vstelm_d(tmp0_m, dst_m + dst_stride, 0, 1);                 \
+    __lsx_vstelm_d(tmp1_m, dst_m + 2 * dst_stride, 0, 0);             \
+    __lsx_vstelm_d(tmp1_m, dst_m + 3 * dst_stride, 0, 1);             \
 }
 
 #define VP9_UNPCK_UB_SH(in, out_h, out_l) \
@@ -144,8 +136,8 @@ static const int32_t sinpi_4_9 = 15212;
     __m128i tmp0_n, tmp1_n, tmp2_n, tmp3_n;                                 \
     __m128i zero_m = __lsx_vldi(0);                                         \
                                                                             \
-    LSX_DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,    \
-                  tmp0_n, tmp1_n, tmp2_n, tmp3_n);                          \
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,        \
+              tmp0_n, tmp1_n, tmp2_n, tmp3_n);                              \
     tmp0_m = __lsx_vilvl_w(tmp1_n, tmp0_n);                                 \
     tmp2_m = __lsx_vilvh_w(tmp1_n, tmp0_n);                                 \
     tmp1_m = __lsx_vilvl_w(tmp3_n, tmp2_n);                                 \
@@ -173,42 +165,28 @@ static const int32_t sinpi_4_9 = 15212;
     madd_s0_m = __lsx_vilvh_h(inp1, inp0);                               \
     madd_s3_m = __lsx_vilvl_h(inp3, inp2);                               \
     madd_s2_m = __lsx_vilvh_h(inp3, inp2);                               \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h,                                         \
-                  madd_s1_m, cst0, madd_s0_m, cst0,                      \
-                  madd_s1_m, cst1, madd_s0_m, cst1,                      \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                       \
-    LSX_DUP4_ARG2(__lsx_vsrari_w,                                        \
-                  tmp0_m, VP9_DCT_CONST_BITS,                            \
-                  tmp1_m, VP9_DCT_CONST_BITS,                            \
-                  tmp2_m, VP9_DCT_CONST_BITS,                            \
-                  tmp3_m, VP9_DCT_CONST_BITS,                            \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                       \
-    LSX_DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m,       \
-                  out0, out1);                                           \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h,                                         \
-                  madd_s3_m, cst2, madd_s2_m, cst2,                      \
-                  madd_s3_m, cst3, madd_s2_m, cst3,                      \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                       \
-    LSX_DUP4_ARG2(__lsx_vsrari_w,                                        \
-                  tmp0_m, VP9_DCT_CONST_BITS,                            \
-                  tmp1_m, VP9_DCT_CONST_BITS,                            \
-                  tmp2_m, VP9_DCT_CONST_BITS,                            \
-                  tmp3_m, VP9_DCT_CONST_BITS,                            \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                       \
-    LSX_DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m,       \
-                  out2, out3);                                           \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s1_m, cst0, madd_s0_m, cst0,          \
+              madd_s1_m, cst1, madd_s0_m, cst1, tmp0_m, tmp1_m, tmp2_m, tmp3_m);\
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                       \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,           \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);      \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1);     \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s3_m, cst2, madd_s2_m, cst2,                 \
+              madd_s3_m, cst3, madd_s2_m, cst3, tmp0_m, tmp1_m, tmp2_m, tmp3_m);\
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                       \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,           \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);      \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out2, out3);     \
 }
 
-#define VP9_SET_CONST_PAIR(mask_h, idx1_h, idx2_h)     \
-( {                                                    \
-    __m128i c0_m, c1_m;                                \
-                                                       \
-    LSX_DUP2_ARG2(__lsx_vreplvei_h,                    \
-                  mask_h, idx1_h, mask_h, idx2_h,      \
-                  c0_m, c1_m);                         \
-    c0_m = __lsx_vpackev_h(c1_m, c0_m);                \
-                                                       \
-    c0_m;                                              \
+#define VP9_SET_CONST_PAIR(mask_h, idx1_h, idx2_h)                           \
+( {                                                                          \
+    __m128i c0_m, c1_m;                                                      \
+                                                                             \
+    DUP2_ARG2(__lsx_vreplvei_h, mask_h, idx1_h, mask_h, idx2_h, c0_m, c1_m); \
+    c0_m = __lsx_vpackev_h(c1_m, c0_m);                                      \
+                                                                             \
+    c0_m;                                                                    \
 } )
 
 /* idct 8x8 macro */
@@ -226,29 +204,26 @@ static const int32_t sinpi_4_9 = 15212;
     k2_m = VP9_SET_CONST_PAIR(mask_m, 6, 3);                                   \
     k3_m = VP9_SET_CONST_PAIR(mask_m, 3, 2);                                   \
     VP9_MADD(in1, in7, in3, in5, k0_m, k1_m, k2_m, k3_m, in1, in7, in3, in5);  \
-    LSX_DUP2_ARG2(__lsx_vsub_h, in1, in3, in7, in5, res0_m, res1_m);           \
+    DUP2_ARG2(__lsx_vsub_h, in1, in3, in7, in5, res0_m, res1_m);               \
     k0_m = VP9_SET_CONST_PAIR(mask_m, 4, 7);                                   \
     k1_m = __lsx_vreplvei_h(mask_m, 4);                                        \
                                                                                \
     res2_m = __lsx_vilvl_h(res0_m, res1_m);                                    \
     res3_m = __lsx_vilvh_h(res0_m, res1_m);                                    \
-    LSX_DUP4_ARG2(__lsx_dp2_w_h,                                               \
-                  res2_m, k0_m, res3_m, k0_m, res2_m, k1_m, res3_m, k1_m,      \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                             \
-    LSX_DUP4_ARG2(__lsx_vsrari_w,                                              \
-                  tmp0_m, VP9_DCT_CONST_BITS, tmp1_m, VP9_DCT_CONST_BITS,      \
-                  tmp2_m, VP9_DCT_CONST_BITS, tmp3_m, VP9_DCT_CONST_BITS,      \
-                  tmp0_m, tmp1_m, tmp2_m, tmp3_m);                             \
+    DUP4_ARG2(__lsx_vdp2_w_h, res2_m, k0_m, res3_m, k0_m, res2_m, k1_m,        \
+              res3_m, k1_m, tmp0_m, tmp1_m, tmp2_m, tmp3_m);                   \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                      \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,          \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);     \
     tp4_m = __lsx_vadd_h(in1, in3);                                            \
-    LSX_DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m,             \
-                  tp5_m, tp6_m);                                               \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tp5_m, tp6_m);  \
     tp7_m = __lsx_vadd_h(in7, in5);                                            \
     k2_m = VP9_SET_COSPI_PAIR(cospi_24_64, -cospi_8_64);                       \
     k3_m = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);                        \
     VP9_MADD(in0, in4, in2, in6, k1_m, k0_m, k2_m, k3_m,                       \
              in0, in4, in2, in6);                                              \
-    BUTTERFLY_4_H(in0, in4, in2, in6, tp0_m, tp1_m, tp2_m, tp3_m);             \
-    BUTTERFLY_8_H(tp0_m, tp1_m, tp2_m, tp3_m, tp4_m, tp5_m, tp6_m, tp7_m,      \
+    LSX_BUTTERFLY_4_H(in0, in4, in2, in6, tp0_m, tp1_m, tp2_m, tp3_m);         \
+    LSX_BUTTERFLY_8_H(tp0_m, tp1_m, tp2_m, tp3_m, tp4_m, tp5_m, tp6_m, tp7_m,  \
                   out0, out1, out2, out3, out4, out5, out6, out7);             \
 }
 
@@ -280,12 +255,8 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements of 8x8 block */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 0, input, 16, input, 32, input, 48,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 64, input, 80, input, 96, input, 112,
-                  in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112, in4, in5, in6, in7);
     __lsx_vst(zero, input, 0);
     __lsx_vst(zero, input, 16);
     __lsx_vst(zero, input, 32);
@@ -294,73 +265,55 @@ static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     __lsx_vst(zero, input, 80);
     __lsx_vst(zero, input, 96);
     __lsx_vst(zero, input, 112);
-    LSX_DUP4_ARG2(__lsx_vilvl_d,
-                  in1, in0, in3, in2, in5, in4, in7, in6,
-                  in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vilvl_d,in1, in0, in3, in2, in5, in4, in7, in6, in0, in1, in2, in3);
 
     /* stage1 */
-    LSX_DUP2_ARG2(__lsx_vilvh_h, in3, in0, in2, in1, s0, s1);
+    DUP2_ARG2(__lsx_vilvh_h, in3, in0, in2, in1, s0, s1);
     k0 = VP9_SET_COSPI_PAIR(cospi_28_64, -cospi_4_64);
     k1 = VP9_SET_COSPI_PAIR(cospi_4_64, cospi_28_64);
     k2 = VP9_SET_COSPI_PAIR(-cospi_20_64, cospi_12_64);
     k3 = VP9_SET_COSPI_PAIR(cospi_12_64, cospi_20_64);
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
-                  tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vsrari_w,
-                  tmp0, VP9_DCT_CONST_BITS,
-                  tmp1, VP9_DCT_CONST_BITS,
-                  tmp2, VP9_DCT_CONST_BITS,
-                  tmp3, VP9_DCT_CONST_BITS,
-                  tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vpickev_h,
-                  zero, tmp0, zero, tmp1,
-                  zero, tmp2, zero, tmp3,
-                  s0, s1, s2, s3);
-    BUTTERFLY_4_H(s0, s1, s3, s2, s4, s7, s6, s5);
+    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1, VP9_DCT_CONST_BITS,
+              tmp2, VP9_DCT_CONST_BITS, tmp3, VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
+              s0, s1, s2, s3);
+    LSX_BUTTERFLY_4_H(s0, s1, s3, s2, s4, s7, s6, s5);
 
     /* stage2 */
-    LSX_DUP2_ARG2(__lsx_vilvl_h, in3, in1, in2, in0, s1, s0);
+    DUP2_ARG2(__lsx_vilvl_h, in3, in1, in2, in0, s1, s0);
     k0 = VP9_SET_COSPI_PAIR(cospi_16_64, cospi_16_64);
     k1 = VP9_SET_COSPI_PAIR(cospi_16_64, -cospi_16_64);
     k2 = VP9_SET_COSPI_PAIR(cospi_24_64, -cospi_8_64);
     k3 = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);
-    LSX_DUP4_ARG2(__lsx_dp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
-                  tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vsrari_w,
-                  tmp0, VP9_DCT_CONST_BITS,
-                  tmp1, VP9_DCT_CONST_BITS,
-                  tmp2, VP9_DCT_CONST_BITS,
-                  tmp3, VP9_DCT_CONST_BITS,
+    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
                   tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vpickev_h,
-                  zero, tmp0, zero, tmp1,
-                  zero, tmp2, zero, tmp3,
-                  s0, s1, s2, s3);
-    BUTTERFLY_4_H(s0, s1, s2, s3, m0, m1, m2, m3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1, VP9_DCT_CONST_BITS,
+              tmp2, VP9_DCT_CONST_BITS, tmp3, VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
+              s0, s1, s2, s3);
+    LSX_BUTTERFLY_4_H(s0, s1, s2, s3, m0, m1, m2, m3);
 
     /* stage3 */
     s0 = __lsx_vilvl_h(s6, s5);
 
     k1 = VP9_SET_COSPI_PAIR(-cospi_16_64, cospi_16_64);
-    LSX_DUP2_ARG2(__lsx_dp2_w_h, s0, k1, s0, k0, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vsrari_w,
-                  tmp0, VP9_DCT_CONST_BITS, tmp1,
-                  VP9_DCT_CONST_BITS, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, s2, s3);
+    DUP2_ARG2(__lsx_vdp2_w_h, s0, k1, s0, k0, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, s2, s3);
 
     /* stage4 */
-    BUTTERFLY_8_H(m0, m1, m2, m3, s4, s2, s3, s7,
-                  in0, in1, in2, in3, in4, in5, in6, in7);
+    LSX_BUTTERFLY_8_H(m0, m1, m2, m3, s4, s2, s3, s7,
+                      in0, in1, in2, in3, in4, in5, in6, in7);
     VP9_ILVLTRANS4x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
                        in0, in1, in2, in3, in4, in5, in6, in7);
     VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
                    in0, in1, in2, in3, in4, in5, in6, in7);
 
     /* final rounding (add 2^4, divide by 2^5) and shift */
-    LSX_DUP4_ARG2(__lsx_vsrari_h, in0 , 5, in1, 5, in2, 5, in3, 5,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, in4 , 5, in5, 5, in6, 5, in7, 5,
-                  in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vsrari_h, in0 , 5, in1, 5, in2, 5, in3, 5, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4 , 5, in5, 5, in6, 5, in7, 5, in4, in5, in6, in7);
 
     /* add block and store 8x8 */
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
@@ -375,12 +328,10 @@ static void vp9_idct8x8_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements of 8x8 block */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 0, input, 16, input, 32, input, 48,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 64, input, 80, input, 96, input, 112,
-                  in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              in4, in5, in6, in7);
     __lsx_vst(zero, input, 0);
     __lsx_vst(zero, input, 16);
     __lsx_vst(zero, input, 32);
@@ -393,18 +344,14 @@ static void vp9_idct8x8_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
     VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
                    in0, in1, in2, in3, in4, in5, in6, in7);
     /* columns transform */
-    TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
-                   in0, in1, in2, in3, in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
     /* 1D idct8x8 */
     VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
                    in0, in1, in2, in3, in4, in5, in6, in7);
     /* final rounding (add 2^4, divide by 2^5) and shift */
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  in0, 5, in1, 5, in2, 5, in3, 5,
-                  in0, in1, in2, in3);
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  in4, 5, in5, 5, in6, 5, in7, 5,
-                  in4, in5, in6, in7);
+    DUP4_ARG2(__lsx_vsrari_h, in0, 5, in1, 5, in2, 5, in3, 5, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4, 5, in5, 5, in6, 5, in7, 5, in4, in5, in6, in7);
     /* add block and store 8x8 */
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
     dst += (4 * dst_stride);
@@ -421,22 +368,14 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
     __m128i zero = __lsx_vldi(0);
     int32_t offset = dst_stride << 2;
 
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*0, input, 32*1,
-                  input, 32*2, input, 32*3,
-                  reg0, reg1, reg2, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*4, input, 32*5,
-                  input, 32*6, input, 32*7,
-                  reg4, reg5, reg6, reg7);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*8, input, 32*9,
-                  input, 32*10, input, 32*11,
-                  reg8, reg9, reg10, reg11);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*12, input, 32*13,
-                  input, 32*14, input, 32*15,
-                  reg12, reg13, reg14, reg15);
+    DUP4_ARG2(__lsx_vld, input, 32*0, input, 32*1, input, 32*2, input, 32*3,
+              reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, input, 32*4, input, 32*5, input, 32*6, input, 32*7,
+              reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
+              reg8, reg9, reg10, reg11);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input, 32*15,
+              reg12, reg13, reg14, reg15);
 
     __lsx_vst(zero, input, 32*0);
     __lsx_vst(zero, input, 32*1);
@@ -457,11 +396,11 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
 
     VP9_DOTP_CONST_PAIR(reg2, reg14, cospi_28_64, cospi_4_64, reg2, reg14);
     VP9_DOTP_CONST_PAIR(reg10, reg6, cospi_12_64, cospi_20_64, reg10, reg6);
-    BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
+    LSX_BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
     VP9_DOTP_CONST_PAIR(reg14, reg2, cospi_16_64, cospi_16_64, loc2, loc3);
     VP9_DOTP_CONST_PAIR(reg0, reg8, cospi_16_64, cospi_16_64, reg0, reg8);
     VP9_DOTP_CONST_PAIR(reg4, reg12, cospi_24_64, cospi_8_64, reg4, reg12);
-    BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
+    LSX_BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
 
     reg0 = __lsx_vsub_h(reg2, loc1);
     reg2 = __lsx_vadd_h(reg2, loc1);
@@ -483,7 +422,7 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
 
     VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_22_64, cospi_10_64, reg5, reg11);
     VP9_DOTP_CONST_PAIR(reg13, reg3, cospi_6_64, cospi_26_64, loc0, loc1);
-    BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
+    LSX_BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
 
     loc1 = __lsx_vadd_h(reg15, reg3);
     reg3 = __lsx_vsub_h(reg15, reg3);
@@ -514,13 +453,13 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
     tmp5 = loc1;
 
     VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_16_64, cospi_16_64, reg5, reg11);
-    BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
+    LSX_BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
 
     reg10 = loc0;
     reg11 = loc1;
 
     VP9_DOTP_CONST_PAIR(reg3, reg13, cospi_16_64, cospi_16_64, reg3, reg13);
-    BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
+    LSX_BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
     reg13 = loc2;
 
     /* Transpose and store the output */
@@ -528,24 +467,20 @@ static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
     reg14 = tmp6;
     reg3 = tmp7;
 
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  reg0, 6, reg2, 6, reg4, 6, reg6, 6,
-                  reg0, reg2, reg4, reg6);
+    DUP4_ARG2(__lsx_vsrari_h, reg0, 6, reg2, 6, reg4, 6, reg6, 6,
+              reg0, reg2, reg4, reg6);
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg0, reg2, reg4, reg6);
     dst += offset;
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  reg8, 6, reg10, 6, reg12, 6, reg14, 6,
-                  reg8, reg10, reg12, reg14);
+    DUP4_ARG2(__lsx_vsrari_h, reg8, 6, reg10, 6, reg12, 6, reg14, 6,
+              reg8, reg10, reg12, reg14);
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg8, reg10, reg12, reg14);
     dst += offset;
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  reg3, 6, reg5, 6, reg11, 6, reg13, 6,
-                  reg3, reg5, reg11, reg13);
+    DUP4_ARG2(__lsx_vsrari_h, reg3, 6, reg5, 6, reg11, 6, reg13, 6,
+              reg3, reg5, reg11, reg13);
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg3, reg13, reg11, reg5);
     dst += offset;
-    LSX_DUP4_ARG2(__lsx_vsrari_h,
-                  reg1, 6, reg7, 6, reg9, 6, reg15, 6,
-                  reg1, reg7, reg9, reg15);
+    DUP4_ARG2(__lsx_vsrari_h, reg1, 6, reg7, 6, reg9, 6, reg15, 6,
+              reg1, reg7, reg9, reg15);
     VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg7, reg9, reg1, reg15);
 }
 
@@ -558,22 +493,14 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
     __m128i zero = __lsx_vldi(0);
     int16_t *offset;
 
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*0, input, 32*1,
-                  input, 32*2, input, 32*3,
-                  reg0, reg1, reg2, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*4, input, 32*5,
-                  input, 32*6, input, 32*7,
-                  reg4, reg5, reg6, reg7);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*8, input, 32*9,
-                  input, 32*10, input, 32*11,
-                  reg8, reg9, reg10, reg11);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  input, 32*12, input, 32*13,
-                  input, 32*14, input, 32*15,
-                  reg12, reg13, reg14, reg15);
+    DUP4_ARG2(__lsx_vld, input, 32*0, input, 32*1, input, 32*2, input, 32*3,
+              reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, input, 32*4, input, 32*5, input, 32*6, input, 32*7,
+              reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
+              reg8, reg9, reg10, reg11);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input, 32*15,
+              reg12, reg13, reg14, reg15);
 
     __lsx_vst(zero, input, 32*0);
     __lsx_vst(zero, input, 32*1);
@@ -594,11 +521,11 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
 
     VP9_DOTP_CONST_PAIR(reg2, reg14, cospi_28_64, cospi_4_64, reg2, reg14);
     VP9_DOTP_CONST_PAIR(reg10, reg6, cospi_12_64, cospi_20_64, reg10, reg6);
-    BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
+    LSX_BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
     VP9_DOTP_CONST_PAIR(reg14, reg2, cospi_16_64, cospi_16_64, loc2, loc3);
     VP9_DOTP_CONST_PAIR(reg0, reg8, cospi_16_64, cospi_16_64, reg0, reg8);
     VP9_DOTP_CONST_PAIR(reg4, reg12, cospi_24_64, cospi_8_64, reg4, reg12);
-    BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
+    LSX_BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
 
     reg0 = __lsx_vsub_h(reg2, loc1);
     reg2 = __lsx_vadd_h(reg2, loc1);
@@ -620,7 +547,7 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
 
     VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_22_64, cospi_10_64, reg5, reg11);
     VP9_DOTP_CONST_PAIR(reg13, reg3, cospi_6_64, cospi_26_64, loc0, loc1);
-    BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
+    LSX_BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
 
     loc1 = __lsx_vadd_h(reg15, reg3);
     reg3 = __lsx_vsub_h(reg15, reg3);
@@ -652,13 +579,13 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
     tmp5 = loc1;
 
     VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_16_64, cospi_16_64, reg5, reg11);
-    BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
+    LSX_BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
 
     reg10 = loc0;
     reg11 = loc1;
 
     VP9_DOTP_CONST_PAIR(reg3, reg13, cospi_16_64, cospi_16_64, reg3, reg13);
-    BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
+    LSX_BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
     reg13 = loc2;
 
     /* Transpose and store the output */
@@ -667,8 +594,8 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
     reg3 = tmp7;
 
     /* transpose block */
-    TRANSPOSE8x8_H(reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14,
-                   reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14);
+    LSX_TRANSPOSE8x8_H(reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14,
+                       reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14);
 
     __lsx_vst(reg0, output, 32*0);
     __lsx_vst(reg2, output, 32*1);
@@ -680,8 +607,8 @@ static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
     __lsx_vst(reg14, output, 32*7);
 
     /* transpose block */
-    TRANSPOSE8x8_H(reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15,
-                   reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15);
+    LSX_TRANSPOSE8x8_H(reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15,
+                       reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15);
 
     offset = output + 8;
     __lsx_vst(reg3, offset, 32*0);
@@ -711,31 +638,20 @@ static void vp9_idct16x16_1_add_lsx(int16_t *input, uint8_t *dst,
     vec = __lsx_vreplgr2vr_h(out);
 
     for (i = 4; i--;) {
-        LSX_DUP4_ARG2(__lsx_vld,
-                      dst, 0,
-                      dst + dst_stride, 0,
-                      dst + dst_stride * 2, 0,
-                      dst + dst_stride * 3, 0,
-                      dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vld, dst, 0, dst + dst_stride, 0, dst + dst_stride * 2, 0,
+                  dst + dst_stride * 3, 0, dst0, dst1, dst2, dst3);
         VP9_UNPCK_UB_SH(dst0, res4, res0);
         VP9_UNPCK_UB_SH(dst1, res5, res1);
         VP9_UNPCK_UB_SH(dst2, res6, res2);
         VP9_UNPCK_UB_SH(dst3, res7, res3);
-        LSX_DUP4_ARG2(__lsx_vadd_h,
-                      res0, vec, res1, vec, res2, vec, res3, vec,
-                      res0, res1, res2, res3);
-        LSX_DUP4_ARG2(__lsx_vadd_h,
-                      res4, vec, res5, vec, res6, vec, res7, vec,
-                      res4, res5, res6, res7);
-        LSX_DUP4_ARG1(__lsx_clamp255_h,
-                      res0, res1, res2, res3,
-                      res0, res1, res2, res3);
-        LSX_DUP4_ARG1(__lsx_clamp255_h,
-                      res4, res5, res6, res7,
-                      res4, res5, res6, res7);
-        LSX_DUP4_ARG2(__lsx_vpickev_b,
-                      res4, res0, res5, res1, res6, res2, res7, res3,
-                      tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vadd_h, res0, vec, res1, vec, res2, vec, res3, vec,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vadd_h, res4, vec, res5, vec, res6, vec, res7, vec,
+                  res4, res5, res6, res7);
+        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
+        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7, res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6, res2, res7, res3,
+                  tmp0, tmp1, tmp2, tmp3);
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst + dst_stride, 0);
         __lsx_vst(tmp2, dst + dst_stride * 2, 0);
@@ -818,9 +734,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 4 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 12 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h,
-                 loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                 m0, m4, m2, m6);
+    DUP4_ARG2(__lsx_vadd_h,loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m0, m4, m2, m6);
 
     #define SUB(a, b) __lsx_vsub_h(a, b)
 
@@ -839,9 +754,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 6 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 14 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h,
-                 loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                 m1, m5, m3, m7);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m1, m5, m3, m7);
 
     __lsx_vst(SUB(loc0, vec3), tmp_buf, 29 * 16);
     __lsx_vst(SUB(loc1, vec2), tmp_buf, 21 * 16);
@@ -858,9 +772,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 5 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 13 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h,
-                 loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                 n0, n4, n2, n6);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n0, n4, n2, n6);
 
     __lsx_vst(SUB(loc0, vec3), tmp_buf, 30 * 16);
     __lsx_vst(SUB(loc1, vec2), tmp_buf, 22 * 16);
@@ -877,9 +790,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 7 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 15 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h,
-                 loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                 n1, n5, n3, n7);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n1, n5, n3, n7);
 
     __lsx_vst(SUB(loc0, vec3), tmp_buf, 28 * 16);
     __lsx_vst(SUB(loc1, vec2), tmp_buf, 20 * 16);
@@ -888,8 +800,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
 
     /* Transpose : 16 vectors */
     /* 1st & 2nd 8x8 */
-    TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
-                   m0, n0, m1, n1, m2, n2, m3, n3);
+    LSX_TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
+                       m0, n0, m1, n1, m2, n2, m3, n3);
     __lsx_vst(m0, dst, 0);
     __lsx_vst(n0, dst, 32 * 2);
     __lsx_vst(m1, dst, 32 * 4);
@@ -899,8 +811,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     __lsx_vst(m3, dst, 32 * 12);
     __lsx_vst(n3, dst, 32 * 14);
 
-    TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
-                   m4, n4, m5, n5, m6, n6, m7, n7);
+    LSX_TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
+                       m4, n4, m5, n5, m6, n6, m7, n7);
 
     __lsx_vst(m4, dst, 16);
     __lsx_vst(n4, dst, 16 + 32 * 2);
@@ -912,26 +824,18 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     __lsx_vst(n7, dst, 16 + 32 * 14);
 
     /* 3rd & 4th 8x8 */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 16 * 16, tmp_buf, 16 * 17,
-                  tmp_buf, 16 * 18, tmp_buf, 16 * 19,
-                  m0, n0, m1, n1);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 16 * 20, tmp_buf, 16 * 21,
-                  tmp_buf, 16 * 22, tmp_buf, 16 * 23,
-                  m2, n2, m3, n3);
-
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 16 * 24, tmp_buf, 16 * 25,
-                  tmp_buf, 16 * 26, tmp_buf, 16 * 27,
-                  m4, n4, m5, n5);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 16 * 28, tmp_buf, 16 * 29,
-                  tmp_buf, 16 * 30, tmp_buf, 16 * 31,
-                  m6, n6, m7, n7);
-
-    TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
-                   m0, n0, m1, n1, m2, n2, m3, n3);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 16, tmp_buf, 16 * 17,
+              tmp_buf, 16 * 18, tmp_buf, 16 * 19, m0, n0, m1, n1);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 20, tmp_buf, 16 * 21,
+              tmp_buf, 16 * 22, tmp_buf, 16 * 23, m2, n2, m3, n3);
+
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 24, tmp_buf, 16 * 25,
+              tmp_buf, 16 * 26, tmp_buf, 16 * 27, m4, n4, m5, n5);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 28, tmp_buf, 16 * 29,
+              tmp_buf, 16 * 30, tmp_buf, 16 * 31, m6, n6, m7, n7);
+
+    LSX_TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
+                       m0, n0, m1, n1, m2, n2, m3, n3);
 
     __lsx_vst(m0, dst, 32);
     __lsx_vst(n0, dst, 32 + 32 * 2);
@@ -942,8 +846,8 @@ static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
     __lsx_vst(m3, dst, 32 + 32 * 12);
     __lsx_vst(n3, dst, 32 + 32 * 14);
 
-    TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
-                   m4, n4, m5, n5, m6, n6, m7, n7);
+    LSX_TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
+                       m4, n4, m5, n5, m6, n6, m7, n7);
 
     __lsx_vst(m4, dst, 48);
     __lsx_vst(n4, dst, 48 + 32 * 2);
@@ -964,14 +868,10 @@ static void vp9_idct8x32_column_even_process_store(int16_t *tmp_buf,
     __m128i zero = __lsx_vldi(0);
 
     /* Even stage 1 */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 0, tmp_buf, 32 * 8,
-                  tmp_buf, 32 * 16, tmp_buf, 32 * 24,
-                  reg0, reg1, reg2, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 32 * 32, tmp_buf, 32 * 40,
-                  tmp_buf, 32 * 48, tmp_buf, 32 * 56,
-                  reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 32 * 8,
+              tmp_buf, 32 * 16, tmp_buf, 32 * 24, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 32 * 32, tmp_buf, 32 * 40,
+              tmp_buf, 32 * 48, tmp_buf, 32 * 56, reg4, reg5, reg6, reg7);
 
     __lsx_vst(zero, tmp_buf, 0);
     __lsx_vst(zero, tmp_buf, 32 * 8);
@@ -986,7 +886,7 @@ static void vp9_idct8x32_column_even_process_store(int16_t *tmp_buf,
 
     VP9_DOTP_CONST_PAIR(reg1, reg7, cospi_28_64, cospi_4_64, reg1, reg7);
     VP9_DOTP_CONST_PAIR(reg5, reg3, cospi_12_64, cospi_20_64, reg5, reg3);
-    BUTTERFLY_4_H(reg1, reg7, reg3, reg5, vec1, vec3, vec2, vec0);
+    LSX_BUTTERFLY_4_H(reg1, reg7, reg3, reg5, vec1, vec3, vec2, vec0);
     VP9_DOTP_CONST_PAIR(vec2, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
 
     loc1 = vec3;
@@ -994,20 +894,16 @@ static void vp9_idct8x32_column_even_process_store(int16_t *tmp_buf,
 
     VP9_DOTP_CONST_PAIR(reg0, reg4, cospi_16_64, cospi_16_64, reg0, reg4);
     VP9_DOTP_CONST_PAIR(reg2, reg6, cospi_24_64, cospi_8_64, reg2, reg6);
-    BUTTERFLY_4_H(reg4, reg0, reg2, reg6, vec1, vec3, vec2, vec0);
-    BUTTERFLY_4_H(vec0, vec1, loc1, loc0, stp3, stp0, stp7, stp4);
-    BUTTERFLY_4_H(vec2, vec3, loc3, loc2, stp2, stp1, stp6, stp5);
+    LSX_BUTTERFLY_4_H(reg4, reg0, reg2, reg6, vec1, vec3, vec2, vec0);
+    LSX_BUTTERFLY_4_H(vec0, vec1, loc1, loc0, stp3, stp0, stp7, stp4);
+    LSX_BUTTERFLY_4_H(vec2, vec3, loc3, loc2, stp2, stp1, stp6, stp5);
 
     /* Even stage 2 */
     /* Load 8 */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 0, tmp_buf, 32 * 8,
-                  tmp_buf, 32 * 16, tmp_buf, 32 * 24,
-                  reg0, reg1, reg2, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_buf, 32 * 32, tmp_buf, 32 * 40,
-                  tmp_buf, 32 * 48, tmp_buf, 32 * 56,
-                  reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 32 * 8,
+              tmp_buf, 32 * 16, tmp_buf, 32 * 24, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 32 * 32, tmp_buf, 32 * 40,
+              tmp_buf, 32 * 48, tmp_buf, 32 * 56, reg4, reg5, reg6, reg7);
 
     __lsx_vst(zero, tmp_buf, 0);
     __lsx_vst(zero, tmp_buf, 32 * 8);
@@ -1053,25 +949,25 @@ static void vp9_idct8x32_column_even_process_store(int16_t *tmp_buf,
 
     /* Even stage 3 : Dependency on Even stage 1 & Even stage 2 */
     /* Store 8 */
-    BUTTERFLY_4_H(stp0, stp1, reg7, reg5, loc1, loc3, loc2, loc0);
+    LSX_BUTTERFLY_4_H(stp0, stp1, reg7, reg5, loc1, loc3, loc2, loc0);
     __lsx_vst(loc1, tmp_eve_buf, 0);
     __lsx_vst(loc3, tmp_eve_buf, 16);
     __lsx_vst(loc2, tmp_eve_buf, 14 * 16);
     __lsx_vst(loc0, tmp_eve_buf, 14 * 16 + 16);
-    BUTTERFLY_4_H(stp2, stp3, reg4, reg1, loc1, loc3, loc2, loc0);
+    LSX_BUTTERFLY_4_H(stp2, stp3, reg4, reg1, loc1, loc3, loc2, loc0);
     __lsx_vst(loc1, tmp_eve_buf, 2 * 16);
     __lsx_vst(loc3, tmp_eve_buf, 2 * 16 + 16);
     __lsx_vst(loc2, tmp_eve_buf, 12 * 16);
     __lsx_vst(loc0, tmp_eve_buf, 12 * 16 + 16);
 
     /* Store 8 */
-    BUTTERFLY_4_H(stp4, stp5, reg6, reg3, loc1, loc3, loc2, loc0);
+    LSX_BUTTERFLY_4_H(stp4, stp5, reg6, reg3, loc1, loc3, loc2, loc0);
     __lsx_vst(loc1, tmp_eve_buf, 4 * 16);
     __lsx_vst(loc3, tmp_eve_buf, 4 * 16 + 16);
     __lsx_vst(loc2, tmp_eve_buf, 10 * 16);
     __lsx_vst(loc0, tmp_eve_buf, 10 * 16 + 16);
 
-    BUTTERFLY_4_H(stp6, stp7, reg2, reg0, loc1, loc3, loc2, loc0);
+    LSX_BUTTERFLY_4_H(stp6, stp7, reg2, reg0, loc1, loc3, loc2, loc0);
     __lsx_vst(loc1, tmp_eve_buf, 6 * 16);
     __lsx_vst(loc3, tmp_eve_buf, 6 * 16 + 16);
     __lsx_vst(loc2, tmp_eve_buf, 8 * 16);
@@ -1120,10 +1016,10 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     reg5 = vec0;
 
     /* 4 Stores */
-    LSX_DUP2_ARG2(__lsx_vadd_h, reg5, reg4, reg3, reg2, vec0, vec1);
+    DUP2_ARG2(__lsx_vadd_h, reg5, reg4, reg3, reg2, vec0, vec1);
     __lsx_vst(vec0, tmp_odd_buf, 4 * 16);
     __lsx_vst(vec1, tmp_odd_buf, 4 * 16 + 16);
-    LSX_DUP2_ARG2(__lsx_vsub_h, reg5, reg4, reg3, reg2, vec0, vec1);
+    DUP2_ARG2(__lsx_vsub_h, reg5, reg4, reg3, reg2, vec0, vec1);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_24_64, cospi_8_64, vec0, vec1);
     __lsx_vst(vec0, tmp_odd_buf, 0);
     __lsx_vst(vec1, tmp_odd_buf, 16);
@@ -1131,7 +1027,7 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     /* 4 Stores */
     VP9_DOTP_CONST_PAIR(reg7, reg0, cospi_28_64, cospi_4_64, reg0, reg7);
     VP9_DOTP_CONST_PAIR(reg6, reg1, -cospi_4_64, cospi_28_64, reg1, reg6);
-    BUTTERFLY_4_H(reg0, reg7, reg6, reg1, vec0, vec1, vec2, vec3);
+    LSX_BUTTERFLY_4_H(reg0, reg7, reg6, reg1, vec0, vec1, vec2, vec3);
     __lsx_vst(vec0, tmp_odd_buf, 6 * 16);
     __lsx_vst(vec1, tmp_odd_buf, 6 * 16 + 16);
     VP9_DOTP_CONST_PAIR(vec2, vec3, cospi_24_64, cospi_8_64, vec2, vec3);
@@ -1164,11 +1060,11 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     VP9_DOTP_CONST_PAIR(reg7, reg0, cospi_3_64, cospi_29_64, reg0, reg7);
 
     /* 4 Stores */
-    LSX_DUP4_ARG2(__lsx_vsub_h,reg1, reg2, reg6, reg5, reg0, reg3, reg7, reg4,
-                  vec0, vec1, vec2, vec3);
+    DUP4_ARG2(__lsx_vsub_h,reg1, reg2, reg6, reg5, reg0, reg3, reg7, reg4,
+              vec0, vec1, vec2, vec3);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_12_64, cospi_20_64, loc0, loc1);
     VP9_DOTP_CONST_PAIR(vec3, vec2, -cospi_20_64, cospi_12_64, loc2, loc3);
-    BUTTERFLY_4_H(loc2, loc3, loc1, loc0, vec0, vec1, vec3, vec2);
+    LSX_BUTTERFLY_4_H(loc2, loc3, loc1, loc0, vec0, vec1, vec3, vec2);
     __lsx_vst(vec0, tmp_odd_buf, 12 * 16);
     __lsx_vst(vec1, tmp_odd_buf, 12 * 16 + 3 * 16);
     VP9_DOTP_CONST_PAIR(vec3, vec2, -cospi_8_64, cospi_24_64, vec0, vec1);
@@ -1176,9 +1072,9 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     __lsx_vst(vec1, tmp_odd_buf, 10 * 16 + 16);
 
     /* 4 Stores */
-    LSX_DUP4_ARG2(__lsx_vadd_h, reg0, reg3, reg1, reg2, reg5, reg6, reg4, reg7,
-                  vec0, vec1, vec2, vec3);
-    BUTTERFLY_4_H(vec0, vec3, vec2, vec1, reg0, reg1, reg3, reg2);
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg3, reg1, reg2, reg5, reg6, reg4, reg7,
+              vec0, vec1, vec2, vec3);
+    LSX_BUTTERFLY_4_H(vec0, vec3, vec2, vec1, reg0, reg1, reg3, reg2);
     __lsx_vst(reg0, tmp_odd_buf, 13 * 16);
     __lsx_vst(reg1, tmp_odd_buf, 13 * 16 + 16);
     VP9_DOTP_CONST_PAIR(reg3, reg2, -cospi_8_64, cospi_24_64,
@@ -1188,25 +1084,21 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
 
     /* Odd stage 3 : Dependency on Odd stage 1 & Odd stage 2 */
     /* Load 8 & Store 8 */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_odd_buf, 0, tmp_odd_buf, 16,
-                  tmp_odd_buf, 32, tmp_odd_buf, 48,
-                  reg0, reg1, reg2, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_odd_buf, 8 * 16, tmp_odd_buf, 8 * 16 + 16,
-                  tmp_odd_buf, 8 * 16 + 32, tmp_odd_buf, 8 * 16 + 48,
-                  reg4, reg5, reg6, reg7);
-
-    LSX_DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 0, tmp_odd_buf, 16,
+              tmp_odd_buf, 32, tmp_odd_buf, 48, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 8 * 16, tmp_odd_buf, 8 * 16 + 16,
+              tmp_odd_buf, 8 * 16 + 32, tmp_odd_buf, 8 * 16 + 48, reg4, reg5, reg6, reg7);
+
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
                   loc0, loc1, loc2, loc3);
     __lsx_vst(loc0, tmp_odd_buf, 0);
     __lsx_vst(loc1, tmp_odd_buf, 16);
     __lsx_vst(loc2, tmp_odd_buf, 32);
     __lsx_vst(loc3, tmp_odd_buf, 48);
-    LSX_DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg1, reg5, vec0, vec1);
+    DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg1, reg5, vec0, vec1);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc0, loc1);
 
-    LSX_DUP2_ARG2(__lsx_vsub_h, reg2, reg6, reg3, reg7, vec0, vec1);
+    DUP2_ARG2(__lsx_vsub_h, reg2, reg6, reg3, reg7, vec0, vec1);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
     __lsx_vst(loc0, tmp_odd_buf, 8 * 16);
     __lsx_vst(loc1, tmp_odd_buf, 8 * 16 + 16);
@@ -1214,26 +1106,22 @@ static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
     __lsx_vst(loc3, tmp_odd_buf, 8 * 16 + 48);
 
     /* Load 8 & Store 8 */
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_odd_buf, 4 * 16, tmp_odd_buf, 4 * 16 + 16,
-                  tmp_odd_buf, 4 * 16 + 32, tmp_odd_buf, 4 * 16 + 48,
-                  reg1, reg2, reg0, reg3);
-    LSX_DUP4_ARG2(__lsx_vld,
-                  tmp_odd_buf, 12 * 16, tmp_odd_buf, 12 * 16 + 16,
-                  tmp_odd_buf, 12 * 16 + 32, tmp_odd_buf, 12 * 16 + 48,
-                  reg4, reg5, reg6, reg7);
-
-    LSX_DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
-                  loc0, loc1, loc2, loc3);
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 4 * 16, tmp_odd_buf, 4 * 16 + 16,
+              tmp_odd_buf, 4 * 16 + 32, tmp_odd_buf, 4 * 16 + 48, reg1, reg2, reg0, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 12 * 16, tmp_odd_buf, 12 * 16 + 16,
+              tmp_odd_buf, 12 * 16 + 32, tmp_odd_buf, 12 * 16 + 48, reg4, reg5, reg6, reg7);
+
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
+              loc0, loc1, loc2, loc3);
     __lsx_vst(loc0, tmp_odd_buf, 4 * 16);
     __lsx_vst(loc1, tmp_odd_buf, 4 * 16 + 16);
     __lsx_vst(loc2, tmp_odd_buf, 4 * 16 + 32);
     __lsx_vst(loc3, tmp_odd_buf, 4 * 16 + 48);
 
-    LSX_DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg3, reg7, vec0, vec1);
+    DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg3, reg7, vec0, vec1);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc0, loc1);
 
-    LSX_DUP2_ARG2(__lsx_vsub_h, reg1, reg5, reg2, reg6, vec0, vec1);
+    DUP2_ARG2(__lsx_vsub_h, reg1, reg5, reg2, reg6, vec0, vec1);
     VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
     __lsx_vst(loc0, tmp_odd_buf, 12 * 16);
     __lsx_vst(loc1, tmp_odd_buf, 12 * 16 + 16);
@@ -1259,14 +1147,14 @@ static void vp9_idct8x32_column_butterfly_addblk(int16_t *tmp_eve_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 4 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 12 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  m0, m4, m2, m6);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m0, m4, m2, m6);
+    DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
     VP9_ADDBLK_ST8x4_UB(dst, (4 * dst_stride), m0, m2, m4, m6);
 
-    LSX_DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  m6, m2, m4, m0);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m6, m2, m4, m0);
+    DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
     VP9_ADDBLK_ST8x4_UB((dst + 19 * dst_stride), (4 * dst_stride),
                         m0, m2, m4, m6);
 
@@ -1280,15 +1168,15 @@ static void vp9_idct8x32_column_butterfly_addblk(int16_t *tmp_eve_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 6 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 14 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  m1, m5, m3, m7);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+               m1, m5, m3, m7);
+    DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
     VP9_ADDBLK_ST8x4_UB((dst + 2 * dst_stride), (4 * dst_stride),
                         m1, m3, m5, m7);
 
-    LSX_DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  m7, m3, m5, m1);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m7, m3, m5, m1);
+    DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
     VP9_ADDBLK_ST8x4_UB((dst + 17 * dst_stride), (4 * dst_stride),
                         m1, m3, m5, m7);
 
@@ -1302,14 +1190,14 @@ static void vp9_idct8x32_column_butterfly_addblk(int16_t *tmp_eve_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 5 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 13 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  n0, n4, n2, n6);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n0, n4, n2, n6);
+    DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
     VP9_ADDBLK_ST8x4_UB((dst + 1 * dst_stride), (4 * dst_stride),
                         n0, n2, n4, n6);
-    LSX_DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  n6, n2, n4, n0);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n6, n2, n4, n0);
+    DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
     VP9_ADDBLK_ST8x4_UB((dst + 18 * dst_stride), (4 * dst_stride),
                         n0, n2, n4, n6);
 
@@ -1323,14 +1211,14 @@ static void vp9_idct8x32_column_butterfly_addblk(int16_t *tmp_eve_buf,
     loc2 = __lsx_vld(tmp_eve_buf, 7 * 16);
     loc3 = __lsx_vld(tmp_eve_buf, 15 * 16);
 
-    LSX_DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  n1, n5, n3, n7);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n1, n5, n3, n7);
+    DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
     VP9_ADDBLK_ST8x4_UB((dst + 3 * dst_stride), (4 * dst_stride),
                         n1, n3, n5, n7);
-    LSX_DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
-                  n7, n3, n5, n1);
-    LSX_DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n7, n3, n5, n1);
+    DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
     VP9_ADDBLK_ST8x4_UB((dst + 16 * dst_stride), (4 * dst_stride),
                         n1, n3, n5, n7);
 }
@@ -1376,31 +1264,21 @@ static void vp9_idct32x32_1_add_lsx(int16_t *input, uint8_t *dst,
     vec = __lsx_vreplgr2vr_h(out);
 
     for (i = 16; i--;) {
-        LSX_DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
-        LSX_DUP2_ARG2(__lsx_vld, dst + dst_stride, 0,
-                      dst + dst_stride, 16, dst2, dst3);
-
-        LSX_DUP4_ARG2(__lsx_vilvl_b,
-                      zero, dst0, zero, dst1, zero, dst2, zero, dst3,
-                      res0, res1, res2, res3);
-        LSX_DUP4_ARG2(__lsx_vilvh_b,
-                      zero, dst0, zero, dst1, zero, dst2, zero, dst3,
-                      res4, res5, res6, res7);
-        LSX_DUP4_ARG2(__lsx_vadd_h,
-                      res0, vec, res1, vec, res2, vec, res3, vec,
-                      res0, res1, res2, res3);
-        LSX_DUP4_ARG2(__lsx_vadd_h,
-                      res4, vec, res5, vec, res6, vec, res7, vec,
-                      res4, res5, res6, res7);
-        LSX_DUP4_ARG1(__lsx_clamp255_h,
-                      res0, res1, res2, res3,
-                      res0, res1, res2, res3);
-        LSX_DUP4_ARG1(__lsx_clamp255_h,
-                      res4, res5, res6, res7,
-                      res4, res5, res6, res7);
-        LSX_DUP4_ARG2(__lsx_vpickev_b,
-                      res4, res0, res5, res1, res6, res2, res7, res3,
-                      tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
+        DUP2_ARG2(__lsx_vld, dst + dst_stride, 0, dst + dst_stride, 16, dst2, dst3);
+
+        DUP4_ARG2(__lsx_vilvl_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vilvh_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
+                  res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vadd_h, res0, vec, res1, vec, res2, vec, res3, vec,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vadd_h, res4, vec, res5, vec, res6, vec, res7, vec,
+                  res4, res5, res6, res7);
+        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
+        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7, res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6, res2, res7, res3,
+                  tmp0, tmp1, tmp2, tmp3);
 
         __lsx_vst(tmp0, dst, 0);
         __lsx_vst(tmp1, dst, 16);
diff --git a/libavcodec/loongarch/vp9_intra_lsx.c b/libavcodec/loongarch/vp9_intra_lsx.c
index 6adb3f9e3c..b3d4f88158 100644
--- a/libavcodec/loongarch/vp9_intra_lsx.c
+++ b/libavcodec/loongarch/vp9_intra_lsx.c
@@ -20,7 +20,7 @@
  */
 
 #include "libavcodec/vp9dsp.h"
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "vp9dsp_loongarch.h"
 
 #define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4,   \
@@ -89,7 +89,7 @@ void ff_vert_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *left,
     uint32_t row;
     __m128i src0, src1;
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
     for (row = 32; row--;) {
         __lsx_vst(src0, dst, 0);
         __lsx_vst(src1, dst, 16);
@@ -285,7 +285,7 @@ void ff_dc_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 
     tmp0 = __lsx_vld(src_top, 0);
     tmp1 = __lsx_vld(src_left, 0);
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);
     dst0 = __lsx_vadd_h(tmp0, tmp1);
     dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
     dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
@@ -322,11 +322,11 @@ void ff_dc_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 {
     __m128i tmp0, tmp1, tmp2, tmp3, dst0;
 
-    LSX_DUP2_ARG2(__lsx_vld, src_top, 0, src_top, 16, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vld, src_left, 0, src_left, 16, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp2, tmp2,
-                  tmp3, tmp3, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vadd_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vld, src_top, 0, src_top, 16, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vld, src_left, 0, src_left, 16, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp2, tmp2,
+              tmp3, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vadd_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1);
     dst0 = __lsx_vadd_h(tmp0, tmp1);
     dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
     dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
@@ -346,8 +346,8 @@ void ff_dc_##dir##_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 {                                                                                \
     __m128i tmp0, tmp1, dst0;                                                    \
                                                                                  \
-    LSX_DUP2_ARG2(__lsx_vld, dir, 0, dir, 16, tmp0, tmp1);                       \
-    LSX_DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);       \
+    DUP2_ARG2(__lsx_vld, dir, 0, dir, 16, tmp0, tmp1);                           \
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);           \
     dst0 = __lsx_vadd_h(tmp0, tmp1);                                             \
     dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                       \
     dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                       \
@@ -403,17 +403,17 @@ void ff_tm_4x4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 
     reg0 = __lsx_vreplgr2vr_h(top_left);
     reg1 = __lsx_vld(src_top_ptr, 0);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
-                  3, tmp3, tmp2, tmp1, tmp0);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
-                  src3, dst0, dst1, dst2, dst3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
-                  dst0, dst1, dst2, dst3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
-                  dst0, dst1, dst2, dst3);
-    LSX_DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
+              src3, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
+              dst0, dst1, dst2, dst3);
+    DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
     __lsx_vstelm_w(dst0, dst, 0, 0);
     dst += dst_stride;
     __lsx_vstelm_w(dst0, dst, 0, 2);
@@ -433,28 +433,26 @@ void ff_tm_8x8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 
     reg0 = __lsx_vreplgr2vr_h(top_left);
     reg1 = __lsx_vld(src_top_ptr, 0);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
-                  3, tmp7, tmp6, tmp5, tmp4);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
-                  7, tmp3, tmp2, tmp1, tmp0);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
-                  src3, src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vhaddw_hu_bu, src4, src4, src5, src5, src6, src6, src7,
-                  src7, src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vpickev_b, src1, src0, src3, src2, src5, src4, src7, src6,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp7, tmp6, tmp5, tmp4);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
+              7, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vilvl_b, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
+              src3, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src4, src4, src5, src5, src6, src6, src7,
+              src7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpickev_b, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src1, src2, src3);
     __lsx_vstelm_d(src0, dst, 0, 0);
     dst += dst_stride;
     __lsx_vstelm_d(src0, dst, 0, 1);
@@ -483,70 +481,64 @@ void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
 
     reg0 = __lsx_vreplgr2vr_h(top_left);
     reg1 = __lsx_vld(src_top_ptr, 0);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
-                  3, tmp15, tmp14, tmp13, tmp12);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
-                  7, tmp11, tmp10, tmp9, tmp8);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 8, src_left, 9, src_left, 10, src_left,
-                  11, tmp7, tmp6, tmp5, tmp4);
-    LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 12, src_left, 13, src_left, 14, src_left,
-                  15, tmp3, tmp2, tmp1, tmp0);
-    LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                  tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                  tmp4, tmp5, tmp6, tmp7);
-    LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                  tmp8, tmp9, tmp10, tmp11);
-    LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                  src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                  src4, src5, src6, src7);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                  tmp12, tmp13, tmp14, tmp15);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp15, tmp14, tmp13, tmp12);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
+              7, tmp11, tmp10, tmp9, tmp8);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 8, src_left, 9, src_left, 10, src_left,
+              11, tmp7, tmp6, tmp5, tmp4);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 12, src_left, 13, src_left, 14, src_left,
+              15, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp8, tmp9, tmp10, tmp11);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1, tmp15, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp12, tmp13, tmp14, tmp15);
     LSX_ST_8(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, dst, dst_stride);
     LSX_ST_8(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15, dst, dst_stride);
 }
@@ -561,41 +553,41 @@ void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
     __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
 
     reg0 = __lsx_vreplgr2vr_h(top_left);
-    LSX_DUP2_ARG2(__lsx_vld, src_top_ptr, 0, src_top_ptr, 16, reg1, reg2);
+    DUP2_ARG2(__lsx_vld, src_top_ptr, 0, src_top_ptr, 16, reg1, reg2);
 
     src_left += 28;
     for (loop_cnt = 8; loop_cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left, 3,
-                      tmp3, tmp2, tmp1, tmp0);
+        DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left, 3,
+                  tmp3, tmp2, tmp1, tmp0);
         src_left -= 4;
-        LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
-                      src4, src5, src6, src7);
-        LSX_DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
-                      src4, src5, src6, src7);
-        LSX_DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
-                      dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
-                      dst4, dst5, dst6, dst7);
-        LSX_DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
-                      dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG2(__lsx_vssub_hu, dst4, reg0, dst5, reg0, dst6, reg0, dst7, reg0,
-                      dst4, dst5, dst6, dst7);
-        LSX_DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
-                      src4, src5, src6, src7);
-        LSX_DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
-                      dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG2(__lsx_vsat_hu, dst4, 7, dst5, 7, dst6, 7, dst7, 7,
-                      dst4, dst5, dst6, dst7);
-        LSX_DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vpackev_b, dst4, dst0, dst5, dst1, dst6, dst2, dst7, dst3,
-                      dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+                  src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+                  src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2, tmp3, reg2,
+                  dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vssub_hu, dst4, reg0, dst5, reg0, dst6, reg0, dst7, reg0,
+                  dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+                  src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vsat_hu, dst4, 7, dst5, 7, dst6, 7, dst7, 7,
+                  dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vpackev_b, dst4, dst0, dst5, dst1, dst6, dst2, dst7, dst3,
+                  dst0, dst1, dst2, dst3);
         __lsx_vst(src0, dst, 0);
         __lsx_vst(dst0, dst, 16);
         dst += dst_stride;
diff --git a/libavcodec/loongarch/vp9_lpf_lsx.c b/libavcodec/loongarch/vp9_lpf_lsx.c
index f8bde7e300..58619e6950 100644
--- a/libavcodec/loongarch/vp9_lpf_lsx.c
+++ b/libavcodec/loongarch/vp9_lpf_lsx.c
@@ -20,7 +20,7 @@
  */
 
 #include "libavcodec/vp9dsp.h"
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "libavutil/common.h"
 #include "vp9dsp_loongarch.h"
 
@@ -237,45 +237,6 @@
     mask_dst = __lsx_vxori_b(mask_dst, 0xff);                               \
 }
 
-#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    LSX_DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,   \
-                  _tmp0, _tmp1, _tmp2, _tmp3);                                     \
-    LSX_DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _tmp4, _tmp6);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _tmp5, _tmp7);        \
-    LSX_DUP2_ARG2(__lsx_vilvl_w, _tmp6, _tmp4, _tmp7, _tmp5, _out0, _out4);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_w, _tmp6, _tmp4, _tmp7, _tmp5, _out2, _out6);        \
-    LSX_DUP4_ARG2(__lsx_vbsrl_v, _out0, 8, _out2, 8, _out4, 8, _out6, 8,           \
-                  _out1, _out3, _out5, _out7);                                     \
-}
-
-#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
-                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-    LSX_DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,   \
-                  _tmp0, _tmp1, _tmp2, _tmp3);                                     \
-    LSX_DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,    \
-                  _in13, _tmp4, _tmp5, _tmp6, _tmp7);                              \
-    LSX_DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);            \
-    LSX_DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);            \
-    LSX_DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);            \
-    LSX_DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);            \
-    LSX_DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                \
-    LSX_DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                \
-    LSX_DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                \
-    LSX_DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                \
-    LSX_DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);        \
-    LSX_DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);        \
-}
-
-
 void ff_loop_filter_v_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
                               int32_t b_limit_ptr,
                               int32_t limit_ptr,
@@ -287,11 +248,10 @@ void ff_loop_filter_v_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i mask, hev, flat, thresh, b_limit, limit;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0, p1_out, p0_out, q0_out, q1_out;
 
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
-
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -321,10 +281,10 @@ void ff_loop_filter_v_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i limit0, thresh1, b_limit1, limit1;
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
 
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh0 = __lsx_vreplgr2vr_b(thresh_ptr);
     thresh1 = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -364,10 +324,10 @@ void ff_loop_filter_v_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i p3_l, p2_l, p1_l, p0_l, q3_l, q2_l, q1_l, q0_l;
     __m128i zero = __lsx_vldi(0);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -388,19 +348,19 @@ void ff_loop_filter_v_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_d(q0_out, dst          , 0, 0);
         __lsx_vstelm_d(q1_out, dst +  stride, 0, 0);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filter8,
                     p1_filter8, p0_filter8, q0_filter8, q1_filter8, q2_filter8);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
-                      zero, p0_filter8, zero, q0_filter8, p2_filter8,
-                      p1_filter8, p0_filter8, q0_filter8);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
-                      q1_filter8, q2_filter8);
+        DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
+                  zero, p0_filter8, zero, q0_filter8, p2_filter8,
+                  p1_filter8, p0_filter8, q0_filter8);
+        DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
+                  q1_filter8, q2_filter8);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filter8, flat);
@@ -439,10 +399,10 @@ void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -470,27 +430,26 @@ void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vst(q0_out, dst, 0);
         __lsx_vst(q1_out, dst + stride, 0);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                   q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
-                      p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h, p1_filt8_l,
+                  p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l, p2_filt8_l,
+                  p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h, q2_filt8_l,
+                  q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -527,10 +486,10 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -560,20 +519,19 @@ void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vst(q0_out, dst, 0);
         __lsx_vst(q1_out, dst + stride, 0);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
-                      p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l, p1_filt8_l,
+                  p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l, q2_filt8_l,
+                  q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -609,10 +567,10 @@ void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = { 0 };
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh = __lsx_vreplgr2vr_b(thresh_ptr);
     tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
@@ -642,20 +600,19 @@ void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vst(q0_out, dst, 0);
         __lsx_vst(q1_out, dst + stride, 0);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
-                      p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h,
-                      q0_filt8_h, p2_filt8_h, p1_filt8_h, p0_filt8_h,
-                      q0_filt8_h);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
-                      q2_filt8_h, q1_filt8_h, q2_filt8_h);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
+                  p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h, q0_filt8_h,
+                  p2_filt8_h, p1_filt8_h, p0_filt8_h, q0_filt8_h);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
+                  q2_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_h, flat);
@@ -695,10 +652,10 @@ static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -719,27 +676,26 @@ static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
         __lsx_vst(q1_out, dst + stride, 0);
         return 1;
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
-                      p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -784,21 +740,21 @@ static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride, uint8_t *filter48
 
     flat = __lsx_vld(filter48, 96);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
-                  dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp + stride, 0, dst_tmp + stride2, 0,
-                  dst_tmp + stride3, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3,
-                  0, q0, q1, q2, q3);
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2, 0,
-                  dst_tmp1 + stride3, 0, q4, q5, q6, q7);
+    DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
+              dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp + stride, 0, dst_tmp + stride2, 0,
+              dst_tmp + stride3, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3,
+              0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2, 0,
+              dst_tmp1 + stride3, 0, q4, q5, q6, q7);
     VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
 
     /* if flat2 is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat2)) {
-        LSX_DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32, filter48,
-                      48, p2, p1, p0, q0);
-        LSX_DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32, filter48,
+                  48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
 
         __lsx_vst(p2, dst - stride3, 0);
         __lsx_vst(p1, dst - stride2, 0);
@@ -1157,10 +1113,10 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i tmp0, tmp1, tmp2;
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
-                  dst - stride, 0, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
-                  dst + stride3, 0, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -1182,20 +1138,20 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_d(q1_out, dst +   stride, 0, 0);
     } else {
         /* convert 8 bit input data into 16 bit */
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l,
                     p2_filter8, p1_filter8, p0_filter8, q0_filter8,
                     q1_filter8, q2_filter8);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
-                      zero, p0_filter8, zero, q0_filter8, p2_filter8,
-                      p1_filter8, p0_filter8, q0_filter8);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
-                      q1_filter8, q2_filter8);
+        DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
+                  zero, p0_filter8, zero, q0_filter8, p2_filter8,
+                  p1_filter8, p0_filter8, q0_filter8);
+        DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
+                  q1_filter8, q2_filter8);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filter8, flat);
@@ -1206,10 +1162,10 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
         q2_out = __lsx_vbitsel_v(q2, q2_filter8, flat);
 
         /* load 16 vector elements */
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
-                      dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2,
-                      0, dst_tmp1 + stride3, 0, q4, q5, q6, q7);
+        DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
+                  dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
+        DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0, dst_tmp1 + stride2,
+                  0, dst_tmp1 + stride3, 0, q4, q5, q6, q7);
 
         VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
 
@@ -1229,10 +1185,10 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             __lsx_vstelm_d(q2_out, dst, 0, 0);
         } else {
             /* LSB(right) 8 pixel operation */
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p7, zero, p6, zero, p5, zero, p4,
-                          p7_l, p6_l, p5_l, p4_l);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q4, zero, q5, zero, q6, zero, q7,
-                          q4_l, q5_l, q6_l, q7_l);
+            DUP4_ARG2(__lsx_vilvl_b, zero, p7, zero, p6, zero, p5, zero, p4,
+                      p7_l, p6_l, p5_l, p4_l);
+            DUP4_ARG2(__lsx_vilvl_b, zero, q4, zero, q5, zero, q6, zero, q7,
+                      q4_l, q5_l, q6_l, q7_l);
 
             tmp0 = __lsx_vslli_h(p7_l, 3);
             tmp0 = __lsx_vsub_h(tmp0, p7_l);
@@ -1257,8 +1213,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             tmp1 = __lsx_vadd_h(tmp1, tmp0);
 
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(p6, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(p5, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1277,8 +1233,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h(tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(p4, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(p3, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1297,8 +1253,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h(tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(p2_out, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(p1_out, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1317,8 +1273,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h((__m128i)tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h((__m128i)tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(p0_out, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(q0_out, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1337,8 +1293,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h(tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(q1_out, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(q2_out, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1357,8 +1313,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h(tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(q3, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(q4, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1377,8 +1333,8 @@ void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
             p0_filter16 = __lsx_vsrari_h(tmp1, 4);
             tmp1 = __lsx_vadd_h(tmp1, tmp2);
             p1_filter16 = __lsx_vsrari_h(tmp1, 4);
-            LSX_DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
-                          p1_filter16, p0_filter16, p1_filter16);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
             p0_filter16 = __lsx_vbitsel_v(q5, p0_filter16, flat2);
             p1_filter16 = __lsx_vbitsel_v(q6, p1_filter16, flat2);
             __lsx_vstelm_d(p0_filter16, dst, 0, 0);
@@ -1401,10 +1357,10 @@ void ff_loop_filter_h_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i p3, p2, p1, p0, q3, q2, q1, q0;
     __m128i vec0, vec1, vec2, vec3;
 
-    LSX_DUP4_ARG2(__lsx_vld, dst, -4, dst + stride, -4, dst + stride2, -4,
-                  dst + stride3, -4, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, -4, dst + stride, -4, dst + stride2, -4,
+              dst + stride3, -4, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -1415,7 +1371,7 @@ void ff_loop_filter_h_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
                  hev, mask, flat);
     VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1, p0, q0, q1);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, vec0, vec1);
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, vec0, vec1);
     vec2 = __lsx_vilvl_h(vec1, vec0);
     vec3 = __lsx_vilvh_h(vec1, vec0);
 
@@ -1447,17 +1403,17 @@ void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i row8, row9, row10, row11, row12, row13, row14, row15;
     __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row0, row1, row2, row3);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row0, row1, row2, row3);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row4, row5, row6, row7);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row8, row9, row10, row11);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row8, row9, row10, row11);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row12, row13, row14, row15);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row12, row13, row14, row15);
 
     LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
                         row8, row9, row10, row11, row12, row13, row14, row15,
@@ -1478,10 +1434,10 @@ void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
     LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit0, b_limit0, thresh0,
                  hev, mask, flat);
     VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1, p0, q0, q1);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, tmp0, tmp1);
     tmp2 = __lsx_vilvl_h(tmp1, tmp0);
     tmp3 = __lsx_vilvh_h(tmp1, tmp0);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, p0, p1, q1, q0, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vilvh_b, p0, p1, q1, q0, tmp0, tmp1);
     tmp4 = __lsx_vilvl_h(tmp1, tmp0);
     tmp5 = __lsx_vilvh_h(tmp1, tmp0);
 
@@ -1526,11 +1482,11 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, p3, p2, p1, p0);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, q0, q1, q2, q3);
 
     LSX_TRANSPOSE8x8_B(p3, p2, p1, p0, q0, q1, q2, q3,
                        p3, p2, p1, p0, q0, q1, q2, q3);
@@ -1553,7 +1509,7 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
         /* Store 4 pixels p1-_q1 */
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
 
@@ -1568,19 +1524,19 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
         __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
-                      p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l,
+                  q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
+                  q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -1591,7 +1547,7 @@ void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
         q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
 
         /* Store 6 pixels p2-_q2 */
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
         vec4 = __lsx_vilvl_b(q2, q1);
@@ -1646,17 +1602,17 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i zero = __lsx_vldi(0);
 
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, p0, p1, p2, p3);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, p0, p1, p2, p3);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row4, row5, row6, row7);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, q3, q2, q1, q0);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, q3, q2, q1, q0);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row12, row13, row14, row15);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row12, row13, row14, row15);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -1686,12 +1642,10 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out,
-                      vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out,
-                      vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec4 = __lsx_vilvl_h(vec1, vec0);
         vec5 = __lsx_vilvh_h(vec1, vec0);
 
@@ -1716,29 +1670,28 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
         __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
 
         /* filter8 */
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
-                      p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h, p1_filt8_l,
+                  p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h, q2_filt8_l,
+                  q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -1748,10 +1701,10 @@ void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q1 = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
         q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec3 = __lsx_vilvl_h(vec1, vec0);
         vec4 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
         vec6 = __lsx_vilvl_h(vec1, vec0);
         vec7 = __lsx_vilvh_h(vec1, vec0);
         vec2 = __lsx_vilvl_b(q2, q1);
@@ -1827,17 +1780,17 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i zero = __lsx_vldi(0);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, p0, p1, p2, p3);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, p0, p1, p2, p3);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row4, row5, row6, row7);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, q3, q2, q1, q0);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, q3, q2, q1, q0);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row12, row13, row14, row15);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row12, row13, row14, row15);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -1869,10 +1822,10 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec4 = __lsx_vilvl_h(vec1, vec0);
         vec5 = __lsx_vilvh_h(vec1, vec0);
 
@@ -1897,20 +1850,19 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
         __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
-                      p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l,
-                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
-                      q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l, p1_filt8_l,
+                  p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l, p2_filt8_l,
+                  p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l, q2_filt8_l,
+                  q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -1920,10 +1872,10 @@ void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q1 = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
         q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec3 = __lsx_vilvl_h(vec1, vec0);
         vec4 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
         vec6 = __lsx_vilvl_h(vec1, vec0);
         vec7 = __lsx_vilvh_h(vec1, vec0);
         vec2 = __lsx_vilvl_b(q2, q1);
@@ -1999,17 +1951,17 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
     __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     __m128i zero = __lsx_vldi(0);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, p0, p1, p2, p3);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, p0, p1, p2, p3);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row4, row5, row6, row7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row4, row5, row6, row7);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, q3, q2, q1, q0);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, q3, q2, q1, q0);
     dst_tmp += stride4;
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
-                  dst_tmp + stride3, -4, row12, row13, row14, row15);
+    DUP4_ARG2(__lsx_vld, dst_tmp, -4, dst_tmp + stride, -4, dst_tmp + stride2, -4,
+              dst_tmp + stride3, -4, row12, row13, row14, row15);
 
     /* transpose 16x8 matrix into 8x16 */
     LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
@@ -2041,10 +1993,10 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec4 = __lsx_vilvl_h(vec1, vec0);
         vec5 = __lsx_vilvh_h(vec1, vec0);
 
@@ -2069,21 +2021,20 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
         __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
         __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
 
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
-                      p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h,
-                      q0_filt8_h, p2_filt8_h, p1_filt8_h, p0_filt8_h,
-                      q0_filt8_h);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
-                      q2_filt8_h, q1_filt8_h, q2_filt8_h);
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
+                  p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h, q0_filt8_h,
+                  p2_filt8_h, p1_filt8_h, p0_filt8_h, q0_filt8_h);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
+                  q2_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* store pixel values */
         p2 = __lsx_vbitsel_v(p2, p2_filt8_h, flat);
@@ -2093,10 +2044,10 @@ void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
         q1 = __lsx_vbitsel_v(q1_out, q1_filt8_h, flat);
         q2 = __lsx_vbitsel_v(q2, q2_filt8_h, flat);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec3 = __lsx_vilvl_h(vec1, vec0);
         vec4 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
         vec6 = __lsx_vilvl_h(vec1, vec0);
         vec7 = __lsx_vilvh_h(vec1, vec0);
         vec2 = __lsx_vilvl_b(q2, q1);
@@ -2166,13 +2117,13 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     LSX_TRANSPOSE8x8_B(p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org,
                        p0_org, p7, p6, p5, p4, p3, p2, p1, p0);
     /* 8x8 transpose */
-    LSX_DUP4_ARG2(__lsx_vilvh_b, p5_org, p7_org, p4_org, p6_org, p1_org,
-                  p3_org, p0_org, p2_org, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, tmp1, tmp0, tmp3, tmp2, tmp4, tmp6);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp7);
-    LSX_DUP2_ARG2(__lsx_vilvl_w, tmp6, tmp4, tmp7, tmp5, q0, q4);
-    LSX_DUP2_ARG2(__lsx_vilvh_w, tmp6, tmp4, tmp7, tmp5, q2, q6);
-    LSX_DUP4_ARG2(__lsx_vbsrl_v, q0, 8, q2, 8, q4, 8, q6, 8, q1, q3, q5, q7);
+    DUP4_ARG2(__lsx_vilvh_b, p5_org, p7_org, p4_org, p6_org, p1_org,
+              p3_org, p0_org, p2_org, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, tmp1, tmp0, tmp3, tmp2, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp7);
+    DUP2_ARG2(__lsx_vilvl_w, tmp6, tmp4, tmp7, tmp5, q0, q4);
+    DUP2_ARG2(__lsx_vilvh_w, tmp6, tmp4, tmp7, tmp5, q2, q6);
+    DUP4_ARG2(__lsx_vbsrl_v, q0, 8, q2, 8, q4, 8, q6, 8, q1, q3, q5, q7);
 
     LSX_ST_8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_pitch);
     output += out_pitch;
@@ -2222,13 +2173,13 @@ static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
     q1 = __lsx_vpackod_d(row14, row6);
     q0 = __lsx_vpackod_d(row15, row7);
 
-    LSX_DUP2_ARG2(__lsx_vpackev_b, q6, q7, q4, q5, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vpackod_b, q6, q7, q4, q5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vpackev_b, q6, q7, q4, q5, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpackod_b, q6, q7, q4, q5, tmp4, tmp5);
 
-    LSX_DUP2_ARG2(__lsx_vpackev_b, q2, q3, q0, q1, q5, q7);
-    LSX_DUP2_ARG2(__lsx_vpackod_b, q2, q3, q0, q1, tmp6, tmp7);
+    DUP2_ARG2(__lsx_vpackev_b, q2, q3, q0, q1, q5, q7);
+    DUP2_ARG2(__lsx_vpackod_b, q2, q3, q0, q1, tmp6, tmp7);
 
-    LSX_DUP2_ARG2(__lsx_vpackev_h, tmp1, tmp0, q7, q5, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpackev_h, tmp1, tmp0, q7, q5, tmp2, tmp3);
     q0 = __lsx_vpackev_w(tmp3, tmp2);
     q4 = __lsx_vpackod_w(tmp3, tmp2);
 
@@ -2237,7 +2188,7 @@ static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
     q2 = __lsx_vpackev_w(tmp3, tmp2);
     q6 = __lsx_vpackod_w(tmp3, tmp2);
 
-    LSX_DUP2_ARG2(__lsx_vpackev_h, tmp5, tmp4, tmp7, tmp6, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpackev_h, tmp5, tmp4, tmp7, tmp6, tmp2, tmp3);
     q1 = __lsx_vpackev_w(tmp3, tmp2);
     q5 = __lsx_vpackod_w(tmp3, tmp2);
 
@@ -2267,10 +2218,8 @@ static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, src, -64, src, -48, src, -32,
-                  src, -16, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                  q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, src, -64, src, -48, src, -32, src, -16, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -2289,7 +2238,7 @@ static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
 
@@ -2311,10 +2260,10 @@ static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
         __lsx_vstelm_w(vec3, src_org, 0, 3);
         return 1;
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
 
@@ -2362,14 +2311,12 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *dst, uint8_t *dst_org,
     uint8_t *dst_tmp = dst - 128;
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
-                  dst_tmp, 48, p7, p6, p5, p4);
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
-                  dst_tmp, 112, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32,
-                  dst, 48, q0, q1, q2, q3);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96,
-                  dst, 112, q4, q5, q6, q7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
+              dst_tmp, 48, p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
+              dst_tmp, 112, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96, dst, 112, q4, q5, q6, q7);
 
     flat = __lsx_vld(filter48, 96);
 
@@ -2380,11 +2327,11 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *dst, uint8_t *dst_org,
     if (__lsx_bz_v(flat2)) {
         __m128i vec0, vec1, vec2, vec3, vec4;
 
-        LSX_DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
-                      filter48, 48, p2, p1, p0, q0);
-        LSX_DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
+                  filter48, 48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec3 = __lsx_vilvl_h(vec1, vec0);
         vec4 = __lsx_vilvh_h(vec1, vec0);
         vec2 = __lsx_vilvl_b(q2, q1);
@@ -2651,10 +2598,8 @@ static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *dst, uint8_t *filter48,
     __m128i zero = __lsx_vldi(0);
 
     /* load vector elements */
-    LSX_DUP4_ARG2(__lsx_vld, dst, -64, dst, -48, dst, -32,
-                  dst, -16, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32,
-                  dst, 48, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, -64, dst, -48, dst, -32, dst, -16, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
 
     thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
     b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
@@ -2671,10 +2616,10 @@ static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *dst, uint8_t *filter48,
 
     /* if flat is zero for all pixels, then no need to calculate other filter */
     if (__lsx_bz_v(flat)) {
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec2 = __lsx_vilvl_h(vec1, vec0);
         vec3 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         vec4 = __lsx_vilvl_h(vec1, vec0);
         vec5 = __lsx_vilvh_h(vec1, vec0);
 
@@ -2701,26 +2646,26 @@ static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *dst, uint8_t *filter48,
 
         return 1;
     } else {
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_l, p2_l, p1_l, p0_l);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_l, q1_l, q2_l, q3_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
         VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
                     p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
-                      p3_h, p2_h, p1_h, p0_h);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
-                      q0_h, q1_h, q2_h, q3_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
         VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
                     p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
 
         /* convert 16 bit output data into 8 bit */
-        LSX_DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
                       p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h,
                       q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
                       q0_filt8_l);
-        LSX_DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
-                      q2_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
 
         /* store pixel values */
         p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
@@ -2763,14 +2708,12 @@ static int32_t vp9_vt_lpf_t16_16w(uint8_t *dst, uint8_t *dst_org,
 
     flat = __lsx_vld(filter48, 96);
 
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
-                  dst_tmp, 48, p7, p6, p5, p4);
-    LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
-                  dst_tmp, 112, p3, p2, p1, p0);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32,
-                  dst, 48, q0, q1, q2, q3);
-    LSX_DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96,
-                  dst, 112, q4, q5, q6, q7);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
+              dst_tmp, 48, p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
+              dst_tmp, 112, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96, dst, 112, q4, q5, q6, q7);
 
     VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
 
@@ -2778,14 +2721,14 @@ static int32_t vp9_vt_lpf_t16_16w(uint8_t *dst, uint8_t *dst_org,
     if (__lsx_bz_v(flat2)) {
         __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
 
-        LSX_DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
-                      filter48, 48, p2, p1, p0, q0);
-        LSX_DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
+                  filter48, 48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
 
-        LSX_DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
         vec3 = __lsx_vilvl_h(vec1, vec0);
         vec4 = __lsx_vilvh_h(vec1, vec0);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
         vec6 = __lsx_vilvl_h(vec1, vec0);
         vec7 = __lsx_vilvh_h(vec1, vec0);
         vec2 = __lsx_vilvl_b(q2, q1);
diff --git a/libavcodec/loongarch/vp9_mc_lsx.c b/libavcodec/loongarch/vp9_mc_lsx.c
index 02ee916447..9d91675781 100644
--- a/libavcodec/loongarch/vp9_mc_lsx.c
+++ b/libavcodec/loongarch/vp9_mc_lsx.c
@@ -20,7 +20,7 @@
  */
 
 #include "libavcodec/vp9dsp.h"
-#include "libavutil/loongarch/generic_macros_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "vp9dsp_loongarch.h"
 
 static const uint8_t mc_filt_mask_arr[16 * 3] = {
@@ -33,57 +33,56 @@ static const uint8_t mc_filt_mask_arr[16 * 3] = {
 };
 
 
-#define HORIZ_8TAP_4WID_4VECS_FILT(_src0, _src1, _src2, _src3,                     \
-                                   _mask0, _mask1, _mask2, _mask3,                 \
-                                   _filter0, _filter1, _filter2, _filter3,         \
-                                   _out0, _out1)                                   \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    __m128i _reg0, _reg1, _reg2, _reg3;                                            \
-                                                                                   \
-    LSX_DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0,                             \
-                  _src3, _src2, _mask0, _tmp0, _tmp1);                             \
-    LSX_DUP2_ARG2(__lsx_dp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _reg0, _reg1);  \
-    LSX_DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask1,                             \
-                  _src3, _src2, _mask1, _tmp2, _tmp3);                             \
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, _reg0, _tmp2, _filter1,                        \
-                  _reg1, _tmp3, _filter1, _reg0, _reg1);                           \
-    LSX_DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask2,                             \
-                  _src3, _src2, _mask2, _tmp4, _tmp5);                             \
-    LSX_DUP2_ARG2(__lsx_dp2_h_b, _tmp4, _filter2, _tmp5, _filter2, _reg2, _reg3);  \
-    LSX_DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask3,                             \
-                  _src3, _src2, _mask3, _tmp6, _tmp7);                             \
-    LSX_DUP2_ARG3(__lsx_dp2add_h_b, _reg2, _tmp6, _filter3,                        \
-                  _reg3, _tmp7, _filter3, _reg2, _reg3);                           \
-    LSX_DUP2_ARG2(__lsx_vsadd_h, _reg0, _reg2, _reg1, _reg3, _out0, _out1);        \
+#define HORIZ_8TAP_4WID_4VECS_FILT(_src0, _src1, _src2, _src3,                         \
+                                   _mask0, _mask1, _mask2, _mask3,                     \
+                                   _filter0, _filter1, _filter2, _filter3,             \
+                                   _out0, _out1)                                       \
+{                                                                                      \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                    \
+    __m128i _reg0, _reg1, _reg2, _reg3;                                                \
+                                                                                       \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src3, _src2, _mask0,               \
+              _tmp0, _tmp1);                                                           \
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _reg0, _reg1);         \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask1, _src3, _src2, _mask1, _tmp2, _tmp3);\
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp2, _filter1, _reg1, _tmp3, _filter1,       \
+              _reg0, _reg1);                                                           \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask2, _src3, _src2, _mask2, _tmp4, _tmp5);\
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp4, _filter2, _tmp5, _filter2, _reg2, _reg3);         \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask3, _src3, _src2, _mask3, _tmp6, _tmp7);\
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg2, _tmp6, _filter3, _reg3, _tmp7, _filter3,       \
+              _reg2, _reg3);                                                           \
+    DUP2_ARG2(__lsx_vsadd_h, _reg0, _reg2, _reg1, _reg3, _out0, _out1);                \
 }
 
-#define HORIZ_8TAP_8WID_4VECS_FILT(_src0, _src1, _src2, _src3,                                \
-                                   _mask0, _mask1, _mask2, _mask3,                            \
-                                   _filter0, _filter1, _filter2, _filter3,                    \
-                                   _out0, _out1, _out2, _out3)                                \
-{                                                                                             \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                           \
-    __m128i _reg0, _reg1, _reg2, _reg3, _reg4, _reg5, _reg6, _reg7;                           \
-                                                                                              \
-    LSX_DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask0, _src1, _src1, _mask0, _src2,           \
-                  _src2, _mask0, _src3, _src3, _mask0, _tmp0, _tmp1, _tmp2, _tmp3);           \
-    LSX_DUP4_ARG2(__lsx_dp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _tmp2, _filter0,           \
-                  _tmp3, _filter0, _reg0, _reg1, _reg2, _reg3);                               \
-    LSX_DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask2, _src1, _src1, _mask2, _src2,           \
-                  _src2, _mask2, _src3, _src3, _mask2, _tmp0, _tmp1, _tmp2, _tmp3);           \
-    LSX_DUP4_ARG2(__lsx_dp2_h_b, _tmp0, _filter2, _tmp1, _filter2, _tmp2, _filter2,           \
-                  _tmp3, _filter2, _reg4, _reg5, _reg6, _reg7);                               \
-    LSX_DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask1, _src1, _src1, _mask1, _src2,           \
-                  _src2, _mask1, _src3, _src3, _mask1, _tmp4, _tmp5, _tmp6, _tmp7);           \
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, _reg0, _tmp4, _filter1, _reg1, _tmp5, _filter1,           \
-                  _reg2, _tmp6, _filter1, _reg3, _tmp7, _filter1, _reg0, _reg1, _reg2, _reg3);\
-    LSX_DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask3, _src1, _src1, _mask3, _src2,           \
-                  _src2, _mask3, _src3, _src3, _mask3, _tmp4, _tmp5, _tmp6, _tmp7);           \
-    LSX_DUP4_ARG3(__lsx_dp2add_h_b, _reg4, _tmp4, _filter3, _reg5, _tmp5, _filter3,           \
-                  _reg6, _tmp6, _filter3, _reg7, _tmp7, _filter3, _reg4, _reg5, _reg6, _reg7);\
-    LSX_DUP4_ARG2(__lsx_vsadd_h, _reg0, _reg4, _reg1, _reg5, _reg2, _reg6, _reg3, _reg7,      \
-                  _out0, _out1, _out2, _out3);                                                \
+#define HORIZ_8TAP_8WID_4VECS_FILT(_src0, _src1, _src2, _src3,                        \
+                                   _mask0, _mask1, _mask2, _mask3,                    \
+                                   _filter0, _filter1, _filter2, _filter3,            \
+                                   _out0, _out1, _out2, _out3)                        \
+{                                                                                     \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                   \
+    __m128i _reg0, _reg1, _reg2, _reg3, _reg4, _reg5, _reg6, _reg7;                   \
+                                                                                      \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask0, _src1, _src1, _mask0, _src2,       \
+              _src2, _mask0, _src3, _src3, _mask0, _tmp0, _tmp1, _tmp2, _tmp3);       \
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _tmp2, _filter0,      \
+              _tmp3, _filter0, _reg0, _reg1, _reg2, _reg3);                           \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask2, _src1, _src1, _mask2, _src2,       \
+              _src2, _mask2, _src3, _src3, _mask2, _tmp0, _tmp1, _tmp2, _tmp3);       \
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter2, _tmp1, _filter2, _tmp2, _filter2,      \
+              _tmp3, _filter2, _reg4, _reg5, _reg6, _reg7);                           \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask1, _src1, _src1, _mask1, _src2,       \
+              _src2, _mask1, _src3, _src3, _mask1, _tmp4, _tmp5, _tmp6, _tmp7);       \
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp4, _filter1, _reg1, _tmp5, _filter1,      \
+              _reg2, _tmp6, _filter1, _reg3, _tmp7, _filter1, _reg0, _reg1, _reg2,    \
+              _reg3);                                                                 \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask3, _src1, _src1, _mask3, _src2,       \
+              _src2, _mask3, _src3, _src3, _mask3, _tmp4, _tmp5, _tmp6, _tmp7);       \
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg4, _tmp4, _filter3, _reg5, _tmp5, _filter3,      \
+              _reg6, _tmp6, _filter3, _reg7, _tmp7, _filter3, _reg4, _reg5, _reg6,    \
+              _reg7);                                                                 \
+    DUP4_ARG2(__lsx_vsadd_h, _reg0, _reg4, _reg1, _reg5, _reg2, _reg6, _reg3, _reg7,  \
+              _out0, _out1, _out2, _out3);                                            \
 }
 
 #define FILT_8TAP_DPADD_S_H(_reg0, _reg1, _reg2, _reg3,                            \
@@ -91,10 +90,10 @@ static const uint8_t mc_filt_mask_arr[16 * 3] = {
 ( {                                                                                \
     __m128i _vec0, _vec1;                                                          \
                                                                                    \
-    _vec0 = __lsx_dp2_h_b(_reg0, _filter0);                                        \
-    _vec0 = __lsx_dp2add_h_b(_vec0, _reg1, _filter1);                              \
-    _vec1 = __lsx_dp2_h_b(_reg2, _filter2);                                        \
-    _vec1 = __lsx_dp2add_h_b(_vec1, _reg3, _filter3);                              \
+    _vec0 = __lsx_vdp2_h_b(_reg0, _filter0);                                       \
+    _vec0 = __lsx_vdp2add_h_b(_vec0, _reg1, _filter1);                             \
+    _vec1 = __lsx_vdp2_h_b(_reg2, _filter2);                                       \
+    _vec1 = __lsx_vdp2add_h_b(_vec1, _reg3, _filter3);                             \
     _vec0 = __lsx_vsadd_h(_vec0, _vec1);                                           \
                                                                                    \
     _vec0;                                                                         \
@@ -106,8 +105,8 @@ static const uint8_t mc_filt_mask_arr[16 * 3] = {
     __m128i _tmp0, _tmp1, _tmp2, _tmp3;                                            \
     __m128i _out;                                                                  \
                                                                                    \
-    LSX_DUP4_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src1, _src0, _mask1, _src1,\
-                  _src0, _mask2, _src1, _src0, _mask3, _tmp0, _tmp1, _tmp2, _tmp3);\
+    DUP4_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src1, _src0, _mask1, _src1,    \
+              _src0, _mask2, _src1, _src0, _mask3, _tmp0, _tmp1, _tmp2, _tmp3);    \
     _out = FILT_8TAP_DPADD_S_H(_tmp0, _tmp1, _tmp2, _tmp3, _filt_h0, _filt_h1,     \
                                _filt_h2, _filt_h3);                                \
     _out = __lsx_vsrari_h(_out, 7);                                                \
@@ -171,14 +170,14 @@ static void common_hz_8t_4x4_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= 3;
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, out0, out1);
     out = __lsx_vssrarni_b_h(out1, out0, 7);
@@ -204,28 +203,28 @@ static void common_hz_8t_4x8_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src += stride;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, out0, out1);
     src += stride;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
     src += stride;
-    LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, out2, out3);
-    LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-    LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
     __lsx_vstelm_w(out0, dst, 0, 0);
     dst += dst_stride;
     __lsx_vstelm_w(out0, dst, 0, 1);
@@ -265,18 +264,18 @@ static void common_hz_8t_8x4_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-    LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-    LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
     __lsx_vstelm_d(out0, dst, 0, 0);
     dst += dst_stride;
     __lsx_vstelm_d(out0, dst, 0, 1);
@@ -299,21 +298,21 @@ static void common_hz_8t_8x8mult_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
         src += stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(out0, dst, 0, 1);
@@ -350,20 +349,20 @@ static void common_hz_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
-        LSX_DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src1, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
         dst += dst_stride;
         __lsx_vst(out1, dst, 0);
@@ -385,37 +384,37 @@ static void common_hz_8t_32w_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
         src3 = __lsx_vld(src, 24);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
         src3 = __lsx_vld(src, 24);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
         src += src_stride;
 
         dst += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
         dst += dst_stride;
@@ -435,33 +434,33 @@ static void common_hz_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (; loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
         src3 = __lsx_vld(src, 24);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
         src3 = __lsx_vld(src, 56);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vst(out0, dst, 32);
         __lsx_vst(out1, dst, 48);
         src += src_stride;
@@ -481,8 +480,8 @@ static void common_vt_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i out0, out1;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
     src4 = __lsx_vld(src, 0);
@@ -491,19 +490,21 @@ static void common_vt_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
-    LSX_DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0,
+              tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
     reg2 = __lsx_vilvl_d(tmp5, tmp2);
-    LSX_DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
+    DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
     reg2 = __lsx_vxori_b(reg2, 128);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0,
+                  tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
+        DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
         out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1, filter2, filter3);
         out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1, filter2, filter3);
         out0 = __lsx_vssrarni_b_h(out1, out0, 7);
@@ -536,8 +537,8 @@ static void common_vt_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i out0, out1, out2, out3;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
@@ -547,25 +548,25 @@ static void common_vt_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  tmp0, tmp1, tmp2, tmp3);
         out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1, filter2, filter3);
         out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1, filter2, filter3);
         out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1, filter2, filter3);
         out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1, filter2, filter3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(out0, dst, 0, 1);
@@ -597,8 +598,8 @@ static void common_vt_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
     __m128i tmp0, tmp1, tmp2, tmp3;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
     src4 = __lsx_vld(src, 0);
@@ -607,29 +608,32 @@ static void common_vt_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
-    LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1, reg6, reg7, reg8, reg9);
-    LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1,
+              reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1, reg6, reg7,
+              reg8, reg9);
+    DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      src4, src5, src7, src8);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src4, src5, src7, src8);
         tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
         tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
         tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
         tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         dst += dst_stride;
         __lsx_vst(tmp1, dst, 0);
@@ -638,8 +642,8 @@ static void common_vt_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
         tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
         tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
         tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
         __lsx_vst(tmp0, dst, 0);
         dst += dst_stride;
         __lsx_vst(tmp1, dst, 0);
@@ -676,8 +680,8 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
     __m128i tmp0, tmp1, tmp2, tmp3;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     for (;cnt--;) {
         uint32_t loop_cnt = height >> 2;
 
@@ -692,31 +696,32 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
         src_tmp += src_stride;
         src6 = __lsx_vld(src_tmp, 0);
         src_tmp += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+                  src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      reg0, reg1, reg2, reg3);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      reg6, reg7, reg8, reg9);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg0, reg1, reg2, reg3);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg6, reg7, reg8, reg9);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
         for (;loop_cnt--;) {
             LSX_LD_4(src_tmp, src_stride, src7, src8, src9, src10);
             src_tmp += src_stride;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                          src7, src8, src9, src10);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src0, src1, src2, src3);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src4, src5, src7, src8);
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                      src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src4, src5, src7, src8);
             tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
             tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
             tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
             tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-            LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             __lsx_vst(tmp0, dst_tmp, 0);
             dst_tmp += dst_stride;
             __lsx_vst(tmp1, dst_tmp, 0);
@@ -725,8 +730,8 @@ static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
             tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
             tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
             tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-            LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             __lsx_vst(tmp0, dst_tmp, 0);
             dst_tmp += dst_stride;
             __lsx_vst(tmp1, dst_tmp, 0);
@@ -782,9 +787,9 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= (3 + 3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-                  filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
+              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
@@ -795,8 +800,8 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
     tmp0 = HORIZ_8TAP_FILT(src0, src1, mask0, mask1, mask2, mask3, filt_hz0,
@@ -807,17 +812,17 @@ static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
                            filt_hz1, filt_hz2, filt_hz3);
     tmp5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                            filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
-                  filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
-    LSX_DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
     tmp2 = __lsx_vpackev_b(tmp5, tmp4);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
         tmp3 = HORIZ_8TAP_FILT(src7, src8, mask0, mask1, mask2, mask3, filt_hz0,
                                filt_hz1, filt_hz2, filt_hz3);
         tmp4 = __lsx_vshuf_b(tmp3, tmp5, shuff);
@@ -864,9 +869,9 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= (3 + 3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-                  filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
+              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
@@ -877,8 +882,9 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1,
+              src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
     src0 = HORIZ_8TAP_FILT(src0, src0, mask0, mask1, mask2, mask3, filt_hz0,
@@ -896,17 +902,17 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
     src6 = HORIZ_8TAP_FILT(src6, src6, mask0, mask1, mask2, mask3, filt_hz0,
                            filt_hz1, filt_hz2, filt_hz3);
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
-                  filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
-                  src2, src1, tmp0, tmp1, tmp2, tmp4);
-    LSX_DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
+              src2, src1, tmp0, tmp1, tmp2, tmp4);
+    DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
         src7 = HORIZ_8TAP_FILT(src7, src7, mask0, mask1, mask2, mask3, filt_hz0,
                                filt_hz1, filt_hz2, filt_hz3);
         tmp3 = __lsx_vpackev_b(src7, src6);
@@ -927,8 +933,8 @@ static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
         src2 = __lsx_vpackev_b(src10, src9);
         src4 = FILT_8TAP_DPADD_S_H(tmp5, tmp6, src0, src2, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(out0, dst, 0, 1);
@@ -1069,13 +1075,13 @@ static void copy_width64_lsx(const uint8_t *src, int32_t src_stride,
     __m128i src8, src9, src10, src11, src12, src13, src14, src15;
 
     for (;cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src0, src1, src2, src3);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src4, src5, src6, src7);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src8, src9, src10, src11);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src8, src9, src10, src11);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src12, src13, src14, src15);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, src12, src13, src14, src15);
         src += src_stride;
         __lsx_vst(src0, dst, 0);
         __lsx_vst(src1, dst, 16);
@@ -1114,13 +1120,13 @@ static void common_hz_8t_and_aver_dst_4x4_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, tmp0, tmp1);
     dst0 = __lsx_vldrepl_w(dst_tmp, 0);
@@ -1157,15 +1163,15 @@ static void common_hz_8t_and_aver_dst_4x8_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     tmp0 = __lsx_vldrepl_w(dst_tmp, 0);
     dst_tmp += dst_stride;
     tmp1 = __lsx_vldrepl_w(dst_tmp, 0);
@@ -1191,15 +1197,15 @@ static void common_hz_8t_and_aver_dst_4x8_lsx(const uint8_t *src,
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, tmp0, tmp1);
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                  src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
     HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                filter0, filter1, filter2, filter3, tmp2, tmp3);
-    LSX_DUP4_ARG3(__lsx_vssrarni_b_h, tmp0, tmp0, 7, tmp1, tmp1, 7, tmp2, tmp2, 7,
-                  tmp3, tmp3, 7, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-    LSX_DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
+    DUP4_ARG3(__lsx_vssrarni_b_h, tmp0, tmp0, 7, tmp1, tmp1, 7, tmp2, tmp2, 7,
+              tmp3, tmp3, 7, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
     __lsx_vstelm_w(dst0, dst, 0, 0);
     dst += dst_stride;
     __lsx_vstelm_w(dst0, dst, 0, 1);
@@ -1245,16 +1251,16 @@ static void common_hz_8t_and_aver_dst_8w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src0, src1, src2, src3);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, tmp0, tmp1, tmp2, tmp3);
         dst0 = __lsx_vldrepl_d(dst_tmp, 0);
@@ -1265,10 +1271,10 @@ static void common_hz_8t_and_aver_dst_8w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         dst3 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, dst1, dst0, dst3, dst2, dst0, dst1);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
+        DUP2_ARG2(__lsx_vilvl_d, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
         __lsx_vstelm_d(dst0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(dst0, dst, 0, 1);
@@ -1295,43 +1301,43 @@ static void common_hz_8t_and_aver_dst_16w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     for (;loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 8, src0, src1);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src0, src1);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
         src += src_stride;
         dst0 = __lsx_vld(dst_tmp, 0);
         dst_tmp += dst_stride;
         dst1 = __lsx_vld(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
-                      mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
-                      mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
-                      mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
-                      mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
-                      filter0, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
-                      filter2, tmp8, tmp9, tmp10, tmp11);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
-                      tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
-                      tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
-        LSX_DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
-                      tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, dst2, dst3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, dst2, 128, dst3, 128, dst2, dst3);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, dst0, dst2, dst1, dst3, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
+                  mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
+                  mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
+                  mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
+                  mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
+                  filter0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
+                  filter2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
+                  tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
+                  tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, dst2, dst3);
+        DUP2_ARG2(__lsx_vxori_b, dst2, 128, dst3, 128, dst2, dst3);
+        DUP2_ARG2(__lsx_vavgr_bu, dst0, dst2, dst1, dst3, dst0, dst1);
         __lsx_vst(dst0, dst, 0);
         dst += dst_stride;
         __lsx_vst(dst1, dst, 0);
@@ -1355,41 +1361,41 @@ static void common_hz_8t_and_aver_dst_32w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
                   filter0, filter1, filter2, filter3);
 
     for (;loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
         src3 = __lsx_vld(src, 24);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vld, dst_tmp, 0, dst, 16, dst0, dst1);
+        DUP2_ARG2(__lsx_vld, dst_tmp, 0, dst, 16, dst0, dst1);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
-                      mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
-                      mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
-                      mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
-        LSX_DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
-                      mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
-                      filter0, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG2(__lsx_dp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
-                      filter2, tmp8, tmp9, tmp10, tmp11);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
-                      tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP4_ARG3(__lsx_dp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
-                      tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
-        LSX_DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
-                      tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, dst0, tmp0, dst1, tmp1, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
+                  mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
+                  mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
+                  mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
+                  mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
+                  filter0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
+                  filter2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
+                  tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
+                  tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vavgr_bu, dst0, tmp0, dst1, tmp1, dst0, dst1);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         dst += dst_stride;
@@ -1411,37 +1417,37 @@ static void common_hz_8t_and_aver_dst_64w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= 3;
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
                   filter0, filter1, filter2, filter3);
 
     for (;loop_cnt--;) {
-        LSX_DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
         src3 = __lsx_vld(src, 24);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
-        LSX_DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
         __lsx_vst(out0, dst, 0);
         __lsx_vst(out1, dst, 16);
 
-        LSX_DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
         src3 = __lsx_vld(src, 56);
         src1 = __lsx_vshuf_b(src2, src0, shuff);
-        LSX_DUP2_ARG2(__lsx_vld, dst, 32, dst, 48, dst0, dst1);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                      src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vld, dst, 32, dst, 48, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
         HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
                                    filter0, filter1, filter2, filter3, out0, out1, out2, out3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
         __lsx_vst(out0, dst, 32);
         __lsx_vst(out1, dst, 48);
         src += src_stride;
@@ -1464,8 +1470,8 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
     __m128i out0, out1;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
     src4 = __lsx_vld(src, 0);
@@ -1474,11 +1480,12 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0, tmp1, tmp2, tmp3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
-    LSX_DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0, tmp1,
+              tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
     reg2 = __lsx_vilvl_d(tmp5, tmp2);
-    LSX_DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
+    DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
     reg2 = __lsx_vxori_b(reg2, 128);
 
     for (;loop_cnt--;) {
@@ -1492,11 +1499,12 @@ static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         src3 = __lsx_vldrepl_w(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
         src0 = __lsx_vilvl_d(src1, src0);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0, tmp1, tmp2, tmp3);
-        LSX_DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
-        LSX_DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9, tmp0,
+                  tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
+        DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
         out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1, filter2, filter3);
         out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1, filter2, filter3);
         out0 = __lsx_vssrarni_b_h(out1, out0, 7);
@@ -1532,8 +1540,8 @@ static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
     __m128i out0, out1, out2, out3;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
     src += src_stride;
@@ -1543,11 +1551,11 @@ static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
-    LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
-    LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, reg0, reg1, reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
@@ -1560,18 +1568,18 @@ static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         src3 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, src1, src0, src3, src2, src0, src1);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                      tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vilvl_d, src1, src0, src3, src2, src0, src1);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  tmp0, tmp1, tmp2, tmp3);
         out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1, filter2, filter3);
         out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1, filter2, filter3);
         out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1, filter2, filter3);
         out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1, filter2, filter3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, out0, src0, out1, src1, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, src0, out1, src1, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(out0, dst, 0, 1);
@@ -1609,8 +1617,8 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
     __m128i tmp0, tmp1, tmp2, tmp3;
 
     src -= (3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
-                  filter0, filter1, filter2, filter3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
     for (;cnt--;) {
         uint32_t loop_cnt = height >> 2;
         uint8_t *dst_reg = dst;
@@ -1626,36 +1634,36 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
         src_tmp += src_stride;
         src6 = __lsx_vld(src_tmp, 0);
         src_tmp += src_stride;
-        LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-        LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
         src6 = __lsx_vxori_b(src6, 128);
-        LSX_DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      reg0, reg1, reg2, reg3);
-        LSX_DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
-        LSX_DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
-                      reg6, reg7, reg8, reg9);
-        LSX_DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg0, reg1, reg2, reg3);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg6, reg7, reg8, reg9);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
 
         for (;loop_cnt--;) {
             LSX_LD_4(src_tmp, src_stride, src7, src8, src9, src10);
             src_tmp += src_stride;
-            LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                          src7, src8, src9, src10);
-            LSX_DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src0, src1, src2, src3);
-            LSX_DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
-                          src4, src5, src7, src8);
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                      src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                      src4, src5, src7, src8);
             tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1, filter2, filter3);
             tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1, filter2, filter3);
             tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1, filter2, filter3);
             tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1, filter2, filter3);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-            LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             tmp2 = __lsx_vld(dst_reg, 0);
             dst_reg += dst_stride;
             tmp3 = __lsx_vld(dst_reg, 0);
             dst_reg += dst_stride;
-            LSX_DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
             __lsx_vst(tmp0, dst_tmp, 0);
             dst_tmp += dst_stride;
             __lsx_vst(tmp1, dst_tmp, 0);
@@ -1664,13 +1672,13 @@ static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
             tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1, filter2, filter3);
             tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1, filter2, filter3);
             tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1, filter2, filter3);
-            LSX_DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
-            LSX_DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
             tmp2 = __lsx_vld(dst_reg, 0);
             dst_reg += dst_stride;
             tmp3 = __lsx_vld(dst_reg, 0);
             dst_reg += dst_stride;
-            LSX_DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
+            DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
             __lsx_vst(tmp0, dst_tmp, 0);
             dst_tmp += dst_stride;
             __lsx_vst(tmp1, dst_tmp, 0);
@@ -1745,9 +1753,9 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 16);
     src -= (3 + 3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-                  filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
+              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
@@ -1758,8 +1766,8 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
     tmp0 = HORIZ_8TAP_FILT(src0, src1, mask0, mask1, mask2, mask3, filt_hz0,
@@ -1770,10 +1778,10 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
                            filt_hz1, filt_hz2, filt_hz3);
     tmp5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                            filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
-                  filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
-    LSX_DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
     tmp2 = __lsx_vpackev_b(tmp5, tmp4);
 
     for (;loop_cnt--;) {
@@ -1787,10 +1795,10 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         src5 = __lsx_vldrepl_w(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_w, src3, src2, src5, src4, src2, src3);
+        DUP2_ARG2(__lsx_vilvl_w, src3, src2, src5, src4, src2, src3);
         src2 = __lsx_vilvl_d(src3, src2);
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
         tmp3 = HORIZ_8TAP_FILT(src7, src8, mask0, mask1, mask2, mask3, filt_hz0,
                                filt_hz1, filt_hz2, filt_hz3);
         tmp4 = __lsx_vshuf_b(tmp3, tmp5, shuff);
@@ -1841,9 +1849,9 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
 
     mask0 = __lsx_vld(mc_filt_mask_arr, 0);
     src -= (3 + 3 * src_stride);
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
-                  filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
-    LSX_DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
+              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
     mask3 = __lsx_vaddi_bu(mask0, 6);
 
     LSX_LD_4(src, src_stride, src0, src1, src2, src3);
@@ -1854,8 +1862,8 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
     src += src_stride;
     src6 = __lsx_vld(src, 0);
     src += src_stride;
-    LSX_DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
-    LSX_DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
     src6 = __lsx_vxori_b(src6, 128);
 
     src0 = HORIZ_8TAP_FILT(src0, src0, mask0, mask1, mask2, mask3, filt_hz0,
@@ -1873,18 +1881,18 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
     src6 = HORIZ_8TAP_FILT(src6, src6, mask0, mask1, mask2, mask3, filt_hz0,
                            filt_hz1, filt_hz2, filt_hz3);
 
-    LSX_DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
-                  filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
-    LSX_DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
-                  src2, src1, tmp0, tmp1, tmp2, tmp4);
-    LSX_DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
+              src2, src1, tmp0, tmp1, tmp2, tmp4);
+    DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
 
     for (;loop_cnt--;) {
         LSX_LD_4(src, src_stride, src7, src8, src9, src10);
         src += src_stride;
 
-        LSX_DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
-                      src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
         src7 = HORIZ_8TAP_FILT(src7, src7, mask0, mask1, mask2, mask3, filt_hz0,
                                filt_hz1, filt_hz2, filt_hz3);
         tmp3 = __lsx_vpackev_b(src7, src6);
@@ -1905,8 +1913,8 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
         src2 = __lsx_vpackev_b(src10, src9);
         src4 = FILT_8TAP_DPADD_S_H(tmp5, tmp6, src0, src2, filt_vt0, filt_vt1,
                                    filt_vt2, filt_vt3);
-        LSX_DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
-        LSX_DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
         src5 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
         src7 = __lsx_vldrepl_d(dst_tmp, 0);
@@ -1915,8 +1923,8 @@ static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
         dst_tmp += dst_stride;
         src9 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, src7, src5, src9, src8, src5, src7);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, out0, src5, out1, src7, out0, out1);
+        DUP2_ARG2(__lsx_vilvl_d, src7, src5, src9, src8, src5, src7);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, src5, out1, src7, out0, out1);
         __lsx_vstelm_d(out0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(out0, dst, 0, 1);
@@ -2014,7 +2022,7 @@ static void avg_width8_lsx(const uint8_t *src, int32_t src_stride,
         src += src_stride;
         tmp3 = __lsx_vldrepl_d(src, 0);
         src += src_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, src0, src1);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, src0, src1);
         tmp0 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
         tmp1 = __lsx_vldrepl_d(dst_tmp, 0);
@@ -2023,8 +2031,8 @@ static void avg_width8_lsx(const uint8_t *src, int32_t src_stride,
         dst_tmp += dst_stride;
         tmp3 = __lsx_vldrepl_d(dst_tmp, 0);
         dst_tmp += dst_stride;
-        LSX_DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, dst0, dst1);
-        LSX_DUP2_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1, dst0, dst1);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, dst0, dst1);
+        DUP2_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1, dst0, dst1);
         __lsx_vstelm_d(dst0, dst, 0, 0);
         dst += dst_stride;
         __lsx_vstelm_d(dst0, dst, 0, 1);
@@ -2050,8 +2058,8 @@ static void avg_width16_lsx(const uint8_t *src, int32_t src_stride,
         src += src_stride;
         LSX_LD_4(dst_tmp, dst_stride, dst0, dst1, dst2, dst3);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
-                      src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
         LSX_ST_4(dst0, dst1, dst2, dst3, dst, dst_stride);
         dst += dst_stride;
     }
@@ -2077,10 +2085,10 @@ static void avg_width32_lsx(const uint8_t *src, int32_t src_stride,
         dst_tmp1 += dst_stride;
         LSX_LD_4_16(dst_tmp2, dst_stride, dst1, dst3, dst5, dst7);
         dst_tmp2 += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
-                      src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
-                      src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
+                  src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
         LSX_ST_4(dst0, dst2, dst4, dst6, dst_tmp, dst_stride);
         dst_tmp += dst_stride;
         LSX_ST_4_16(dst1, dst3, dst5, dst7, dst, dst_stride);
@@ -2100,38 +2108,38 @@ static void avg_width64_lsx(const uint8_t *src, int32_t src_stride,
     __m128i dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
 
     for (;cnt--;) {
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                      src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                      src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                      src8, src9, src10, src11);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src8, src9, src10, src11);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
-                      src12, src13, src14, src15);
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src12, src13, src14, src15);
         src += src_stride;
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
-                      dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst0, dst1, dst2, dst3);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
-                      dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst4, dst5, dst6, dst7);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
-                      dst8, dst9, dst10, dst11);
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst8, dst9, dst10, dst11);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
-                      dst12, dst13, dst14, dst15);
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst12, dst13, dst14, dst15);
         dst_tmp += dst_stride;
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
-                      src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
-                      src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src8, dst8, src9, dst9, src10,
-                      dst10, src11, dst11, dst8, dst9, dst10, dst11);
-        LSX_DUP4_ARG2(__lsx_vavgr_bu, src12, dst12, src13, dst13, src14,
-                      dst14, src15, dst15, dst12, dst13, dst14, dst15);
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
+                  src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vavgr_bu, src8, dst8, src9, dst9, src10,
+                  dst10, src11, dst11, dst8, dst9, dst10, dst11);
+        DUP4_ARG2(__lsx_vavgr_bu, src12, dst12, src13, dst13, src14,
+                  dst14, src15, dst15, dst12, dst13, dst14, dst15);
         __lsx_vst(dst0, dst, 0);
         __lsx_vst(dst1, dst, 16);
         __lsx_vst(dst2, dst, 32);
diff --git a/libavutil/loongarch/generic_macros_lasx.h b/libavutil/loongarch/generic_macros_lasx.h
deleted file mode 100644
index aca12e6959..0000000000
--- a/libavutil/loongarch/generic_macros_lasx.h
+++ /dev/null
@@ -1,3561 +0,0 @@
-/*
- * Copyright (c) 2020 Loongson Technology Corporation Limited
- * All rights reserved.
- * Contributed by Shiyou Yin   <yinshiyou-hf@loongson.cn>
- *                Xiwei Gu     <guxiwei-hf@loongson.cn>
- *                Jin Bo       <jinbo@loongson.cn>
- *                Hao Chen     <chenhao@loongson.cn>
- *                Lu Wang      <wanglu@loongson.cn>
- *                Peng Zhou    <zhoupeng@loongson.cn>
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
- */
-
-#ifndef AVUTIL_LOONGARCH_GENERIC_MACROS_LASX_H
-#define AVUTIL_LOONGARCH_GENERIC_MACROS_LASX_H
-
-/*
- * Copyright (c) 2020 Loongson Technology Corporation Limited
- * All rights reserved.
- * Contributed by Shiyou Yin   <yinshiyou-hf@loongson.cn>
- *                Xiwei Gu     <guxiwei-hf@loongson.cn>
- *                Jin Bo       <jinbo@loongson.cn>
- *                Hao Chen     <chenhao@loongson.cn>
- *                Lu Wang      <wanglu@loongson.cn>
- *                Peng Zhou    <zhoupeng@loongson.cn>
- *
- * This file is maintained in LSOM project, don't change it directly.
- * You can get the latest version of this header from: ***
- *
- */
-
-#ifndef GENERIC_MACROS_LASX_H
-#define GENERIC_MACROS_LASX_H
-
-#include <stdint.h>
-#include <lasxintrin.h>
-
-/**
- * MAJOR version: Macro usage changes.
- * MINOR version: Add new macros, or bug fix.
- * MICRO version: Comment changes or implementation changes。
- */
-#define LSOM_LASX_VERSION_MAJOR 3
-#define LSOM_LASX_VERSION_MINOR 0
-#define LSOM_LASX_VERSION_MICRO 0
-
-/* Description : Load 256-bit vector data with stride
- * Arguments   : Inputs  - psrc    (source pointer to load from)
- *                       - stride
- *               Outputs - out0, out1, ~
- * Details     : Load 256-bit data in 'out0' from (psrc)
- *               Load 256-bit data in 'out1' from (psrc + stride)
- */
-#define LASX_LD(psrc) *((__m256i *)(psrc))
-
-#define LASX_LD_2(psrc, stride, out0, out1)                                 \
-{                                                                           \
-    out0 = LASX_LD(psrc);                                                   \
-    out1 = LASX_LD((psrc) + stride);                                        \
-}
-
-#define LASX_LD_4(psrc, stride, out0, out1, out2, out3)                     \
-{                                                                           \
-    LASX_LD_2((psrc), stride, out0, out1);                                  \
-    LASX_LD_2((psrc) + 2 * stride , stride, out2, out3);                    \
-}
-
-#define LASX_LD_8(psrc, stride, out0, out1, out2, out3, out4, out5,         \
-                  out6, out7)                                               \
-{                                                                           \
-    LASX_LD_4((psrc), stride, out0, out1, out2, out3);                      \
-    LASX_LD_4((psrc) + 4 * stride, stride, out4, out5, out6, out7);         \
-}
-
-/* Description : Store 256-bit vector data with stride
- * Arguments   : Inputs  - in0, in1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store 256-bit data from 'in0' to (pdst)
- *               Store 256-bit data from 'in1' to (pdst + stride)
- */
-#define LASX_ST(in, pdst) *((__m256i *)(pdst)) = (in)
-
-#define LASX_ST_2(in0, in1, pdst, stride)                                   \
-{                                                                           \
-    LASX_ST(in0, (pdst));                                                   \
-    LASX_ST(in1, (pdst) + stride);                                          \
-}
-
-#define LASX_ST_4(in0, in1, in2, in3, pdst, stride)                         \
-{                                                                           \
-    LASX_ST_2(in0, in1, (pdst), stride);                                    \
-    LASX_ST_2(in2, in3, (pdst) + 2 * stride, stride);                       \
-}
-
-#define LASX_ST_8(in0, in1, in2, in3, in4, in5, in6, in7, pdst, stride)     \
-{                                                                           \
-    LASX_ST_4(in0, in1, in2, in3, (pdst), stride);                          \
-    LASX_ST_4(in4, in5, in6, in7, (pdst) + 4 * stride, stride);             \
-}
-
-/* Description : Store half word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1,  ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store half word 'idx0' from 'in' to (pdst)
- *               Store half word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : LASX_ST_H(in, idx, pdst)
- *          in : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
- *        idx0 : 0x01
- *        out0 : 2
- */
-#define LASX_ST_H(in, idx, pdst)                                          \
-{                                                                         \
-    __lasx_xvstelm_h(in, pdst, 0, idx);                                   \
-}
-
-#define LASX_ST_H_2(in, idx0, idx1, pdst, stride)                         \
-{                                                                         \
-    LASX_ST_H(in, idx0, (pdst));                                          \
-    LASX_ST_H(in, idx1, (pdst) + stride);                                 \
-}
-
-#define LASX_ST_H_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
-{                                                                         \
-    LASX_ST_H_2(in, idx0, idx1, (pdst), stride);                          \
-    LASX_ST_H_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
-}
-
-
-/* Description : Store word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1,  ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store word 'idx0' from 'in' to (pdst)
- *               Store word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : LASX_ST_W(in, idx, pdst)
- *          in : 1, 2, 3, 4, 5, 6, 7, 8
- *        idx0 : 0x01
- *        out0 : 2
- */
-#define LASX_ST_W(in, idx, pdst)                                          \
-{                                                                         \
-    __lasx_xvstelm_w(in, pdst, 0, idx);                                   \
-}
-
-#define LASX_ST_W_2(in, idx0, idx1, pdst, stride)                         \
-{                                                                         \
-    LASX_ST_W(in, idx0, (pdst));                                          \
-    LASX_ST_W(in, idx1, (pdst) + stride);                                 \
-}
-
-#define LASX_ST_W_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
-{                                                                         \
-    LASX_ST_W_2(in, idx0, idx1, (pdst), stride);                          \
-    LASX_ST_W_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
-}
-
-#define LASX_ST_W_8(in, idx0, idx1, idx2, idx3, idx4, idx5, idx6, idx7,   \
-                    pdst, stride)                                         \
-{                                                                         \
-    LASX_ST_W_4(in, idx0, idx1, idx2, idx3, (pdst), stride);              \
-    LASX_ST_W_4(in, idx4, idx5, idx6, idx7, (pdst) + 4 * stride, stride); \
-}
-
-/* Description : Store double word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store double word 'idx0' from 'in' to (pdst)
- *               Store double word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : See LASX_ST_W(in, idx, pdst)
- */
-#define LASX_ST_D(in, idx, pdst)                                         \
-{                                                                        \
-    __lasx_xvstelm_d(in, pdst, 0, idx);                                  \
-}
-
-#define LASX_ST_D_2(in, idx0, idx1, pdst, stride)                        \
-{                                                                        \
-    LASX_ST_D(in, idx0, (pdst));                                         \
-    LASX_ST_D(in, idx1, (pdst) + stride);                                \
-}
-
-#define LASX_ST_D_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
-{                                                                        \
-    LASX_ST_D_2(in, idx0, idx1, (pdst), stride);                         \
-    LASX_ST_D_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
-}
-
-/* Description : Store quad word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store quad word 'idx0' from 'in' to (pdst)
- *               Store quad word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : See LASX_ST_W(in, idx, pdst)
- */
-#define LASX_ST_Q(in, idx, pdst)                                         \
-{                                                                        \
-    LASX_ST_D(in, (idx << 1), pdst);                                     \
-    LASX_ST_D(in, (( idx << 1) + 1), (char*)(pdst) + 8);                 \
-}
-
-#define LASX_ST_Q_2(in, idx0, idx1, pdst, stride)                        \
-{                                                                        \
-    LASX_ST_Q(in, idx0, (pdst));                                         \
-    LASX_ST_Q(in, idx1, (pdst) + stride);                                \
-}
-
-#define LASX_ST_Q_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
-{                                                                        \
-    LASX_ST_Q_2(in, idx0, idx1, (pdst), stride);                         \
-    LASX_ST_Q_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
-}
-
-/* Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - unsigned halfword
- * Details     : Unsigned byte elements from in0 are iniplied with
- *               unsigned byte elements from in0 producing a result
- *               twice the size of input i.e. unsigned halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector
- *               (2 unsigned halfword results)
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_H_BU(in0, in1, out0)                   \
-{                                                       \
-    __m256i _tmp0_m ;                                   \
-                                                        \
-    _tmp0_m = __lasx_xvmulwev_h_bu( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_h_bu( _tmp0_m, in0, in1 );  \
-}
-#define LASX_DP2_H_BU_2(in0, in1, in2, in3, out0, out1) \
-{                                                       \
-    LASX_DP2_H_BU(in0, in1, out0);                      \
-    LASX_DP2_H_BU(in2, in3, out1);                      \
-}
-#define LASX_DP2_H_BU_4(in0, in1, in2, in3,             \
-                        in4, in5, in6, in7,             \
-                        out0, out1, out2, out3)         \
-{                                                       \
-    LASX_DP2_H_BU_2(in0, in1, in0, in1, out0, out1);    \
-    LASX_DP2_H_BU_2(in4, in5, in6, in7, out2, out3);    \
-}
-
-/* Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - signed halfword
- * Details     : Signed byte elements from in0 are iniplied with
- *               signed byte elements from in0 producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector
- *               (2 signed halfword results)
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_H_B(in0, in1, out0)                      \
-{                                                         \
-    __m256i _tmp0_m ;                                     \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_h_b( in0, in1 );            \
-    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in0, in1 );     \
-}
-#define LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1)    \
-{                                                         \
-    LASX_DP2_H_B(in0, in1, out0);                         \
-    LASX_DP2_H_B(in2, in3, out1);                         \
-}
-#define LASX_DP2_H_B_4(in0, in1, in2, in3,                \
-                       in4, in5, in6, in7,                \
-                       out0, out1, out2, out3)            \
-{                                                         \
-    LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1);       \
-    LASX_DP2_H_B_2(in4, in5, in6, in7, out2, out3);       \
-}
-
-/* Description : Dot product of half word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Return Type - signed word
- * Details     : Signed half word elements from in* are iniplied with
- *               signed half word elements from in* producing a result
- *               twice the size of input i.e. signed word.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector.
- * Example     : LASX_DP2_W_H(in0, in1, out0)
- *               in0:   1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *               in0:   8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *               out0:  22,38,38,22, 22,38,38,22
- */
-#define LASX_DP2_W_H(in0, in1, out0)                   \
-{                                                      \
-    __m256i _tmp0_m ;                                  \
-                                                       \
-    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );  \
-}
-#define LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1)             \
-{                                                                  \
-    LASX_DP2_W_H(in0, in1, out0);                                  \
-    LASX_DP2_W_H(in2, in3, out1);                                  \
-}
-#define LASX_DP2_W_H_4(in0, in1, in2, in3,                         \
-                       in4, in5, in6, in7, out0, out1, out2, out3) \
-{                                                                  \
-    LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1);                \
-    LASX_DP2_W_H_2(in4, in5, in6, in7, out2, out3);                \
-}
-#define LASX_DP2_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                       in8, in9, in10, in11, in12, in13, in14, in15,   \
-                       out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                      \
-    LASX_DP2_W_H_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
-                   out0, out1, out2, out3);                            \
-    LASX_DP2_W_H_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
-                   out4, out5, out6, out7);                            \
-}
-
-/* Description : Dot product of word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Retrun Type - signed double
- * Details     : Signed word elements from in* are iniplied with
- *               signed word elements from in* producing a result
- *               twice the size of input i.e. signed double word.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector.
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_D_W(in0, in1, out0)                    \
-{                                                       \
-    __m256i _tmp0_m ;                                   \
-                                                        \
-    _tmp0_m = __lasx_xvmulwev_d_w( in0, in1 );          \
-    out0 = __lasx_xvmaddwod_d_w( _tmp0_m, in0, in1 );   \
-}
-#define LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1)  \
-{                                                       \
-    LASX_DP2_D_W(in0, in1, out0);                       \
-    LASX_DP2_D_W(in2, in3, out1);                       \
-}
-#define LASX_DP2_D_W_4(in0, in1, in2, in3,                             \
-                       in4, in5, in6, in7, out0, out1, out2, out3)     \
-{                                                                      \
-    LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1);                    \
-    LASX_DP2_D_W_2(in4, in5, in6, in7, out2, out3);                    \
-}
-#define LASX_DP2_D_W_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                       in8, in9, in10, in11, in12, in13, in14, in15,   \
-                       out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                      \
-    LASX_DP2_D_W_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
-                   out0, out1, out2, out3);                            \
-    LASX_DP2_D_W_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
-                   out4, out5, out6, out7);                            \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2_W_HU_H(in0, in1, out0)                   \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_w_hu_h( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in0, in1 );  \
-}
-
-#define LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1) \
-{                                                         \
-    LASX_DP2_W_HU_H(in0, in1, out0);                      \
-    LASX_DP2_W_HU_H(in2, in3, out1);                      \
-}
-
-#define LASX_DP2_W_HU_H_4(in0, in1, in2, in3,             \
-                          in4, in5, in6, in7,             \
-                          out0, out1, out2, out3)         \
-{                                                         \
-    LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1);    \
-    LASX_DP2_W_HU_H_2(in4, in5, in6, in7, out2, out3);    \
-}
-
-/* Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in0 are iniplied with
- *               signed byte elements from in0 producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added to the out vector
- *               (2 signed halfword results)
- * Example     : LASX_DP2ADD_H_B(in0, in1, in2, out0)
- *               in0:  1,2,3,4, 1,2,3,4, 1,2,3,4, 1,2,3,4,
- *               in1:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *                     1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *               in2:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *                     8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *               out0: 23,40,41,26, 23,40,41,26, 23,40,41,26, 23,40,41,26
- */
-#define LASX_DP2ADD_H_B(in0, in1, in2, out0)                 \
-{                                                            \
-    __m256i _tmp0_m;                                         \
-                                                             \
-    _tmp0_m = __lasx_xvmaddwev_h_b( in0, in1, in2 );         \
-    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in1, in2 );        \
-}
-#define LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1)  \
-{                                                                    \
-    LASX_DP2ADD_H_B(in0, in1, in2, out0);                            \
-    LASX_DP2ADD_H_B(in3, in4, in5, out1);                            \
-}
-#define LASX_DP2ADD_H_B_4(in0, in1, in2, in3, in4, in5,                \
-                          in6, in7, in8, in9, in10, in11,              \
-                          out0, out1, out2, out3)                      \
-{                                                                      \
-    LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_DP2ADD_H_B_2(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Signed halfword elements from 'in0' are iniplied with
- *               signed halfword elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_H(in0, in1, in2, out0)                 \
-{                                                            \
-    __m256i _tmp0_m;                                         \
-                                                             \
-    _tmp0_m = __lasx_xvmaddwev_w_h( in0, in1, in2 );         \
-    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 );        \
-}
-#define LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1 ) \
-{                                                                    \
-    LASX_DP2ADD_W_H(in0, in1, in2, out0);                            \
-    LASX_DP2ADD_W_H(in3, in4, in5, out1);                            \
-}
-#define LASX_DP2ADD_W_H_4(in0, in1, in2, in3, in4, in5,              \
-                          in6, in7, in8, in9, in10, in11,            \
-                          out0, out1, out2, out3)                    \
-{                                                                    \
-    LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);     \
-    LASX_DP2ADD_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);   \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               unsigned halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_HU(in0, in1, in2, out0)          \
-{                                                      \
-    __m256i _tmp0_m;                                   \
-                                                       \
-    _tmp0_m = __lasx_xvmaddwev_w_hu( in0, in1, in2 );  \
-    out0 = __lasx_xvmaddwod_w_hu( _tmp0_m, in1, in2 ); \
-}
-#define LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                    \
-    LASX_DP2ADD_W_HU(in0, in1, in2, out0);                           \
-    LASX_DP2ADD_W_HU(in3, in4, in5, out1);                           \
-}
-#define LASX_DP2ADD_W_HU_4(in0, in1, in2, in3, in4, in5,             \
-                           in6, in7, in8, in9, in10, in11,           \
-                           out0, out1, out2, out3)                   \
-{                                                                    \
-    LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2ADD_W_HU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_HU_H(in0, in1, in2, out0)           \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmaddwev_w_hu_h( in0, in1, in2 );   \
-    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in1, in2 );  \
-}
-
-#define LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                      \
-    LASX_DP2ADD_W_HU_H(in0, in1, in2, out0);                           \
-    LASX_DP2ADD_W_HU_H(in3, in4, in5, out1);                           \
-}
-
-#define LASX_DP2ADD_W_HU_H_4(in0, in1, in2, in3, in4, in5,             \
-                             in6, in7, in8, in9, in10, in11,           \
-                             out0, out1, out2, out3)                   \
-{                                                                      \
-    LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2ADD_W_HU_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Vector Unsigned Dot Product and Subtract.
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned byte elements from 'in0' are iniplied with
- *               unsigned byte elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtract from double width elements,
- *               then written to the 'out0' vector.
- */
-#define LASX_DP2SUB_H_BU(in0, in1, in2, out0)             \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_h_bu( in1, in2 );           \
-    _tmp0_m = __lasx_xvmaddwod_h_bu( _tmp0_m, in1, in2 ); \
-    out0 = __lasx_xvsub_h( in0, _tmp0_m );                \
-}
-
-#define LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                    \
-    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
-    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
-}
-
-#define LASX_DP2SUB_H_BU_4(in0, in1, in2, in3, in4, in5,             \
-                           in6, in7, in8, in9, in10, in11,           \
-                           out0, out1, out2, out3)                   \
-{                                                                    \
-    LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2SUB_H_BU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Vector Signed Dot Product and Subtract.
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Signed halfword elements from 'in0' are iniplied with
- *               signed halfword elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtract from double width elements,
- *               then written to the 'out0' vector.
- */
-#define LASX_DP2SUB_W_H(in0, in1, in2, out0)             \
-{                                                        \
-    __m256i _tmp0_m;                                     \
-                                                         \
-    _tmp0_m = __lasx_xvmulwev_w_h( in1, in2 );           \
-    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 ); \
-    out0 = __lasx_xvsub_w( in0, _tmp0_m );               \
-}
-
-#define LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                   \
-    LASX_DP2SUB_W_H(in0, in1, in2, out0);                           \
-    LASX_DP2SUB_W_H(in3, in4, in5, out1);                           \
-}
-
-#define LASX_DP2SUB_W_H_4(in0, in1, in2, in3, in4, in5,             \
-                          in6, in7, in8, in9, in10, in11,           \
-                          out0, out1, out2, out3)                   \
-{                                                                   \
-    LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2SUB_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Dot product of half word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Return Type - signed word
- * Details     : Signed half word elements from in* are iniplied with
- *               signed half word elements from in* producing a result
- *               twice the size of input i.e. signed word.
- *               Then this iniplication results of four adjacent elements
- *               are added together and stored to the out vector.
- * Example     : LASX_DP2_W_H(in0, in0, out0)
- *               in0:   3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1,
- *               in0:   -2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1,
- *               out0:  -2,0,1,1,
- */
-#define LASX_DP4_D_H(in0, in1, out0)                         \
-{                                                            \
-    __m256i _tmp0_m ;                                        \
-                                                             \
-    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );               \
-    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );     \
-    out0  = __lasx_xvhaddw_d_w( _tmp0_m, _tmp0_m );          \
-}
-#define LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1)       \
-{                                                            \
-    LASX_DP4_D_H(in0, in1, out0);                            \
-    LASX_DP4_D_H(in2, in3, out1);                            \
-}
-#define LASX_DP4_D_H_4(in0, in1, in2, in3,                            \
-                       in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                     \
-    LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1);                   \
-    LASX_DP4_D_H_2(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The high half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               higher half of the two-fold sign extension ( signed byte
- *               to signed half word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWH_H_B_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_b( in0, in0 );                                    \
-    _tmp1_m = __lasx_xvilvh_b( in1, in1 );                                    \
-    out0 = __lasx_xvaddwev_h_b( _tmp0_m, _tmp1_m );                           \
-}
-#define LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWH_H_B_128SV(in0, in1, out0);                                     \
-    LASX_ADDWH_H_B_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWH_H_B_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWH_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The high half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               higher half of the two-fold sign extension ( signed half word
- *               to signed word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWH_W_H_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                                    \
-    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                                    \
-    out0 = __lasx_xvaddwev_w_h( _tmp0_m, _tmp1_m );                           \
-}
-#define LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWH_W_H_128SV(in0, in1, out0);                                     \
-    LASX_ADDWH_W_H_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWH_W_H_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold sign extension ( signed byte
- *               to signed half word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWL_H_B_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvsllwil_h_b( in0, 0 );                                  \
-    _tmp1_m = __lasx_xvsllwil_h_b( in1, 0 );                                  \
-    out0 = __lasx_xvadd_h( _tmp0_m, _tmp1_m );                                \
-}
-#define LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWL_H_B_128SV(in0, in1, out0);                                     \
-    LASX_ADDWL_H_B_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWL_H_B_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWL_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold sign extension ( signed half word
- *               to signed word ) and stored to the out vector.
- * Example     : LASX_ADDWL_W_H_128SV(in0, in1, out0)
- *               in0   3,0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0, 1,0,1,0, 1,0,0,1,
- *               out0  5,-1,4,2, 1,0,2,-1,
- */
-#define LASX_ADDWL_W_H_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m;                                                          \
-                                                                              \
-    _tmp0_m = __lasx_xvilvl_h(in1, in0);                                      \
-    out0 = __lasx_xvhaddw_w_h( _tmp0_m, _tmp0_m );                            \
-}
-#define LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWL_W_H_128SV(in0, in1, out0);                                     \
-    LASX_ADDWL_W_H_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWL_W_H_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold zero extension ( unsigned byte
- *               to unsigned half word ) and stored to the out vector.
- */
-#define LASX_ADDWL_H_BU_128SV(in0, in1, out0)                                 \
-{                                                                             \
-    __m256i _tmp0_m;                                                          \
-                                                                              \
-    _tmp0_m = __lasx_xvilvl_b(in1, in0);                                      \
-    out0 = __lasx_xvhaddw_hu_bu( _tmp0_m, _tmp0_m );                          \
-}
-#define LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)               \
-{                                                                             \
-    LASX_ADDWL_H_BU_128SV(in0, in1, out0);                                    \
-    LASX_ADDWL_H_BU_128SV(in2, in3, out1);                                    \
-}
-#define LASX_ADDWL_H_BU_4_128SV(in0, in1, in2, in3,                           \
-                                in4, in5, in6, in7, out0, out1, out2, out3)   \
-{                                                                             \
-    LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                  \
-    LASX_ADDWL_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                  \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : In1 vector plus in0 vector after double zero extension
- *               ( unsigned byte to half word ),add and stored to the out vector.
- * Example     : reference to LASX_ADDW_W_W_H_128SV(in0, in1, out0)
- */
-#define LASX_ADDW_H_H_BU_128SV(in0, in1, out0)                                \
-{                                                                             \
-    __m256i _tmp1_m;                                                          \
-                                                                              \
-    _tmp1_m = __lasx_xvsllwil_hu_bu( in1, 0 );                                \
-    out0 = __lasx_xvadd_h( in0, _tmp1_m );                                    \
-}
-#define LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)              \
-{                                                                             \
-    LASX_ADDW_H_H_BU_128SV(in0, in1, out0);                                   \
-    LASX_ADDW_H_H_BU_128SV(in2, in3, out1);                                   \
-}
-#define LASX_ADDW_H_H_BU_4_128SV(in0, in1, in2, in3,                          \
-                                 in4, in5, in6, in7, out0, out1, out2, out3)  \
-{                                                                             \
-    LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                 \
-    LASX_ADDW_H_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                 \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : In1 vector plus in0 vector after double sign extension
- *               ( signed half word to word ),add and stored to the out vector.
- * Example     : LASX_ADDW_W_W_H_128SV(in0, in1, out0)
- *               in0   0,1,0,0, -1,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1,
- *               out0  2,0,1,2, -1,0,1,1,
- */
-#define LASX_ADDW_W_W_H_128SV(in0, in1, out0)                                 \
-{                                                                             \
-    __m256i _tmp1_m;                                                          \
-                                                                              \
-    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
-    out0 = __lasx_xvadd_w( in0, _tmp1_m );                                    \
-}
-#define LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1)               \
-{                                                                             \
-    LASX_ADDW_W_W_H_128SV(in0, in1, out0);                                    \
-    LASX_ADDW_W_W_H_128SV(in2, in3, out1);                                    \
-}
-#define LASX_ADDW_W_W_H_4_128SV(in0, in1, in2, in3,                           \
-                                in4, in5, in6, in7, out0, out1, out2, out3)   \
-{                                                                             \
-    LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                  \
-    LASX_ADDW_W_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                  \
-}
-
-/* Description : Multiplication and addition calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , and the result is added to
- *               the vector in0, the stored to the out vector.
- * Example     : LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)
- *               in0   1,2,3,4, 5,6,7 8
- *               in1   1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
- *               in2   200,300,400,500, 2000,3000,4000,5000,
- *                     -200,-300,-400,-500, -2000,-3000,-4000,-5000
- *               out0  5,-1,4,2, 1,0,2,-1,
- */
-#define LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)                            \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
-    _tmp1_m = __lasx_xvsllwil_w_h( in2, 0 );                                  \
-    _tmp0_m = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                             \
-    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
-}
-#define LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
-{                                                                             \
-    LASX_MADDWL_W_H_128SV(in0, in1, in2, out0);                               \
-    LASX_MADDWL_W_H_128SV(in3, in4, in5, out1);                               \
-}
-#define LASX_MADDWL_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
-                                in6, in7, in8, in9, in10, in11,              \
-                                out0, out1, out2, out3)                      \
-{                                                                            \
-    LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_MADDWL_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Multiplication and addition calculation after expansion
- *               of the higher half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the higher half of the two-fold sign extension ( signed
- *               half word to signed word ) , and the result is added to
- *               the vector in0, the stored to the out vector.
- * Example     : see LASX_MADDWL_W_H_128SV
- */
-#define LASX_MADDWH_W_H_128SV(in0, in1, in2, out0)                            \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_h( in1, in1 );                                    \
-    _tmp1_m = __lasx_xvilvh_h( in2, in2 );                                    \
-    _tmp0_m = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );                        \
-    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
-}
-#define LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
-{                                                                             \
-    LASX_MADDWH_W_H_128SV(in0, in1, in2, out0);                               \
-    LASX_MADDWH_W_H_128SV(in3, in4, in5, out1);                               \
-}
-#define LASX_MADDWH_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
-                                in6, in7, in8, in9, in10, in11,              \
-                                out0, out1, out2, out3)                      \
-{                                                                            \
-    LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_MADDWH_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Multiplication calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , the stored to the out vector.
- * Example     : LASX_MULWL_W_H_128SV(in0, in1, out0)
- *               in0   3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0,  0,0,1,0, 1,0,0,1,
- *               out0  6,1,3,0, 0,0,1,0,
- */
-#define LASX_MULWL_W_H_128SV(in0, in1, out0)                    \
-{                                                               \
-    __m256i _tmp0_m, _tmp1_m;                                   \
-                                                                \
-    _tmp0_m = __lasx_xvsllwil_w_h( in0, 0 );                    \
-    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                    \
-    out0 = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                  \
-}
-#define LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                               \
-    LASX_MULWL_W_H_128SV(in0, in1, out0);                       \
-    LASX_MULWL_W_H_128SV(in2, in3, out1);                       \
-}
-#define LASX_MULWL_W_H_4_128SV(in0, in1, in2, in3,              \
-                               in4, in5, in6, in7,              \
-                               out0, out1, out2, out3)          \
-{                                                               \
-    LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_MULWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : Multiplication calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , the stored to the out vector.
- * Example     : see LASX_MULWL_W_H_128SV
- */
-#define LASX_MULWH_W_H_128SV(in0, in1, out0)                    \
-{                                                               \
-    __m256i _tmp0_m, _tmp1_m;                                   \
-                                                                \
-    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                      \
-    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                      \
-    out0 = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );             \
-}
-#define LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                               \
-    LASX_MULWH_W_H_128SV(in0, in1, out0);                       \
-    LASX_MULWH_W_H_128SV(in2, in3, out1);                       \
-}
-#define LASX_MULWH_W_H_4_128SV(in0, in1, in2, in3,              \
-                               in4, in5, in6, in7,              \
-                               out0, out1, out2, out3)          \
-{                                                               \
-    LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_MULWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector add the in0 vector after the
- *               lower half of the two-fold zero extension ( unsigned byte
- *               to unsigned half word ) and stored to the out vector.
- */
-#define LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0)                    \
-{                                                                    \
-    __m256i _tmp1_m;                                                 \
-    __m256i _zero_m = { 0 };                                         \
-                                                                     \
-    _tmp1_m = __lasx_xvilvl_b( _zero_m, in1 );                       \
-    out0 = __lasx_xvsadd_hu( in0, _tmp1_m );                         \
-}
-#define LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                                    \
-    LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0);                       \
-    LASX_SADDW_HU_HU_BU_128SV(in2, in3, out1);                       \
-}
-#define LASX_SADDW_HU_HU_BU_4_128SV(in0, in1, in2, in3,              \
-                                    in4, in5, in6, in7,              \
-                                    out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_SADDW_HU_HU_BU_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : Low 8-bit vector elements unsigned extension to halfword
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 unsigned extension to halfword,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_HU_BU(in0, out0)                                          \
-{                                                                              \
-    out0 = __lasx_vext2xv_hu_bu(in0);                                          \
-}
-
-#define LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1)                             \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU(in0, out0);                                             \
-    LASX_UNPCK_L_HU_BU(in1, out1);                                             \
-}
-
-#define LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1);                                \
-    LASX_UNPCK_L_HU_BU_2(in2, in3, out2, out3);                                \
-}
-
-#define LASX_UNPCK_L_HU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,           \
-                             out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);          \
-    LASX_UNPCK_L_HU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);          \
-}
-
-/* Description : Low 8-bit vector elements unsigned extension to word
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 unsigned extension to word,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_WU_BU(in0, out0)                                         \
-{                                                                             \
-    out0 = __lasx_vext2xv_wu_bu(in0);                                         \
-}
-
-#define LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1)                            \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU(in0, out0);                                            \
-    LASX_UNPCK_L_WU_BU(in1, out1);                                            \
-}
-
-#define LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1);                               \
-    LASX_UNPCK_L_WU_BU_2(in2, in3, out2, out3);                               \
-}
-
-#define LASX_UNPCK_L_WU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
-                             out0, out1, out2, out3, out4, out5, out6, out7)  \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
-    LASX_UNPCK_L_WU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
-}
-
-/* Description : Low 8-bit vector elements signed extension to halfword
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 signed extension to halfword,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_H_B(in0, out0)                                          \
-{                                                                            \
-    out0 = __lasx_vext2xv_h_b(in0);                                          \
-}
-
-#define LASX_UNPCK_L_H_B_2(in0, in1, out0, out1)                             \
-{                                                                            \
-    LASX_UNPCK_L_H_B(in0, out0);                                             \
-    LASX_UNPCK_L_H_B(in1, out1);                                             \
-}
-
-#define LASX_UNPCK_L_H_B_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
-{                                                                            \
-    LASX_UNPCK_L_H_B_2(in0, in1, out0, out1);                                \
-    LASX_UNPCK_L_H_B_2(in2, in3, out2, out3);                                \
-}
-
-/* Description : Low halfword vector elements signed extension to word
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low halfword elements from in0 signed extension to
- *               word, written to output vector out0. Similar for in1.
- *               Similar for other pairs.
- * Example     : LASX_UNPCK_L_W_H(in0, out0)
- *         in0 : 3, 0, 3, 0,  0, 0, 0, -1,  0, 0, 1, 1,  0, 0, 0, 1
- *        out0 : 3, 0, 3, 0,  0, 0, 0, -1
- */
-#define LASX_UNPCK_L_W_H(in0, out0)                                         \
-{                                                                           \
-    out0 = __lasx_vext2xv_w_h(in0);                                         \
-}
-
-#define LASX_UNPCK_L_W_H_2(in0, in1, out0, out1)                            \
-{                                                                           \
-    LASX_UNPCK_L_W_H(in0, out0);                                            \
-    LASX_UNPCK_L_W_H(in1, out1);                                            \
-}
-
-#define LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
-{                                                                           \
-    LASX_UNPCK_L_W_H_2(in0, in1, out0, out1);                               \
-    LASX_UNPCK_L_W_H_2(in2, in3, out2, out3);                               \
-}
-
-#define LASX_UNPCK_L_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
-                           out0, out1, out2, out3, out4, out5, out6, out7)  \
-{                                                                           \
-    LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
-    LASX_UNPCK_L_W_H_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
-}
-
-/* Description : Interleave odd byte elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd byte elements of in_h and odd byte
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_B(in_h, in_l, out)                                            \
-{                                                                                \
-    out = __lasx_xvpackod_b((in_h, in1_l);                                       \
-}
-
-#define LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                   \
-{                                                                                \
-    LASX_ILVOD_B(in0_h, in0_l, out0);                                            \
-    LASX_ILVOD_B(in1_h, in1_l, out1);                                            \
-}
-
-#define LASX_ILVOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,   \
-                       out0, out1, out2, out3)                                   \
-{                                                                                \
-    LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                      \
-    LASX_ILVOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                      \
-}
-
-/* Description : Interleave odd half word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd half word elements of in_h and odd half word
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_H(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_h(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_H(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_H(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave odd word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd word elements of in_h and odd word
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- *        in_h : 1, 2, 3, 4,   5, 6, 7, 8
- *        in_l : 1, 0, 3, 1,   1, 2, 3, 4
- *         out : 0, 2, 1, 4,   2, 6, 4, 8
- */
-#define LASX_ILVOD_W(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_w(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_W(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_W(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave odd double word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd double word elements of in_h and odd double word
- *               elements of in_l are interleaved and copied to out.
- * Example     : LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_D(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_d(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_D(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_D(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave right half of byte elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of byte elements of in_l and high half of byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_B(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_b(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_b(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_B(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_B(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of byte elements of in_l and low half of byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_B_128SV(in_h, in_l, out0)                                   \
-{                                                                             \
-    out0 = __lasx_xvilvl_b(in_h, in_l);                                       \
-}
-
-#define LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_B_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_B_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of half word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of half word elements of in_l and right half of
- *               half word elements of in_h are interleaved and copied to
- *               out0. Similar for other pairs.
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_H(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_h(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_h(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_H(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_H(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of half word elements of in_l and low half of half
- *               word elements of in_h are interleaved and copied to
- *               out0. Similar for other pairs.
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_H_128SV(in_h, in_l, out0)                                   \
-{                                                                             \
-    out0 = __lasx_xvilvl_h(in_h, in_l);                                       \
-}
-
-#define LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_H_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_H_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of word elements from vectors
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of halfword elements of in_l and low half of word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : LASX_ILVL_W(in_h, in_l, out0)
- *        in_h : 0, 1, 0, 1,  0, 1, 0, 1
- *        in_l : 1, 2, 3, 4,  5, 6, 7, 8
- *        out0 : 1, 0, 2, 1,  3, 0, 4, 1
- */
-#define LASX_ILVL_W(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_w(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_w(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_W(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_W(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of halfword elements of in_l and low half of word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : LASX_ILVL_W_128SV(in_h, in_l, out0)
- *        in_h : 0, 1, 0, 1, 0, 1, 0, 1
- *        in_l : 1, 2, 3, 4, 5, 6, 7, 8
- *        out0 : 1, 0, 2, 1, 5, 0, 6, 1
- */
-#define LASX_ILVL_W_128SV(in_h, in_l, out0)                             \
-{                                                                       \
-    out0 = __lasx_xvilvl_w(in_h, in_l);                                 \
-}
-
-#define LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_W_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_W_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of double word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of double word elements of in_l and low half of
- *               double word elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_D(in_h, in_l, out0)                                   \
-{                                                                       \
-    __m256i tmp0, tmp1;                                                 \
-    tmp0 = __lasx_xvilvl_d(in_h, in_l);                                 \
-    tmp1 = __lasx_xvilvh_d(in_h, in_l);                                 \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-}
-
-#define LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_D(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_D(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave right half of double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Right half of double word elements of in_l and right half of
- *               double word elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_D_128SV(in_h, in_l, out0)                              \
-{                                                                        \
-    out0 = __lasx_xvilvl_d(in_h, in_l);                                  \
-}
-
-#define LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_D_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_D_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of byte elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of byte elements of in_l and high half of
- *               byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_B(in_h, in_l, out0)                            \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_b(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_b(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_B(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_B(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of  byte elements  of  in_l and high half
- *               of byte elements of in_h are interleaved and copied
- *               to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_B_128SV(in_h, in_l, out0)                     \
-{                                                               \
-    out0 = __lasx_xvilvh_b(in_h, in_l);                         \
-}
-
-#define LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_B_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_B_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of half word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of half word elements of in_l and high half of
- *               half word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_H(in_h, in_l, out0)                           \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_h(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_h(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_H(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_H(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of  half word elements  of  in_l and high half
- *               of half word elements of in_h are interleaved and copied
- *               to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_H_128SV(in_h, in_l, out0)                     \
-{                                                               \
-    out0 = __lasx_xvilvh_h(in_h, in_l);                         \
-}
-
-#define LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_H_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_H_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of word elements from vectors
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of word elements of in_l and high half of
- *               word elements of in_h are interleaved and copied to
- *               out0.
- *               Similar for other pairs.
- * Example     : LASX_ILVH_W(in_h, in_l, out0)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *         out0: 5, -5,  6, -6,  7, -7,  8, -8
- */
-#define LASX_ILVH_W(in_h, in_l, out0)                            \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_w(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_w(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_W(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_W(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of word elements of every 128-bit of in_l
- *               and high half of word elements of every 128-bit of
- *               in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : LASX_ILVH_W_128SV(in_h, in_l, out0)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *         out0: 3, -3,  4, -4,  7, -7,  8, -8*
- */
-#define LASX_ILVH_W_128SV(in_h, in_l, out0)                        \
-{                                                                  \
-    out0 = __lasx_xvilvh_w(in_h, in_l);                            \
-}
-
-#define LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_W_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_W_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of double word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out0, out1, ~
- :* Details    : High half of double word elements of in_l and high half of
- *               double word elements of in_h are interleaved and copied to
- *               out0.
- *               Similar for other pairs.
- * Example    : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_D(in_h, in_l, out0)                           \
-{                                                               \
-    __m256i tmp0, tmp1;                                         \
-    tmp0 = __lasx_xvilvl_d(in_h, in_l);                         \
-    tmp1 = __lasx_xvilvh_d(in_h, in_l);                         \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                  \
-}
-
-#define LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_D(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_D(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : High half of double word elements of every 128-bit in_l and
- *               high half of double word elements of every 128-bit in_h are
- *               interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_D_128SV(in_h, in_l, out0)                             \
-{                                                                       \
-    out0 = __lasx_xvilvh_d(in_h, in_l);                                 \
-}
-
-#define LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_D_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_D_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave byte elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  byte elements  of in_l and low half of byte
- *               elements  of in_h  are interleaved  and copied  to  out_l.
- *               High half of byte elements of in_l and high half of byte
- *               elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_l, out_h)
- */
-#define LASX_ILVLH_B(in_h, in_l, out_h, out_l)                          \
-{                                                                       \
-    __m256i tmp0, tmp1;                                                 \
-    tmp0  = __lasx_xvilvl_b(in_h, in_l);                                \
-    tmp1  = __lasx_xvilvh_b(in_h, in_l);                                \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                         \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                         \
-}
-
-#define LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l,                      \
-                       out0_h, out0_l, out1_h, out1_l)                  \
-{                                                                       \
-    LASX_ILVLH_B(in0_h, in0_l, out0_h, out0_l);                         \
-    LASX_ILVLH_B(in1_h, in1_l, out1_h, out1_l);                         \
-}
-
-#define LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_B_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of byte elements of in_l and low half of byte elements
- *               of in_h are interleaved and copied to out_l. High  half  of byte
- *               elements  of in_h  and high half  of byte elements  of in_l  are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_B_128SV(in_h, in_l, out_h, out_l)                           \
-{                                                                              \
-    LASX_ILVL_B_128SV(in_h, in_l, out_l);                                      \
-    LASX_ILVH_B_128SV(in_h, in_l, out_h);                                      \
-}
-
-#define LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_B_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_B_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave half word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  half word elements  of in_l and low half of half
- *               word elements of in_h  are  interleaved  and  copied  to out_l.
- *               High half of half word elements of in_l and high half of half
- *               word elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_H(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_h(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_h(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_H(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_H(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_H_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0_h, out0_l, ~
- * Details     : Low half of half word elements  of every 128-bit of in_l and
- *               low half of half word elements  of every 128-bit of in_h are
- *               interleaved and copied to out_l.
- *               High half of half word elements of every 128-bit of in_l and
- *               high half of half word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_H_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_H_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_H_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_H_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_H_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  word elements  of in_l and low half of word
- *               elements of in_h  are  interleaved  and  copied  to out_l.
- *               High half of word elements of in_l and high half of word
- *               elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *        out_h: 5, -5,  6, -6,  7, -7,  8, -8
- *        out_l: 1, -1,  2, -2,  3, -3,  4, -4
- */
-#define LASX_ILVLH_W(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_w(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_w(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_W(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_W(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_W_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0_h, out0_l, ~
- * Details     : Low half of word elements  of every 128-bit of in_l and
- *               low half of word elements  of every 128-bit of in_h are
- *               interleaved and copied to out_l.
- *               High half of word elements of every 128-bit of in_l and
- *               high half of word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *        out_h: 3, -3,  4, -4,  7, -7,  8, -8
- *        out_l: 1, -1,  2, -2,  5, -5,  6, -6
- */
-#define LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_W_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_W_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_W_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_W_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave double word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of double word  elements  of in_l and low half of
- *               double word elements of in_h are interleaved and copied to
- *               out_l. High half of double word  elements  of in_l and high
- *               half of double word  elements  of in_h are interleaved and
- *               copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_D(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_d(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_d(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_D(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_D(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_D_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of double word elements of every 128-bit  of in_l and
- *               low half of double word elements of every 128-bit  of in_h are
- *               interleaved and copied to out_l.
- *               High half of double word elements of every 128-bit of in_l and
- *               high half of double word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_D_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_D_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_D_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_D_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_D_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Immediate number of columns to slide with zero
- * Arguments   : Inputs  - in0, in1, slide_val, ~
- *               Outputs - out0, out1, ~
- * Details     : Byte elements from every 128-bit of in0 vector
- *               are slide into  out0  by  number  of  elements
- *               specified by slide_val.
- * Example     : LASX_SLDI_B_0_128SV(in0, out0, slide_val)
- *          in0: 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,
- *               19,20,21,22,23,24,25,26,27,28,29,30,31,32
- *         out0: 4, 5,6,7,8,9,10,11,12,13,14,15,16,0,0,0,20,21,
- *               22,23,24,25,26,27,28,29,30,31,32,0,0,0
- *    slide_val: 3
- */
-#define LASX_SLDI_B_0_128SV(in0, out0, slide_val)                   \
-{                                                                   \
-    out0 = __lasx_xvbsrl_v(in0, slide_val);                         \
-}
-
-#define LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val)      \
-{                                                                   \
-    LASX_SLDI_B_0_128SV(in0, out0, slide_val);                      \
-    LASX_SLDI_B_0_128SV(in1, out1, slide_val);                      \
-}
-
-#define LASX_SLDI_B_4_0_128SV(in0, in1, in2, in3,                   \
-                              out0, out1, out2, out3, slide_val)    \
-{                                                                   \
-    LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val);         \
-    LASX_SLDI_B_2_0_128SV(in2, in3, out2, out3, slide_val);         \
-}
-
-/* Description : Pack even byte elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even byte elements of in_l are copied to the low half of
- *               out0.  Even byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_B(in_h, in_l, out0)                                  \
-{                                                                       \
-    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                \
-}
-
-#define LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_B(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_B(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even byte elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even byte elements of in_l are copied to the low half of
- *               out0.  Even byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKEV_B_128SV(in_h, in_l, out0)                            \
-{                                                                       \
-    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
-}
-
-#define LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
-{                                                                       \
-    LASX_PCKEV_B_128SV(in0_h, in0_l, out0);                             \
-    LASX_PCKEV_B_128SV(in1_h, in1_l, out1);                             \
-}
-
-#define LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, out0, out1, out2, out3)      \
-{                                                                       \
-    LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
-    LASX_PCKEV_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
-}
-
-#define LASX_PCKEV_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
-                             out2, out3, out4, out5, out6, out7)        \
-{                                                                       \
-    LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                         in3_h, in3_l, out0, out1, out2, out3);         \
-    LASX_PCKEV_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
-                         in7_h, in7_l, out4, out5, out6, out7);         \
-}
-
-/* Description : Pack even half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_H(in_h, in_l, out0)                                 \
-{                                                                      \
-    out0 = __lasx_xvpickev_h(in_h, in_l);                              \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                               \
-}
-
-#define LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_H(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_H(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even half word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKEV_H_128SV(in_h, in_l, out0)                            \
-{                                                                       \
-    out0 = __lasx_xvpickev_h(in_h, in_l);                               \
-}
-
-#define LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
-{                                                                       \
-    LASX_PCKEV_H_128SV(in0_h, in0_l, out0);                             \
-    LASX_PCKEV_H_128SV(in1_h, in1_l, out1);                             \
-}
-
-#define LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
-    LASX_PCKEV_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
-}
-
-#define LASX_PCKEV_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
-                             out2, out3, out4, out5, out6, out7)        \
-{                                                                       \
-    LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                   in3_h, in3_l, out0, out1, out2, out3);               \
-    LASX_PCKEV_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
-                   in7_h, in7_l, out4, out5, out6, out7);               \
-}
-
-/* Description : Pack even word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even word  elements  of  in_l are copied to
- *               the low  half of out0.  Even word elements
- *               of in_h are copied to the high half of out0.
- * Example     : LASX_PCKEV_W(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  1,  3,  5,  7, -1, -3, -5, -7
- */
-#define LASX_PCKEV_W(in_h, in_l, out0)                    \
-{                                                         \
-    out0 = __lasx_xvpickev_w(in_h, in_l);                 \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                  \
-}
-
-#define LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_W(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_W(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even word  elements  of  in_l are copied to
- *               the low  half of out0.  Even word elements
- *               of in_h are copied to the high half of out0.
- * Example     : LASX_PCKEV_W_128SV(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  1,  3, -1, -3,  5,  7, -5, -7
- */
-#define LASX_PCKEV_W_128SV(in_h, in_l, out0)                           \
-{                                                                      \
-    out0 = __lasx_xvpickev_w(in_h, in_l);                              \
-}
-
-#define LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)   \
-{                                                                      \
-    LASX_PCKEV_W_128SV(in0_h, in0_l, out0);                            \
-    LASX_PCKEV_W_128SV(in1_h, in1_l, out1);                            \
-}
-
-#define LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
-                             in3_h, in3_l, out0, out1, out2, out3)     \
-{                                                                      \
-    LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);      \
-    LASX_PCKEV_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);      \
-}
-
-#define LASX_PCKEV_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l, \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,   \
-                             out2, out3, out4, out5, out6, out7)       \
-{                                                                      \
-    LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,     \
-                         in3_h, in3_l, out0, out1, out2, out3);        \
-    LASX_PCKEV_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,     \
-                         in7_h, in7_l, out4, out5, out6, out7);        \
-}
-
-/* Description : Pack even half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_D(in_h, in_l, out0)                                        \
-{                                                                             \
-    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
-}
-
-#define LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
-{                                                                             \
-    LASX_PCKEV_D(in0_h, in0_l, out0)                                          \
-    LASX_PCKEV_D(in1_h, in1_l, out1)                                          \
-}
-
-#define LASX_PCKEV_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
-                       in3_h, in3_l, out0, out1, out2, out3)                  \
-{                                                                             \
-    LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
-    LASX_PCKEV_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
-}
-
-/* Description : Pack even half word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : LASX_PCKEV_D_128SV(in_h, in_l, out0)
- *        in_h : 1, 2, 3, 4
- *        in_l : 5, 6, 7, 8
- *        out0 : 5, 1, 7, 3
- */
-#define LASX_PCKEV_D_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
-}
-
-#define LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKEV_D_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKEV_D_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKEV_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKEV_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Pack even quad word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even quad elements of in_l are copied to the low
- *               half of out0. Even  quad  elements  of  in_h are
- *               copied to the high half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_Q(in_h, in_l, out0)                          \
-{                                                               \
-    out0 = __lasx_xvpermi_q(in_h, in_l, 0x20);                  \
-}
-
-#define LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_Q(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_Q(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd byte elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd byte elements of in_l are copied to the low half of
- *               out0. Odd byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_B(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_b(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_B(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_B(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0. Odd half word elements of in_h are copied
- *               to the high half of out0.
- * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_H(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_h(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_H(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_H(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd word elements of in_l are copied to the low half of out0.
- *               Odd word elements of in_h are copied to the high half of out0.
- * Example     : LASX_PCKOD_W(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  2,  4,  6,  8, -2, -4, -6, -8
- */
-#define LASX_PCKOD_W(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_w(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_W(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_W(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0. Odd half word elements of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_D(in_h, in_l, out0)                                        \
-{                                                                             \
-    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
-}
-
-#define LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
-{                                                                             \
-    LASX_PCKOD_D(in0_h, in0_l, out0)                                          \
-    LASX_PCKOD_D(in1_h, in1_l, out1)                                          \
-}
-
-#define LASX_PCKOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
-                       in3_h, in3_l, out0, out1, out2, out3)                  \
-{                                                                             \
-    LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
-    LASX_PCKOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
-}
-
-/* Description : Pack odd quad word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd quad elements of in0_h are copied to the high half of
- *               out0 & odd quad elements of in0_l are copied to the low
- *               half of out0.
- *               Odd quad elements of in1_h are copied to the high half of
- *               out1 & odd quad elements of in1_l are copied to the low
- *               half of out1.
- *               LASX_PCKOD_Q(in_h, in_l, out0)
- *               in_h:   0,0,0,0, 0,0,0,0, 19,10,11,12, 13,14,15,16
- *               in_l:   0,0,0,0, 0,0,0,0, 1,2,3,4, 5,6,7,8
- *               out0:  1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
- */
-#define LASX_PCKOD_Q(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpermi_q(in_h, in_l, 0x31);                                 \
-}
-
-#define LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_Q(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_Q(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd half word elements of vector pairsi
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0 of . Odd half word elements of in_h are
- *               copied to the high half of out0.
- * Example     : LASX_PCKOD_D_128SV(in_h, in_l, out0)
- *        in_h : 1, 2, 3, 4
- *        in_l : 5, 6, 7, 8
- *        out0 : 6, 2, 8, 4
- */
-#define LASX_PCKOD_D_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
-}
-
-#define LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKOD_D_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKOD_D_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKOD_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKOD_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-
-/* Description : Transposes 8x8 block with half word elements in vectors.
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0, out1, ~
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE8x8_H_128SV
- *         in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *         in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *         in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *         in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *
- *        out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
- *        out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
- *        out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
- *        out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
- *        out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
- *        out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
- *        out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
- *        out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
- */
-#define LASX_TRANSPOSE8x8_H_128SV(in0, in1, in2, in3, in4, in5, in6, in7,           \
-                                  out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                                   \
-    __m256i s0_m, s1_m;                                                             \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                         \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                         \
-                                                                                    \
-    LASX_ILVL_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                                 \
-    LASX_ILVH_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                                 \
-                                                                                    \
-    LASX_ILVL_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                                 \
-    LASX_ILVH_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                                 \
-                                                                                    \
-    LASX_PCKEV_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
-                         tmp3_m, tmp7_m, out0, out2, out4, out6);                   \
-    LASX_PCKOD_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
-                         tmp3_m, tmp7_m, out1, out3, out5, out7);                   \
-}
-
-/* Description : Transposes 8x8 block with word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- */
-#define LASX_TRANSPOSE8x8_W(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                            out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                           \
-    __m256i s0_m, s1_m;                                                     \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
-                                                                            \
-    LASX_ILVL_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                         \
-    LASX_ILVH_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                         \
-                                                                            \
-    LASX_ILVL_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                         \
-    LASX_ILVH_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                         \
-    LASX_PCKEV_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
-                   tmp7_m, tmp3_m, out0, out1, out2, out3);                 \
-    LASX_PCKOD_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
-                   tmp7_m, tmp3_m, out4, out5, out6, out7);                 \
-}
-
-/* Description : Transposes 2x2 block with quad word elements in vectors
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- * Details     :
- */
-#define LASX_TRANSPOSE2x2_Q(in0, in1, out0, out1) \
-{                                                 \
-    __m256i tmp0;                                 \
-    tmp0 = __lasx_xvpermi_q(in1, in0, 0x02);      \
-    out1 = __lasx_xvpermi_q(in1, in0, 0x13);      \
-    out0 = tmp0;                                  \
-}
-
-/* Description : Transposes 4x4 block with double word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     :
- */
-#define LASX_TRANSPOSE4x4_D(in0, in1, in2, in3, out0, out1, out2, out3) \
-{                                                                       \
-    __m256i tmp0, tmp1, tmp2, tmp3;                                     \
-    LASX_ILVLH_D_2_128SV(in1, in0, in3, in2, tmp0, tmp1, tmp2, tmp3);   \
-    out0 = __lasx_xvpermi_q(tmp2, tmp0, 0x20);                          \
-    out2 = __lasx_xvpermi_q(tmp2, tmp0, 0x31);                          \
-    out1 = __lasx_xvpermi_q(tmp3, tmp1, 0x20);                          \
-    out3 = __lasx_xvpermi_q(tmp3, tmp1, 0x31);                          \
-}
-
-/* Description : Transpose 4x4 block with half word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- *               Return Type - signed halfword
- */
-#define LASX_TRANSPOSE4x4_H_128SV(in0, in1, in2, in3, out0, out1, out2, out3) \
-{                                                                             \
-    __m256i s0_m, s1_m;                                                       \
-                                                                              \
-    LASX_ILVL_H_2_128SV(in1, in0, in3, in2, s0_m, s1_m);                      \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, out2, out0);                               \
-    out1 = __lasx_xvilvh_d(out0, out0);                                       \
-    out3 = __lasx_xvilvh_d(out2, out2);                                       \
-}
-
-/* Description : Transposes input 8x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *                         (input 8x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x8 byte block)
- * Details     :
- */
-#define LASX_TRANSPOSE8x8_B(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                            out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                           \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
-    LASX_ILVL_B_4_128SV(in2, in0, in3, in1, in6, in4, in7, in5,             \
-                       tmp0_m, tmp1_m, tmp2_m, tmp3_m);                     \
-    LASX_ILVLH_B_128SV(tmp1_m, tmp0_m, tmp5_m, tmp4_m);                     \
-    LASX_ILVLH_B_128SV(tmp3_m, tmp2_m, tmp7_m, tmp6_m);                     \
-    LASX_ILVLH_W_128SV(tmp6_m, tmp4_m, out2, out0);                         \
-    LASX_ILVLH_W_128SV(tmp7_m, tmp5_m, out6, out4);                         \
-    LASX_SLDI_B_2_0_128SV(out0, out2, out1, out3, 8);                       \
-    LASX_SLDI_B_2_0_128SV(out4, out6, out5, out7, 8);                       \
-}
-
-/* Description : Transposes input 16x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
- *                         in8, in9, in10, in11, in12, in13, in14, in15
- *                         (input 16x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x16 byte block)
- * Details     :
- */
-#define LASX_TRANSPOSE16x8_B(in0, in1, in2, in3, in4, in5, in6, in7,              \
-                             in8, in9, in10, in11, in12, in13, in14, in15,        \
-                             out0, out1, out2, out3, out4, out5, out6, out7)      \
-{                                                                                 \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
-    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
-    LASX_ILVL_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out0, out3, out2); \
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out5, out4, out7, out6); \
-}
-
-/* Description : Transposes input 16x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
- *                         in8, in9, in10, in11, in12, in13, in14, in15
- *                         (input 16x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE16x8_H
- *         in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *
- *        out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
- *        out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
- *        out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
- *        out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
- *        out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
- *        out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
- *        out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
- *        out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
- */
-#define LASX_TRANSPOSE16x8_H(in0, in1, in2, in3, in4, in5, in6, in7,              \
-                             in8, in9, in10, in11, in12, in13, in14, in15,        \
-                             out0, out1, out2, out3, out4, out5, out6, out7)      \
-{                                                                                 \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
-    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
-    LASX_ILVL_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1);                   \
-    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out2, out3);                   \
-                                                                                  \
-    LASX_ILVH_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out4, out5);                   \
-    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out6, out7);                   \
-}
-
-/* Description : Clips all signed word elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs  - in       (input vector)
- *               Outputs - out_m    (output vector with clipped elements)
- *               Return Type - signed word
- */
-#define LASX_CLIP_W_0_255(in, out_m)        \
-{                                           \
-    out_m = __lasx_xvmaxi_w(in, 0);         \
-    out_m = __lasx_xvsat_wu(out_m, 7);      \
-}
-
-#define LASX_CLIP_W_0_255_2(in0, in1, out0, out1)  \
-{                                                  \
-    LASX_CLIP_W_0_255(in0, out0);                  \
-    LASX_CLIP_W_0_255(in1, out1);                  \
-}
-
-#define LASX_CLIP_W_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                        \
-    LASX_CLIP_W_0_255_2(in0, in1, out0, out1);                           \
-    LASX_CLIP_W_0_255_2(in2, in3, out2, out3);                           \
-}
-
-/* Description : Clips all signed halfword elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs  - in       (input vector)
- *               Outputs - out_m    (output vector with clipped elements)
- *               Return Type - signed halfword
- */
-#define LASX_CLIP_H_0_255(in, out_m)        \
-{                                           \
-    out_m = __lasx_xvmaxi_h(in, 0);         \
-    out_m = __lasx_xvsat_hu(out_m, 7);      \
-}
-
-#define LASX_CLIP_H_0_255_2(in0, in1, out0, out1)  \
-{                                                  \
-    LASX_CLIP_H_0_255(in0, out0);                  \
-    LASX_CLIP_H_0_255(in1, out1);                  \
-}
-
-#define LASX_CLIP_H_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                        \
-    LASX_CLIP_H_0_255_2(in0, in1, out0, out1);                           \
-    LASX_CLIP_H_0_255_2(in2, in3, out2, out3);                           \
-}
-
-/* Description : Clips all halfword elements of input vector between min & max
- *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
- * Arguments   : Inputs  - in    (input vector)
- *                       - min   (min threshold)
- *                       - max   (max threshold)
- *               Outputs - in    (output vector with clipped elements)
- *               Return Type - signed halfword
- */
-#define LASX_CLIP_H(in, min, max)    \
-{                                    \
-    in = __lasx_xvmax_h(min, in);    \
-    in = __lasx_xvmin_h(max, in);    \
-}
-
-/* Description : Dot product and addition of 3 signed byte input vectors
- * Arguments   : Inputs  - in0, in1, in2, coeff0, coeff1, coeff2
- *               Outputs - out0_m
- *               Return Type - signed halfword
- * Details     : Dot product of 'in0' with 'coeff0'
- *               Dot product of 'in1' with 'coeff1'
- *               Dot product of 'in2' with 'coeff2'
- *               Addition of all the 3 vector results
- *               out0_m = (in0 * coeff0) + (in1 * coeff1) + (in2 * coeff2)
- */
-#define LASX_DP2ADD_H_B_3(in0, in1, in2, out0_m, coeff0, coeff1, coeff2) \
-{                                                                        \
-    LASX_DP2_H_B(in0, coeff0, out0_m);                                   \
-    LASX_DP2ADD_H_B(out0_m, in1, coeff1, out0_m);                        \
-    LASX_DP2ADD_H_B(out0_m, in2, coeff2, out0_m);                        \
-}
-
-/* Description : Each byte element is logically xor'ed with immediate 128
- * Arguments   : Inputs  - in0, in1
- *               Outputs - in0, in1 (in-place)
- * Details     : Each unsigned byte element from input vector 'in0' is
- *               logically xor'ed with 128 and result is in-place stored in
- *               'in0' vector
- *               Each unsigned byte element from input vector 'in1' is
- *               logically xor'ed with 128 and result is in-place stored in
- *               'in1' vector
- *               Similar for other pairs
- * Example     : LASX_XORI_B_128(in0)
- *               in0: 9,10,11,12, 13,14,15,16, 121,122,123,124, 125,126,127,128, 17,18,19,20, 21,22,23,24,
- *               248,249,250,251, 252,253,254,255,
- *               in0: 137,138,139,140, 141,142,143,144, 249,250,251,252, 253,254,255,0, 145,146,147,148,
- *               149,150,151,152, 120,121,122,123, 124,125,126,127
- */
-#define LASX_XORI_B_128(in0)                                 \
-{                                                            \
-    in0 = __lasx_xvxori_b(in0, 128);                         \
-}
-#define LASX_XORI_B_2_128(in0, in1)                          \
-{                                                            \
-    LASX_XORI_B_128(in0);                                    \
-    LASX_XORI_B_128(in1);                                    \
-}
-#define LASX_XORI_B_4_128(in0, in1, in2, in3)                \
-{                                                            \
-    LASX_XORI_B_2_128(in0, in1);                             \
-    LASX_XORI_B_2_128(in2, in3);                             \
-}
-#define LASX_XORI_B_8_128(in0, in1, in2, in3, in4, in5, in6, in7)  \
-{                                                                  \
-    LASX_XORI_B_4_128(in0, in1, in2, in3);                         \
-    LASX_XORI_B_4_128(in4, in5, in6, in7);                         \
-}
-
-/* Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx0 < 8' use SPLATI_R_*,
- *               if 'indx0 >= 8' use SPLATI_L_*
- * Arguments   : Inputs  - in, idx0, idx1
- *               Outputs - out0, out1
- * Details     : 'idx0' element value from 'in' vector is replicated to all
- *                elements in 'out0' vector
- *                Valid index range for halfword operation is 0-7
- */
-#define LASX_SPLATI_L_H(in, idx0, out0)                        \
-{                                                              \
-    in = __lasx_xvpermi_q(in, in, 0x02);                       \
-    out0 = __lasx_xvrepl128vei_h(in, idx0);                    \
-}
-#define LASX_SPLATI_H_H(in, idx0, out0)                        \
-{                                                              \
-    in = __lasx_xvpermi_q(in, in, 0X13);                       \
-    out0 = __lasx_xvrepl128vei_h(in, idx0 - 8);                \
-}
-#define LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1)          \
-{                                                              \
-    LASX_SPLATI_L_H(in, idx0, out0);                           \
-    out1 = __lasx_xvrepl128vei_h(in, idx1);                    \
-}
-#define LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1)          \
-{                                                              \
-    LASX_SPLATI_H_H(in, idx0, out0);                           \
-    out1 = __lasx_xvrepl128vei_h(in, idx1 - 8);                \
-}
-#define LASX_SPLATI_L_H_4(in, idx0, idx1, idx2, idx3,          \
-                          out0, out1, out2, out3)              \
-{                                                              \
-    LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1);             \
-    out2 = __lasx_xvrepl128vei_h(in, idx2);                    \
-    out3 = __lasx_xvrepl128vei_h(in, idx3);                    \
-}
-#define SPLATI_H_H_4(in, idx0, idx1, idx2, idx3,               \
-                     out0, out1, out2, out3)                   \
-{                                                              \
-    LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1);             \
-    out2 = __lasx_xvrepl128vei_h(in, idx2 - 8);                \
-    out3 = __lasx_xvrepl128vei_h(in, idx3 - 8);                \
-}
-
-/* Description : Pack even elements of input vectors & xor with 128
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out_m
- * Details     : Signed byte even elements from 'in0' and 'in1' are packed
- *               together in one vector and the resulted vector is xor'ed with
- *               128 to shift the range from signed to unsigned byte
- */
-#define LASX_PICKEV_XORI128_B(in0, in1, out_m)  \
-{                                               \
-    out_m = __lasx_xvpickev_b(in1, in0);        \
-    out_m = __lasx_xvxori_b(out_m, 128);        \
-}
-
-/* Description : Shift right logical all byte elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
-     */
-#define LASX_SRL_B(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_b(in, shift);                                   \
-}
-
-#define LASX_SRL_B_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_B(in0, shift);                                           \
-    LASX_SRL_B(in1, shift);                                           \
-}
-
-#define LASX_SRL_B_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_B_2(in0, in1, shift);                                    \
-    LASX_SRL_B_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all halfword elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
- */
-#define LASX_SRL_H(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_h(in, shift);                                   \
-}
-
-#define LASX_SRL_H_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_H(in0, shift);                                           \
-    LASX_SRL_H(in1, shift);                                           \
-}
-
-#define LASX_SRL_H_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_H_2(in0, in1, shift);                                    \
-    LASX_SRL_H_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all word elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : LASX_SRL_W(in, shift)
- *          in : 1, 3, 2, -4,      0, -2, 25, 0
- *       shift : 1, 1, 1, 1,       2, 2, 2, 2
- *  in(output) : 0, 1, 1, 32766,   0, 16383, 6, 0
- */
-#define LASX_SRL_W(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_w(in, shift);                                   \
-}
-
-#define LASX_SRL_W_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_W(in0, shift);                                           \
-    LASX_SRL_W(in1, shift);                                           \
-}
-
-#define LASX_SRL_W_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_W_2(in0, in1, shift);                                    \
-    LASX_SRL_W_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all double word elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
- */
-#define LASX_SRL_D(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_d(in, shift);                                   \
-}
-
-#define LASX_SRL_D_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_D(in0, shift);                                           \
-    LASX_SRL_D(in1, shift);                                           \
-}
-
-#define LASX_SRL_D_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_D_2(in0, in1, shift);                                    \
-    LASX_SRL_D_2(in2, in3, shift);                                    \
-}
-
-
-/* Description : Shift right arithmetic rounded (immediate)
- * Arguments   : Inputs  - in0, in1, shift
- *               Outputs - in0, in1, (in place)
- * Details     : Each element of vector 'in0' is shifted right arithmetic by
- *               value in 'shift'.
- *               The last discarded bit is added to shifted value for rounding
- *               and the result is in place written to 'in0'
- *               Similar for other pairs
- * Example     : LASX_SRARI_H(in0, out0, shift)
- *               in0:   1,2,3,4, -5,-6,-7,-8, 19,10,11,12, 13,14,15,16
- *               shift: 2
- *               out0:  0,1,1,1, -1,-1,-2,-2, 5,3,3,3, 3,4,4,4
- */
-#define LASX_SRARI_H(in0, out0, shift)                              \
-{                                                                   \
-    out0 = __lasx_xvsrari_h(in0, shift);                            \
-}
-#define LASX_SRARI_H_2(in0, in1, out0, out1, shift)                 \
-{                                                                   \
-    LASX_SRARI_H(in0, out0, shift);                                 \
-    LASX_SRARI_H(in1, out1, shift);                                 \
-}
-#define LASX_SRARI_H_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
-{                                                                         \
-    LASX_SRARI_H_2(in0, in1, out0, out1, shift);                          \
-    LASX_SRARI_H_2(in2, in3, out2, out3, shift);                          \
-}
-
-/* Description : Shift right arithmetic (immediate)
- * Arguments   : Inputs  - in0, in1, shift
- *               Outputs - in0, in1, (in place)
- * Details     : Each element of vector 'in0' is shifted right arithmetic by
- *               value in 'shift'.
- *               Similar for other pairs
- * Example     : see LASX_SRARI_H(in0, out0, shift)
- */
-#define LASX_SRAI_W(in0, out0, shift)                                    \
-{                                                                        \
-    out0 = __lasx_xvsrai_w(in0, shift);                                  \
-}
-#define LASX_SRAI_W_2(in0, in1, out0, out1, shift)                       \
-{                                                                        \
-    LASX_SRAI_W(in0, out0, shift);                                       \
-    LASX_SRAI_W(in1, out1, shift);                                       \
-}
-#define LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
-{                                                                        \
-    LASX_SRAI_W_2(in0, in1, out0, out1, shift);                          \
-    LASX_SRAI_W_2(in2, in3, out2, out3, shift);                          \
-}
-#define LASX_SRAI_W_8(in0, in1, in2, in3, in4, in5, in6, in7,                 \
-                      out0, out1, out2, out3, out4, out5, out6, out7, shift)  \
-{                                                                             \
-    LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift);         \
-    LASX_SRAI_W_4(in4, in5, in6, in7, out4, out5, out6, out7, shift);         \
-}
-
-/* Description : Saturate the halfword element values to the max
- *               unsigned value of (sat_val+1 bits)
- *               The element data width remains unchanged
- * Arguments   : Inputs  - in0, in1, in2, in3, sat_val
- *               Outputs - in0, in1, in2, in3 (in place)
- *               Return Type - unsigned halfword
- * Details     : Each unsigned halfword element from 'in0' is saturated to the
- *               value generated with (sat_val+1) bit range
- *               Results are in placed to original vectors
- * Example     : LASX_SAT_H(in0, out0, sat_val)
- *               in0:    1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
- *               sat_val:3
- *               out0:   1,2,3,4, 5,6,7,7, 7,7,7,7, 7,7,7,7
- */
-#define LASX_SAT_H(in0, out0, sat_val)                                     \
-{                                                                          \
-    out0 = __lasx_xvsat_h(in0, sat_val);                                   \
-} //some error in xvsat_h built-in function
-#define LASX_SAT_H_2(in0, in1, out0, out1, sat_val)                        \
-{                                                                          \
-    LASX_SAT_H(in0, out0, sat_val);                                        \
-    LASX_SAT_H(in1, out1, sat_val);                                        \
-}
-#define LASX_SAT_H_4(in0, in1, in2, in3, out0, out1, out2, out3, sat_val)  \
-{                                                                          \
-    LASX_SAT_H_2(in0, in1, out0, out1, sat_val);                           \
-    LASX_SAT_H_2(in2, in3, out2, out3, sat_val);                           \
-}
-
-/* Description : Addition of 2 pairs of vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1
- * Details     : Each halfwords element from 2 pairs vectors is added
- *               and 2 results are produced
- * Example     : LASX_ADD_H(in0, in1, out)
- *               in0:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *               in1:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *               out:  9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
- */
-#define LASX_ADD_H(in0, in1, out)             \
-{                                             \
-    out = __lasx_xvadd_h(in0, in1);           \
-}
-#define LASX_ADD_H_2(in0, in1, in2, in3, out0, out1) \
-{                                                    \
-    LASX_ADD_H(in0, in1, out0);                      \
-    LASX_ADD_H(in2, in3, out1);                      \
-}
-#define LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3)      \
-{                                                                                         \
-    LASX_ADD_H_2(in0, in1, in2, in3, out0, out1);                                         \
-    LASX_ADD_H_2(in4, in5, in6, in7, out2, out3);                                         \
-}
-#define LASX_ADD_H_8(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, \
-                     in13, in14, in15, out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                                        \
-    LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3);        \
-    LASX_ADD_H_4(in8, in9, in10, in11, in12, in13, in14, in15, out4, out5, out6, out7);  \
-}
-
-/* Description : Horizontal subtraction of unsigned byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Each unsigned odd byte element from 'in0' is subtracted from
- *               even unsigned byte element from 'in0' (pairwise) and the
- *               halfword result is written to 'out0'
- */
-#define LASX_HSUB_UB_2(in0, in1, out0, out1)   \
-{                                              \
-    out0 = __lasx_xvhsubw_hu_bu(in0, in0);     \
-    out1 = __lasx_xvhsubw_hu_bu(in1, in1);     \
-}
-
-#define LASX_HSUB_UB_4(in0, in1, in2, in3, out0, out1, out2, out3)    \
-{                                                                     \
-    LASX_HSUB_UB_2(in0, in1, out0, out1);                                   \
-    LASX_HSUB_UB_2(in2, in3, out2, out3);                                   \
-}
-
-/* Description : Shuffle byte vector elements as per mask vector
- * Arguments   : Inputs  - in0, in1, in2, in3, mask0, mask1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Selective byte elements from in0 & in1 are copied to out0 as
- *               per control vector mask0
- *               Selective byte elements from in2 & in3 are copied to out1 as
- *               per control vector mask1
- * Example     : LASX_SHUF_B_128SV(in0, in1,  mask0, out0)
- *               in_h :  9,10,11,12, 13,14,15,16, 0,0,0,0, 0,0,0,0,
- *                      17,18,19,20, 21,22,23,24, 0,0,0,0, 0,0,0,0
- *               in_l :  1, 2, 3, 4,  5, 6, 7, 8, 0,0,0,0, 0,0,0,0,
- *                      25,26,27,28, 29,30,31,32, 0,0,0,0, 0,0,0,0
- *               mask0:  0, 1, 2, 3,  4, 5, 6, 7, 16,17,18,19, 20,21,22,23,
- *                      16,17,18,19, 20,21,22,23,  0, 1, 2, 3,  4, 5, 6, 7
- *               out0 :  1, 2, 3, 4,  5, 6, 7, 8,  9,10,11,12, 13,14,15,16,
- *                      17,18,19,20, 21,22,23,24, 25,26,27,28, 29,30,31,32
- */
-
-#define LASX_SHUF_B_128SV(in_h, in_l,  mask0, out0)                            \
-{                                                                              \
-    out0 = __lasx_xvshuf_b(in_h, in_l, mask0);                                 \
-}
-#define LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1,          \
-                            out0, out1)                                        \
-{                                                                              \
-    LASX_SHUF_B_128SV(in0_h, in0_l,  mask0, out0);                             \
-    LASX_SHUF_B_128SV(in1_h, in1_l,  mask1, out1);                             \
-}
-#define LASX_SHUF_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,          \
-                            in3_h, in3_l, mask0, mask1, mask2, mask3,          \
-                            out0, out1, out2, out3)                            \
-{                                                                              \
-    LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1, out0, out1); \
-    LASX_SHUF_B_2_128SV(in2_h, in2_l, in3_h, in3_l, mask2, mask3, out2, out3); \
-}
-
-/* Description : Addition of signed halfword elements and signed saturation
- * Arguments   : Inputs  - in0, in1, in2, in3 ~
- *               Outputs - out0, out1 ~
- * Details     : Signed halfword elements from 'in0' are added to signed
- *               halfword elements of 'in1'. The result is then signed saturated
- *               between -32768 to +32767 (as per halfword data type)
- *               Similar for other pairs
- * Example     : LASX_SADD_H(in0, in1, out0)
- *               in0:   1,2,32766,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *               in1:   8,7,30586,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *               out0:  9,9,32767,9, 9,9,9,9, 9,9,9,9, 9,9,9,9,
- */
-#define LASX_SADD_H(in0, in1, out0)                            \
-{                                                              \
-    out0 = __lasx_xvsadd_h(in0, in1);                          \
-}
-#define LASX_SADD_H_2(in0, in1, in2, in3, out0, out1)          \
-{                                                              \
-    LASX_SADD_H(in0, in1, out0);                               \
-    LASX_SADD_H(in2, in3, out1);                               \
-}
-#define LASX_SADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7,  \
-                      out0, out1, out2, out3)                  \
-{                                                              \
-    LASX_SADD_H_2(in0, in1, in2, in3, out0, out1);             \
-    LASX_SADD_H_2(in4, in5, in6, in7, out2, out3);             \
-}
-
-/* Description : Average with rounding (in0 + in1 + 1) / 2.
- * Arguments   : Inputs  - in0, in1, in2, in3,
- *               Outputs - out0, out1
- * Details     : Each unsigned byte element from 'in0' vector is added with
- *               each unsigned byte element from 'in1' vector.
- *               Average with rounding is calculated and written to 'out0'
- */
-#define LASX_AVER_BU( in0, in1, out0 )   \
-{                                        \
-    out0 = __lasx_xvavgr_bu( in0, in1 ); \
-}
-
-#define LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 )  \
-{                                                         \
-    LASX_AVER_BU( in0, in1, out0 );                       \
-    LASX_AVER_BU( in2, in3, out1 );                       \
-}
-
-#define LASX_AVER_BU_4( in0, in1, in2, in3, in4, in5, in6, in7,  \
-                        out0, out1, out2, out3 )                 \
-{                                                                \
-    LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 );            \
-    LASX_AVER_BU_2( in4, in5, in6, in7, out2, out3 );            \
-}
-
-/* Description : Butterfly of 4 input vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     : Butterfly operationuu
- */
-#define LASX_BUTTERFLY_4(RTYPE, in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                            \
-    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in3 );                             \
-    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in2 );                             \
-                                                                             \
-    out2 = (__m256i)( (RTYPE)in1 - (RTYPE)in2 );                             \
-    out3 = (__m256i)( (RTYPE)in0 - (RTYPE)in3 );                             \
-}
-
-/* Description : Butterfly of 8 input vectors
- * Arguments   : Inputs  - in0 in1 in2 ~
- *               Outputs - out0 out1 out2 ~
- * Details     : Butterfly operation
- */
-#define LASX_BUTTERFLY_8(RTYPE, in0, in1, in2, in3, in4, in5, in6, in7,    \
-                         out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                          \
-    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in7 );                           \
-    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in6 );                           \
-    out2 = (__m256i)( (RTYPE)in2 + (RTYPE)in5 );                           \
-    out3 = (__m256i)( (RTYPE)in3 + (RTYPE)in4 );                           \
-                                                                           \
-    out4 = (__m256i)( (RTYPE)in3 - (RTYPE)in4 );                           \
-    out5 = (__m256i)( (RTYPE)in2 - (RTYPE)in5 );                           \
-    out6 = (__m256i)( (RTYPE)in1 - (RTYPE)in6 );                           \
-    out7 = (__m256i)( (RTYPE)in0 - (RTYPE)in7 );                           \
-}
-
-
-#endif /* GENERIC_MACROS_LASX_H */
-#endif /* AVUTIL_LOONGARCH_GENERIC_MACROS_LASX_H */
diff --git a/libavutil/loongarch/generic_macros_lsx.h b/libavutil/loongarch/generic_macros_lsx.h
deleted file mode 100644
index 8edebb4be0..0000000000
--- a/libavutil/loongarch/generic_macros_lsx.h
+++ /dev/null
@@ -1,670 +0,0 @@
-/*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
- * All rights reserved.
- * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
- *                Xiwei Gu   <guxiwei-hf@loongson.cn>
- *                Lu Wang    <wanglu@loongson.cn>
- *
- * This file is part of FFmpeg.
- *
- * FFmpeg is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License as published by the Free Software Foundation; either
- * version 2.1 of the License, or (at your option) any later version.
- *
- * FFmpeg is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * Lesser General Public License for more details.
- *
- * You should have received a copy of the GNU Lesser General Public
- * License along with FFmpeg; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
- *
- */
-
-#ifndef AVUTIL_LOONGARCH_GENERIC_MACROS_LSX_H
-#define AVUTIL_LOONGARCH_GENERIC_MACROS_LSX_H
-
-/*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
- * All rights reserved.
- * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
- *                Xiwei Gu   <guxiwei-hf@loongson.cn>
- *                Lu Wang    <wanglu@loongson.cn>
- *                Hecai Yuan <yuanhecai@loongson.cn>
- *
- * This file is maintained in LSOM project, don't change it directly.
- * You can get the latest version of this header from: ***
- *
- */
-
-#ifndef GENERIC_MACROS_LSX_H
-#define GENERIC_MACROS_LSX_H
-
-#include <lsxintrin.h>
-
-/**
- * MAJOR version: Macro usage changes.
- * MINOR version: Add new functions, or bug fix.
- * MICRO version: Comment changes or implementation changes.
- */
-#define LSOM_LSX_VERSION_MAJOR 0
-#define LSOM_LSX_VERSION_MINOR 7
-#define LSOM_LSX_VERSION_MICRO 0
-
-#define LSX_DUP2_ARG1(_LSX_INS, _IN0, _IN1, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _LSX_INS(_IN0); \
-    _OUT1 = _LSX_INS(_IN1); \
-}
-
-#define LSX_DUP2_ARG2(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _LSX_INS(_IN0, _IN1); \
-    _OUT1 = _LSX_INS(_IN2, _IN3); \
-}
-
-#define LSX_DUP2_ARG3(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _LSX_INS(_IN0, _IN1, _IN2); \
-    _OUT1 = _LSX_INS(_IN3, _IN4, _IN5); \
-}
-
-#define LSX_DUP4_ARG1(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    LSX_DUP2_ARG1(_LSX_INS, _IN0, _IN1, _OUT0, _OUT1); \
-    LSX_DUP2_ARG1(_LSX_INS, _IN2, _IN3, _OUT2, _OUT3); \
-}
-
-#define LSX_DUP4_ARG2(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                      _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    LSX_DUP2_ARG2(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
-    LSX_DUP2_ARG2(_LSX_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
-}
-
-#define LSX_DUP4_ARG3(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                      _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    LSX_DUP2_ARG3(_LSX_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
-    LSX_DUP2_ARG3(_LSX_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               Then the results plus to signed half word elements from in_c.
- * Example     : out = __lsx_dp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26, 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_dp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               unsigned byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               The results plus to signed half word elements from in_c.
- * Example     : out = __lsx_dp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26, 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_dp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of half word vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - __m128i
- * Details     : Signed half word elements from in_h are multiplied by
- *               signed half word elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               Then the results plus to signed word elements from in_c.
- * Example     : out = __lsx_dp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_dp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_dp2_h_b(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22, 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_dp2_h_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               unsigned byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_dp2_h_bu(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22, 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_dp2_h_bu(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_bu(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_dp2_h_bu_b(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
- *         out : 22,38,38,22, 22,38,38,6
- * =============================================================================
- */
-static inline __m128i __lsx_dp2_h_bu_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_dp2_w_h(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_dp2_w_h(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_w_h(in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Clip all halfword elements of input vector between min & max
-                 out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
-   Arguments   : Inputs  - _in  (input vector)
-                         - min  (min threshold)
-                         - max  (max threshold)
-                 Outputs - out  (output vector with clipped elements)
-                 Return Type - signed halfword
- * Example     : out = __lsx_clip_h(_in)
- *         _in : -8,2,280,249, -8,255,280,249
- *         min : 1,1,1,1, 1,1,1,1
- *         max : 9,9,9,9, 9,9,9,9
- *         out : 1,2,9,9, 1,9,9,9
- * =============================================================================
- */
-static inline __m128i __lsx_clip_h(__m128i _in, __m128i min, __m128i max)
-{
-    __m128i out;
-
-    out = __lsx_vmax_h(min, _in);
-    out = __lsx_vmin_h(max, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Set each element of vector between 0 and 255
- * Arguments   : Inputs  - _in
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from _in are clamped between 0 and 255.
- * Example     : out = __lsx_clamp255_h(_in)
- *         _in : -8,255,280,249, -8,255,280,249
- *         out : 0,255,255,249, 0,255,255,249
- * =============================================================================
- */
-static inline __m128i __lsx_clamp255_h(__m128i _in)
-{
-    __m128i out;
-
-    out = __lsx_vmaxi_h(_in, 0);
-    out = __lsx_vsat_hu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Set each element of vector between 0 and 255
- * Arguments   : Inputs  - _in
- *               Outputs - out
- *               Retrun Type - word
- * Details     : Signed byte elements from _in are clamped between 0 and 255.
- * Example     : out = __lsx_clamp255_w(_in)
- *         _in : -8,255,280,249
- *         out : 0,255,255,249
- * =============================================================================
- */
-static inline __m128i __lsx_clamp255_w(__m128i _in)
-{
-    __m128i out;
-
-    out = __lsx_vmaxi_w(_in, 0);
-    out = __lsx_vsat_wu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Swap two variables
-   Arguments   : Inputs  - _in0, _in1
-                 Outputs - _in0, _in1 (in-place)
-   Details     : Swapping of two input variables using xor
- * Example     : SWAP(_in0, _in1)
- *        _in0 : 1,2,3,4
- *        _in1 : 5,6,7,8
- *   _in0(out) : 5,6,7,8
- *   _in1(out) : 1,2,3,4
- * =============================================================================
- */
-#define SWAP(_in0, _in1)                                                \
-{                                                                       \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-}                                                                       \
-
-/*
- * =============================================================================
- * Description : Transpose 4x4 block with word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     :
- * Example     :
- *               1, 2, 3, 4            1, 5, 9,13
- *               5, 6, 7, 8    to      2, 6,10,14
- *               9,10,11,12  =====>    3, 7,11,15
- *              13,14,15,16            4, 8,12,16
- * =============================================================================
- */
-#define TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                          \
-    __m128i _t0, _t1, _t2, _t3;                                            \
-                                                                           \
-    _t0   = __lsx_vilvl_w(_in1, _in0);                                     \
-    _t1   = __lsx_vilvh_w(_in1, _in0);                                     \
-    _t2   = __lsx_vilvl_w(_in3, _in2);                                     \
-    _t3   = __lsx_vilvh_w(_in3, _in2);                                     \
-    _out0 = __lsx_vilvl_d(_t2, _t0);                                       \
-    _out1 = __lsx_vilvh_d(_t2, _t0);                                       \
-    _out2 = __lsx_vilvl_d(_t3, _t1);                                       \
-    _out3 = __lsx_vilvh_d(_t3, _t1);                                       \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with byte elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : TRANSPOSE8x8_B
- *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
- *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
- *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
- *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
- *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
- *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
- *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
- *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
- *
- *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
- *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
- *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
- *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
- *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
- *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
- *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
- *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
- * =============================================================================
- */
-#define TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                       _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                              \
-   __m128i zero = {0};                                                         \
-   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                   \
-   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                             \
-                                                                               \
-   _t0 = __lsx_vilvl_b(_in2, _in0);                                            \
-   _t1 = __lsx_vilvl_b(_in3, _in1);                                            \
-   _t2 = __lsx_vilvl_b(_in6, _in4);                                            \
-   _t3 = __lsx_vilvl_b(_in7, _in5);                                            \
-   _t4 = __lsx_vilvl_b(_t1, _t0);                                              \
-   _t5 = __lsx_vilvh_b(_t1, _t0);                                              \
-   _t6 = __lsx_vilvl_b(_t3, _t2);                                              \
-   _t7 = __lsx_vilvh_b(_t3, _t2);                                              \
-   _out0 = __lsx_vilvl_w(_t6, _t4);                                            \
-   _out2 = __lsx_vilvh_w(_t6, _t4);                                            \
-   _out4 = __lsx_vilvl_w(_t7, _t5);                                            \
-   _out6 = __lsx_vilvh_w(_t7, _t5);                                            \
-   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                  \
-   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                  \
-   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                  \
-   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                  \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with half word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- * Example     :
- *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
- *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
- *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
- *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
- *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
- *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
- *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
- *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
- * =============================================================================
- */
-#define TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                       _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                              \
-    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                  \
-                                                                               \
-    _s0 = __lsx_vilvl_h(_in6, _in4);                                           \
-    _s1 = __lsx_vilvl_h(_in7, _in5);                                           \
-    _t0 = __lsx_vilvl_h(_s1, _s0);                                             \
-    _t1 = __lsx_vilvh_h(_s1, _s0);                                             \
-    _s0 = __lsx_vilvh_h(_in6, _in4);                                           \
-    _s1 = __lsx_vilvh_h(_in7, _in5);                                           \
-    _t2 = __lsx_vilvl_h(_s1, _s0);                                             \
-    _t3 = __lsx_vilvh_h(_s1, _s0);                                             \
-    _s0 = __lsx_vilvl_h(_in2, _in0);                                           \
-    _s1 = __lsx_vilvl_h(_in3, _in1);                                           \
-    _t4 = __lsx_vilvl_h(_s1, _s0);                                             \
-    _t5 = __lsx_vilvh_h(_s1, _s0);                                             \
-    _s0 = __lsx_vilvh_h(_in2, _in0);                                           \
-    _s1 = __lsx_vilvh_h(_in3, _in1);                                           \
-    _t6 = __lsx_vilvl_h(_s1, _s0);                                             \
-    _t7 = __lsx_vilvh_h(_s1, _s0);                                             \
-                                                                               \
-    _out0 = __lsx_vpickev_d(_t0, _t4);                                         \
-    _out2 = __lsx_vpickev_d(_t1, _t5);                                         \
-    _out4 = __lsx_vpickev_d(_t2, _t6);                                         \
-    _out6 = __lsx_vpickev_d(_t3, _t7);                                         \
-    _out1 = __lsx_vpickod_d(_t0, _t4);                                         \
-    _out3 = __lsx_vpickod_d(_t1, _t5);                                         \
-    _out5 = __lsx_vpickod_d(_t2, _t6);                                         \
-    _out7 = __lsx_vpickod_d(_t3, _t7);                                         \
-}
-
-/*
- * =============================================================================
- * Description : Transpose input 8x4 byte block into 4x8
-   Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
-                 Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
-                 Return Type - as per RTYPE
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : TRANSPOSE8x4_B
- *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
- *
- *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
- *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
- *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
- *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
- * =============================================================================
- */
-#define TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,           \
-                       _out0, _out1, _out2, _out3)                               \
-{                                                                                \
-    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
-                                                                                 \
-    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
-    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
-                                                                                 \
-    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
-    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
-                                                                                 \
-    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
-    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
-    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
-    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 16x8 block with byte elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
- *                         in9, in10, in11, in12, in13, in14, in15
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- * Example     :
- *              000,001,002,003,004,005,006,007
- *              008,009,010,011,012,013,014,015
- *              016,017,018,019,020,021,022,023
- *              024,025,026,027,028,029,030,031
- *              032,033,034,035,036,037,038,039
- *              040,041,042,043,044,045,046,047        000,008,...,112,120
- *              048,049,050,051,052,053,054,055        001,009,...,113,121
- *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
- *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
- *              072,073,074,075,076,077,078,079        004,012,...,116,124
- *              080,081,082,083,084,085,086,087        005,013,...,117,125
- *              088,089,090,091,092,093,094,095        006,014,...,118,126
- *              096,097,098,099,100,101,102,103        007,015,...,119,127
- *              104,105,106,107,108,109,110,111
- *              112,113,114,115,116,117,118,119
- *              120,121,122,123,124,125,126,127
- * =============================================================================
- */
-#define TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,      \
-                        _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0,     \
-                        _out1, _out2, _out3, _out4, _out5, _out6, _out7)           \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-    LSX_DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,   \
-                  _tmp0, _tmp1, _tmp2, _tmp3);                                     \
-    LSX_DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,    \
-                  _in13, _tmp4, _tmp5, _tmp6, _tmp7);                              \
-    LSX_DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);            \
-    LSX_DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);            \
-    LSX_DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);            \
-    LSX_DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);            \
-    LSX_DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                \
-    LSX_DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                \
-    LSX_DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                \
-    LSX_DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                \
-    LSX_DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);        \
-    LSX_DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);        \
-    LSX_DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);        \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 4 input vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     : Butterfly operation
- * Example     :
- *               out0 = in0 + in3;
- *               out1 = in1 + in2;
- *               out2 = in1 - in2;
- *               out3 = in0 - in3;
- * =============================================================================
- */
-#define BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                         \
-    _out0 = __lsx_vadd_b(_in0, _in3);                                     \
-    _out1 = __lsx_vadd_b(_in1, _in2);                                     \
-    _out2 = __lsx_vsub_b(_in1, _in2);                                     \
-    _out3 = __lsx_vsub_b(_in0, _in3);                                     \
-}
-#define BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                         \
-    _out0 = __lsx_vadd_h(_in0, _in3);                                     \
-    _out1 = __lsx_vadd_h(_in1, _in2);                                     \
-    _out2 = __lsx_vsub_h(_in1, _in2);                                     \
-    _out3 = __lsx_vsub_h(_in0, _in3);                                     \
-}
-#define BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                         \
-    _out0 = __lsx_vadd_w(_in0, _in3);                                     \
-    _out1 = __lsx_vadd_w(_in1, _in2);                                     \
-    _out2 = __lsx_vsub_w(_in1, _in2);                                     \
-    _out3 = __lsx_vsub_w(_in0, _in3);                                     \
-}
-#define BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                         \
-    _out0 = __lsx_vadd_d(_in0, _in3);                                     \
-    _out1 = __lsx_vadd_d(_in1, _in2);                                     \
-    _out2 = __lsx_vsub_d(_in1, _in2);                                     \
-    _out3 = __lsx_vsub_d(_in0, _in3);                                     \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 8 input vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
- *               Outputs - _out0, _out1, _out2, _out3, ~
- * Details     : Butterfly operation
- * Example     :
- *              _out0 = _in0 + _in7;
- *              _out1 = _in1 + _in6;
- *              _out2 = _in2 + _in5;
- *              _out3 = _in3 + _in4;
- *              _out4 = _in3 - _in4;
- *              _out5 = _in2 - _in5;
- *              _out6 = _in1 - _in6;
- *              _out7 = _in0 - _in7;
- * =============================================================================
- */
-#define BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,          \
-                      _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)  \
-{                                                                              \
-    _out0 = __lsx_vadd_h(_in0, _in7);                                          \
-    _out1 = __lsx_vadd_h(_in1, _in6);                                          \
-    _out2 = __lsx_vadd_h(_in2, _in5);                                          \
-    _out3 = __lsx_vadd_h(_in3, _in4);                                          \
-    _out4 = __lsx_vsub_h(_in3, _in4);                                          \
-    _out5 = __lsx_vsub_h(_in2, _in5);                                          \
-    _out6 = __lsx_vsub_h(_in1, _in6);                                          \
-    _out7 = __lsx_vsub_h(_in0, _in7);                                          \
-}
-
-/*
- * =============================================================================
- * Description : Print out elements in vector.
- * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
- *               Outputs -
- * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
- *               '_enter' is TRUE, prefix "\nVP:" will be added first.
- * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
- *               VP:1,2,3,4,
- * =============================================================================
- */
-#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
-{                                                     \
-    RTYPE _tmp0 = (RTYPE)in0;                         \
-    int _i = 0;                                       \
-    if (enter)                                        \
-        printf("\nVP:");                              \
-    for(_i = 0; _i < element_num; _i++)               \
-        printf("%d,",_tmp0[_i]);                      \
-}
-
-#endif /* GENERIC_MACROS_LSX_H */
-#endif /* AVUTIL_LOONGARCH_GENERIC_MACROS_LSX_H */
diff --git a/libavutil/loongarch/loongson_intrinsics.h b/libavutil/loongarch/loongson_intrinsics.h
new file mode 100644
index 0000000000..865d6ae9bd
--- /dev/null
+++ b/libavutil/loongarch/loongson_intrinsics.h
@@ -0,0 +1,1881 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ *
+ */
+
+#ifndef AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H
+#define AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H
+
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is maintained in LSOM project, don't change it directly.
+ * You can get the latest version of this header from: ***
+ *
+ */
+
+#ifndef LOONGSON_INTRINSICS_H
+#define LOONGSON_INTRINSICS_H
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fix.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LSOM_VERSION_MAJOR 1
+#define LSOM_VERSION_MINOR 0
+#define LSOM_VERSION_MICRO 0
+
+#define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0); \
+    _OUT1 = _INS(_IN1); \
+}
+
+#define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1); \
+    _OUT1 = _INS(_IN2, _IN3); \
+}
+
+#define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1, _IN2); \
+    _OUT1 = _INS(_IN3, _IN4, _IN5); \
+}
+
+#define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1); \
+    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
+    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
+    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
+}
+
+/*
+ * =============================================================================
+ * Description : Print out elements in vector.
+ * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
+ *               Outputs -
+ * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
+ *               '_enter' is TRUE, prefix "\nVP:" will be added first.
+ * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
+ *               VP:1,2,3,4,
+ * =============================================================================
+ */
+#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
+{                                                     \
+    RTYPE _tmp0 = (RTYPE)in0;                         \
+    int _i = 0;                                       \
+    if (enter)                                        \
+        printf("\nVP:");                              \
+    for(_i = 0; _i < element_num; _i++)               \
+        printf("%d,",_tmp0[_i]);                      \
+}
+
+#ifdef __loongarch_sx
+#include <lsxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of half word vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - __m128i
+ * Details     : Signed half word elements from in_h are multiplied by
+ *               signed half word elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
+ *         out : 22,38,38,22, 22,38,38,6
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_w_h(in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
+ * Arguments   : Inputs  - _in  (input vector)
+ *                       - min  (min threshold)
+ *                       - max  (max threshold)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lsx_vclip_h(_in)
+ *         _in : -8,2,280,249, -8,255,280,249
+ *         min : 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
+{
+    __m128i out;
+
+    out = __lsx_vmax_h(min, _in);
+    out = __lsx_vmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_h(_in)
+ *         _in : -8,255,280,249, -8,255,280,249
+ *         out : 0,255,255,249, 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_h(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_h(_in, 0);
+    out = __lsx_vsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - word
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_w(_in)
+ *         _in : -8,255,280,249
+ *         out : 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_w(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_w(_in, 0);
+    out = __lsx_vsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Swap two variables
+ * Arguments   : Inputs  - _in0, _in1
+ *               Outputs - _in0, _in1 (in-place)
+ * Details     : Swapping of two input variables using xor
+ * Example     : LSX_SWAP(_in0, _in1)
+ *        _in0 : 1,2,3,4
+ *        _in1 : 5,6,7,8
+ *   _in0(out) : 5,6,7,8
+ *   _in1(out) : 1,2,3,4
+ * =============================================================================
+ */
+#define LSX_SWAP(_in0, _in1)                                            \
+{                                                                       \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+}                                                                       \
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                              \
+    __m128i _t0, _t1, _t2, _t3;                                                \
+                                                                               \
+    _t0   = __lsx_vilvl_w(_in1, _in0);                                         \
+    _t1   = __lsx_vilvh_w(_in1, _in0);                                         \
+    _t2   = __lsx_vilvl_w(_in3, _in2);                                         \
+    _t3   = __lsx_vilvh_w(_in3, _in2);                                         \
+    _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
+    _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
+    _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
+    _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with byte elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x8_B
+ *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
+ *
+ *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
+ *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
+ *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
+ *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+   __m128i zero = {0};                                                            \
+   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                      \
+   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+                                                                                  \
+   _t0 = __lsx_vilvl_b(_in2, _in0);                                               \
+   _t1 = __lsx_vilvl_b(_in3, _in1);                                               \
+   _t2 = __lsx_vilvl_b(_in6, _in4);                                               \
+   _t3 = __lsx_vilvl_b(_in7, _in5);                                               \
+   _t4 = __lsx_vilvl_b(_t1, _t0);                                                 \
+   _t5 = __lsx_vilvh_b(_t1, _t0);                                                 \
+   _t6 = __lsx_vilvl_b(_t3, _t2);                                                 \
+   _t7 = __lsx_vilvh_b(_t3, _t2);                                                 \
+   _out0 = __lsx_vilvl_w(_t6, _t4);                                               \
+   _out2 = __lsx_vilvh_w(_t6, _t4);                                               \
+   _out4 = __lsx_vilvl_w(_t7, _t5);                                               \
+   _out6 = __lsx_vilvh_w(_t7, _t5);                                               \
+   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                     \
+   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                     \
+   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                     \
+   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                     \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
+ *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
+ *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
+ *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
+ *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
+ *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
+ *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
+ *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                     \
+                                                                                  \
+    _s0 = __lsx_vilvl_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvl_h(_in7, _in5);                                              \
+    _t0 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t1 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvh_h(_in7, _in5);                                              \
+    _t2 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t3 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvl_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvl_h(_in3, _in1);                                              \
+    _t4 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t5 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvh_h(_in3, _in1);                                              \
+    _t6 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t7 = __lsx_vilvh_h(_s1, _s0);                                                \
+                                                                                  \
+    _out0 = __lsx_vpickev_d(_t0, _t4);                                            \
+    _out2 = __lsx_vpickev_d(_t1, _t5);                                            \
+    _out4 = __lsx_vpickev_d(_t2, _t6);                                            \
+    _out6 = __lsx_vpickev_d(_t3, _t7);                                            \
+    _out1 = __lsx_vpickod_d(_t0, _t4);                                            \
+    _out3 = __lsx_vpickod_d(_t1, _t5);                                            \
+    _out5 = __lsx_vpickod_d(_t2, _t6);                                            \
+    _out7 = __lsx_vpickod_d(_t3, _t7);                                            \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x4 byte block into 4x8
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
+ *               Return Type - as per RTYPE
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x4_B
+ *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
+ *
+ *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,       \
+                           _out0, _out1, _out2, _out3)                           \
+{                                                                                \
+    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
+                                                                                 \
+    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
+    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
+                                                                                 \
+    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
+    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
+                                                                                 \
+    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
+    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
+    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
+    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
+ *                         in9, in10, in11, in12, in13, in14, in15
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              000,001,002,003,004,005,006,007
+ *              008,009,010,011,012,013,014,015
+ *              016,017,018,019,020,021,022,023
+ *              024,025,026,027,028,029,030,031
+ *              032,033,034,035,036,037,038,039
+ *              040,041,042,043,044,045,046,047        000,008,...,112,120
+ *              048,049,050,051,052,053,054,055        001,009,...,113,121
+ *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
+ *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
+ *              072,073,074,075,076,077,078,079        004,012,...,116,124
+ *              080,081,082,083,084,085,086,087        005,013,...,117,125
+ *              088,089,090,091,092,093,094,095        006,014,...,118,126
+ *              096,097,098,099,100,101,102,103        007,015,...,119,127
+ *              104,105,106,107,108,109,110,111
+ *              112,113,114,115,116,117,118,119
+ *              120,121,122,123,124,125,126,127
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
+                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
+{                                                                                  \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,       \
+              _tmp0, _tmp1, _tmp2, _tmp3);                                         \
+    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,        \
+              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                                  \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);                \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);                \
+    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                    \
+    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                    \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);            \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);            \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     : Butterfly operation
+ * Example     :
+ *               out0 = in0 + in3;
+ *               out1 = in1 + in2;
+ *               out2 = in1 - in2;
+ *               out3 = in0 - in3;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_b(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_b(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_b(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_b(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_h(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_h(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_h(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_h(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_w(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_w(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_w(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_w(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_d(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_d(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_d(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     :
+ *              _out0 = _in0 + _in7;
+ *              _out1 = _in1 + _in6;
+ *              _out2 = _in2 + _in5;
+ *              _out3 = _in3 + _in4;
+ *              _out4 = _in3 - _in4;
+ *              _out5 = _in2 - _in5;
+ *              _out6 = _in1 - _in6;
+ *              _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_b(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_b(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_b(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_b(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_b(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_b(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_b(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_b(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_h(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_h(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_h(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_h(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_h(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_h(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_h(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_h(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_w(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_w(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_w(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_w(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_w(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_w(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_w(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_w(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_d(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_d(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_d(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_d(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_d(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_d(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_d(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_d(_in0, _in7);                                            \
+}
+
+#endif //LSX
+
+#ifdef __loongarch_asx
+#include <lasxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_b(in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : out = __lasx_xvdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of word vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - signed double
+ * Details     : Signed word elements from in_h are multiplied with
+ *               signed word elements from in_l producing a result
+ *               twice the size of input i.e. signed double word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_d_w(in_h, in_l);
+    out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - per RTYPE
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               unsigned halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Unsigned Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    out = __lasx_xvsub_h(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Signed Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               Signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ *        in_c : 0,0,0,0, 0,0,0,0
+ *        in_h : 3,1,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *        in_l : 2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *         out : -7,-3,0,0, 0,-1,0,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvsub_w(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are iniplied with
+ *               signed halfword elements from in_l producing a result
+ *               four times the size of input i.e. signed doubleword.
+ *               Then this iniplication results of four adjacent elements
+ *               are added together and stored to the out vector.
+ * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
+ *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
+ *        in_l : -2,1,1,0, 1,0,0,0, 0,0,1, 0, 1,0,0,1
+ *         out : -2,0,1,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvhaddw_d_w(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwh_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 1,0,0,-1, 1,0,0, 2
+ * =============================================================================
+ */
+ static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwl_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 5,-1,4,2, 1,0,2,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The out vector and the out vector are added after the
+ *               lower half of the two-fold zero extension (unsigned byte
+ *               to unsigned halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_hu_bu(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double zero extension (unsigned byte to
+ *               signed halfword)，added to the in_h vector.
+ * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_hu_bu(in_l, 0);
+    out = __lasx_xvadd_h(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double sign extension (signed halfword to
+ *               signed word), added to the in_h vector.
+ * Example     : out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ *        in_h : 0, 1,0,0, -1,0,0,1,
+ *        in_l : 2,-1,1,2,  1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *         out : 2, 0,1,2, -1,0,1,1,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_w_h(in_l, 0);
+    out = __lasx_xvadd_w(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the lower half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed halfword
+ *               to signed word), and the result is added to the vector in_c,
+ *               then stored to the out vector.
+ * Example     : out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 5,6,7,8
+ *        in_h : 1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
+ *        in_l : 200, 300, 400, 500,  2000, 3000, 4000, 5000,
+ *              -200,-300,-400,-500, -2000,-3000,-4000,-5000
+ *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    tmp0 = __lasx_xvmul_w(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the higher half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the higher half of the two-fold sign extension (signed
+ *               halfword to signed word), and the result is added to
+ *               the vector in_c, then stored to the out vector.
+ * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwl_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 6,1,3,0, 0,0,1,0
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    out  = __lasx_xvmul_w(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwh_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 0,0,0,0, 0,0,0,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    out  = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added saturately after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector adds the in_l vector saturately after the lower
+ *               half of the two-fold zero extension (unsigned byte to unsigned
+ *               halfword) and the results are stored to the out vector.
+ * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
+ *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *         out : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp1, out;
+    __m256i zero = {0};
+
+    tmp1 = __lasx_xvilvl_b(zero, in_l);
+    out  = __lasx_xvsadd_hu(in_h, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
+ * Arguments   : Inputs  - in    (input vector)
+ *                       - min   (min threshold)
+ *                       - max   (max threshold)
+ *               Outputs - in    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lasx_xvclip_h(in, min, max)
+ *          in : -8,2,280,249, -8,255,280,249, 4,4,4,4, 5,5,5,5
+ *         min : 1,1,1,1, 1,1,1,1, 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
+{
+    __m256i out;
+
+    out = __lasx_xvmax_h(min, in);
+    out = __lasx_xvmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed halfword elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in   (input vector)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : See out = __lasx_xvclamp255_w(in)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_h(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_h(in, 0);
+    out = __lasx_xvsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed word elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs - in   (input vector)
+ *               Output - out  (output vector with clipped elements)
+ *               Return Type - signed word
+ * Example     : out = __lasx_xvclamp255_w(in)
+ *          in : -8,255,280,249, -8,255,280,249
+ *         out :  0,255,255,249,  0,255,255,249
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_w(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_w(in, 0);
+    out = __lasx_xvsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_l_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,0,2,0, 0,0,0,0
+ *         idx : 0x02
+ *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x02);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_h_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,2,0,0, 0,0,0,0
+ *         idx : 0x09
+ *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ * =============================================================================
+ */
+
+static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x13);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with double word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *         _in0 : 1,2,3,4
+ *         _in1 : 1,2,3,4
+ *         _in2 : 1,2,3,4
+ *         _in3 : 1,2,3,4
+ *
+ *        _out0 : 1,1,1,1
+ *        _out1 : 2,2,2,2
+ *        _out2 : 3,3,3,3
+ *        _out3 : 4,4,4,4
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                               \
+    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                         \
+    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                        \
+    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                        \
+    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                        \
+    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                               \
+    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *         _in0 : 1,2,3,4,5,6,7,8
+ *         _in1 : 2,2,3,4,5,6,7,8
+ *         _in2 : 3,2,3,4,5,6,7,8
+ *         _in3 : 4,2,3,4,5,6,7,8
+ *         _in4 : 5,2,3,4,5,6,7,8
+ *         _in5 : 6,2,3,4,5,6,7,8
+ *         _in6 : 7,2,3,4,5,6,7,8
+ *         _in7 : 8,2,3,4,5,6,7,8
+ *
+ *        _out0 : 1,2,3,4,5,6,7,8
+ *        _out1 : 2,2,2,2,2,2,2,2
+ *        _out2 : 3,3,3,3,3,3,3,3
+ *        _out3 : 4,4,4,4,4,4,4,4
+ *        _out4 : 5,5,5,5,5,5,5,5
+ *        _out5 : 6,6,6,6,6,6,6,6
+ *        _out6 : 7,7,7,7,7,7,7,7
+ *        _out7 : 8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in3, _in1);                                          \
+    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvl_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in7, _in5);                                          \
+    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                               \
+    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                               \
+    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                               \
+    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                               \
+    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                               \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE16x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_w(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_w(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_w(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_w(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_w(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_w(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_w(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_w(_t7, _t5);                                             \
+    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                       \
+    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                       \
+    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                       \
+    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                       \
+    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                       \
+    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                       \
+    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                       \
+    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                       \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE16x8_H
+ *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
+ *       _out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+   {                                                                                 \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with halfword elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ *               Return Type - signed halfword
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)     \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+                                                                                    \
+    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                            \
+    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                            \
+    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                                          \
+    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                                          \
+    _out1 = __lasx_xvilvh_d(_out0, _out0);                                          \
+    _out3 = __lasx_xvilvh_d(_out2, _out2);                                          \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *                         (input 8x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x8 byte block)
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                          \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                          \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                    \
+    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                    \
+    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                                      \
+    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                                      \
+    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                                      \
+    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                                      \
+    _out1 = __lasx_xvbsrl_v(_out0, 8);                                              \
+    _out3 = __lasx_xvbsrl_v(_out2, 8);                                              \
+    _out5 = __lasx_xvbsrl_v(_out4, 8);                                              \
+    _out7 = __lasx_xvbsrl_v(_out6, 8);                                              \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with halfword elements in vectors.
+ * Arguments   : Inputs  - _in0, _in1, ~
+ *               Outputs - _out0, _out1, ~
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE8x8_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *        _in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *
+ *       _out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
+ *       _out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ *       _out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
+ *       _out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
+ *       _out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
+ *       _out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
+ *       _out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
+ *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in7, _in5);                                          \
+    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in7, _in5);                                          \
+    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in3, _in1);                                          \
+    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in3, _in1);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                                    \
+    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                                    \
+    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                                    \
+    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                                    \
+    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                                    \
+    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                                    \
+    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                                    \
+    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                                    \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_4
+ *               _out0 = _in0 + _in3;
+ *               _out1 = _in1 + _in2;
+ *               _out2 = _in1 - _in2;
+ *               _out3 = _in0 - _in3;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_b(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_b(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_b(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_b(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_h(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_h(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_h(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_h(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_w(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_w(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_w(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_w(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_d(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_d(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_d(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_8
+ *               _out0 = _in0 + _in7;
+ *               _out1 = _in1 + _in6;
+ *               _out2 = _in2 + _in5;
+ *               _out3 = _in3 + _in4;
+ *               _out4 = _in3 - _in4;
+ *               _out5 = _in2 - _in5;
+ *               _out6 = _in1 - _in6;
+ *               _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_b(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_b(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_b(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_b(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_b(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_b(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_b(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_b(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_h(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_h(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_h(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_h(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_h(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_h(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_h(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_h(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_w(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_w(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_w(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_w(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_w(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_w(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_w(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_w(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_d(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_d(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_d(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_d(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_d(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_d(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_d(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_d(_in0, _in7);                                           \
+}
+
+#endif //LASX
+
+#endif /* LOONGSON_INTRINSICS_H */
+#endif /* AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H */
+
diff --git a/libswscale/loongarch/input_lasx.c b/libswscale/loongarch/input_lasx.c
index 49b37cd0f6..bc18dd5c14 100644
--- a/libswscale/loongarch/input_lasx.c
+++ b/libswscale/loongarch/input_lasx.c
@@ -20,7 +20,7 @@
  */
 
 #include "swscale_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
                            int width, int32_t *rgb2yuv)
@@ -50,16 +50,15 @@ void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4]
         __m256i g_l, g_h, b_l, b_h, r_l, r_h;
         __m256i v_l, v_h, u_l, u_h, u_lh, v_lh;
 
-        _g   = LASX_LD((src0 + i));
-        _b   = LASX_LD((src1 + i));
-        _r   = LASX_LD((src2 + i));
-        LASX_UNPCK_L_WU_BU(_g, g_l);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_l, r_l);
+        DUP2_ARG2(__lasx_xvld, src0 + i, 0, src1 + i, 0, _g, _b);
+        _r   = __lasx_xvld(src2 + i, 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
         _g   = __lasx_xvpermi_d(_g, 0x01);
         _b   = __lasx_xvpermi_d(_b, 0x01);
         _r   = __lasx_xvpermi_d(_r, 0x01);
-        LASX_UNPCK_L_WU_BU(_g, g_h);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_h, r_h);
+        g_h = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_h, r_h);
         u_l  = __lasx_xvmadd_w(temp, ru, r_l);
         u_h  = __lasx_xvmadd_w(temp, ru, r_h);
         v_l  = __lasx_xvmadd_w(temp, rv, r_l);
@@ -76,7 +75,7 @@ void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4]
         u_h  = __lasx_xvsra_w(u_h, sra);
         v_l  = __lasx_xvsra_w(v_l, sra);
         v_h  = __lasx_xvsra_w(v_h, sra);
-        LASX_SHUF_B_2_128SV(u_h, u_l, v_h, v_l, mask, mask, u_lh, v_lh);
+        DUP2_ARG3(__lasx_xvshuf_b, u_h, u_l, mask, v_h, v_l, mask, u_lh, v_lh);
         u_lh = __lasx_xvpermi_d(u_lh, 0xD8);
         v_lh = __lasx_xvpermi_d(v_lh, 0xD8);
         __lasx_xvst(u_lh, (dstU + i), 0);
@@ -90,8 +89,8 @@ void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4]
         _g  = __lasx_xvldrepl_d((src0 + i), 0);
         _b  = __lasx_xvldrepl_d((src1 + i), 0);
         _r  = __lasx_xvldrepl_d((src2 + i), 0);
-        LASX_UNPCK_L_WU_BU(_g, g_l);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_l, r_l);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
         u_l = __lasx_xvmadd_w(temp, ru, r_l);
         v_l = __lasx_xvmadd_w(temp, rv, r_l);
         u_l = __lasx_xvmadd_w(u_l, gu, g_l);
@@ -100,7 +99,7 @@ void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4]
         v_l = __lasx_xvmadd_w(v_l, bv, b_l);
         u_l = __lasx_xvsra_w(u_l, sra);
         v_l = __lasx_xvsra_w(v_l, sra);
-        LASX_SHUF_B_2_128SV(u_l, u_l, v_l, v_l, mask, mask, u, v);
+        DUP2_ARG3(__lasx_xvshuf_b, u_l, u_l, mask, v_l, v_l, mask, u, v);
         __lasx_xvstelm_d(u, (dstU + i), 0, 0);
         __lasx_xvstelm_d(u, (dstU + i), 8, 2);
         __lasx_xvstelm_d(v, (dstV + i), 0, 0);
@@ -140,16 +139,15 @@ void planar_rgb_to_y_lasx(uint8_t *_dst, const uint8_t *src[4], int width,
         __m256i g_l, g_h, b_l, b_h, r_l, r_h;
         __m256i y_l, y_h, y_lh;
 
-        _g   = LASX_LD((src0 + i));
-        _b   = LASX_LD((src1 + i));
-        _r   = LASX_LD((src2 + i));
-        LASX_UNPCK_L_WU_BU(_g, g_l);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_l, r_l);
+        DUP2_ARG2(__lasx_xvld, src0 + i, 0, src1 + i, 0, _g, _b);
+        _r   = __lasx_xvld(src2 + i, 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
         _g   = __lasx_xvpermi_d(_g, 0x01);
         _b   = __lasx_xvpermi_d(_b, 0x01);
         _r   = __lasx_xvpermi_d(_r, 0x01);
-        LASX_UNPCK_L_WU_BU(_g, g_h);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_h, r_h);
+        g_h  = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu,_b, _r, b_h, r_h);
         y_l  = __lasx_xvmadd_w(temp, ry, r_l);
         y_h  = __lasx_xvmadd_w(temp, ry, r_h);
         y_l  = __lasx_xvmadd_w(y_l, gy, g_l);
@@ -158,7 +156,7 @@ void planar_rgb_to_y_lasx(uint8_t *_dst, const uint8_t *src[4], int width,
         y_h  = __lasx_xvmadd_w(y_h, by, b_h);
         y_l  = __lasx_xvsra_w(y_l, sra);
         y_h  = __lasx_xvsra_w(y_h, sra);
-        LASX_SHUF_B_128SV(y_h, y_l, mask, y_lh);
+        y_lh = __lasx_xvshuf_b(y_h, y_l, mask);
         y_lh = __lasx_xvpermi_d(y_lh, 0xD8);
         __lasx_xvst(y_lh, (dst + i), 0);
     }
@@ -170,13 +168,13 @@ void planar_rgb_to_y_lasx(uint8_t *_dst, const uint8_t *src[4], int width,
         _g  = __lasx_xvldrepl_d((src0 + i), 0);
         _b  = __lasx_xvldrepl_d((src1 + i), 0);
         _r  = __lasx_xvldrepl_d((src2 + i), 0);
-        LASX_UNPCK_L_WU_BU(_g, g_l);
-        LASX_UNPCK_L_WU_BU_2(_b, _r, b_l, r_l);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
         y_l = __lasx_xvmadd_w(temp, ry, r_l);
         y_l = __lasx_xvmadd_w(y_l, gy, g_l);
         y_l = __lasx_xvmadd_w(y_l, by, b_l);
         y_l = __lasx_xvsra_w(y_l, sra);
-        LASX_SHUF_B_128SV(y_l, y_l, mask, y);
+        y = __lasx_xvshuf_b(y_l, y_l, mask);
         __lasx_xvstelm_d(y, (dst + i), 0, 0);
         __lasx_xvstelm_d(y, (dst + i), 8, 2);
         i += 8;
diff --git a/libswscale/loongarch/output_lasx.c b/libswscale/loongarch/output_lasx.c
index b3fb1f5ce0..fbcf51cf0c 100644
--- a/libswscale/loongarch/output_lasx.c
+++ b/libswscale/loongarch/output_lasx.c
@@ -20,7 +20,7 @@
  */
 
 #include "swscale_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
                           const int16_t **src, uint8_t *dest, int dstW,
@@ -42,9 +42,8 @@ void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
     int val_2[8] = {dither4, dither5, dither6, dither7, dither4, dither5, dither6, dither7};
     int val_3[8] = {dither0, dither1, dither2, dither3, dither4, dither5, dither6, dither7};
 
-    val1 = LASX_LD(val_1);
-    val2 = LASX_LD(val_2);
-    val3 = LASX_LD(val_3);
+    DUP2_ARG2(__lasx_xvld, val_1, 0, val_2, 0, val1, val2);
+    val3 = __lasx_xvld(val_3, 0);
 
     for (i = 0; i < len; i += 16) {
         int j;
@@ -55,16 +54,16 @@ void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
         val_h = __lasx_xvslli_w(val2, 12);
 
         for (j = 0; j < filterSize; j++) {
-            src0  = LASX_LD(src[j]+ i);
+            src0  = __lasx_xvld(src[j]+ i, 0);
             filter0 = __lasx_xvldrepl_h((filter + j), 0);
-            LASX_MADDWL_W_H_128SV(val_l, src0, filter0, val_l);
-            LASX_MADDWH_W_H_128SV(val_h, src0, filter0, val_h);
+            val_l = __lasx_xvmaddwl_w_h(val_l, src0, filter0);
+            val_h = __lasx_xvmaddwh_w_h(val_h, src0, filter0);
         }
         val_l = __lasx_xvsrai_w(val_l, 19);
         val_h = __lasx_xvsrai_w(val_h, 19);
-        LASX_CLIP_W_0_255(val_l, val_l);
-        LASX_CLIP_W_0_255(val_h, val_h);
-        LASX_SHUF_B_128SV(val_h, val_l, mask, val_lh);
+        val_l = __lasx_xvclip255_w(val_l);
+        val_h = __lasx_xvclip255_w(val_h);
+        val_lh = __lasx_xvshuf_b(val_h, val_l, mask);
         __lasx_xvstelm_d(val_lh, (dest + i), 0, 0);
         __lasx_xvstelm_d(val_lh, (dest + i), 8, 2);
     }
@@ -76,15 +75,15 @@ void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
         val_l = __lasx_xvslli_w(val3, 12);
 
         for (j = 0; j < filterSize; j++) {
-            src0  = LASX_LD(src[j] + i);
+            src0  = __lasx_xvld(src[j] + i, 0);
             src0  = __lasx_xvpermi_d(src0, 0xD8);
             filter0 = __lasx_xvldrepl_h((filter + j), 0);
-            LASX_MADDWL_W_H_128SV(val_l, src0, filter0, val_l);
+            val_l = __lasx_xvmaddwl_w_h(val_l, src0, filter0);
         }
         val_l = __lasx_xvsrai_w(val_l, 19);
-        LASX_CLIP_W_0_255(val_l, val_l);
+        val_l = __lasx_xvclip255_w(val_l);
         val_h = __lasx_xvpermi_d(val_l, 0x4E);
-        LASX_SHUF_B_128SV(val_h, val_l, mask, val_l);
+        val_l = __lasx_xvshuf_b(val_h, val_l, mask);
         __lasx_xvstelm_d(val_l, (dest + i), 0, 0);
         i += 8;
     }
@@ -763,7 +762,10 @@ yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         v2_od  = yl1_ev;
         for (j = 0; j < lumFilterSize; j++) {
             temp    = __lasx_xvldrepl_h((lumFilter + j), 0);
-            LASX_LD_4((lumSrc[j] + count_lum), 16, l_src1, l_src2, l_src3, l_src4);
+            DUP4_ARG2(__lasx_xvld, lumSrc[j] + count_lum, 0, lumSrc[j] + count_lum,
+                      32, lumSrc[j] + count_lum, 64, lumSrc[j] + count_lum, 96, l_src1,
+                      l_src2, l_src3, l_src4);
+
             yl1_ev  = __lasx_xvmaddwev_w_h(yl1_ev, temp, l_src1);
             yl1_od  = __lasx_xvmaddwod_w_h(yl1_od, temp, l_src1);
             yh1_ev  = __lasx_xvmaddwev_w_h(yh1_ev, temp, l_src2);
@@ -774,8 +776,10 @@ yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
             yh2_od  = __lasx_xvmaddwod_w_h(yh2_od, temp, l_src4);
         }
         for (j = 0; j < chrFilterSize; j++) {
-            LASX_LD_2((chrUSrc[j] + count), 16, u_src1, u_src2);
-            LASX_LD_2((chrVSrc[j] + count), 16, v_src1, v_src2);
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrUSrc[j] + count, 32,
+                      u_src1, u_src2);
+            DUP2_ARG2(__lasx_xvld, chrVSrc[j] + count, 0, chrVSrc[j] + count, 32,
+                      v_src1, v_src2);
             temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
             u1_ev  = __lasx_xvmaddwev_w_h(u1_ev, temp, u_src1);
             u1_od  = __lasx_xvmaddwod_w_h(u1_od, temp, u_src1);
@@ -835,16 +839,16 @@ yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         v_od  = yl_ev;
         for (j = 0; j < lumFilterSize; j++) {
             temp   = __lasx_xvldrepl_h((lumFilter + j), 0);
-            l_src1 = LASX_LD((lumSrc[j] + count_lum));
-            l_src2 = LASX_LD((lumSrc[j] + count_lum + 16));
+            DUP2_ARG2(__lasx_xvld, lumSrc[j] + count_lum, 0, lumSrc[j] + count_lum,
+                      32, l_src1, l_src2);
             yl_ev  = __lasx_xvmaddwev_w_h(yl_ev, temp, l_src1);
             yl_od  = __lasx_xvmaddwod_w_h(yl_od, temp, l_src1);
             yh_ev  = __lasx_xvmaddwev_w_h(yh_ev, temp, l_src2);
             yh_od  = __lasx_xvmaddwod_w_h(yh_od, temp, l_src2);
         }
         for (j = 0; j < chrFilterSize; j++) {
-            u_src = LASX_LD((chrUSrc[j] + count));
-            v_src = LASX_LD((chrVSrc[j] + count));
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src, v_src);
             temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
             u_ev  = __lasx_xvmaddwev_w_h(u_ev, temp, u_src);
             u_od  = __lasx_xvmaddwod_w_h(u_od, temp, u_src);
@@ -881,13 +885,13 @@ yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         v    = y_ev;
         for (j = 0; j < lumFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
-            l_src = LASX_LD((lumSrc[j] + count_lum));
+            l_src = __lasx_xvld(lumSrc[j] + count_lum, 0);
             y_ev  = __lasx_xvmaddwev_w_h(y_ev, temp, l_src);
             y_od  = __lasx_xvmaddwod_w_h(y_od, temp, l_src);
         }
         for (j = 0; j < chrFilterSize; j++) {
-            u_src = LASX_LD((chrUSrc[j] + count));
-            v_src = LASX_LD((chrVSrc[j] + count));
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrVSrc[j] + count,
+                      0, u_src, v_src);
             temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
             u_src = __lasx_vext2xv_w_h(u_src);
             v_src = __lasx_vext2xv_w_h(v_src);
@@ -913,7 +917,7 @@ yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         uv   = y_ev;
         for (j = 0; j < lumFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
-            l_src = LASX_LD((lumSrc[j] + count_lum));
+            l_src = __lasx_xvld(lumSrc[j] + count_lum, 0);
             l_src = __lasx_vext2xv_w_h(l_src);
             y_ev  = __lasx_xvmaddwev_w_h(y_ev, temp, l_src);
         }
@@ -987,17 +991,14 @@ yuv2rgb_2_template_lasx(SwsContext *c, const int16_t *buf[2],
         __m256i y1_h, y1_l, y1, u1, v1;
         __m256i y_l, y_h, u, v;
 
-        y0   = LASX_LD(buf0 + i);
-        u0   = LASX_LD(ubuf0 + count);
-        v0   = LASX_LD(vbuf0 + count);
-        y1   = LASX_LD(buf1 + i);
-        u1   = LASX_LD(ubuf1 + count);
-        v1   = LASX_LD(vbuf1 + count);
-        LASX_UNPCK_L_W_H_2(y0, y1, y0_l, y1_l);
+        DUP4_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + count, 0, vbuf0 + count,
+                  0, buf1 + i, 0, y0, u0, v0, y1);
+        DUP2_ARG2(__lasx_xvld, ubuf1 + count, 0, vbuf1 + count, 0, u1, v1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, y0, y1, y0_l, y1_l);
         y0   = __lasx_xvpermi_d(y0, 0x4E);
         y1   = __lasx_xvpermi_d(y1, 0x4E);
-        LASX_UNPCK_L_W_H_2(y0, y1, y0_h, y1_h);
-        LASX_UNPCK_L_W_H_4(u0, u1, v0, v1, u0, u1, v0, v1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, y0, y1, y0_h, y1_h);
+        DUP4_ARG1(__lasx_vext2xv_w_h, u0, u1, v0, v1, u0, u1, v0, v1);
         y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
         y0_h = __lasx_xvmul_w(y0_h, v_yalpha1);
         u0   = __lasx_xvmul_w(u0, v_uvalpha1);
@@ -1021,14 +1022,14 @@ yuv2rgb_2_template_lasx(SwsContext *c, const int16_t *buf[2],
         __m256i y1_l, y1, u1, v1;
         __m256i y_l, u, v;
 
-        y0   = LASX_LD(buf0 + i);
+        y0   = __lasx_xvld(buf0 + i, 0);
         u0   = __lasx_xvldrepl_d((ubuf0 + count), 0);
         v0   = __lasx_xvldrepl_d((vbuf0 + count), 0);
-        y1   = LASX_LD(buf1 + i);
+        y1   = __lasx_xvld(buf1 + i, 0);
         u1   = __lasx_xvldrepl_d((ubuf1 + count), 0);
         v1   = __lasx_xvldrepl_d((vbuf1 + count), 0);
-        LASX_UNPCK_L_W_H_2(y0, y1, y0_l, y1_l);
-        LASX_UNPCK_L_W_H_4(u0, u1, v0, v1, u0, u1, v0, v1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, y0, y1, y0_l, y1_l);
+        DUP4_ARG1(__lasx_vext2xv_w_h, u0, u1, v0, v1, u0, u1, v0, v1);
         y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
         u0   = __lasx_xvmul_w(u0, v_uvalpha1);
         v0   = __lasx_xvmul_w(v0, v_uvalpha1);
@@ -1086,14 +1087,13 @@ yuv2rgb_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i src_y, src_u, src_v;
             __m256i y_h, y_l, u, v;
 
-            src_y = LASX_LD(buf0 + i);
-            src_u = LASX_LD(ubuf0 + count);
-            src_v = LASX_LD(vbuf0 + count);
-            LASX_ADDWL_W_H_128SV(src_y, bias_64, y_l);
-            LASX_ADDWH_W_H_128SV(src_y, bias_64, y_h);
+            DUP2_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + count, 0, src_y, src_u);
+            src_v = __lasx_xvld(vbuf0 + count, 0);
+            y_l = __lasx_xvaddwl_w_h(src_y, bias_64);
+            y_h = __lasx_xvaddwh_w_h(src_y, bias_64);
             src_u = __lasx_xvpermi_d(src_u, 0xD8);
             src_v = __lasx_xvpermi_d(src_v, 0xD8);
-            LASX_ADDWL_W_H_2_128SV(src_u, bias_64, src_v, bias_64, u, v);
+            DUP2_ARG2(__lasx_xvaddwl_w_h, src_u, bias_64, src_v, bias_64, u, v);
             y_l   = __lasx_xvsrai_w(y_l, 7);
             y_h   = __lasx_xvsrai_w(y_h, 7);
             u     = __lasx_xvsrai_w(u, 7);
@@ -1107,12 +1107,12 @@ yuv2rgb_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i src_y, src_u, src_v;
             __m256i y_l, u, v;
 
-            src_y = LASX_LD(buf0 + i);
+            src_y = __lasx_xvld(buf0 + i, 0);
             src_u = __lasx_xvldrepl_d((ubuf0 + count), 0);
             src_v = __lasx_xvldrepl_d((vbuf0 + count), 0);
             src_y = __lasx_xvpermi_d(src_y, 0xD8);
-            LASX_ADDWL_W_H_2_128SV(src_y, bias_64, src_u, bias_64, y_l, u);
-            LASX_ADDWL_W_H_128SV(src_v, bias_64, v);
+            DUP2_ARG2(__lasx_xvaddwl_w_h, src_y, bias_64, src_u, bias_64, y_l, u);
+            v = __lasx_xvaddwl_w_h(src_v, bias_64);
             y_l   = __lasx_xvsrai_w(y_l, 7);
             u     = __lasx_xvsrai_w(u, 7);
             v     = __lasx_xvsrai_w(v, 7);
@@ -1150,18 +1150,16 @@ yuv2rgb_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i src_y, src_u0, src_v0, src_u1, src_v1;
             __m256i y_h, y_l, u, v;
 
-            src_y  = LASX_LD(buf0 + i);
-            src_u0 = LASX_LD(ubuf0 + count);
-            src_v0 = LASX_LD(vbuf0 + count);
-            src_u1 = LASX_LD(ubuf1 + count);
-            src_v1 = LASX_LD(vbuf1 + count);
+            DUP4_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + count, 0, vbuf0 + count,
+                      0, ubuf1 + count, 0, src_y, src_u0, src_v0, src_u1);
+            src_v1 = __lasx_xvld(vbuf1 + count, 0);
             src_u0 = __lasx_xvpermi_d(src_u0, 0xD8);
             src_v0 = __lasx_xvpermi_d(src_v0, 0xD8);
             src_u1 = __lasx_xvpermi_d(src_u1, 0xD8);
             src_v1 = __lasx_xvpermi_d(src_v1, 0xD8);
-            LASX_ADDWL_W_H_128SV(src_y, bias_64, y_l);
-            LASX_ADDWH_W_H_128SV(src_y, bias_64, y_h);
-            LASX_ADDWL_W_H_2_128SV(src_u0, src_u1, src_v0, src_v1, u, v);
+            y_l = __lasx_xvaddwl_w_h(src_y, bias_64);
+            y_h =  __lasx_xvaddwh_w_h(src_y, bias_64);
+            DUP2_ARG2(__lasx_xvaddwl_w_h, src_u0, src_u1, src_v0, src_v1, u, v);
             u      = __lasx_xvadd_w(u, bias_128);
             v      = __lasx_xvadd_w(v, bias_128);
             y_l    = __lasx_xvsrai_w(y_l, 7);
@@ -1177,15 +1175,15 @@ yuv2rgb_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i src_y, src_u0, src_v0, src_u1, src_v1;
             __m256i y_l, u, v;
 
-            src_y  = LASX_LD(buf0 + i);
+            src_y  = __lasx_xvld(buf0 + i, 0);
             src_u0 = __lasx_xvldrepl_d((ubuf0 + count), 0);
             src_v0 = __lasx_xvldrepl_d((vbuf0 + count), 0);
             src_u1 = __lasx_xvldrepl_d((ubuf1 + count), 0);
             src_v1 = __lasx_xvldrepl_d((vbuf1 + count), 0);
 
             src_y  = __lasx_xvpermi_d(src_y, 0xD8);
-            LASX_ADDWL_W_H_128SV(src_y, bias_64, y_l);
-            LASX_ADDWL_W_H_2_128SV(src_u0, src_u1, src_v0, src_v1, u, v);
+            y_l = __lasx_xvaddwl_w_h(src_y, bias_64);
+            DUP2_ARG2(__lasx_xvaddwl_w_h, src_u0, src_u1, src_v0, src_v1, u, v);
             u      = __lasx_xvadd_w(u, bias_128);
             v      = __lasx_xvadd_w(v, bias_128);
             y_l    = __lasx_xvsrai_w(y_l, 7);
@@ -1582,16 +1580,15 @@ yuv2rgb_full_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         u_l = u_h = v_l = v_h = __lasx_xvreplgr2vr_w(tempc);
         for (j = 0; j < lumFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
-            l_src = LASX_LD((lumSrc[j] + i));
-            LASX_MADDWL_W_H_128SV(y_l, l_src, temp, y_l);
-            LASX_MADDWH_W_H_128SV(y_h, l_src, temp, y_h);
+            l_src = __lasx_xvld(lumSrc[j] + i, 0);
+            y_l = __lasx_xvmaddwl_w_h(y_l, l_src, temp);
+            y_h = __lasx_xvmaddwh_w_h(y_h, l_src, temp);
         }
         for (j = 0; j < chrFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
-            u_src = LASX_LD((chrUSrc[j] + i));
-            v_src = LASX_LD((chrVSrc[j] + i));
-            LASX_MADDWL_W_H_2_128SV(u_l, u_src, temp, v_l, v_src, temp, u_l, v_l);
-            LASX_MADDWH_W_H_2_128SV(u_h, u_src, temp, v_h, v_src, temp, u_h, v_h);
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + i, 0, chrVSrc[j] + i, 0, u_src, v_src);
+            DUP2_ARG3(__lasx_xvmaddwl_w_h, u_l, u_src, temp, v_l, v_src, temp, u_l, v_l);
+            DUP2_ARG3(__lasx_xvmaddwh_w_h, u_h, u_src, temp, v_h, v_src, temp, u_h, v_h);
         }
         y_l = __lasx_xvsrai_w(y_l, 10);
         y_h = __lasx_xvsrai_w(y_h, 10);
@@ -1610,9 +1607,9 @@ yuv2rgb_full_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
             a_l = a_h = __lasx_xvreplgr2vr_w(a_temp);
             for (j = 0; j < lumFilterSize; j++) {
                 temp  = __lasx_xvldrepl_h(lumFilter + j, 0);
-                a_src = LASX_LD((alpSrc[j] + i));
-                LASX_MADDWL_W_H_128SV(a_l, a_src, temp, a_l);
-                LASX_MADDWH_W_H_128SV(a_h, a_src, temp, a_h);
+                a_src = __lasx_xvld(alpSrc[j] + i, 0);
+                a_l = __lasx_xvmaddwl_w_h(a_l, a_src, temp);
+                a_h = __lasx_xvmaddwh_w_h(a_h, a_src, temp);
             }
             a_h = __lasx_xvsrai_w(a_h, 19);
             a_l = __lasx_xvsrai_w(a_l, 19);
@@ -1644,17 +1641,16 @@ yuv2rgb_full_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
         u_l = v_l = __lasx_xvreplgr2vr_w(tempc);
         for (j = 0; j < lumFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
-            l_src = LASX_LD((lumSrc[j] + i));
+            l_src = __lasx_xvld(lumSrc[j] + i, 0);
             l_src = __lasx_xvpermi_d(l_src, 0xD8);
-            LASX_MADDWL_W_H_128SV(y_l, l_src, temp, y_l);
+            y_l = __lasx_xvmaddwl_w_h(y_l, l_src, temp);
         }
         for (j = 0; j < chrFilterSize; j++) {
             temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
-            u_src = LASX_LD((chrUSrc[j] + i));
-            v_src = LASX_LD((chrVSrc[j] + i));
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + i, 0, chrVSrc[j] + i, 0, u_src, v_src);
             u_src = __lasx_xvpermi_d(u_src, 0xD8);
             v_src = __lasx_xvpermi_d(v_src, 0xD8);
-            LASX_MADDWL_W_H_2_128SV(u_l, u_src, temp, v_l, v_src, temp, u_l, v_l);
+            DUP2_ARG3(__lasx_xvmaddwl_w_h, u_l, u_src, temp, v_l, v_src, temp, u_l, v_l);
         }
         y_l = __lasx_xvsrai_w(y_l, 10);
         u_l = __lasx_xvsrai_w(u_l, 10);
@@ -1668,9 +1664,9 @@ yuv2rgb_full_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
             a_l = __lasx_xvreplgr2vr_w(a_temp);
             for (j = 0; j < lumFilterSize; j++) {
                 temp  = __lasx_xvldrepl_h(lumFilter + j, 0);
-                a_src = LASX_LD((alpSrc[j] + i));
+                a_src = __lasx_xvld(alpSrc[j] + i, 0);
                 a_src = __lasx_xvpermi_d(a_src, 0xD8);
-                LASX_MADDWL_W_H_128SV(a_l, a_src, temp, a_l);
+                a_l =  __lasx_xvmaddwl_w_h(a_l, a_src, temp);
             }
             a_l = __lasx_xvsrai_w(a_l, 19);
             WRITE_FULL_L_A(R_l, G_l, B_l, a_l, c, dest, i, R, A, G, B,
@@ -1768,22 +1764,19 @@ yuv2rgb_full_2_template_lasx(SwsContext *c, const int16_t *buf[2],
         __m256i y_l, y_h, v_l, v_h, u_l, u_h;
         __m256i R_l, R_h, G_l, G_h, B_l, B_h;
 
-        b0   = LASX_LD((buf0 + i));
-        b1   = LASX_LD((buf1 + i));
-        ub0  = LASX_LD((ubuf0 + i));
-        ub1  = LASX_LD((ubuf1 + i));
-        vb0  = LASX_LD((vbuf0 + i));
-        vb1  = LASX_LD((vbuf1 + i));
-        LASX_UNPCK_L_W_H_2(b0, b1, y0_l, y1_l);
-        LASX_UNPCK_L_W_H_4(ub0, ub1, vb0, vb1, u0_l, u1_l, v0_l, v1_l);
+        DUP4_ARG2(__lasx_xvld, buf0 + i, 0, buf1 + i, 0, ubuf0 + i, 0, ubuf1 + i, 0,
+                  b0, b1, ub0, ub1);
+        DUP2_ARG2(__lasx_xvld, vbuf0 + i, 0, vbuf1 + i, 0, vb0 , vb1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, b0, b1, y0_l, y1_l);
+        DUP4_ARG1(__lasx_vext2xv_w_h, ub0, ub1, vb0, vb1, u0_l, u1_l, v0_l, v1_l);
         b0   = __lasx_xvpermi_d(b0, 0x4E);
         b1   = __lasx_xvpermi_d(b1, 0x4E);
         ub0  = __lasx_xvpermi_d(ub0, 0x4E);
         ub1  = __lasx_xvpermi_d(ub1, 0x4E);
         vb0  = __lasx_xvpermi_d(vb0, 0x4E);
         vb1  = __lasx_xvpermi_d(vb1, 0x4E);
-        LASX_UNPCK_L_W_H_2(b0, b1, y0_h, y1_h);
-        LASX_UNPCK_L_W_H_4(ub0, ub1, vb0, vb1, u0_h, u1_h, v0_h, v1_h);
+        DUP2_ARG1(__lasx_vext2xv_w_h, b0, b1, y0_h, y1_h);
+        DUP4_ARG1(__lasx_vext2xv_w_h, ub0, ub1, vb0, vb1, u0_h, u1_h, v0_h, v1_h);
         y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
         y0_h = __lasx_xvmul_w(y0_h, v_yalpha1);
         u0_l = __lasx_xvmul_w(u0_l, v_uvalpha1);
@@ -1815,12 +1808,11 @@ yuv2rgb_full_2_template_lasx(SwsContext *c, const int16_t *buf[2],
             __m256i a0, a1, a0_l, a0_h;
             __m256i a_l, a_h, a1_l, a1_h;
 
-            a0  = LASX_LD((abuf0 + i));
-            a1  = LASX_LD((abuf1 + i));
-            LASX_UNPCK_L_W_H_2(a0, a1, a0_l, a1_l);
+            DUP2_ARG2(__lasx_xvld, abuf0 + i, 0, abuf1 + i, 0, a0, a1);
+            DUP2_ARG1(__lasx_vext2xv_w_h, a0, a1, a0_l, a1_l);
             a0  = __lasx_xvpermi_d(a0, 0x4E);
             a1  = __lasx_xvpermi_d(a1, 0x4E);
-            LASX_UNPCK_L_W_H_2(a0, a1, a0_h, a1_h);
+            DUP2_ARG1(__lasx_vext2xv_w_h, a0, a1, a0_h, a1_h);
             a_l = __lasx_xvmadd_w(a_bias, a0_l, v_yalpha1);
             a_h = __lasx_xvmadd_w(a_bias, a0_h, v_yalpha1);
             a_l = __lasx_xvmadd_w(a_l, v_yalpha, a1_l);
@@ -1853,14 +1845,11 @@ yuv2rgb_full_2_template_lasx(SwsContext *c, const int16_t *buf[2],
         __m256i y_l, u_l, v_l;
         __m256i R_l, G_l, B_l;
 
-        b0   = LASX_LD((buf0 + i));
-        b1   = LASX_LD((buf1 + i));
-        ub0  = LASX_LD((ubuf0 + i));
-        ub1  = LASX_LD((ubuf1 + i));
-        vb0  = LASX_LD((vbuf0 + i));
-        vb1  = LASX_LD((vbuf1 + i));
-        LASX_UNPCK_L_W_H_2(b0, b1, y0_l, y1_l);
-        LASX_UNPCK_L_W_H_4(ub0, ub1, vb0, vb1, u0_l, u1_l, v0_l, v1_l);
+        DUP4_ARG2(__lasx_xvld, buf0 + i, 0, buf1 + i, 0, ubuf0 + i, 0, ubuf1 + i, 0,
+                  b0, b1, ub0, ub1);
+        DUP2_ARG2(__lasx_xvld, vbuf0 + i, 0, vbuf1 + i, 0, vb0, vb1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, b0, b1, y0_l, y1_l);
+        DUP4_ARG1(__lasx_vext2xv_w_h, ub0, ub1, vb0, vb1, u0_l, u1_l, v0_l, v1_l);
         y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
         u0_l = __lasx_xvmul_w(u0_l, v_uvalpha1);
         v0_l = __lasx_xvmul_w(v0_l, v_uvalpha1);
@@ -1879,9 +1868,8 @@ yuv2rgb_full_2_template_lasx(SwsContext *c, const int16_t *buf[2],
             __m256i a0, a1, a0_l;
             __m256i a_l, a1_l;
 
-            a0  = LASX_LD((abuf0 + i));
-            a1  = LASX_LD((abuf1 + i));
-            LASX_UNPCK_L_W_H_2(a0, a1, a0_l, a1_l);
+            DUP2_ARG2(__lasx_xvld, abuf0 + i, 0, abuf1 + i, 0, a0, a1);
+            DUP2_ARG1(__lasx_vext2xv_w_h, a0, a1, a0_l, a1_l);
             a_l = __lasx_xvmadd_w(a_bias, a0_l, v_yalpha1);
             a_l = __lasx_xvmadd_w(a_l, v_yalpha, a1_l);
             a_l = __lasx_xvsrai_w(a_l, 19);
@@ -1953,16 +1941,15 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i y_l, y_h, u_l, u_h, v_l, v_h;
             __m256i R_l, R_h, G_l, G_h, B_l, B_h;
 
-            b   = LASX_LD((buf0 + i));
-            ub  = LASX_LD((ubuf0 + i));
-            vb  = LASX_LD((vbuf0 + i));
-            LASX_UNPCK_L_W_H(b, y_l);
-            LASX_UNPCK_L_W_H_2(ub, vb, ub_l, vb_l);
+            DUP2_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + i, 0, b, ub);
+            vb  = __lasx_xvld(vbuf0 + i, 0);
+            y_l = __lasx_vext2xv_w_h(b);
+            DUP2_ARG1(__lasx_vext2xv_w_h, ub, vb, ub_l, vb_l);
             b   = __lasx_xvpermi_d(b, 0x4E);
             ub  = __lasx_xvpermi_d(ub, 0x4E);
             vb  = __lasx_xvpermi_d(vb, 0x4E);
-            LASX_UNPCK_L_W_H(b, y_h);
-            LASX_UNPCK_L_W_H_2(ub, vb, ub_h, vb_h);
+            y_h = __lasx_vext2xv_w_h(b);
+            DUP2_ARG1(__lasx_vext2xv_w_h, ub, vb, ub_h, vb_h);
             y_l = __lasx_xvslli_w(y_l, 2);
             y_h = __lasx_xvslli_w(y_h, 2);
             u_l = __lasx_xvsub_w(ub_l, uv);
@@ -1982,11 +1969,11 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
                 __m256i a_src;
                 __m256i a_l, a_h;
 
-                a_src = LASX_LD((abuf0 + i));
+                a_src = __lasx_xvld(abuf0 + i, 0);
                 a_src = __lasx_xvpermi_d(a_src, 0xD8);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_l);
+                a_l = __lasx_xvaddw_h_h_bu(bias, a_src);
                 a_src = __lasx_xvpermi_d(a_src, 0xB1);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_h);
+                a_h = __lasx_xvaddw_w_w_h(bias, a_src);
                 a_l   = __lasx_xvsrai_w(a_l, 7);
                 a_h   = __lasx_xvsrai_w(a_h, 7);
                 WRITE_FULL_L_A(R_l, G_l, B_l, a_l, c, dest, i, R, A, G, B,
@@ -2013,11 +2000,10 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i y_l, u_l, v_l;
             __m256i R_l, G_l, B_l;
 
-            b   = LASX_LD((buf0 + i));
-            ub  = LASX_LD((ubuf0 + i));
-            vb  = LASX_LD((vbuf0 + i));
-            LASX_UNPCK_L_W_H(b, y_l);
-            LASX_UNPCK_L_W_H_2(ub, vb, ub_l, vb_l);
+            DUP2_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + i, 0, b, ub);
+            vb  = __lasx_xvld(vbuf0 + i, 0);
+            y_l = __lasx_vext2xv_w_h(b);
+            DUP2_ARG1(__lasx_vext2xv_w_h, ub, vb, ub_l, vb_l);
             y_l = __lasx_xvslli_w(y_l, 2);
             u_l = __lasx_xvsub_w(ub_l, uv);
             v_l = __lasx_xvsub_w(vb_l, uv);
@@ -2029,9 +2015,9 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
             if(hasAlpha) {
                 __m256i a_src, a_l;
 
-                a_src = LASX_LD((abuf0 + i));
+                a_src = __lasx_xvld(abuf0 + i, 0);
                 a_src = __lasx_xvpermi_d(a_src, 0xD8);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_l);
+                a_l = __lasx_xvaddw_w_w_h(bias, a_src);
                 a_l   = __lasx_xvsrai_w(a_l, 7);
                 WRITE_FULL_L_A(R_l, G_l, B_l, a_l, c, dest, i, R, A, G, B,
                                y, target, hasAlpha, err);
@@ -2075,22 +2061,21 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i y_l, y_h, u_l, u_h, v_l, v_h;
             __m256i R_l, R_h, G_l, G_h, B_l, B_h;
 
-            b   = LASX_LD((buf0 + i));
-            ub0 = LASX_LD((ubuf0 + i));
-            vb0 = LASX_LD((vbuf0 + i));
-            ub1 = LASX_LD((ubuf1 + i));
-            vb1 = LASX_LD((vbuf1 + i));
-            LASX_UNPCK_L_W_H(b, y_l);
+            DUP4_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + i, 0, vbuf0 + i, 0, ubuf1 + i, 0,
+                      b, ub0, vb0, ub1);
+            vb1 = __lasx_xvld(vbuf1 + i, 0);
+            y_l = __lasx_vext2xv_w_h(b);
             b   = __lasx_xvpermi_d(b, 0X4E);
-            LASX_UNPCK_L_W_H(b, y_h);
+            y_h = __lasx_vext2xv_w_h(b);
             y_l = __lasx_xvslli_w(y_l, 2);
             y_h = __lasx_xvslli_w(y_h, 2);
             ub0 = __lasx_xvpermi_d(ub0, 0xD8);
             vb0 = __lasx_xvpermi_d(vb0, 0xD8);
             ub1 = __lasx_xvpermi_d(ub1, 0xD8);
             vb1 = __lasx_xvpermi_d(vb1, 0xD8);
-            LASX_ADDWL_W_H_2_128SV(ub0, ub1, vb0, vb1, u_l, v_l);
-            LASX_ADDWH_W_H_2_128SV(ub0, ub1, vb0, vb1, u_h, v_h);
+
+            DUP2_ARG2(__lasx_xvaddwl_w_h, ub0, ub1, vb0, vb1, u_l, v_l);
+            DUP2_ARG2(__lasx_xvaddwh_w_h, ub0, ub1, vb0, vb1, u_h, v_h);
             u_l = __lasx_xvsub_w(u_l, uv);
             u_h = __lasx_xvsub_w(u_h, uv);
             v_l = __lasx_xvsub_w(v_l, uv);
@@ -2108,11 +2093,11 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
                 __m256i a_src;
                 __m256i a_l, a_h;
 
-                a_src = LASX_LD((abuf0 + i));
+                a_src = __lasx_xvld(abuf0 + i, 0);
                 a_src = __lasx_xvpermi_d(a_src, 0xD8);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_l);
+                a_l = __lasx_xvaddw_w_w_h(bias, a_src);
                 a_src = __lasx_xvpermi_d(a_src, 0xB1);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_h);
+                a_h = __lasx_xvaddw_w_w_h(bias, a_src);
                 a_l   = __lasx_xvsrai_w(a_l, 7);
                 a_h   = __lasx_xvsrai_w(a_h, 7);
                 WRITE_FULL_L_A(R_l, G_l, B_l, a_l, c, dest, i, R, A, G, B,
@@ -2139,18 +2124,16 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
             __m256i y_l, u_l, v_l;
             __m256i R_l, G_l, B_l;
 
-            b   = LASX_LD((buf0 + i));
-            ub0 = LASX_LD((ubuf0 + i));
-            vb0 = LASX_LD((vbuf0 + i));
-            ub1 = LASX_LD((ubuf1 + i));
-            vb1 = LASX_LD((vbuf1 + i));
-            LASX_UNPCK_L_W_H(b, y_l);
+            DUP4_ARG2(__lasx_xvld, buf0 + i, 0, ubuf0 + i, 0, vbuf0 + i, 0, ubuf1 + i, 0,
+                      b, ub0, vb0, ub1);
+            vb1 = __lasx_xvld(vbuf1 + i, 0);
+            y_l = __lasx_vext2xv_w_h(b);
             y_l = __lasx_xvslli_w(y_l, 2);
             ub0 = __lasx_xvpermi_d(ub0, 0xD8);
             vb0 = __lasx_xvpermi_d(vb0, 0xD8);
             ub1 = __lasx_xvpermi_d(ub1, 0xD8);
             vb1 = __lasx_xvpermi_d(vb1, 0xD8);
-            LASX_ADDWL_W_H_2_128SV(ub0, ub1, vb0, vb1, u_l, v_l);
+            DUP2_ARG2(__lasx_xvaddwl_w_h, ub0, ub1, vb0, vb1, u_l, v_l);
             u_l = __lasx_xvsub_w(u_l, uv);
             v_l = __lasx_xvsub_w(v_l, uv);
             u_l = __lasx_xvslli_w(u_l, 1);
@@ -2162,9 +2145,9 @@ yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
                 __m256i a_src;
                 __m256i a_l;
 
-                a_src = LASX_LD((abuf0 + i));
+                a_src = __lasx_xvld(abuf0 + i, 0);
                 a_src = __lasx_xvpermi_d(a_src, 0xD8);
-                LASX_ADDW_W_W_H_128SV(bias, a_src, a_l);
+                a_l =  __lasx_xvaddw_w_w_h(bias, a_src);
                 a_l   = __lasx_xvsrai_w(a_l, 7);
                 WRITE_FULL_L_A(R_l, G_l, B_l, a_l, c, dest, i, R, A, G, B,
                                y, target, hasAlpha, err);
diff --git a/libswscale/loongarch/rgb2rgb_lasx.c b/libswscale/loongarch/rgb2rgb_lasx.c
index d04cecd57f..78b5a03bb4 100644
--- a/libswscale/loongarch/rgb2rgb_lasx.c
+++ b/libswscale/loongarch/rgb2rgb_lasx.c
@@ -21,7 +21,7 @@
  */
 
 #include "swscale_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 void ff_interleave_bytes_lasx(const uint8_t *src1, const uint8_t *src2,
                               uint8_t *dest, int width, int height,
@@ -35,12 +35,11 @@ void ff_interleave_bytes_lasx(const uint8_t *src1, const uint8_t *src2,
         __m256i src_1, src_2, dst;
 
         for (w = 0; w < len; w += 16) {
-            src_1 = LASX_LD(src1 + w);
-            src_2 = LASX_LD(src2 + w);
+            DUP2_ARG2(__lasx_xvld, src1 + w, 0, src2 + w, 0, src_1, src_2);
             src_1 = __lasx_xvpermi_d(src_1, 0xD8);
             src_2 = __lasx_xvpermi_d(src_2, 0xD8);
             dst   = __lasx_xvilvl_b(src_2, src_1);
-            LASX_ST(dst, dest + index);
+            __lasx_xvst(dst, dest + index, 0);
             index  += 32;
         }
         for (w = 0; w < width; w++) {
diff --git a/libswscale/loongarch/swscale_lasx.c b/libswscale/loongarch/swscale_lasx.c
index b3dc96681e..fb03d1291f 100644
--- a/libswscale/loongarch/swscale_lasx.c
+++ b/libswscale/loongarch/swscale_lasx.c
@@ -20,262 +20,271 @@
  */
 
 #include "swscale_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 #include "libavutil/intreadwrite.h"
 
-#define SCALE_8_16(_sh)                                           \
-{                                                                 \
-    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);           \
-    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);           \
-    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);           \
-    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);           \
-    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);           \
-    src8    = __lasx_xvldrepl_d(src + filterPos[8], 0);           \
-    src9    = __lasx_xvldrepl_d(src + filterPos[9], 0);           \
-    src10   = __lasx_xvldrepl_d(src + filterPos[10], 0);          \
-    src11   = __lasx_xvldrepl_d(src + filterPos[11], 0);          \
-    src12   = __lasx_xvldrepl_d(src + filterPos[12], 0);          \
-    src13   = __lasx_xvldrepl_d(src + filterPos[13], 0);          \
-    src14   = __lasx_xvldrepl_d(src + filterPos[14], 0);          \
-    src15   = __lasx_xvldrepl_d(src + filterPos[15], 0);          \
-    LASX_LD_8(filter, 16, filter0, filter1, filter2, filter3,     \
-              filter4, filter5, filter6, filter7);                \
-    LASX_PCKEV_D_4_128SV(src1, src0, src3, src2, src5, src4,      \
-                         src7, src6, src0, src2, src4, src6);     \
-    LASX_PCKEV_D_4_128SV(src9, src8, src11, src10, src13, src12,  \
-                         src15, src14, src8, src10, src12, src14);\
-    LASX_UNPCK_L_HU_BU_8(src0, src2, src4, src6, src8, src10,     \
-                         src12, src14, src0, src2, src4, src6,    \
-                         src8, src10, src12, src14);              \
-    LASX_DP2_W_H_8(filter0, src0, filter1, src2, filter2, src4,   \
-                   filter3, src6, filter4, src8, filter5, src10,  \
-                   filter6, src12, filter7, src14, src0, src1,    \
-                   src2, src3, src4, src5, src6, src7);           \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    src2 = __lasx_xvhaddw_d_w(src2, src2);                        \
-    src3 = __lasx_xvhaddw_d_w(src3, src3);                        \
-    src4 = __lasx_xvhaddw_d_w(src4, src4);                        \
-    src5 = __lasx_xvhaddw_d_w(src5, src5);                        \
-    src6 = __lasx_xvhaddw_d_w(src6, src6);                        \
-    src7 = __lasx_xvhaddw_d_w(src7, src7);                        \
-    LASX_PCKEV_W_4_128SV(src1, src0, src3, src2, src5, src4,      \
-                         src7, src6, src0, src1, src2, src3);     \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    src2 = __lasx_xvhaddw_d_w(src2, src2);                        \
-    src3 = __lasx_xvhaddw_d_w(src3, src3);                        \
-    LASX_PCKEV_W_2_128SV(src1, src0, src3, src2, src0, src1);     \
-    LASX_SRAI_W_2(src0, src1, src0, src1, _sh);                   \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    src1 = __lasx_xvmin_w(src1, vmax);                            \
-    src0 = __lasx_xvperm_w(src0, shuf);                           \
-    src1 = __lasx_xvperm_w(src1, shuf);                           \
-    LASX_PCKEV_H(src1, src0, src0);                               \
-    LASX_ST(src0, dst);                                           \
-    filterPos += 16;                                              \
-    filter    += 128;                                             \
-    dst       += 16;                                              \
+#define SCALE_8_16(_sh)                                               \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);               \
+    src8    = __lasx_xvldrepl_d(src + filterPos[8], 0);               \
+    src9    = __lasx_xvldrepl_d(src + filterPos[9], 0);               \
+    src10   = __lasx_xvldrepl_d(src + filterPos[10], 0);              \
+    src11   = __lasx_xvldrepl_d(src + filterPos[11], 0);              \
+    src12   = __lasx_xvldrepl_d(src + filterPos[12], 0);              \
+    src13   = __lasx_xvldrepl_d(src + filterPos[13], 0);              \
+    src14   = __lasx_xvldrepl_d(src + filterPos[14], 0);              \
+    src15   = __lasx_xvldrepl_d(src + filterPos[15], 0);              \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32, 0, \
+              filter + 48, 0, filter0, filter1, filter2, filter3);    \
+    DUP4_ARG2(__lasx_xvld, filter + 64, 0, filter + 80, 0,            \
+              filter + 96, 0, filter + 112, 0, filter4,               \
+              filter5, filter6, filter7);                             \
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src2, src4, src6);        \
+    DUP4_ARG2(__lasx_xvpickev_d, src9, src8, src11, src10,            \
+              src13, src12, src15, src14, src8, src10, src12, src14); \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src4, src6,           \
+              src0, src2, src4, src6);                                \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src8, src10, src12,               \
+              src14, src8, src10, src12, src14);                      \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2,         \
+              filter2, src4, filter3, src6, src0, src1, src2, src3);  \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter4, src8, filter5, src10,        \
+              filter6, src12, filter7, src14, src4, src5, src6, src7);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    src4 = __lasx_xvhaddw_d_w(src4, src4);                            \
+    src5 = __lasx_xvhaddw_d_w(src5, src5);                            \
+    src6 = __lasx_xvhaddw_d_w(src6, src6);                            \
+    src7 = __lasx_xvhaddw_d_w(src7, src7);                            \
+    DUP4_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src1, src2, src3);        \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    DUP2_ARG2(__lasx_xvsrai_w, src0, _sh, src1, _sh, src0, src1);     \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src1 = __lasx_xvmin_w(src1, vmax);                                \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
+    src1 = __lasx_xvperm_w(src1, shuf);                               \
+    src0 = __lasx_xvpickev_h(src1, src0);                             \
+    src0 = __lasx_xvpermi_d(src0, 0xd8);                              \
+    __lasx_xvst(src0, dst, 0);                                        \
+    filterPos += 16;                                                  \
+    filter    += 128;                                                 \
+    dst       += 16;                                                  \
 }
 
-#define SCALE_8_8(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);           \
-    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);           \
-    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);           \
-    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);           \
-    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);           \
-    LASX_LD_4(filter, 16, filter0, filter1, filter2, filter3);    \
-    LASX_PCKEV_D_4_128SV(src1, src0, src3, src2, src5, src4,      \
-                         src7, src6, src0, src2, src4, src6);     \
-    LASX_UNPCK_L_HU_BU_4(src0, src2, src4, src6,                  \
-                         src0, src2, src4, src6);                 \
-    LASX_DP2_W_H_4(filter0, src0, filter1, src2, filter2, src4,   \
-                   filter3, src6, src0, src1, src2, src3);        \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    src2 = __lasx_xvhaddw_d_w(src2, src2);                        \
-    src3 = __lasx_xvhaddw_d_w(src3, src3);                        \
-    LASX_PCKEV_W_2_128SV(src1, src0, src3, src2, src0, src1);     \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    LASX_PCKEV_W_128SV(src1, src0, src0);                         \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    src0 = __lasx_xvperm_w(src0, shuf);                           \
+#define SCALE_8_8(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);               \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32,    \
+              0, filter + 48, 0, filter0, filter1, filter2, filter3); \
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src2, src4, src6);        \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src4, src6,           \
+              src0, src2, src4, src6);                                \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2,         \
+              filter2, src4, filter3, src6, src0, src1, src2,src3);   \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src0 = __lasx_xvpickev_w(src1, src0);                             \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
 }
 
-#define SCALE_8_4(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);           \
-    LASX_LD_2(filter, 16, filter0, filter1);                      \
-    LASX_PCKEV_D_2_128SV(src1, src0, src3, src2, src0, src2);     \
-    LASX_UNPCK_L_HU_BU_2(src0, src2, src0, src2);                 \
-    LASX_DP2_W_H_2(filter0, src0, filter1, src2, src0, src1);     \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    LASX_PCKEV_W_128SV(src1, src0, src0);                         \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    LASX_PCKEV_W_128SV(src0, src0, src0);                         \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    src0 = __lasx_xvperm_w(src0, shuf);                           \
+#define SCALE_8_4(_sh)                                                    \
+{                                                                         \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);                   \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);                   \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);                   \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);                   \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);  \
+    DUP2_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src0, src2);     \
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src0, src2);              \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2, src0, src1);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                                \
+    src0 = __lasx_xvpickev_w(src1, src0);                                 \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src0 = __lasx_xvpickev_w(src0, src0);                                 \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                    \
+    src0 = __lasx_xvmin_w(src0, vmax);                                    \
+    src0 = __lasx_xvperm_w(src0, shuf);                                   \
 }
 
-#define SCALE_8_2(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);           \
-    filter0 = LASX_LD(filter);                                    \
-    LASX_PCKEV_D_128SV(src1, src0, src0);                         \
-    LASX_UNPCK_L_HU_BU(src0, src0);                               \
-    LASX_DP2_W_H(filter0, src0, src0);                            \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src0 = __lasx_xvhaddw_q_d(src0, src0);                        \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                       \
-    dst[1] = __lasx_xvpickve2gr_w(src0, 4);                       \
-    filterPos += 2;                                               \
-    filter    += 16;                                              \
-    dst       += 2;                                               \
+#define SCALE_8_2(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    src0 = __lasx_xvpickev_d(src1, src0);                             \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvhaddw_q_d(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                           \
+    dst[1] = __lasx_xvpickve2gr_w(src0, 4);                           \
+    filterPos += 2;                                                   \
+    filter    += 16;                                                  \
+    dst       += 2;                                                   \
 }
 
-#define SCALE_4_16(_sh)                                           \
-{                                                                 \
-    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);           \
-    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);           \
-    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);           \
-    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);           \
-    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);           \
-    src8    = __lasx_xvldrepl_w(src + filterPos[8], 0);           \
-    src9    = __lasx_xvldrepl_w(src + filterPos[9], 0);           \
-    src10   = __lasx_xvldrepl_w(src + filterPos[10], 0);          \
-    src11   = __lasx_xvldrepl_w(src + filterPos[11], 0);          \
-    src12   = __lasx_xvldrepl_w(src + filterPos[12], 0);          \
-    src13   = __lasx_xvldrepl_w(src + filterPos[13], 0);          \
-    src14   = __lasx_xvldrepl_w(src + filterPos[14], 0);          \
-    src15   = __lasx_xvldrepl_w(src + filterPos[15], 0);          \
-    LASX_LD_4(filter, 16, filter0, filter1, filter2, filter3);    \
-    LASX_ILVL_W_4_128SV(src1, src0, src3, src2, src5, src4,       \
-                        src7, src6, src0, src2, src4, src6);      \
-    LASX_ILVL_W_4_128SV(src9, src8, src11, src10, src13, src12,   \
-                        src15, src14, src8, src10, src12, src14); \
-    LASX_ILVL_D_4_128SV(src2, src0, src6, src4, src10, src8,      \
-                        src14, src12, src0, src1, src2, src3);    \
-    LASX_UNPCK_L_HU_BU_4(src0, src1, src2, src3,                  \
-                         src0, src1, src2, src3);                 \
-    LASX_DP2_W_H_4(filter0, src0, filter1, src1, filter2, src2,   \
-                   filter3, src3, src0, src1, src2, src3);        \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    src2 = __lasx_xvhaddw_d_w(src2, src2);                        \
-    src3 = __lasx_xvhaddw_d_w(src3, src3);                        \
-    LASX_PCKEV_W_2_128SV(src1, src0, src3, src2, src0, src1);     \
-    LASX_SRAI_W_2(src0, src1, src0, src1, _sh);                   \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    src1 = __lasx_xvmin_w(src1, vmax);                            \
-    LASX_PCKEV_H_128SV(src1, src0, src0);                         \
-    src0 = __lasx_xvperm_w(src0, shuf);                           \
-    LASX_ST(src0, dst);                                           \
-    filterPos += 16;                                              \
-    filter    += 64;                                              \
-    dst       += 16;                                              \
+#define SCALE_4_16(_sh)                                               \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);               \
+    src8    = __lasx_xvldrepl_w(src + filterPos[8], 0);               \
+    src9    = __lasx_xvldrepl_w(src + filterPos[9], 0);               \
+    src10   = __lasx_xvldrepl_w(src + filterPos[10], 0);              \
+    src11   = __lasx_xvldrepl_w(src + filterPos[11], 0);              \
+    src12   = __lasx_xvldrepl_w(src + filterPos[12], 0);              \
+    src13   = __lasx_xvldrepl_w(src + filterPos[13], 0);              \
+    src14   = __lasx_xvldrepl_w(src + filterPos[14], 0);              \
+    src15   = __lasx_xvldrepl_w(src + filterPos[15], 0);              \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32, 0, \
+              filter + 48, 0, filter0, filter1, filter2, filter3);    \
+    DUP4_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src5,          \
+              src4, src7, src6, src0, src2, src4, src6);              \
+    DUP4_ARG2(__lasx_xvilvl_w, src9, src8, src11, src10, src13,       \
+              src12, src15, src14, src8, src10, src12, src14);        \
+    DUP4_ARG2(__lasx_xvilvl_d, src2, src0, src6, src4, src10,         \
+                   src8, src14, src12, src0, src1, src2, src3);       \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src1, src2, src3,           \
+              src0, src1, src2, src3);                                \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1,         \
+              filter2, src2, filter3, src3, src0, src1, src2, src3);  \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    DUP2_ARG2(__lasx_xvsrai_w, src0, _sh, src1, _sh, src0, src1);     \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src1 = __lasx_xvmin_w(src1, vmax);                                \
+    src0 = __lasx_xvpickev_h(src1, src0);                             \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
+    __lasx_xvst(src0, dst, 0);                                        \
+    filterPos += 16;                                                  \
+    filter    += 64;                                                  \
+    dst       += 16;                                                  \
 }
 
-#define SCALE_4_8(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);           \
-    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);           \
-    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);           \
-    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);           \
-    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);           \
-    LASX_LD_2(filter, 16, filter0, filter1);                      \
-    LASX_ILVL_W_4_128SV(src1, src0, src3, src2, src5, src4,       \
-                        src7, src6, src0, src2, src4, src6);      \
-    LASX_ILVL_D_2_128SV(src2, src0, src6, src4, src0, src1);      \
-    LASX_UNPCK_L_HU_BU_2(src0, src1, src0, src1);                 \
-    LASX_DP2_W_H_2(filter0, src0, filter1, src1, src0, src1);     \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    src1 = __lasx_xvhaddw_d_w(src1, src1);                        \
-    LASX_PCKEV_W_128SV(src1, src0, src0);                         \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
+#define SCALE_4_8(_sh)                                                    \
+{                                                                         \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);                   \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);                   \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);                   \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);                   \
+    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);                   \
+    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);                   \
+    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);                   \
+    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);                   \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);  \
+    DUP4_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src5,              \
+              src4, src7, src6, src0, src2, src4, src6);                  \
+    DUP2_ARG2(__lasx_xvilvl_d, src2, src0, src6, src4, src0, src1);       \
+                                                                          \
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, src0, src1, src0, src1);              \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1, src0, src1);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                                \
+    src0 = __lasx_xvpickev_w(src1, src0);                                 \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                    \
+    src0 = __lasx_xvmin_w(src0, vmax);                                    \
 }
 
-#define SCALE_4_4(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);           \
-    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);           \
-    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);           \
-    filter0 = LASX_LD(filter);                                    \
-    LASX_ILVL_W_2_128SV(src1, src0, src3, src2, src0, src1);      \
-    LASX_ILVL_D_128SV(src1, src0, src0);                          \
-    LASX_UNPCK_L_HU_BU(src0, src0);                               \
-    LASX_DP2_W_H(filter0, src0, src0);                            \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    LASX_PCKEV_W(src0, src0, src0);                               \
+#define SCALE_4_4(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    DUP2_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src0, src1);   \
+                                                                      \
+    src0 = __lasx_xvilvl_d(src1, src0);                               \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src0 = __lasx_xvpickev_w(src0, src0);                             \
+    src0 = __lasx_xvpermi_d(src0, 0xd8);                              \
 }
 
-#define SCALE_4_2(_sh)                                            \
-{                                                                 \
-    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);           \
-    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);           \
-    filter0 = LASX_LD(filter);                                    \
-    LASX_ILVL_W_128SV(src1, src0, src0);                          \
-    LASX_UNPCK_L_HU_BU(src0, src0);                               \
-    LASX_DP2_W_H(filter0, src0, src0);                            \
-    src0 = __lasx_xvhaddw_d_w(src0, src0);                        \
-    LASX_SRAI_W(src0, src0, _sh);                                 \
-    src0 = __lasx_xvmin_w(src0, vmax);                            \
-    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                       \
-    dst[1] = __lasx_xvpickve2gr_w(src0, 2);                       \
-    filterPos += 2;                                               \
-    filter    += 8;                                               \
-    dst       += 2;                                               \
+#define SCALE_4_2(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    src0 = __lasx_xvilvl_w(src1, src0);                               \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                           \
+    dst[1] = __lasx_xvpickve2gr_w(src0, 2);                           \
+    filterPos += 2;                                                   \
+    filter    += 8;                                                   \
+    dst       += 2;                                                   \
 }
 
-#define SCALE_16                                                  \
-{                                                                 \
-    src0     = __lasx_xvldrepl_d((srcPos1 + j), 0);               \
-    src1     = __lasx_xvldrepl_d((srcPos2 + j), 0);               \
-    src2     = __lasx_xvldrepl_d((srcPos3 + j), 0);               \
-    src3     = __lasx_xvldrepl_d((srcPos4 + j), 0);               \
-    filter0  = LASX_LD(filterStart1 + j);                         \
-    filter1  = LASX_LD(filterStart2 + j);                         \
-    filter2  = LASX_LD(filterStart3 + j);                         \
-    filter3  = LASX_LD(filterStart4 + j);                         \
-    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                \
-    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                \
-    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);          \
-    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);          \
-    LASX_ILVL_B_2_128SV(zero, src0, zero, src1, src0, src1);      \
-    LASX_DP2_W_H(filter0, src0, out0);                            \
-    LASX_DP2_W_H(filter1, src1, out1);                            \
-    src0     = __lasx_xvhaddw_d_w(out0, out0);                    \
-    src1     = __lasx_xvhaddw_d_w(out1, out1);                    \
-    out0     = __lasx_xvpackev_d(src1, src0);                     \
-    out1     = __lasx_xvpackod_d(src1, src0);                     \
-    out0     = __lasx_xvadd_w(out0, out1);                        \
-    out      = __lasx_xvadd_w(out, out0);                         \
+#define SCALE_16                                                      \
+{                                                                     \
+    src0     = __lasx_xvldrepl_d((srcPos1 + j), 0);                   \
+    src1     = __lasx_xvldrepl_d((srcPos2 + j), 0);                   \
+    src2     = __lasx_xvldrepl_d((srcPos3 + j), 0);                   \
+    src3     = __lasx_xvldrepl_d((srcPos4 + j), 0);                   \
+    DUP4_ARG2(__lasx_xvld, filterStart1 + j, 0, filterStart2 + j, 0,  \
+              filterStart3 + j, 0, filterStart4 + j, 0, filter0,      \
+              filter1, filter2, filter3);                             \
+    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                    \
+    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                    \
+    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);              \
+    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);              \
+    DUP2_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, src0, src1);   \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1,         \
+              out0, out1);                                            \
+    src0     = __lasx_xvhaddw_d_w(out0, out0);                        \
+    src1     = __lasx_xvhaddw_d_w(out1, out1);                        \
+    out0     = __lasx_xvpackev_d(src1, src0);                         \
+    out1     = __lasx_xvpackod_d(src1, src0);                         \
+    out0     = __lasx_xvadd_w(out0, out1);                            \
+    out      = __lasx_xvadd_w(out, out0);                             \
 }
 
 void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
@@ -299,16 +308,17 @@ void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
         }
         if (res & 8) {
             SCALE_8_8(7);
-            LASX_PCKEV_H_128SV(src0, src0, src0);
-            LASX_ST_D_2(src0, 0, 2, dst, 4);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 2);
             filterPos += 8;
             filter    += 64;
             dst       += 8;
         }
         if (res & 4) {
             SCALE_8_4(7);
-            LASX_PCKEV_H_128SV(src0, src0, src0);
-            LASX_ST_D(src0, 0, dst);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
             filterPos += 4;
             filter    += 32;
             dst       += 4;
@@ -319,9 +329,9 @@ void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
         if (res & 1) {
             int val = 0;
             src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);
-            filter0 = LASX_LD(filter);
-            LASX_UNPCK_L_HU_BU(src0, src0);
-            LASX_DP2_W_H(filter0, src0, src0);
+            filter0 = __lasx_xvld(filter, 0);
+            src0 = __lasx_vext2xv_hu_bu(src0);
+            src0 = __lasx_xvdp2_w_h(filter0, src0);
             src0    = __lasx_xvhaddw_d_w(src0, src0);
             src0    = __lasx_xvhaddw_q_d(src0, src0);
             val     = __lasx_xvpickve2gr_w(src0, 0);
@@ -340,17 +350,18 @@ void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
         }
         if (res & 8) {
             SCALE_4_8(7);
-            LASX_PCKEV_H_128SV(src1, src0, src0);
+            src0 = __lasx_xvpickev_h(src1, src0);
             src0 = __lasx_xvperm_w(src0, shuf);
-            LASX_ST_D_2(src0, 0, 1, dst, 4);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
             filterPos += 8;
             filter    += 32;
             dst       += 8;
         }
         if (res & 4) {
             SCALE_4_4(7);
-            LASX_PCKEV_H_128SV(src0, src0, src0);
-            LASX_ST_D(src0, 0, dst);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
             filterPos += 4;
             filter    += 16;
             dst       += 4;
@@ -415,9 +426,9 @@ void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
 
             for (j = 0; j < filterlen; j += 8) {
                 src1   = __lasx_xvldrepl_d((srcPos + j), 0);
-                filter0 = LASX_LD(filter + j);
-                LASX_ILVL_B_128SV(zero, src1, src1);
-                LASX_DP2_W_H(filter0, src1, out0);
+                filter0 = __lasx_xvld(filter + j, 0);
+                src1 = __lasx_xvilvl_b(zero, src1);
+                out0 = __lasx_xvdp2_w_h(filter0, src1);
                 out0 = __lasx_xvhaddw_d_w(out0, out0);
                 out0 = __lasx_xvhaddw_q_d(out0, out0);
                 val += __lasx_xvpickve2gr_w(out0, 0);
@@ -459,14 +470,15 @@ void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
         int res = dstW & 7;
         while (len--) {
             SCALE_8_8(3);
-            LASX_ST(src0, dst);
+            __lasx_xvst(src0, dst, 0);
             filterPos += 8;
             filter    += 64;
             dst       += 8;
         }
         if (res & 4) {
             SCALE_8_4(3);
-            LASX_ST_D_2(src0, 0, 1, dst, 4);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
             filterPos += 4;
             filter    += 32;
             dst       += 4;
@@ -479,9 +491,9 @@ void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
             __m256i src0, filter0, out0;
 
             src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);
-            filter0 = LASX_LD(filter);
-            LASX_UNPCK_L_HU_BU(src0, src0);
-            LASX_DP2_W_H(filter0, src0, out0);
+            filter0 = __lasx_xvld(filter, 0);
+            src0 = __lasx_vext2xv_hu_bu(src0);
+            out0 = __lasx_xvdp2_w_h(filter0, src0);
             out0    = __lasx_xvhaddw_d_w(out0, out0);
             out0    = __lasx_xvhaddw_q_d(out0, out0);
             val     = __lasx_xvpickve2gr_w(out0, 0);
@@ -497,14 +509,15 @@ void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
         while (len--) {
             SCALE_4_8(3);
             src0 = __lasx_xvperm_w(src0, shuf);
-            LASX_ST(src0, dst);
+            __lasx_xvst(src0, dst, 0);
             filterPos += 8;
             filter    += 32;
             dst       += 8;
         }
         if (res & 4) {
             SCALE_4_4(3);
-            LASX_ST_D_2(src0, 0, 2, dst, 4);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
             filterPos += 4;
             filter    += 16;
             dst       += 4;
@@ -569,9 +582,9 @@ void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
 
             for (j = 0; j < filterlen; j += 8) {
                 src1   = __lasx_xvldrepl_d((srcPos + j), 0);
-                filter0 = LASX_LD(filter + j);
-                LASX_ILVL_B_128SV(zero, src1, src1);
-                LASX_DP2_W_H(filter0, src1, out0);
+                filter0 = __lasx_xvld(filter + j, 0);
+                src1 = __lasx_xvilvl_b(zero, src1);
+                out0 = __lasx_xvdp2_w_h(filter0, src1);
                 out0 = __lasx_xvhaddw_d_w(out0, out0);
                 out0 = __lasx_xvhaddw_q_d(out0, out0);
                 val += __lasx_xvpickve2gr_w(out0, 0);
@@ -598,59 +611,54 @@ void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
 
 #undef SCALE_16
 
-#define SCALE_8                                                      \
-{                                                                    \
-    int val1, val2, val3, val4;                                      \
-    __m256i src0, src1, src2, src3, filter0, filter1, out0, out1;    \
-    src0    = LASX_LD(src + filterPos[0]);                           \
-    src1    = LASX_LD(src + filterPos[1]);                           \
-    src2    = LASX_LD(src + filterPos[2]);                           \
-    src3    = LASX_LD(src + filterPos[3]);                           \
-    filter0 = LASX_LD(filter);                                       \
-    filter1 = LASX_LD(filter + 16);                                  \
-    src0    = __lasx_xvpermi_q(src0, src1, 0x02);                    \
-    src2    = __lasx_xvpermi_q(src2, src3, 0x02);                    \
-    LASX_DP2_W_HU_H_2(src0, filter0, src2, filter1, out0, out1);     \
-    src0    = __lasx_xvhaddw_d_w(out0, out0);                        \
-    src1    = __lasx_xvhaddw_d_w(out1, out1);                        \
-    out0    = __lasx_xvpackev_d(src1, src0);                         \
-    out1    = __lasx_xvpackod_d(src1, src0);                         \
-    out0    = __lasx_xvadd_w(out0, out1);                            \
-    out0    = __lasx_xvsra_w(out0, shift);                           \
-    val1    = __lasx_xvpickve2gr_w(out0, 0);                         \
-    val2    = __lasx_xvpickve2gr_w(out0, 4);                         \
-    val3    = __lasx_xvpickve2gr_w(out0, 2);                         \
-    val4    = __lasx_xvpickve2gr_w(out0, 6);                         \
-    dst[0]  = FFMIN(val1, max);                                      \
-    dst[1]  = FFMIN(val2, max);                                      \
-    dst[2]  = FFMIN(val3, max);                                      \
-    dst[3]  = FFMIN(val4, max);                                      \
-    filterPos += 4;                                                  \
-    filter += 32;                                                    \
-    dst += 4;                                                        \
+#define SCALE_8                                                              \
+{                                                                            \
+    int val1, val2, val3, val4;                                              \
+    __m256i src0, src1, src2, src3, filter0, filter1, out0, out1;            \
+    DUP4_ARG2(__lasx_xvld, src + filterPos[0], 0, src + filterPos[1], 0,     \
+              src + filterPos[2], 0, src + filterPos[3], 0, src0, src1, src2,\
+              src3);                                                         \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);     \
+    src0    = __lasx_xvpermi_q(src0, src1, 0x02);                            \
+    src2    = __lasx_xvpermi_q(src2, src3, 0x02);                            \
+    DUP2_ARG2(__lasx_xvdp2_w_hu_h, src0, filter0, src2, filter1, out0, out1);\
+    src0    = __lasx_xvhaddw_d_w(out0, out0);                                \
+    src1    = __lasx_xvhaddw_d_w(out1, out1);                                \
+    out0    = __lasx_xvpackev_d(src1, src0);                                 \
+    out1    = __lasx_xvpackod_d(src1, src0);                                 \
+    out0    = __lasx_xvadd_w(out0, out1);                                    \
+    out0    = __lasx_xvsra_w(out0, shift);                                   \
+    val1    = __lasx_xvpickve2gr_w(out0, 0);                                 \
+    val2    = __lasx_xvpickve2gr_w(out0, 4);                                 \
+    val3    = __lasx_xvpickve2gr_w(out0, 2);                                 \
+    val4    = __lasx_xvpickve2gr_w(out0, 6);                                 \
+    dst[0]  = FFMIN(val1, max);                                              \
+    dst[1]  = FFMIN(val2, max);                                              \
+    dst[2]  = FFMIN(val3, max);                                              \
+    dst[3]  = FFMIN(val4, max);                                              \
+    filterPos += 4;                                                          \
+    filter += 32;                                                            \
+    dst += 4;                                                                \
 }
 
-#define SCALE_16                                                     \
-{                                                                    \
-    src0     = LASX_LD(srcPos1 + j);                                 \
-    src1     = LASX_LD(srcPos2 + j);                                 \
-    src2     = LASX_LD(srcPos3 + j);                                 \
-    src3     = LASX_LD(srcPos4 + j);                                 \
-    filter0  = LASX_LD(filterStart1 + j);                            \
-    filter1  = LASX_LD(filterStart2 + j);                            \
-    filter2  = LASX_LD(filterStart3 + j);                            \
-    filter3  = LASX_LD(filterStart4 + j);                            \
-    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                   \
-    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                   \
-    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);             \
-    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);             \
-    LASX_DP2_W_HU_H_2(src0, filter0, src1, filter1, out0, out1);     \
-    src0     = __lasx_xvhaddw_d_w(out0, out0);                       \
-    src1     = __lasx_xvhaddw_d_w(out1, out1);                       \
-    out0     = __lasx_xvpackev_d(src1, src0);                        \
-    out1     = __lasx_xvpackod_d(src1, src0);                        \
-    out0     = __lasx_xvadd_w(out0, out1);                           \
-    out      = __lasx_xvadd_w(out, out0);                            \
+#define SCALE_16                                                             \
+{                                                                            \
+    DUP4_ARG2(__lasx_xvld, srcPos1 + j, 0, srcPos2 + j, 0, srcPos3 + j,      \
+              0, srcPos4 + j, 0, src0, src1, src2, src3);                    \
+    DUP4_ARG2(__lasx_xvld, filterStart1 + j, 0, filterStart2 + j, 0,         \
+              filterStart3 + j, 0, filterStart4 + j, 0, filter0,             \
+              filter1, filter2, filter3);                                    \
+    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                           \
+    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                           \
+    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);                     \
+    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);                     \
+    DUP2_ARG2(__lasx_xvdp2_w_hu_h, src0, filter0, src1, filter1, out0, out1);\
+    src0     = __lasx_xvhaddw_d_w(out0, out0);                               \
+    src1     = __lasx_xvhaddw_d_w(out1, out1);                               \
+    out0     = __lasx_xvpackev_d(src1, src0);                                \
+    out1     = __lasx_xvpackod_d(src1, src0);                                \
+    out0     = __lasx_xvadd_w(out0, out1);                                   \
+    out      = __lasx_xvadd_w(out, out0);                                    \
 }
 
 void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
@@ -683,9 +691,9 @@ void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
             int val = 0;
             __m256i src0, filter0, out0;
 
-            src0    = LASX_LD(src + filterPos[i]);
-            filter0 = LASX_LD(filter);
-            LASX_DP2_W_HU_H(src0, filter0, out0);
+            src0    = __lasx_xvld(src + filterPos[i], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
             out0    = __lasx_xvhaddw_d_w(out0, out0);
             out0    = __lasx_xvhaddw_q_d(out0, out0);
             val     = __lasx_xvpickve2gr_w(out0, 0);
@@ -700,11 +708,11 @@ void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
             src2 = __lasx_xvldrepl_d(src + filterPos[1], 0);
             src3 = __lasx_xvldrepl_d(src + filterPos[2], 0);
             src4 = __lasx_xvldrepl_d(src + filterPos[3], 0);
-            filter0 = LASX_LD(filter);
+            filter0 = __lasx_xvld(filter, 0);
             src1 = __lasx_xvextrins_d(src1, src2, 0x10);
             src3 = __lasx_xvextrins_d(src3, src4, 0x10);
             src0 = __lasx_xvpermi_q(src1, src3, 0x02);
-            LASX_DP2_W_HU_H(src0, filter0, out0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
             out0 = __lasx_xvhaddw_d_w(out0, out0);
             out0 = __lasx_xvsra_w(out0, shift);
             dst[0] = FFMIN((__lasx_xvpickve2gr_w(out0, 0)), max);
@@ -769,9 +777,8 @@ void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
             __m256i src0, filter0, out0;
 
             for (j = 0; j < filterlen; j += 8) {
-                src0    = LASX_LD(srcPos + j);
-                filter0 = LASX_LD(filter + j);
-                LASX_DP2_W_HU_H(src0, filter0, out0);
+                DUP2_ARG2(__lasx_xvld, srcPos + j, 0, filter + j, 0, src0, filter0);
+                out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
                 out0    = __lasx_xvhaddw_d_w(out0, out0);
                 out0    = __lasx_xvhaddw_q_d(out0, out0);
                 val    += __lasx_xvpickve2gr_w(out0, 0);
@@ -827,9 +834,8 @@ void ff_hscale_16_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
             int val = 0;
             __m256i src0, filter0, out0;
 
-            src0 = LASX_LD(src + filterPos[i]);
-            filter0 = LASX_LD(filter);
-            LASX_DP2_W_HU_H(src0, filter0, out0);
+            DUP2_ARG2(__lasx_xvld, src + filterPos[i], 0, filter, 0, src0, filter0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
             out0 = __lasx_xvhaddw_d_w(out0, out0);
             out0 = __lasx_xvhaddw_q_d(out0, out0);
             val  = __lasx_xvpickve2gr_w(out0, 0);
@@ -844,11 +850,11 @@ void ff_hscale_16_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
             src2 = __lasx_xvldrepl_d(src + filterPos[1], 0);
             src3 = __lasx_xvldrepl_d(src + filterPos[2], 0);
             src4 = __lasx_xvldrepl_d(src + filterPos[3], 0);
-            filter0 = LASX_LD(filter);
+            filter0 = __lasx_xvld(filter, 0);
             src1 = __lasx_xvextrins_d(src1, src2, 0x10);
             src3 = __lasx_xvextrins_d(src3, src4, 0x10);
             src0 = __lasx_xvpermi_q(src1, src3, 0x02);
-            LASX_DP2_W_HU_H(src0, filter0, out0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
             out0 = __lasx_xvhaddw_d_w(out0, out0);
             out0 = __lasx_xvsra_w(out0, shift);
             dst[0] = FFMIN((__lasx_xvpickve2gr_w(out0, 0)), max);
@@ -913,9 +919,8 @@ void ff_hscale_16_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
             __m256i src0, filter0, out0;
 
             for (j = 0; j < filterlen; j += 8) {
-                src0    = LASX_LD(srcPos + j);
-                filter0 = LASX_LD(filter + j);
-                LASX_DP2_W_HU_H(src0, filter0, out0);
+                DUP2_ARG2(__lasx_xvld, srcPos + j, 0, filter + j, 0, src0, filter0);
+                out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
                 out0    = __lasx_xvhaddw_d_w(out0, out0);
                 out0    = __lasx_xvhaddw_q_d(out0, out0);
                 val    += __lasx_xvpickve2gr_w(out0, 0);
diff --git a/libswscale/loongarch/yuv2rgb_lasx.c b/libswscale/loongarch/yuv2rgb_lasx.c
index be8aca4c66..c1f805e7a1 100644
--- a/libswscale/loongarch/yuv2rgb_lasx.c
+++ b/libswscale/loongarch/yuv2rgb_lasx.c
@@ -21,7 +21,7 @@
  */
 
 #include "swscale_loongarch.h"
-#include "libavutil/loongarch/generic_macros_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
 
 #define YUV2RGB_LOAD_COE                                     \
     /* Load x_offset */                                      \
@@ -36,11 +36,11 @@
     __m256i vr_coeff = __lasx_xvreplgr2vr_d(c->vrCoeff);     \
 
 #define LOAD_YUV_16                                          \
-    m_y  = LASX_LD(py + (w << 4));                           \
+    m_y  = __lasx_xvld(py + (w << 4), 0);                    \
     m_u  = __lasx_xvldrepl_d(pu + (w << 3), 0);              \
     m_v  = __lasx_xvldrepl_d(pv + (w << 3), 0);              \
-    LASX_UNPCK_L_HU_BU(m_y, m_y);                            \
-    LASX_UNPCK_L_HU_BU_2(m_u, m_v, m_u, m_v);                \
+    m_y = __lasx_vext2xv_hu_bu(m_y);                         \
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, m_u, m_v, m_u, m_v);     \
 
 /* YUV2RGB method
  * The conversion method is as follows:
@@ -70,14 +70,14 @@
     v2g = __lasx_xvsadd_h(v2g, u2g);                                       \
     g   = __lasx_xvsadd_h(v2g, y_1);                                       \
     b   = __lasx_xvsadd_h(y_1, u2b);                                       \
-    LASX_CLIP_H_0_255_2(r, g, r, g);                                       \
-    LASX_CLIP_H_0_255(b, b);                                               \
+    DUP2_ARG1(__lasx_xvclip255_h, r, g, r, g);                             \
+    b = __lasx_xvclip255_h(b);                                             \
 
 #define RGB_PACK_16(r, g, b, rgb_l, rgb_h)                                 \
 {                                                                          \
     __m256i rg;                                                            \
     rg = __lasx_xvpackev_b(g, r);                                          \
-    LASX_SHUF_B_2_128SV(b, rg, b, rg, shuf2, shuf3, rgb_l, rgb_h);         \
+    DUP2_ARG3(__lasx_xvshuf_b, b, rg, shuf2, b, rg, shuf3, rgb_l, rgb_h);  \
 }
 
 #define RGB_PACK_32(r, g, b, a, rgb_l, rgb_h)                              \
@@ -85,8 +85,8 @@
     __m256i rg, ba;                                                        \
     rgb_l = __lasx_xvpackev_b(g, r);                                       \
     rgb_h = __lasx_xvpackev_b(a, b);                                       \
-    LASX_ILVL_H_128SV(rgb_h, rgb_l, rg);                                   \
-    LASX_ILVH_H_128SV(rgb_h, rgb_l, ba);                                   \
+    rg = __lasx_xvilvl_h(rgb_h, rgb_l);                                    \
+    ba = __lasx_xvilvh_h(rgb_h, rgb_l);                                    \
     rgb_l = __lasx_xvpermi_q(ba, rg, 0x20);                                \
     rgb_h = __lasx_xvpermi_q(ba, rg, 0x31);                                \
 }
@@ -94,7 +94,8 @@
 #define RGB_STORE_32(rgb_l, rgb_h, iamge, w)                               \
 {                                                                          \
     uint8_t *index = image + (w * 64);                                     \
-    LASX_ST_2(rgb_l, rgb_h, index, 32);                                    \
+    __lasx_xvst(rgb_l, index, 0);                                          \
+    __lasx_xvst(rgb_h, index + 32, 0);                                     \
 }
 
 #define RGB_STORE(rgb_l, rgb_h, image, w)                                      \
-- 
2.20.1

