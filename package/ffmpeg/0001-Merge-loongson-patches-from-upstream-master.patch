From 2ceab685d88663f1c3cba105fb744248e63d9f20 Mon Sep 17 00:00:00 2001
From: Shiyou Yin <yinshiyou-hf@loongson.cn>
Date: Fri, 18 Sep 2020 15:46:43 +0800
Subject: [PATCH 1/2] Merge loongson patches from upstream master.

The latest patch: 5a844b638ac081e763709c38174596d3e02d6ece.
---
 configure                                   |  167 +--
 ffbuild/common.mak                          |   10 +-
 libavcodec/mips/Makefile                    |    7 +
 libavcodec/mips/aacdec_mips.c               |    8 +-
 libavcodec/mips/aacpsdsp_mips.c             |   13 +-
 libavcodec/mips/aacpsy_mips.h               |   14 +-
 libavcodec/mips/aacsbr_mips.c               |    2 +-
 libavcodec/mips/blockdsp_init_mips.c        |   40 +-
 libavcodec/mips/cabac.h                     |    4 +-
 libavcodec/mips/fft_mips.c                  |   12 +-
 libavcodec/mips/h263dsp_init_mips.c         |   18 +-
 libavcodec/mips/h263dsp_msa.c               |    5 +-
 libavcodec/mips/h264chroma_init_mips.c      |   55 +-
 libavcodec/mips/h264chroma_mmi.c            |  970 +++++++--------
 libavcodec/mips/h264chroma_msa.c            |   87 +-
 libavcodec/mips/h264dsp_init_mips.c         |  225 ++--
 libavcodec/mips/h264dsp_mips.h              |   42 +-
 libavcodec/mips/h264dsp_mmi.c               |  100 +-
 libavcodec/mips/h264dsp_msa.c               |  192 +--
 libavcodec/mips/h264idct_msa.c              |   17 +-
 libavcodec/mips/h264pred_init_mips.c        |  210 ++--
 libavcodec/mips/h264pred_mmi.c              |    8 +-
 libavcodec/mips/h264qpel_init_mips.c        |  412 ++++---
 libavcodec/mips/h264qpel_msa.c              |  202 ++--
 libavcodec/mips/hevc_idct_msa.c             |   28 +-
 libavcodec/mips/hevc_lpf_sao_msa.c          |  234 ++--
 libavcodec/mips/hevc_mc_bi_msa.c            |  179 +--
 libavcodec/mips/hevc_mc_biw_msa.c           |  184 +--
 libavcodec/mips/hevc_mc_uni_msa.c           |  146 ++-
 libavcodec/mips/hevc_mc_uniw_msa.c          |  197 +--
 libavcodec/mips/hevcdsp_init_mips.c         |  919 +++++++-------
 libavcodec/mips/hevcdsp_mips.h              |   91 ++
 libavcodec/mips/hevcdsp_mmi.c               | 1183 +++++++++++++++++++
 libavcodec/mips/hevcdsp_msa.c               |   63 +-
 libavcodec/mips/hevcpred_init_mips.c        |   40 +-
 libavcodec/mips/hevcpred_msa.c              |   83 +-
 libavcodec/mips/hpeldsp_init_mips.c         |  180 ++-
 libavcodec/mips/hpeldsp_msa.c               |   76 +-
 libavcodec/mips/idctdsp_init_mips.c         |   74 +-
 libavcodec/mips/idctdsp_msa.c               |    9 +-
 libavcodec/mips/me_cmp_init_mips.c          |   50 +-
 libavcodec/mips/me_cmp_msa.c                |    8 +-
 libavcodec/mips/mpegaudiodsp_mips_float.c   |  492 ++++----
 libavcodec/mips/mpegvideo_init_mips.c       |   48 +-
 libavcodec/mips/mpegvideo_mmi.c             |    2 +-
 libavcodec/mips/mpegvideoencdsp_init_mips.c |   21 +-
 libavcodec/mips/pixblockdsp_init_mips.c     |   63 +-
 libavcodec/mips/qpeldsp_init_mips.c         |  270 +++--
 libavcodec/mips/qpeldsp_msa.c               |  380 +++---
 libavcodec/mips/sbrdsp_mips.c               |   19 +-
 libavcodec/mips/simple_idct_mmi.c           |    2 +-
 libavcodec/mips/simple_idct_msa.c           |   98 +-
 libavcodec/mips/vc1dsp_init_mips.c          |  164 +--
 libavcodec/mips/vc1dsp_mips.h               |   31 +-
 libavcodec/mips/vc1dsp_mmi.c                |    8 +-
 libavcodec/mips/vc1dsp_msa.c                |  461 ++++++++
 libavcodec/mips/videodsp_init.c             |   10 +-
 libavcodec/mips/vp3dsp_idct_mmi.c           |  769 ++++++++++++
 libavcodec/mips/vp3dsp_idct_msa.c           |  598 ++++++++++
 libavcodec/mips/vp3dsp_init_mips.c          |   50 +
 libavcodec/mips/vp3dsp_mips.h               |   43 +
 libavcodec/mips/vp8_idct_msa.c              |   11 +-
 libavcodec/mips/vp8_lpf_msa.c               |   27 +-
 libavcodec/mips/vp8_mc_msa.c                |  113 +-
 libavcodec/mips/vp8dsp_init_mips.c          |  240 ++--
 libavcodec/mips/vp9_idct_msa.c              |   83 +-
 libavcodec/mips/vp9_intra_msa.c             |    5 +-
 libavcodec/mips/vp9_lpf_msa.c               |  146 ++-
 libavcodec/mips/vp9_mc_mmi.c                |  628 ++++++++++
 libavcodec/mips/vp9_mc_msa.c                |  121 +-
 libavcodec/mips/vp9dsp_init_mips.c          |   54 +-
 libavcodec/mips/vp9dsp_mips.h               |   50 +
 libavcodec/mips/wmv2dsp_init_mips.c         |   18 +-
 libavcodec/mips/wmv2dsp_mips.h              |    4 +-
 libavcodec/mips/wmv2dsp_mmi.c               |    4 +-
 libavcodec/mips/xvid_idct_mmi.c             |    4 +-
 libavcodec/mips/xvididct_init_mips.c        |   31 +-
 libavcodec/mips/xvididct_mips.h             |    4 +-
 libavcodec/vp3dsp.c                         |    2 +
 libavcodec/vp3dsp.h                         |    1 +
 libavutil/cpu.c                             |   10 +
 libavutil/cpu.h                             |    3 +
 libavutil/cpu_internal.h                    |    2 +
 libavutil/mips/Makefile                     |    2 +-
 libavutil/mips/asmdefs.h                    |   42 +
 libavutil/mips/cpu.c                        |  134 +++
 libavutil/mips/cpu.h                        |   28 +
 libavutil/mips/generic_macros_msa.h         |  572 ++++-----
 libavutil/mips/mmiutils.h                   |   26 +-
 libavutil/tests/cpu.c                       |    3 +
 tests/checkasm/checkasm.c                   |    3 +
 91 files changed, 8175 insertions(+), 4289 deletions(-)
 create mode 100644 libavcodec/mips/hevcdsp_mmi.c
 create mode 100644 libavcodec/mips/vc1dsp_msa.c
 create mode 100644 libavcodec/mips/vp3dsp_idct_mmi.c
 create mode 100644 libavcodec/mips/vp3dsp_idct_msa.c
 create mode 100644 libavcodec/mips/vp3dsp_init_mips.c
 create mode 100644 libavcodec/mips/vp3dsp_mips.h
 create mode 100644 libavcodec/mips/vp9_mc_mmi.c
 create mode 100644 libavutil/mips/cpu.c
 create mode 100644 libavutil/mips/cpu.h

diff --git a/configure b/configure
index c966689..db57f4d 100755
--- a/configure
+++ b/configure
@@ -439,6 +439,7 @@ Optimization options (experts only):
   --disable-mipsdsp        disable MIPS DSP ASE R1 optimizations
   --disable-mipsdspr2      disable MIPS DSP ASE R2 optimizations
   --disable-msa            disable MSA optimizations
+  --disable-msa2           disable MSA2 optimizations
   --disable-mipsfpu        disable floating point MIPS optimizations
   --disable-mmi            disable Loongson SIMD optimizations
   --disable-fast-unaligned consider unaligned accesses slow
@@ -1953,6 +1954,7 @@ ARCH_EXT_LIST_MIPS="
     mipsdsp
     mipsdspr2
     msa
+    msa2
 "
 
 ARCH_EXT_LIST_LOONGSON="
@@ -2476,8 +2478,9 @@ mips64r6_deps="mips"
 mipsfpu_deps="mips"
 mipsdsp_deps="mips"
 mipsdspr2_deps="mips"
-mmi_deps="mips"
+mmi_deps_any="loongson2 loongson3"
 msa_deps="mipsfpu"
+msa2_deps="msa"
 
 cpunop_deps="i686"
 x86_64_select="i686"
@@ -4811,8 +4814,6 @@ elif enabled bfin; then
 
 elif enabled mips; then
 
-    cpuflags="-march=$cpu"
-
     if [ "$cpu" != "generic" ]; then
         disable mips32r2
         disable mips32r5
@@ -4821,19 +4822,53 @@ elif enabled mips; then
         disable mips64r6
         disable loongson2
         disable loongson3
+        disable mipsdsp
+        disable mipsdspr2
+
+        cpuflags="-march=$cpu"
 
         case $cpu in
-            24kc|24kf*|24kec|34kc|1004kc|24kef*|34kf*|1004kf*|74kc|74kf)
+            # General ISA levels
+            mips1|mips3)
+            ;;
+            mips32r2)
                 enable mips32r2
-                disable msa
             ;;
-            p5600|i6400|p6600)
-                disable mipsdsp
-                disable mipsdspr2
+            mips32r5)
+                enable mips32r2
+                enable mips32r5
             ;;
-            loongson*)
-                enable loongson2
+            mips64r2|mips64r5)
+                enable mips64r2
                 enable loongson3
+            ;;
+            # Cores from MIPS(MTI)
+            24kc)
+                disable mipsfpu
+                enable mips32r2
+            ;;
+            24kf*|24kec|34kc|74Kc|1004kc)
+                enable mips32r2
+            ;;
+            24kef*|34kf*|1004kf*)
+                enable mipsdsp
+                enable mips32r2
+            ;;
+            p5600)
+                enable mips32r2
+                enable mips32r5
+                check_cflags "-mtune=p5600" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops"
+            ;;
+            i6400)
+                enable mips64r6
+                check_cflags "-mtune=i6400 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
+            ;;
+            p6600)
+                enable mips64r6
+                check_cflags "-mtune=p6600 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
+            ;;
+            # Cores from Loongson
+            loongson2e|loongson2f|loongson3*)
                 enable local_aligned
                 enable simd_align_16
                 enable fast_64bit
@@ -4841,76 +4876,42 @@ elif enabled mips; then
                 enable fast_cmov
                 enable fast_unaligned
                 disable aligned_stack
-                disable mipsfpu
-                disable mipsdsp
-                disable mipsdspr2
                 # When gcc version less than 5.3.0, add -fno-expensive-optimizations flag.
-                if [ $cc == gcc ]; then
-                    gcc_version=$(gcc -dumpversion)
-                    if [ "$(echo "$gcc_version 5.3.0" | tr " " "\n" | sort -rV | head -n 1)" == "$gcc_version" ]; then
-                        expensive_optimization_flag=""
-                    else
+                if test "$cc_type" = "gcc"; then
+                    case $gcc_basever in
+                        2|2.*|3.*|4.*|5.0|5.1|5.2)
                         expensive_optimization_flag="-fno-expensive-optimizations"
-                    fi
+                        ;;
+                        *)
+                        expensive_optimization_flag=""
+                        ;;
+                    esac
                 fi
+
                 case $cpu in
                     loongson3*)
+                        enable loongson3
                         cpuflags="-march=loongson3a -mhard-float $expensive_optimization_flag"
                     ;;
                     loongson2e)
+                        enable loongson2
                         cpuflags="-march=loongson2e -mhard-float $expensive_optimization_flag"
                     ;;
                     loongson2f)
+                        enable loongson2
                         cpuflags="-march=loongson2f -mhard-float $expensive_optimization_flag"
                     ;;
                 esac
             ;;
             *)
-                # Unknown CPU. Disable everything.
-                warn "unknown CPU. Disabling all MIPS optimizations."
-                disable mipsfpu
-                disable mipsdsp
-                disable mipsdspr2
-                disable msa
-                disable mmi
+                warn "unknown MIPS CPU"
             ;;
         esac
 
-        case $cpu in
-            24kc)
-                disable mipsfpu
-                disable mipsdsp
-                disable mipsdspr2
-            ;;
-            24kf*)
-                disable mipsdsp
-                disable mipsdspr2
-            ;;
-            24kec|34kc|1004kc)
-                disable mipsfpu
-                disable mipsdspr2
-            ;;
-            24kef*|34kf*|1004kf*)
-                disable mipsdspr2
-            ;;
-            74kc)
-                disable mipsfpu
-            ;;
-            p5600)
-                enable mips32r5
-                check_cflags "-mtune=p5600" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops"
-            ;;
-            i6400)
-                enable mips64r6
-                check_cflags "-mtune=i6400 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
-            ;;
-            p6600)
-                enable mips64r6
-                check_cflags "-mtune=p6600 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
-            ;;
-        esac
     else
-        # We do not disable anything. Is up to the user to disable the unwanted features.
+        disable mipsdsp
+        disable mipsdspr2
+        # Disable DSP stuff for generic CPU, it can't be detected at runtime.
         warn 'generic cpu selected'
     fi
 
@@ -5644,28 +5645,43 @@ EOF
 
 elif enabled mips; then
 
-    enabled loongson2 && check_inline_asm loongson2 '"dmult.g $8, $9, $10"'
-    enabled loongson3 && check_inline_asm loongson3 '"gsldxc1 $f0, 0($2, $3)"'
-    enabled mmi && check_inline_asm mmi '"punpcklhw $f0, $f0, $f0"'
-
-    # Enable minimum ISA based on selected options
+    # Check toolchain ISA level
     if enabled mips64; then
-        enabled mips64r6 && check_inline_asm_flags mips64r6 '"dlsa $0, $0, $0, 1"' '-mips64r6'
-        enabled mips64r2 && check_inline_asm_flags mips64r2 '"dext $0, $0, 0, 1"' '-mips64r2'
-        disabled mips64r6 && disabled mips64r2 && check_inline_asm_flags mips64r1 '"daddi $0, $0, 0"' '-mips64'
+        enabled mips64r6 && check_inline_asm mips64r6 '"dlsa $0, $0, $0, 1"' &&
+            disable mips64r2
+
+        enabled mips64r2 && check_inline_asm mips64r2 '"dext $0, $0, 0, 1"'
+
+        disable mips32r6 && disable mips32r5 && disable mips32r2
     else
-        enabled mips32r6 && check_inline_asm_flags mips32r6 '"aui $0, $0, 0"' '-mips32r6'
-        enabled mips32r5 && check_inline_asm_flags mips32r5 '"eretnc"' '-mips32r5'
-        enabled mips32r2 && check_inline_asm_flags mips32r2 '"ext $0, $0, 0, 1"' '-mips32r2'
-        disabled mips32r6 && disabled mips32r5 && disabled mips32r2 && check_inline_asm_flags mips32r1 '"addi $0, $0, 0"' '-mips32'
+        enabled mips32r6 && check_inline_asm mips32r6 '"aui $0, $0, 0"' &&
+            disable mips32r5 && disable mips32r2
+
+        enabled mips32r5 && check_inline_asm mips32r5 '"eretnc"'
+        enabled mips32r2 && check_inline_asm mips32r2 '"ext $0, $0, 0, 1"'
+
+        disable mips64r6 && disable mips64r5 && disable mips64r2
     fi
 
-    enabled mipsfpu && check_inline_asm_flags mipsfpu '"cvt.d.l $f0, $f2"' '-mhard-float'
+    enabled mipsfpu && check_inline_asm mipsfpu '"cvt.d.l $f0, $f2"'
     enabled mipsfpu && (enabled mips32r5 || enabled mips32r6 || enabled mips64r6) && check_inline_asm_flags mipsfpu '"cvt.d.l $f0, $f1"' '-mfp64'
-    enabled mipsfpu && enabled msa && check_inline_asm_flags msa '"addvi.b $w0, $w1, 1"' '-mmsa' && check_headers msa.h || disable msa
+
     enabled mipsdsp && check_inline_asm_flags mipsdsp '"addu.qb $t0, $t1, $t2"' '-mdsp'
     enabled mipsdspr2 && check_inline_asm_flags mipsdspr2 '"absq_s.qb $t0, $t1"' '-mdspr2'
 
+    # MSA and MSA2 can be detected at runtime so we supply extra flags here
+    enabled mipsfpu && enabled msa && check_inline_asm msa '"addvi.b $w0, $w1, 1"' '-mmsa' && append MSAFLAGS '-mmsa'
+    enabled msa && enabled msa2 && check_inline_asm msa2 '"nxbits.any.b $w0, $w0"' '-mmsa2' && append MSAFLAGS '-mmsa2'
+
+    # loongson2 have no switch cflag so we can only probe toolchain ability
+    enabled loongson2 && check_inline_asm loongson2 '"dmult.g $8, $9, $10"' && disable loongson3
+
+    # loongson3 is paired with MMI
+    enabled loongson3 && check_inline_asm loongson3 '"gsldxc1 $f0, 0($2, $3)"' '-mloongson-ext' && append MMIFLAGS '-mloongson-ext'
+
+    # MMI can be detected at runtime too
+    enabled mmi && check_inline_asm mmi '"punpcklhw $f0, $f0, $f0"' '-mloongson-mmi' && append MMIFLAGS '-mloongson-mmi'
+
     if enabled bigendian && enabled msa; then
         disable msa
     fi
@@ -7011,6 +7027,7 @@ if enabled mips; then
     echo "MIPS DSP R1 enabled       ${mipsdsp-no}"
     echo "MIPS DSP R2 enabled       ${mipsdspr2-no}"
     echo "MIPS MSA enabled          ${msa-no}"
+    echo "MIPS MSA2 enabled         ${msa2-no}"
     echo "LOONGSON MMI enabled      ${mmi-no}"
 fi
 if enabled ppc; then
@@ -7170,6 +7187,8 @@ LDSOFLAGS=$LDSOFLAGS
 SHFLAGS=$(echo $($ldflags_filter $SHFLAGS))
 ASMSTRIPFLAGS=$ASMSTRIPFLAGS
 X86ASMFLAGS=$X86ASMFLAGS
+MSAFLAGS=$MSAFLAGS
+MMIFLAGS=$MMIFLAGS
 BUILDSUF=$build_suffix
 PROGSSUF=$progs_suffix
 FULLNAME=$FULLNAME
diff --git a/ffbuild/common.mak b/ffbuild/common.mak
index eb41b05..a00d40d 100644
--- a/ffbuild/common.mak
+++ b/ffbuild/common.mak
@@ -45,7 +45,7 @@ LDFLAGS    := $(ALLFFLIBS:%=$(LD_PATH)lib%) $(LDFLAGS)
 
 define COMPILE
        $(call $(1)DEP,$(1))
-       $($(1)) $($(1)FLAGS) $($(1)_DEPFLAGS) $($(1)_C) $($(1)_O) $(patsubst $(SRC_PATH)/%,$(SRC_LINK)/%,$<)
+       $($(1)) $($(1)FLAGS) $($(2)) $($(1)_DEPFLAGS) $($(1)_C) $($(1)_O) $(patsubst $(SRC_PATH)/%,$(SRC_LINK)/%,$<)
 endef
 
 COMPILE_C = $(call COMPILE,CC)
@@ -55,6 +55,14 @@ COMPILE_M = $(call COMPILE,OBJCC)
 COMPILE_X86ASM = $(call COMPILE,X86ASM)
 COMPILE_HOSTC = $(call COMPILE,HOSTCC)
 COMPILE_NVCC = $(call COMPILE,NVCC)
+COMPILE_MMI = $(call COMPILE,CC,MMIFLAGS)
+COMPILE_MSA = $(call COMPILE,CC,MSAFLAGS)
+
+%_mmi.o: %_mmi.c
+	$(COMPILE_MMI)
+
+%_msa.o: %_msa.c
+	$(COMPILE_MSA)
 
 %.o: %.c
 	$(COMPILE_C)
diff --git a/libavcodec/mips/Makefile b/libavcodec/mips/Makefile
index 1f659a0..2be4d9b 100644
--- a/libavcodec/mips/Makefile
+++ b/libavcodec/mips/Makefile
@@ -22,6 +22,7 @@ OBJS-$(CONFIG_HEVC_DECODER)               += mips/hevcdsp_init_mips.o      \
                                              mips/hevcpred_init_mips.o
 OBJS-$(CONFIG_VP9_DECODER)                += mips/vp9dsp_init_mips.o
 OBJS-$(CONFIG_VP8_DECODER)                += mips/vp8dsp_init_mips.o
+OBJS-$(CONFIG_VP3DSP)                     += mips/vp3dsp_init_mips.o
 OBJS-$(CONFIG_H264DSP)                    += mips/h264dsp_init_mips.o
 OBJS-$(CONFIG_H264QPEL)                   += mips/h264qpel_init_mips.o
 OBJS-$(CONFIG_H264CHROMA)                 += mips/h264chroma_init_mips.o
@@ -54,6 +55,7 @@ MSA-OBJS-$(CONFIG_VP9_DECODER)            += mips/vp9_mc_msa.o             \
 MSA-OBJS-$(CONFIG_VP8_DECODER)            += mips/vp8_mc_msa.o             \
                                              mips/vp8_idct_msa.o           \
                                              mips/vp8_lpf_msa.o
+MSA-OBJS-$(CONFIG_VP3DSP)                 += mips/vp3dsp_idct_msa.o
 MSA-OBJS-$(CONFIG_H264DSP)                += mips/h264dsp_msa.o            \
                                              mips/h264idct_msa.o
 MSA-OBJS-$(CONFIG_H264QPEL)               += mips/h264qpel_msa.o
@@ -69,6 +71,8 @@ MSA-OBJS-$(CONFIG_IDCTDSP)                += mips/idctdsp_msa.o           \
 MSA-OBJS-$(CONFIG_MPEGVIDEO)              += mips/mpegvideo_msa.o
 MSA-OBJS-$(CONFIG_MPEGVIDEOENC)           += mips/mpegvideoencdsp_msa.o
 MSA-OBJS-$(CONFIG_ME_CMP)                 += mips/me_cmp_msa.o
+MSA-OBJS-$(CONFIG_VC1_DECODER)            += mips/vc1dsp_msa.o
+
 MMI-OBJS                                  += mips/constants.o
 MMI-OBJS-$(CONFIG_H264DSP)                += mips/h264dsp_mmi.o
 MMI-OBJS-$(CONFIG_H264CHROMA)             += mips/h264chroma_mmi.o
@@ -84,3 +88,6 @@ MMI-OBJS-$(CONFIG_VP8_DECODER)            += mips/vp8dsp_mmi.o
 MMI-OBJS-$(CONFIG_HPELDSP)                += mips/hpeldsp_mmi.o
 MMI-OBJS-$(CONFIG_VC1_DECODER)            += mips/vc1dsp_mmi.o
 MMI-OBJS-$(CONFIG_WMV2DSP)                += mips/wmv2dsp_mmi.o
+MMI-OBJS-$(CONFIG_HEVC_DECODER)           += mips/hevcdsp_mmi.o
+MMI-OBJS-$(CONFIG_VP3DSP)                 += mips/vp3dsp_idct_mmi.o
+MMI-OBJS-$(CONFIG_VP9_DECODER)            += mips/vp9_mc_mmi.o
diff --git a/libavcodec/mips/aacdec_mips.c b/libavcodec/mips/aacdec_mips.c
index 253cdeb..d02245d 100644
--- a/libavcodec/mips/aacdec_mips.c
+++ b/libavcodec/mips/aacdec_mips.c
@@ -237,9 +237,9 @@ static void apply_ltp_mips(AACContext *ac, SingleChannelElement *sce)
 
         if (ltp->lag < 1024)
             num_samples = ltp->lag + 1024;
-            j = (2048 - num_samples) >> 2;
-            k = (2048 - num_samples) & 3;
-            p_predTime = &predTime[num_samples];
+        j = (2048 - num_samples) >> 2;
+        k = (2048 - num_samples) & 3;
+        p_predTime = &predTime[num_samples];
 
         for (i = 0; i < num_samples; i++)
             predTime[i] = sce->ltp_state[i + 2048 - ltp->lag] * ltp->coef;
@@ -340,7 +340,7 @@ static void update_ltp_mips(AACContext *ac, SingleChannelElement *sce)
     float *saved_ltp = sce->coeffs;
     const float *lwindow = ics->use_kb_window[0] ? ff_aac_kbd_long_1024 : ff_sine_1024;
     const float *swindow = ics->use_kb_window[0] ? ff_aac_kbd_short_128 : ff_sine_128;
-    float temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
+    uint32_t temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
 
     if (ics->window_sequence[0] == EIGHT_SHORT_SEQUENCE) {
         float *p_saved_ltp = saved_ltp + 576;
diff --git a/libavcodec/mips/aacpsdsp_mips.c b/libavcodec/mips/aacpsdsp_mips.c
index 83fdc2f..ae628a9 100644
--- a/libavcodec/mips/aacpsdsp_mips.c
+++ b/libavcodec/mips/aacpsdsp_mips.c
@@ -293,16 +293,17 @@ static void ps_decorrelate_mips(float (*out)[2], float (*delay)[2],
     float phi_fract0 = phi_fract[0];
     float phi_fract1 = phi_fract[1];
     float temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7, temp8, temp9;
+    float f1, f2, f3;
 
     float *p_delay_end = (p_delay + (len << 1));
 
     /* merged 2 loops */
+    f1 = 0.65143905753106;
+    f2 = 0.56471812200776;
+    f3 = 0.48954165955695;
     __asm__ volatile(
         ".set    push                                                    \n\t"
         ".set    noreorder                                               \n\t"
-        "li.s    %[ag0],        0.65143905753106                         \n\t"
-        "li.s    %[ag1],        0.56471812200776                         \n\t"
-        "li.s    %[ag2],        0.48954165955695                         \n\t"
         "mul.s   %[ag0],        %[ag0],        %[g_decay_slope]          \n\t"
         "mul.s   %[ag1],        %[ag1],        %[g_decay_slope]          \n\t"
         "mul.s   %[ag2],        %[ag2],        %[g_decay_slope]          \n\t"
@@ -378,10 +379,10 @@ static void ps_decorrelate_mips(float (*out)[2], float (*delay)[2],
           [temp3]"=&f"(temp3), [temp4]"=&f"(temp4), [temp5]"=&f"(temp5),
           [temp6]"=&f"(temp6), [temp7]"=&f"(temp7), [temp8]"=&f"(temp8),
           [temp9]"=&f"(temp9), [p_delay]"+r"(p_delay), [p_ap_delay]"+r"(p_ap_delay),
-          [p_Q_fract]"+r"(p_Q_fract), [p_t_gain]"+r"(p_t_gain), [p_out]"+r"(p_out),
-          [ag0]"=&f"(ag0), [ag1]"=&f"(ag1), [ag2]"=&f"(ag2)
+          [p_Q_fract]"+r"(p_Q_fract), [p_t_gain]"+r"(p_t_gain), [p_out]"+r"(p_out)
         : [phi_fract0]"f"(phi_fract0), [phi_fract1]"f"(phi_fract1),
-          [p_delay_end]"r"(p_delay_end), [g_decay_slope]"f"(g_decay_slope)
+          [p_delay_end]"r"(p_delay_end), [g_decay_slope]"f"(g_decay_slope),
+          [ag0]"f"(f1), [ag1]"f"(f2), [ag2]"f"(f3)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/aacpsy_mips.h b/libavcodec/mips/aacpsy_mips.h
index a1fe5cc..7d27d32 100644
--- a/libavcodec/mips/aacpsy_mips.h
+++ b/libavcodec/mips/aacpsy_mips.h
@@ -135,11 +135,11 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
     float coeff3 = psy_fir_coeffs[7];
     float coeff4 = psy_fir_coeffs[9];
 
+    float f1 = 32768.0;
     __asm__ volatile (
         ".set push                                          \n\t"
         ".set noreorder                                     \n\t"
 
-        "li.s   $f12,       32768                           \n\t"
         "1:                                                 \n\t"
         "lwc1   $f0,        40(%[fb])                       \n\t"
         "lwc1   $f1,        4(%[fb])                        \n\t"
@@ -203,14 +203,14 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
         "madd.s %[sum2],    %[sum2],    $f9,    %[coeff4]   \n\t"
         "madd.s %[sum4],    %[sum4],    $f6,    %[coeff4]   \n\t"
         "madd.s %[sum3],    %[sum3],    $f3,    %[coeff4]   \n\t"
-        "mul.s  %[sum1],    %[sum1],    $f12                \n\t"
-        "mul.s  %[sum2],    %[sum2],    $f12                \n\t"
+        "mul.s  %[sum1],    %[sum1],    %[f1]               \n\t"
+        "mul.s  %[sum2],    %[sum2],    %[f1]               \n\t"
         "madd.s %[sum4],    %[sum4],    $f11,   %[coeff4]   \n\t"
         "madd.s %[sum3],    %[sum3],    $f8,    %[coeff4]   \n\t"
         "swc1   %[sum1],    0(%[hp])                        \n\t"
         "swc1   %[sum2],    4(%[hp])                        \n\t"
-        "mul.s  %[sum4],    %[sum4],    $f12                \n\t"
-        "mul.s  %[sum3],    %[sum3],    $f12                \n\t"
+        "mul.s  %[sum4],    %[sum4],    %[f1]               \n\t"
+        "mul.s  %[sum3],    %[sum3],    %[f1]               \n\t"
         "swc1   %[sum4],    12(%[hp])                       \n\t"
         "swc1   %[sum3],    8(%[hp])                        \n\t"
         "bne    %[fb],      %[fb_end],  1b                  \n\t"
@@ -223,9 +223,9 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
           [fb]"+r"(fb), [hp]"+r"(hp)
         : [coeff0]"f"(coeff0), [coeff1]"f"(coeff1),
           [coeff2]"f"(coeff2), [coeff3]"f"(coeff3),
-          [coeff4]"f"(coeff4), [fb_end]"r"(fb_end)
+          [coeff4]"f"(coeff4), [fb_end]"r"(fb_end), [f1]"f"(f1)
         : "$f0", "$f1", "$f2", "$f3", "$f4", "$f5", "$f6",
-          "$f7", "$f8", "$f9", "$f10", "$f11", "$f12",
+          "$f7", "$f8", "$f9", "$f10", "$f11",
           "memory"
     );
 }
diff --git a/libavcodec/mips/aacsbr_mips.c b/libavcodec/mips/aacsbr_mips.c
index 56aa4e8..fe9c08a 100644
--- a/libavcodec/mips/aacsbr_mips.c
+++ b/libavcodec/mips/aacsbr_mips.c
@@ -333,7 +333,7 @@ static void sbr_hf_assemble_mips(float Y1[38][64][2],
     int indexnoise = ch_data->f_indexnoise;
     int indexsine  = ch_data->f_indexsine;
     float *g_temp1, *q_temp1, *pok, *pok1;
-    float temp1, temp2, temp3, temp4;
+    uint32_t temp1, temp2, temp3, temp4;
     int size = m_max;
 
     if (sbr->reset) {
diff --git a/libavcodec/mips/blockdsp_init_mips.c b/libavcodec/mips/blockdsp_init_mips.c
index 30ae95f..c6964fa 100644
--- a/libavcodec/mips/blockdsp_init_mips.c
+++ b/libavcodec/mips/blockdsp_init_mips.c
@@ -19,36 +19,26 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "blockdsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void blockdsp_init_msa(BlockDSPContext *c)
+void ff_blockdsp_init_mips(BlockDSPContext *c)
 {
-    c->clear_block = ff_clear_block_msa;
-    c->clear_blocks = ff_clear_blocks_msa;
+    int cpu_flags = av_get_cpu_flags();
 
-    c->fill_block_tab[0] = ff_fill_block16_msa;
-    c->fill_block_tab[1] = ff_fill_block8_msa;
-}
-#endif  // #if HAVE_MSA
+    if (have_mmi(cpu_flags)) {
+        c->clear_block = ff_clear_block_mmi;
+        c->clear_blocks = ff_clear_blocks_mmi;
 
-#if HAVE_MMI
-static av_cold void blockdsp_init_mmi(BlockDSPContext *c)
-{
-    c->clear_block = ff_clear_block_mmi;
-    c->clear_blocks = ff_clear_blocks_mmi;
+        c->fill_block_tab[0] = ff_fill_block16_mmi;
+        c->fill_block_tab[1] = ff_fill_block8_mmi;
+    }
 
-    c->fill_block_tab[0] = ff_fill_block16_mmi;
-    c->fill_block_tab[1] = ff_fill_block8_mmi;
-}
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        c->clear_block = ff_clear_block_msa;
+        c->clear_blocks = ff_clear_blocks_msa;
 
-void ff_blockdsp_init_mips(BlockDSPContext *c)
-{
-#if HAVE_MSA
-    blockdsp_init_msa(c);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    blockdsp_init_mmi(c);
-#endif /* HAVE_MMI */
+        c->fill_block_tab[0] = ff_fill_block16_msa;
+        c->fill_block_tab[1] = ff_fill_block8_msa;
+    }
 }
diff --git a/libavcodec/mips/cabac.h b/libavcodec/mips/cabac.h
index 82cee29..6a13b77 100644
--- a/libavcodec/mips/cabac.h
+++ b/libavcodec/mips/cabac.h
@@ -25,7 +25,7 @@
 #define AVCODEC_MIPS_CABAC_H
 
 #include "libavcodec/cabac.h"
-#include "libavutil/mips/mmiutils.h"
+#include "libavutil/mips/asmdefs.h"
 #include "config.h"
 
 #define get_cabac_inline get_cabac_inline_mips
@@ -109,7 +109,7 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
       [lps_off]"i"(H264_LPS_RANGE_OFFSET),
       [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
       [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
-      [cabac_mask]"i"(CABAC_MASK)
+      [cabac_mask]"r"(CABAC_MASK)
     : "memory"
     );
 
diff --git a/libavcodec/mips/fft_mips.c b/libavcodec/mips/fft_mips.c
index 03dcbad..69abdc8 100644
--- a/libavcodec/mips/fft_mips.c
+++ b/libavcodec/mips/fft_mips.c
@@ -71,6 +71,7 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
     float temp, temp1, temp3, temp4;
     FFTComplex * tmpz_n2, * tmpz_n34, * tmpz_n4;
     FFTComplex * tmpz_n2_i, * tmpz_n34_i, * tmpz_n4_i, * tmpz_i;
+    float f1 = 0.7071067812;
 
     num_transforms = (21845 >> (17 - s->nbits)) | 1;
 
@@ -148,7 +149,6 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
             "swc1  %[pom2], 4(%[tmpz])                      \n\t"  // tmpz[0].im = tmpz[0].im + tmp6;
             "lwc1  %[pom1], 16(%[tmpz])                     \n\t"
             "lwc1  %[pom3], 20(%[tmpz])                     \n\t"
-            "li.s  %[pom],  0.7071067812                    \n\t"  // float pom = 0.7071067812f;
             "add.s %[temp1],%[tmp1],    %[tmp2]             \n\t"
             "sub.s %[temp], %[pom1],    %[tmp8]             \n\t"
             "add.s %[pom2], %[pom3],    %[tmp7]             \n\t"
@@ -159,10 +159,10 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
             "add.s %[pom1], %[pom1],    %[tmp8]             \n\t"
             "sub.s %[pom3], %[pom3],    %[tmp7]             \n\t"
             "add.s %[tmp3], %[tmp3],    %[tmp4]             \n\t"
-            "mul.s %[tmp5], %[pom],     %[temp1]            \n\t"  // tmp5 = pom * (tmp1 + tmp2);
-            "mul.s %[tmp7], %[pom],     %[temp3]            \n\t"  // tmp7 = pom * (tmp3 - tmp4);
-            "mul.s %[tmp6], %[pom],     %[temp4]            \n\t"  // tmp6 = pom * (tmp2 - tmp1);
-            "mul.s %[tmp8], %[pom],     %[tmp3]             \n\t"  // tmp8 = pom * (tmp3 + tmp4);
+            "mul.s %[tmp5], %[f1],      %[temp1]            \n\t"  // tmp5 = pom * (tmp1 + tmp2);
+            "mul.s %[tmp7], %[f1],      %[temp3]            \n\t"  // tmp7 = pom * (tmp3 - tmp4);
+            "mul.s %[tmp6], %[f1],      %[temp4]            \n\t"  // tmp6 = pom * (tmp2 - tmp1);
+            "mul.s %[tmp8], %[f1],      %[tmp3]             \n\t"  // tmp8 = pom * (tmp3 + tmp4);
             "swc1  %[pom1], 16(%[tmpz])                     \n\t"  // tmpz[2].re = tmpz[2].re + tmp8;
             "swc1  %[pom3], 20(%[tmpz])                     \n\t"  // tmpz[2].im = tmpz[2].im - tmp7;
             "add.s %[tmp1], %[tmp5],    %[tmp7]             \n\t"  // tmp1 = tmp5 + tmp7;
@@ -193,7 +193,7 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
               [tmp3]"=&f"(tmp3), [tmp2]"=&f"(tmp2), [tmp4]"=&f"(tmp4), [tmp5]"=&f"(tmp5),  [tmp7]"=&f"(tmp7),
               [tmp6]"=&f"(tmp6), [tmp8]"=&f"(tmp8), [pom3]"=&f"(pom3),[temp]"=&f"(temp), [temp1]"=&f"(temp1),
               [temp3]"=&f"(temp3), [temp4]"=&f"(temp4)
-            : [tmpz]"r"(tmpz)
+            : [tmpz]"r"(tmpz), [f1]"f"(f1)
             : "memory"
         );
     }
diff --git a/libavcodec/mips/h263dsp_init_mips.c b/libavcodec/mips/h263dsp_init_mips.c
index 09bd937..a73eb12 100644
--- a/libavcodec/mips/h263dsp_init_mips.c
+++ b/libavcodec/mips/h263dsp_init_mips.c
@@ -18,19 +18,15 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h263dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h263dsp_init_msa(H263DSPContext *c)
-{
-    c->h263_h_loop_filter = ff_h263_h_loop_filter_msa;
-    c->h263_v_loop_filter = ff_h263_v_loop_filter_msa;
-}
-#endif  // #if HAVE_MSA
-
 av_cold void ff_h263dsp_init_mips(H263DSPContext *c)
 {
-#if HAVE_MSA
-    h263dsp_init_msa(c);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)){
+        c->h263_h_loop_filter = ff_h263_h_loop_filter_msa;
+        c->h263_v_loop_filter = ff_h263_v_loop_filter_msa;
+    }
 }
diff --git a/libavcodec/mips/h263dsp_msa.c b/libavcodec/mips/h263dsp_msa.c
index 472bcbd..2e1ca01 100644
--- a/libavcodec/mips/h263dsp_msa.c
+++ b/libavcodec/mips/h263dsp_msa.c
@@ -86,10 +86,7 @@ static void h263_h_loop_filter_msa(uint8_t *src, int32_t stride, int32_t qscale)
     ILVR_B2_SH(in3, in0, in1, in2, temp0, temp1);
     in0 = (v16u8) __msa_ilvr_h(temp1, temp0);
     in3 = (v16u8) __msa_ilvl_h(temp1, temp0);
-    ST4x4_UB(in0, in0, 0, 1, 2, 3, src, stride);
-    src += 4 * stride;
-    ST4x4_UB(in3, in3, 0, 1, 2, 3, src, stride);
-    src += 4 * stride;
+    ST_W8(in0, in3, 0, 1, 2, 3, 0, 1, 2, 3, src, stride);
 }
 
 static void h263_v_loop_filter_msa(uint8_t *src, int32_t stride, int32_t qscale)
diff --git a/libavcodec/mips/h264chroma_init_mips.c b/libavcodec/mips/h264chroma_init_mips.c
index 122148d..6bb19d3 100644
--- a/libavcodec/mips/h264chroma_init_mips.c
+++ b/libavcodec/mips/h264chroma_init_mips.c
@@ -19,45 +19,34 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264chroma_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264chroma_init_msa(H264ChromaContext *c, int bit_depth)
-{
-    const int high_bit_depth = bit_depth > 8;
-
-    if (!high_bit_depth) {
-        c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_msa;
-        c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_msa;
-        c->put_h264_chroma_pixels_tab[2] = ff_put_h264_chroma_mc2_msa;
-
-        c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_msa;
-        c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_msa;
-        c->avg_h264_chroma_pixels_tab[2] = ff_avg_h264_chroma_mc2_msa;
-    }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void h264chroma_init_mmi(H264ChromaContext *c, int bit_depth)
+av_cold void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth)
 {
+    int cpu_flags = av_get_cpu_flags();
     int high_bit_depth = bit_depth > 8;
 
-    if (!high_bit_depth) {
-        c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_mmi;
-        c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_mmi;
-        c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_mmi;
-        c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_mmi;
+    /* MMI apears to be faster than MSA here */
+    if (have_msa(cpu_flags)) {
+        if (!high_bit_depth) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_msa;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_msa;
+            c->put_h264_chroma_pixels_tab[2] = ff_put_h264_chroma_mc2_msa;
+
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_msa;
+            c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_msa;
+            c->avg_h264_chroma_pixels_tab[2] = ff_avg_h264_chroma_mc2_msa;
+        }
     }
-}
-#endif /* HAVE_MMI */
 
-av_cold void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth)
-{
-#if HAVE_MSA
-    h264chroma_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    h264chroma_init_mmi(c, bit_depth);
-#endif /* HAVE_MMI */
+    if (have_mmi(cpu_flags)) {
+        if (!high_bit_depth) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_mmi;
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_mmi;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_mmi;
+            c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_mmi;
+        }
+    }
 }
diff --git a/libavcodec/mips/h264chroma_mmi.c b/libavcodec/mips/h264chroma_mmi.c
index 91b2cc4..739dd7d 100644
--- a/libavcodec/mips/h264chroma_mmi.c
+++ b/libavcodec/mips/h264chroma_mmi.c
@@ -30,74 +30,177 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         int h, int x, int y)
 {
     int A = 64, B, C, D, E;
-    double ftmp[10];
+    double ftmp[12];
     uint64_t tmp[1];
 
     if (!(x || y)) {
         /* x=0, y=0, A=64 */
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-            "dli        %[tmp0],    0x06                               \n\t"
-            "mtc1       %[tmp0],    %[ftmp4]                           \n\t"
-
             "1:                                                        \n\t"
+            MMI_ULDC1(%[ftmp0], %[src], 0x00)
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
             MMI_ULDC1(%[ftmp1], %[src], 0x00)
-            "addi       %[h],       %[h],           -0x04              \n\t"
             PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-            MMI_ULDC1(%[ftmp5], %[src], 0x00)
+            MMI_ULDC1(%[ftmp2], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-            MMI_ULDC1(%[ftmp6], %[src], 0x00)
+            MMI_ULDC1(%[ftmp3], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-            MMI_ULDC1(%[ftmp7], %[src], 0x00)
 
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]           \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]           \n\t"
-            "psllh      %[ftmp1],   %[ftmp2],       %[ftmp4]           \n\t"
-            "psllh      %[ftmp2],   %[ftmp3],       %[ftmp4]           \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]           \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-            MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            "addi       %[h],       %[h],           -0x04              \n\t"
 
-            "punpcklbh  %[ftmp2],   %[ftmp5],       %[ftmp0]           \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp5],       %[ftmp0]           \n\t"
-            "psllh      %[ftmp1],   %[ftmp2],       %[ftmp4]           \n\t"
-            "psllh      %[ftmp2],   %[ftmp3],       %[ftmp4]           \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]           \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
+            MMI_SDC1(%[ftmp0], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            MMI_SDC1(%[ftmp2], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            MMI_SDC1(%[ftmp3], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            "bnez       %[h],       1b                                 \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride)
+            : "memory"
+        );
+    } else if (x && y) {
+        /* x!=0, y!=0 */
+        D = x * y;
+        B = (x << 3) - D;
+        C = (y << 3) - D;
+        A = 64 - D - B - C;
 
-            "punpcklbh  %[ftmp2],   %[ftmp6],       %[ftmp0]           \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp6],       %[ftmp0]           \n\t"
-            "psllh      %[ftmp1],   %[ftmp2],       %[ftmp4]           \n\t"
-            "psllh      %[ftmp2],   %[ftmp3],       %[ftmp4]           \n\t"
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
+            "dli        %[tmp0],    0x06                               \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]           \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                           \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]           \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]           \n\t"
+
+            "1:                                                        \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            MMI_ULDC1(%[ftmp2], %[src], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
+            MMI_ULDC1(%[ftmp3], %[src], 0x00)
+            MMI_ULDC1(%[ftmp4], %[src], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
+            MMI_ULDC1(%[ftmp10], %[src], 0x00)
+            MMI_ULDC1(%[ftmp11], %[src], 0x01)
+            "addi       %[h],       %[h],           -0x02              \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]               \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[B]               \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[A]               \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[B]               \n\t"
+            "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]           \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]               \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[D]               \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp7]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[C]               \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[D]               \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp8]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp5]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp6]           \n\t"
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
             "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]           \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]           \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]           \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]               \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[B]               \n\t"
+            "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[A]               \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[B]               \n\t"
+            "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]           \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp10],      %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp10],      %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp11],      %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp11],      %[ftmp0]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]               \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[D]               \n\t"
+            "paddh      %[ftmp5],   %[ftmp5],       %[ftmp7]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[C]               \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[D]               \n\t"
+            "paddh      %[ftmp6],   %[ftmp6],       %[ftmp8]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]           \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp6]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_32]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_32]        \n\t"
+            "psrlh      %[ftmp3],   %[ftmp3],       %[ftmp9]           \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp9]           \n\t"
+            "packushb   %[ftmp3],   %[ftmp3],       %[ftmp4]           \n\t"
+
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            MMI_SDC1(%[ftmp3], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            "bnez       %[h],       1b                                 \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
+              [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
+              [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
+    } else if (x) {
+        /* x!=0, y==0 */
+        E = x << 3;
+        A = 64 - E;
+
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
+            "dli        %[tmp0],    0x06                               \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
+
+            "1:                                                        \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            MMI_ULDC1(%[ftmp2], %[src], 0x01)
+            "addi       %[h],       %[h],           -0x01              \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]               \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]               \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]           \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]               \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]               \n\t"
+            "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]           \n\t"
 
-            "punpcklbh  %[ftmp2],   %[ftmp7],       %[ftmp0]           \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp7],       %[ftmp0]           \n\t"
-            "psllh      %[ftmp1],   %[ftmp2],       %[ftmp4]           \n\t"
-            "psllh      %[ftmp2],   %[ftmp3],       %[ftmp4]           \n\t"
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
             "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]           \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]           \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]           \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
-
-            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             "bnez       %[h],       1b                                 \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
@@ -107,220 +210,80 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
               [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32)
+            : [stride]"r"((mips_reg)stride),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
             : "memory"
         );
     } else {
-        if (x && y) {
-            /* x!=0, y!=0 */
-            D = x * y;
-            B = (x << 3) - D;
-            C = (y << 3) - D;
-            A = 64 - D - B - C;
-
-            __asm__ volatile (
-                "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-                "dli        %[tmp0],    0x06                               \n\t"
-                "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
-                "pshufh     %[B],       %[B],           %[ftmp0]           \n\t"
-                "mtc1       %[tmp0],    %[ftmp9]                           \n\t"
-                "pshufh     %[C],       %[C],           %[ftmp0]           \n\t"
-                "pshufh     %[D],       %[D],           %[ftmp0]           \n\t"
-
-                "1:                                                        \n\t"
-                MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                MMI_ULDC1(%[ftmp2], %[src], 0x01)
-                PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-                MMI_ULDC1(%[ftmp3], %[src], 0x00)
-                MMI_ULDC1(%[ftmp4], %[src], 0x01)
-                "addi       %[h],       %[h],           -0x02              \n\t"
-
-                "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]           \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]           \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[A]               \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[B]               \n\t"
-                "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]           \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[A]               \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[B]               \n\t"
-                "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]           \n\t"
-
-                "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]           \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]           \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[C]               \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[D]               \n\t"
-                "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]           \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[C]               \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[D]               \n\t"
-                "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]           \n\t"
-
-                "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]           \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
-                "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-                "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]           \n\t"
-                "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]           \n\t"
-                "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-                MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
-
-                MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                MMI_ULDC1(%[ftmp2], %[src], 0x01)
-                PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-                MMI_ULDC1(%[ftmp3], %[src], 0x00)
-                MMI_ULDC1(%[ftmp4], %[src], 0x01)
-
-                "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]           \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]           \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[A]               \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[B]               \n\t"
-                "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]           \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[A]               \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[B]               \n\t"
-                "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]           \n\t"
-
-                "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]           \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]           \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]           \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[C]               \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[D]               \n\t"
-                "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]           \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[C]               \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[D]               \n\t"
-                "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]           \n\t"
-
-                "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]           \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]           \n\t"
-                "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-                "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]           \n\t"
-                "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]           \n\t"
-                "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-                MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
-
-                "bnez       %[h],       1b                                 \n\t"
-                : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                  [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                  [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                  [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                  [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
-                  [tmp0]"=&r"(tmp[0]),
-                  [dst]"+&r"(dst),              [src]"+&r"(src),
-                  [h]"+&r"(h)
-                : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-                  [A]"f"(A),                    [B]"f"(B),
-                  [C]"f"(C),                    [D]"f"(D)
-                : "memory"
-            );
-        } else {
-            if (x) {
-                /* x!=0, y==0 */
-                E = x << 3;
-                A = 64 - E;
-
-                __asm__ volatile (
-                    "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-                    "dli        %[tmp0],    0x06                               \n\t"
-                    "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
-                    "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
-                    "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
-
-                    "1:                                                        \n\t"
-                    MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                    MMI_ULDC1(%[ftmp2], %[src], 0x01)
-                    "addi       %[h],       %[h],           -0x01              \n\t"
-                    PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-
-                    "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]           \n\t"
-                    "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]           \n\t"
-                    "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]           \n\t"
-                    "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]           \n\t"
-                    "pmullh     %[ftmp3],   %[ftmp3],       %[A]               \n\t"
-                    "pmullh     %[ftmp5],   %[ftmp5],       %[E]               \n\t"
-                    "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]           \n\t"
-                    "pmullh     %[ftmp4],   %[ftmp4],       %[A]               \n\t"
-                    "pmullh     %[ftmp6],   %[ftmp6],       %[E]               \n\t"
-                    "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]           \n\t"
-
-                    "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-                    "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-                    "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]           \n\t"
-                    "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]           \n\t"
-                    "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-                    MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                    PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
-                    "bnez       %[h],       1b                                 \n\t"
-                    : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                      [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                      [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                      [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                      [tmp0]"=&r"(tmp[0]),
-                      [dst]"+&r"(dst),              [src]"+&r"(src),
-                      [h]"+&r"(h)
-                    : [stride]"r"((mips_reg)stride),
-                      [ff_pw_32]"f"(ff_pw_32),
-                      [A]"f"(A),                    [E]"f"(E)
-                    : "memory"
-                );
-            } else {
-                /* x==0, y!=0 */
-                E = y << 3;
-                A = 64 - E;
-
-                __asm__ volatile (
-                    "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-                    "dli        %[tmp0],    0x06                               \n\t"
-                    "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
-                    "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
-                    "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
-
-                    "1:                                                        \n\t"
-                    MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                    PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
-                    MMI_ULDC1(%[ftmp2], %[src], 0x00)
-                    "addi       %[h],       %[h],           -0x01              \n\t"
-
-                    "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]           \n\t"
-                    "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]           \n\t"
-                    "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]           \n\t"
-                    "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]           \n\t"
-                    "pmullh     %[ftmp3],   %[ftmp3],       %[A]               \n\t"
-                    "pmullh     %[ftmp5],   %[ftmp5],       %[E]               \n\t"
-                    "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]           \n\t"
-                    "pmullh     %[ftmp4],   %[ftmp4],       %[A]               \n\t"
-                    "pmullh     %[ftmp6],   %[ftmp6],       %[E]               \n\t"
-                    "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]           \n\t"
-
-                    "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]        \n\t"
-                    "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]        \n\t"
-                    "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]           \n\t"
-                    "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]           \n\t"
-                    "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]           \n\t"
-                    MMI_SDC1(%[ftmp1], %[dst], 0x00)
-
-                    PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
-                    "bnez       %[h],       1b                                 \n\t"
-                    : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                      [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                      [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                      [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                      [tmp0]"=&r"(tmp[0]),
-                      [dst]"+&r"(dst),              [src]"+&r"(src),
-                      [h]"+&r"(h)
-                    : [stride]"r"((mips_reg)stride),
-                      [ff_pw_32]"f"(ff_pw_32),
-                      [A]"f"(A),                    [E]"f"(E)
-                    : "memory"
-                );
-            }
-        }
+        /* x==0, y!=0 */
+        E = y << 3;
+        A = 64 - E;
+
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
+            "dli        %[tmp0],    0x06                               \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
+
+            "1:                                                        \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
+            MMI_ULDC1(%[ftmp2], %[src], 0x00)
+            PTR_ADDU   "%[src],     %[src],         %[stride]          \n\t"
+            MMI_ULDC1(%[ftmp8], %[src], 0x00)
+            "addi       %[h],       %[h],           -0x02              \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]               \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]               \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]           \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]               \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]               \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp6]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_32]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_32]        \n\t"
+            "psrlh      %[ftmp3],   %[ftmp3],       %[ftmp7]           \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp7]           \n\t"
+            "packushb   %[ftmp1],   %[ftmp3],       %[ftmp4]           \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp2],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp2],       %[ftmp0]           \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp8],       %[ftmp0]           \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp8],       %[ftmp0]           \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]               \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]               \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ftmp5]           \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]               \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]               \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ftmp6]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp3],       %[ff_pw_32]        \n\t"
+            "paddh      %[ftmp4],   %[ftmp4],       %[ff_pw_32]        \n\t"
+            "psrlh      %[ftmp3],   %[ftmp3],       %[ftmp7]           \n\t"
+            "psrlh      %[ftmp4],   %[ftmp4],       %[ftmp7]           \n\t"
+            "packushb   %[ftmp2],   %[ftmp3],       %[ftmp4]           \n\t"
+
+            MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            MMI_SDC1(%[ftmp2], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
+            "bnez       %[h],       1b                                 \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
     }
 }
 
@@ -334,231 +297,200 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
     if(!(x || y)){
         /* x=0, y=0, A=64 */
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
-            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
-            "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
-
             "1:                                                         \n\t"
-            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            MMI_ULDC1(%[ftmp0], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
-            MMI_ULDC1(%[ftmp5], %[src], 0x00)
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
-
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
             MMI_LDC1(%[ftmp2], %[dst], 0x00)
-            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
-            MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
-
-            "punpcklbh  %[ftmp2],   %[ftmp5],       %[ftmp0]            \n\t"
-            "punpckhbh  %[ftmp3],   %[ftmp5],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "pmullh     %[ftmp2],   %[ftmp3],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp4]            \n\t"
-            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp4]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
-            MMI_LDC1(%[ftmp2], %[dst], 0x00)
-            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            MMI_LDC1(%[ftmp3], %[dst], 0x00)
+            PTR_SUBU   "%[dst],     %[dst],         %[stride]           \n\t"
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
+            MMI_SDC1(%[ftmp0], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
-
             "addi       %[h],       %[h],           -0x02               \n\t"
             "bnez       %[h],       1b                                  \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride)
+            : "memory"
+        );
+    } else if (x && y) {
+        /* x!=0, y!=0 */
+        D = x * y;
+        B = (x << 3) - D;
+        C = (y << 3) - D;
+        A = 64 - D - B - C;
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
+            "dli        %[tmp0],    0x06                           \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
+            "pshufh     %[B],       %[B],           %[ftmp0]       \n\t"
+            "mtc1       %[tmp0],    %[ftmp9]                       \n\t"
+            "pshufh     %[C],       %[C],           %[ftmp0]       \n\t"
+            "pshufh     %[D],       %[D],           %[ftmp0]       \n\t"
+
+            "1:                                                    \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            MMI_ULDC1(%[ftmp2], %[src], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
+            MMI_ULDC1(%[ftmp3], %[src], 0x00)
+            MMI_ULDC1(%[ftmp4], %[src], 0x01)
+            "addi       %[h],       %[h],           -0x01          \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]       \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[A]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[B]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]       \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[A]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[B]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]       \n\t"
+
+            "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]       \n\t"
+            "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]       \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[C]           \n\t"
+            "pmullh     %[ftmp7],   %[ftmp7],       %[D]           \n\t"
+            "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]       \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[C]           \n\t"
+            "pmullh     %[ftmp8],   %[ftmp8],       %[D]           \n\t"
+            "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]       \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]       \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]       \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]       \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]       \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_LDC1(%[ftmp2], %[dst], 0x00)
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
+            "bnez       %[h],       1b                             \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
               [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A)
+              [A]"f"(A),                    [B]"f"(B),
+              [C]"f"(C),                    [D]"f"(D)
+            : "memory"
+        );
+    } else if (x) {
+        /* x!=0, y==0 */
+        E = x << 3;
+        A = 64 - E;
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
+            "dli        %[tmp0],    0x06                           \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
+
+            "1:                                                    \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            MMI_ULDC1(%[ftmp2], %[src], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
+            "addi       %[h],       %[h],           -0x01          \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]       \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]       \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]       \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]       \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]       \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_LDC1(%[ftmp2], %[dst], 0x00)
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
+            "bnez       %[h],       1b                             \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
             : "memory"
         );
     } else {
-        if(x && y) {
-            /* x!=0, y!=0 */
-            D = x * y;
-            B = (x << 3) - D;
-            C = (y << 3) - D;
-            A = 64 - D - B - C;
-            __asm__ volatile (
-                "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-                "dli        %[tmp0],    0x06                           \n\t"
-                "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
-                "pshufh     %[B],       %[B],           %[ftmp0]       \n\t"
-                "mtc1       %[tmp0],    %[ftmp9]                       \n\t"
-                "pshufh     %[C],       %[C],           %[ftmp0]       \n\t"
-                "pshufh     %[D],       %[D],           %[ftmp0]       \n\t"
-
-                "1:                                                    \n\t"
-                MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                MMI_ULDC1(%[ftmp2], %[src], 0x01)
-                PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
-                MMI_ULDC1(%[ftmp3], %[src], 0x00)
-                MMI_ULDC1(%[ftmp4], %[src], 0x01)
-                "addi       %[h],       %[h],           -0x01          \n\t"
-
-                "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]       \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp1],       %[ftmp0]       \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp2],       %[ftmp0]       \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp2],       %[ftmp0]       \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[A]           \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[B]           \n\t"
-                "paddh      %[ftmp1],   %[ftmp5],       %[ftmp7]       \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[A]           \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[B]           \n\t"
-                "paddh      %[ftmp2],   %[ftmp6],       %[ftmp8]       \n\t"
-
-                "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]       \n\t"
-                "punpckhbh  %[ftmp6],   %[ftmp3],       %[ftmp0]       \n\t"
-                "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]       \n\t"
-                "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]       \n\t"
-                "pmullh     %[ftmp5],   %[ftmp5],       %[C]           \n\t"
-                "pmullh     %[ftmp7],   %[ftmp7],       %[D]           \n\t"
-                "paddh      %[ftmp3],   %[ftmp5],       %[ftmp7]       \n\t"
-                "pmullh     %[ftmp6],   %[ftmp6],       %[C]           \n\t"
-                "pmullh     %[ftmp8],   %[ftmp8],       %[D]           \n\t"
-                "paddh      %[ftmp4],   %[ftmp6],       %[ftmp8]       \n\t"
-
-                "paddh      %[ftmp1],   %[ftmp1],       %[ftmp3]       \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ftmp4]       \n\t"
-                "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
-                "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
-                "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp9]       \n\t"
-                "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp9]       \n\t"
-                "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                MMI_LDC1(%[ftmp2], %[dst], 0x00)
-                "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
-                "bnez       %[h],       1b                             \n\t"
-                : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                  [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                  [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                  [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                  [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
-                  [tmp0]"=&r"(tmp[0]),
-                  [dst]"+&r"(dst),              [src]"+&r"(src),
-                  [h]"+&r"(h)
-                : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-                  [A]"f"(A),                    [B]"f"(B),
-                  [C]"f"(C),                    [D]"f"(D)
-                : "memory"
-            );
-        } else {
-            if(x) {
-                /* x!=0, y==0 */
-                E = x << 3;
-                A = 64 - E;
-                __asm__ volatile (
-                    "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-                    "dli        %[tmp0],    0x06                           \n\t"
-                    "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
-                    "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
-                    "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
-
-                    "1:                                                    \n\t"
-                    MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                    MMI_ULDC1(%[ftmp2], %[src], 0x01)
-                    PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
-                    "addi       %[h],       %[h],           -0x01          \n\t"
-
-                    "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]       \n\t"
-                    "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]       \n\t"
-                    "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]       \n\t"
-                    "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]       \n\t"
-                    "pmullh     %[ftmp3],   %[ftmp3],       %[A]           \n\t"
-                    "pmullh     %[ftmp5],   %[ftmp5],       %[E]           \n\t"
-                    "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]       \n\t"
-                    "pmullh     %[ftmp4],   %[ftmp4],       %[A]           \n\t"
-                    "pmullh     %[ftmp6],   %[ftmp6],       %[E]           \n\t"
-                    "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]       \n\t"
-
-                    "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
-                    "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
-                    "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]       \n\t"
-                    "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]       \n\t"
-                    "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                    MMI_LDC1(%[ftmp2], %[dst], 0x00)
-                    "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                    MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                    PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
-                    "bnez       %[h],       1b                             \n\t"
-                    : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                      [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                      [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                      [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                      [tmp0]"=&r"(tmp[0]),
-                      [dst]"+&r"(dst),              [src]"+&r"(src),
-                      [h]"+&r"(h)
-                    : [stride]"r"((mips_reg)stride),
-                      [ff_pw_32]"f"(ff_pw_32),
-                      [A]"f"(A),                    [E]"f"(E)
-                    : "memory"
-                );
-            } else {
-                /* x==0, y!=0 */
-                E = y << 3;
-                A = 64 - E;
-                __asm__ volatile (
-                    "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-                    "dli        %[tmp0],    0x06                           \n\t"
-                    "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
-                    "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
-                    "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
-
-                    "1:                                                    \n\t"
-                    MMI_ULDC1(%[ftmp1], %[src], 0x00)
-                    PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
-                    MMI_ULDC1(%[ftmp2], %[src], 0x00)
-                    "addi       %[h],       %[h],           -0x01          \n\t"
-
-                    "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]       \n\t"
-                    "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]       \n\t"
-                    "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]       \n\t"
-                    "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]       \n\t"
-                    "pmullh     %[ftmp3],   %[ftmp3],       %[A]           \n\t"
-                    "pmullh     %[ftmp5],   %[ftmp5],       %[E]           \n\t"
-                    "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]       \n\t"
-                    "pmullh     %[ftmp4],   %[ftmp4],       %[A]           \n\t"
-                    "pmullh     %[ftmp6],   %[ftmp6],       %[E]           \n\t"
-                    "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]       \n\t"
-
-                    "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
-                    "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
-                    "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]       \n\t"
-                    "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]       \n\t"
-                    "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                    MMI_LDC1(%[ftmp2], %[dst], 0x00)
-                    "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
-                    MMI_SDC1(%[ftmp1], %[dst], 0x00)
-                    PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
-                    "bnez       %[h],       1b                             \n\t"
-                    : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-                      [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-                      [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-                      [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-                      [tmp0]"=&r"(tmp[0]),
-                      [dst]"+&r"(dst),              [src]"+&r"(src),
-                      [h]"+&r"(h)
-                    : [stride]"r"((mips_reg)stride),
-                      [ff_pw_32]"f"(ff_pw_32),
-                      [A]"f"(A),                    [E]"f"(E)
-                    : "memory"
-                );
-            }
-        }
+        /* x==0, y!=0 */
+        E = y << 3;
+        A = 64 - E;
+        __asm__ volatile (
+            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
+            "dli        %[tmp0],    0x06                           \n\t"
+            "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
+            "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
+            "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
+
+            "1:                                                    \n\t"
+            MMI_ULDC1(%[ftmp1], %[src], 0x00)
+            PTR_ADDU   "%[src],     %[src],         %[stride]      \n\t"
+            MMI_ULDC1(%[ftmp2], %[src], 0x00)
+            "addi       %[h],       %[h],           -0x01          \n\t"
+
+            "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp4],   %[ftmp1],       %[ftmp0]       \n\t"
+            "punpcklbh  %[ftmp5],   %[ftmp2],       %[ftmp0]       \n\t"
+            "punpckhbh  %[ftmp6],   %[ftmp2],       %[ftmp0]       \n\t"
+            "pmullh     %[ftmp3],   %[ftmp3],       %[A]           \n\t"
+            "pmullh     %[ftmp5],   %[ftmp5],       %[E]           \n\t"
+            "paddh      %[ftmp1],   %[ftmp3],       %[ftmp5]       \n\t"
+            "pmullh     %[ftmp4],   %[ftmp4],       %[A]           \n\t"
+            "pmullh     %[ftmp6],   %[ftmp6],       %[E]           \n\t"
+            "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]       \n\t"
+
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
+            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]       \n\t"
+            "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]       \n\t"
+            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_LDC1(%[ftmp2], %[dst], 0x00)
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
+            MMI_SDC1(%[ftmp1], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
+            "bnez       %[h],       1b                             \n\t"
+            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
+              [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
+              [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
+              [tmp0]"=&r"(tmp[0]),
+              [dst]"+&r"(dst),              [src]"+&r"(src),
+              [h]"+&r"(h)
+            : [stride]"r"((mips_reg)stride),
+              [ff_pw_32]"f"(ff_pw_32),
+              [A]"f"(A),                    [E]"f"(E)
+            : "memory"
+        );
     }
 }
 
@@ -567,8 +499,8 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
 {
     const int A = (8 - x) * (8 - y);
     const int B = x * (8 - y);
-    const int C = (8 - x) *  y;
-    const int D = x *  y;
+    const int C = (8 - x) * y;
+    const int D = x * y;
     const int E = B + C;
     double ftmp[8];
     uint64_t tmp[1];
@@ -586,31 +518,29 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
 
             "1:                                                         \n\t"
-            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
             MMI_ULWC1(%[ftmp2], %[src], 0x01)
-            MMI_ULWC1(%[ftmp3], %[addr0], 0x00)
-            MMI_ULWC1(%[ftmp4], %[addr0], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            MMI_ULWC1(%[ftmp3], %[src], 0x00)
+            MMI_ULWC1(%[ftmp4], %[src], 0x01)
 
             "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
             "pmullh     %[ftmp6],   %[ftmp6],       %[B]                \n\t"
             "paddh      %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
-
             "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp6],   %[ftmp4],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
             "pmullh     %[ftmp6],   %[ftmp6],       %[D]                \n\t"
             "paddh      %[ftmp2],   %[ftmp5],       %[ftmp6]            \n\t"
-
             "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
             "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+
             "addi       %[h],       %[h],           -0x01               \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
@@ -619,7 +549,6 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
               [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
-              [addr0]"=&r"(addr[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
@@ -629,7 +558,6 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else if (E) {
         const int step = C ? stride : 1;
-
         __asm__ volatile (
             "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "dli        %[tmp0],    0x06                                \n\t"
@@ -638,22 +566,20 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
 
             "1:                                                         \n\t"
-            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
             MMI_ULWC1(%[ftmp2], %[addr0], 0x00)
-
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
             "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp4],   %[ftmp2],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
             "pmullh     %[ftmp4],   %[ftmp4],       %[E]                \n\t"
             "paddh      %[ftmp1],   %[ftmp3],       %[ftmp4]            \n\t"
-
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
             "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
-            "addi       %[h],       %[h],           -0x01               \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
@@ -671,42 +597,22 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
-            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
-            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
-
             "1:                                                         \n\t"
-            MMI_ULWC1(%[ftmp1], %[src], 0x00)
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            MMI_ULWC1(%[ftmp0], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
-            MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
-
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             "addi       %[h],       %[h],           -0x02               \n\t"
+            MMI_SWC1(%[ftmp0], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
-              [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-              [tmp0]"=&r"(tmp[0]),
-              RESTRICT_ASM_LOW32
               [dst]"+&r"(dst),              [src]"+&r"(src),
+              RESTRICT_ASM_LOW32
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A)
+            : [stride]"r"((mips_reg)stride)
             : "memory"
         );
     }
@@ -736,33 +642,31 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             "pshufh     %[D],       %[D],           %[ftmp0]            \n\t"
 
             "1:                                                         \n\t"
-            PTR_ADDU   "%[addr0],   %[src],         %[stride]           \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
             MMI_ULWC1(%[ftmp2], %[src], 0x01)
-            MMI_ULWC1(%[ftmp3], %[addr0], 0x00)
-            MMI_ULWC1(%[ftmp4], %[addr0], 0x01)
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            MMI_ULWC1(%[ftmp3], %[src], 0x00)
+            MMI_ULWC1(%[ftmp4], %[src], 0x01)
 
             "punpcklbh  %[ftmp5],   %[ftmp1],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp6],   %[ftmp2],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp5],   %[ftmp5],       %[A]                \n\t"
             "pmullh     %[ftmp6],   %[ftmp6],       %[B]                \n\t"
             "paddh      %[ftmp1],   %[ftmp5],       %[ftmp6]            \n\t"
-
             "punpcklbh  %[ftmp5],   %[ftmp3],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp6],   %[ftmp4],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp5],   %[ftmp5],       %[C]                \n\t"
             "pmullh     %[ftmp6],   %[ftmp6],       %[D]                \n\t"
             "paddh      %[ftmp2],   %[ftmp5],       %[ftmp6]            \n\t"
-
             "paddh      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
             "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]            \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
             MMI_LWC1(%[ftmp2], %[dst], 0x00)
             "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+
             "addi       %[h],       %[h],           -0x01               \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
@@ -771,7 +675,6 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
               [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
-              [addr0]"=&r"(addr[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
@@ -781,32 +684,30 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else if (E) {
         const int step = C ? stride : 1;
-
         __asm__ volatile (
             "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "dli        %[tmp0],    0x06                                \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
             "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
+
             "1:                                                         \n\t"
-            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
+            PTR_ADDU   "%[addr0],   %[src],         %[step]             \n\t"
             MMI_ULWC1(%[ftmp2], %[addr0], 0x00)
-
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
+            "addi       %[h],       %[h],           -0x01               \n\t"
             "punpcklbh  %[ftmp3],   %[ftmp1],       %[ftmp0]            \n\t"
             "punpcklbh  %[ftmp4],   %[ftmp2],       %[ftmp0]            \n\t"
             "pmullh     %[ftmp3],   %[ftmp3],       %[A]                \n\t"
             "pmullh     %[ftmp4],   %[ftmp4],       %[E]                \n\t"
             "paddh      %[ftmp1],   %[ftmp3],       %[ftmp4]            \n\t"
-
             "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
             "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp5]            \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
             MMI_LWC1(%[ftmp2], %[dst], 0x00)
             "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
-            "addi       %[h],       %[h],           -0x01               \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
@@ -824,46 +725,27 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
-            "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
-            "mtc1       %[tmp0],    %[ftmp3]                            \n\t"
-
             "1:                                                         \n\t"
-            MMI_ULWC1(%[ftmp1], %[src], 0x00)
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
-            MMI_LWC1(%[ftmp2], %[dst], 0x00)
-            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            MMI_ULWC1(%[ftmp0], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
-            MMI_SWC1(%[ftmp1], %[dst], 0x00)
-            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
-
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
-            "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]            \n\t"
-            "pmullh     %[ftmp1],   %[ftmp2],       %[A]                \n\t"
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]         \n\t"
-            "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
-            "packushb   %[ftmp1],   %[ftmp1],       %[ftmp0]            \n\t"
-            MMI_LWC1(%[ftmp2], %[dst], 0x00)
-            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp2]            \n\t"
+            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             "addi       %[h],       %[h],           -0x02               \n\t"
+            MMI_LWC1(%[ftmp2], %[dst], 0x00)
+            "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]            \n\t"
+            MMI_SWC1(%[ftmp0], %[dst], 0x00)
+            PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
+            MMI_LWC1(%[ftmp3], %[dst], 0x00)
+            "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]            \n\t"
             MMI_SWC1(%[ftmp1], %[dst], 0x00)
-
-            PTR_ADDU   "%[src],     %[src],         %[stride]           \n\t"
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "bnez       %[h],       1b                                  \n\t"
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
-              [tmp0]"=&r"(tmp[0]),
-              RESTRICT_ASM_LOW32
               [dst]"+&r"(dst),              [src]"+&r"(src),
+              RESTRICT_ASM_LOW32
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A)
+            : [stride]"r"((mips_reg)stride)
             : "memory"
         );
     }
diff --git a/libavcodec/mips/h264chroma_msa.c b/libavcodec/mips/h264chroma_msa.c
index 4c25761..4a68d9e 100644
--- a/libavcodec/mips/h264chroma_msa.c
+++ b/libavcodec/mips/h264chroma_msa.c
@@ -85,7 +85,7 @@ static void avc_chroma_hz_2x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     res_r = __msa_sat_u_h(res_r, 7);
     res = (v8i16) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
 
-    ST2x4_UB(res, 0, dst, stride);
+    ST_H4(res, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_2w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -121,7 +121,7 @@ static void avc_chroma_hz_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     res_r = __msa_sat_u_h(res_r, 7);
     res = (v4i32) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
 
-    ST4x2_UB(res, dst, stride);
+    ST_W2(res, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -144,7 +144,7 @@ static void avc_chroma_hz_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H2_UH(res0_r, res1_r, 6);
     SAT_UH2_UH(res0_r, res1_r, 7);
     out = (v16u8) __msa_pckev_b((v16i8) res1_r, (v16i8) res0_r);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -168,7 +168,7 @@ static void avc_chroma_hz_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res0, res1, res2, res3, 6);
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
-    ST4x8_UB(out0, out1, dst, stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_4w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -204,7 +204,7 @@ static void avc_chroma_hz_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res0, res1, res2, res3, 6);
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -237,7 +237,7 @@ static void avc_chroma_hz_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SAT_UH4_UH(res4, res5, res6, res7, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     PCKEV_B2_UB(res5, res4, res7, res6, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_nonmult_msa(uint8_t *src, uint8_t *dst,
@@ -266,7 +266,7 @@ static void avc_chroma_hz_nonmult_msa(uint8_t *src, uint8_t *dst,
         SRARI_H4_UH(res0, res1, res2, res3, 6);
         SAT_UH4_UH(res0, res1, res2, res3, 7);
         PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
-        ST8x4_UB(out0, out1, dst, stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
         dst += (4 * stride);
     }
 
@@ -283,7 +283,7 @@ static void avc_chroma_hz_nonmult_msa(uint8_t *src, uint8_t *dst,
             res0 = __msa_sat_u_h(res0, 7);
             res0 = (v8u16) __msa_pckev_b((v16i8) res0, (v16i8) res0);
 
-            ST8x1_UB(res0, dst);
+            ST_D1(res0, 0, dst);
             dst += stride;
         }
     }
@@ -359,7 +359,7 @@ static void avc_chroma_vt_2x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 
     res = (v8i16) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
 
-    ST2x4_UB(res, 0, dst, stride);
+    ST_H4(res, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_2w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -394,7 +394,7 @@ static void avc_chroma_vt_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     res_r = __msa_sat_u_h(res_r, 7);
     res = (v4i32) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
 
-    ST4x2_UB(res, dst, stride);
+    ST_W2(res, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -418,7 +418,7 @@ static void avc_chroma_vt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H2_UH(res0_r, res1_r, 6);
     SAT_UH2_UH(res0_r, res1_r, 7);
     out = (v16u8) __msa_pckev_b((v16i8) res1_r, (v16i8) res0_r);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -446,7 +446,7 @@ static void avc_chroma_vt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res0, res1, res2, res3, 6);
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
-    ST4x8_UB(out0, out1, dst, stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_4w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -480,7 +480,7 @@ static void avc_chroma_vt_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res0, res1, res2, res3, 6);
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -512,7 +512,7 @@ static void avc_chroma_vt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     PCKEV_B2_UB(res5, res4, res7, res6, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_8w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -592,7 +592,7 @@ static void avc_chroma_hv_2x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 
     res = (v8i16) __msa_pckev_b((v16i8) res_vt0, (v16i8) res_vt0);
 
-    ST2x4_UB(res, 0, dst, stride);
+    ST_H4(res, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hv_2w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -634,7 +634,7 @@ static void avc_chroma_hv_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     res_vt0 = __msa_sat_u_h(res_vt0, 7);
     res = (v4i32) __msa_pckev_b((v16i8) res_vt0, (v16i8) res_vt0);
 
-    ST4x2_UB(res, dst, stride);
+    ST_W2(res, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -666,7 +666,8 @@ static void avc_chroma_hv_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H2_UH(res_vt0, res_vt1, 6);
     SAT_UH2_UH(res_vt0, res_vt1, 7);
     PCKEV_B2_SW(res_vt0, res_vt0, res_vt1, res_vt1, res0, res1);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, stride);
+    ST_W2(res0, 0, 1, dst, stride);
+    ST_W2(res1, 0, 1, dst + 2 * stride, stride);
 }
 
 static void avc_chroma_hv_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -706,7 +707,7 @@ static void avc_chroma_hv_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 6);
     SAT_UH4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 7);
     PCKEV_B2_UB(res_vt1, res_vt0, res_vt3, res_vt2, res0, res1);
-    ST4x8_UB(res0, res1, dst, stride);
+    ST_W8(res0, res1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hv_4w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -766,7 +767,7 @@ static void avc_chroma_hv_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRARI_H4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 6);
     SAT_UH4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 7);
     PCKEV_B2_UB(res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -822,7 +823,7 @@ static void avc_chroma_hv_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SAT_UH4_UH(res_vt4, res_vt5, res_vt6, res_vt7, 7);
     PCKEV_B2_UB(res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
     PCKEV_B2_UB(res_vt5, res_vt4, res_vt7, res_vt6, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_8w_msa(uint8_t *src, uint8_t *dst, int32_t stride,
@@ -918,7 +919,7 @@ static void avc_chroma_hz_and_aver_dst_2x4_msa(uint8_t *src, uint8_t *dst,
     dst0 = (v16u8) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
     dst0 = __msa_aver_u_b(dst0, dst_data);
 
-    ST2x4_UB(dst0, 0, dst, stride);
+    ST_H4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_2w_msa(uint8_t *src, uint8_t *dst,
@@ -962,7 +963,7 @@ static void avc_chroma_hz_and_aver_dst_4x2_msa(uint8_t *src, uint8_t *dst,
     res = __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
     dst_data = __msa_aver_u_b((v16u8) res, dst_data);
 
-    ST4x2_UB(dst_data, dst, stride);
+    ST_W2(dst_data, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
@@ -991,7 +992,7 @@ static void avc_chroma_hz_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH2_UH(res0_r, res1_r, 7);
     out = (v16u8) __msa_pckev_b((v16i8) res1_r, (v16i8) res0_r);
     out = __msa_aver_u_b(out, dst_data);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
@@ -1023,7 +1024,7 @@ static void avc_chroma_hz_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
-    ST4x8_UB(out0, out1, dst, stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_4w_msa(uint8_t *src, uint8_t *dst,
@@ -1066,7 +1067,7 @@ static void avc_chroma_hz_and_aver_dst_8x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
@@ -1110,7 +1111,7 @@ static void avc_chroma_hz_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
     PCKEV_B2_UB(res5, res4, res7, res6, out2, out3);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
     AVER_UB2_UB(out2, dst2, out3, dst3, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hz_and_aver_dst_8w_msa(uint8_t *src, uint8_t *dst,
@@ -1200,7 +1201,7 @@ static void avc_chroma_vt_and_aver_dst_2x4_msa(uint8_t *src, uint8_t *dst,
     res = (v8i16) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
     res = (v8i16) __msa_aver_u_b((v16u8) res, dst_data);
 
-    ST2x4_UB(res, 0, dst, stride);
+    ST_H4(res, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_2w_msa(uint8_t *src, uint8_t *dst,
@@ -1243,7 +1244,7 @@ static void avc_chroma_vt_and_aver_dst_4x2_msa(uint8_t *src, uint8_t *dst,
     res = (v16u8) __msa_pckev_b((v16i8) res_r, (v16i8) res_r);
     res = __msa_aver_u_b(res, dst_data);
 
-    ST4x2_UB(res, dst, stride);
+    ST_W2(res, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
@@ -1273,7 +1274,7 @@ static void avc_chroma_vt_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH2_UH(res0_r, res1_r, 7);
     out = (v16u8) __msa_pckev_b((v16i8) res1_r, (v16i8) res0_r);
     out = __msa_aver_u_b(out, dst0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
@@ -1309,7 +1310,7 @@ static void avc_chroma_vt_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
-    ST4x8_UB(out0, out1, dst, stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_4w_msa(uint8_t *src, uint8_t *dst,
@@ -1351,7 +1352,7 @@ static void avc_chroma_vt_and_aver_dst_8x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res0, res1, res2, res3, 7);
     PCKEV_B2_UB(res1, res0, res3, res2, out0, out1);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
@@ -1394,7 +1395,7 @@ static void avc_chroma_vt_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
     PCKEV_B2_UB(res5, res4, res7, res6, out2, out3);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
     AVER_UB2_UB(out2, dst2, out3, dst3, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_vt_and_aver_dst_8w_msa(uint8_t *src, uint8_t *dst,
@@ -1492,7 +1493,7 @@ static void avc_chroma_hv_and_aver_dst_2x4_msa(uint8_t *src, uint8_t *dst,
     res = __msa_pckev_b((v16i8) res_vt0, (v16i8) res_vt0);
     dst0 = __msa_aver_u_b((v16u8) res, dst0);
 
-    ST2x4_UB(dst0, 0, dst, stride);
+    ST_H4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_2w_msa(uint8_t *src, uint8_t *dst,
@@ -1545,7 +1546,7 @@ static void avc_chroma_hv_and_aver_dst_4x2_msa(uint8_t *src, uint8_t *dst,
     dst0 = (v16u8) __msa_pckev_b((v16i8) res_vt0, (v16i8) res_vt0);
     dst0 = __msa_aver_u_b(dst0, dst_data);
 
-    ST4x2_UB(dst0, dst, stride);
+    ST_W2(dst0, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
@@ -1584,7 +1585,7 @@ static void avc_chroma_hv_and_aver_dst_4x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH2_UH(res_vt0, res_vt1, 7);
     out = (v16u8) __msa_pckev_b((v16i8) res_vt1, (v16i8) res_vt0);
     out = __msa_aver_u_b(out, dst_data);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
@@ -1633,7 +1634,7 @@ static void avc_chroma_hv_and_aver_dst_4x8_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 7);
     PCKEV_B2_UB(res_vt1, res_vt0, res_vt3, res_vt2, res0, res1);
     AVER_UB2_UB(res0, dst0, res1, dst1, res0, res1);
-    ST4x8_UB(res0, res1, dst, stride);
+    ST_W8(res0, res1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_4w_msa(uint8_t *src, uint8_t *dst,
@@ -1701,7 +1702,7 @@ static void avc_chroma_hv_and_aver_dst_8x4_msa(uint8_t *src, uint8_t *dst,
     SAT_UH4_UH(res_vt0, res_vt1, res_vt2, res_vt3, 7);
     PCKEV_B2_UB(res_vt1, res_vt0, res_vt3, res_vt2, out0, out1);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
@@ -1770,7 +1771,7 @@ static void avc_chroma_hv_and_aver_dst_8x8_msa(uint8_t *src, uint8_t *dst,
     PCKEV_B2_UB(res_vt5, res_vt4, res_vt7, res_vt6, out2, out3);
     AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
     AVER_UB2_UB(out2, dst2, out3, dst3, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_chroma_hv_and_aver_dst_8w_msa(uint8_t *src, uint8_t *dst,
@@ -1848,21 +1849,21 @@ static void avg_width4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
         LW4(dst + 4 * stride, stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, dst1);
         AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);
-        ST4x8_UB(dst0, dst1, dst, stride);
+        ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
     } else if (4 == height) {
         LW4(src, stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);
         LW4(dst, stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
         dst0 = __msa_aver_u_b(src0, dst0);
-        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+        ST_W4(dst0, 0, 1, 2, 3, dst, stride);
     } else if (2 == height) {
         LW2(src, stride, tp0, tp1);
         INSERT_W2_UB(tp0, tp1, src0);
         LW2(dst, stride, tp0, tp1);
         INSERT_W2_UB(tp0, tp1, dst0);
         dst0 = __msa_aver_u_b(src0, dst0);
-        ST4x2_UB(dst0, dst, stride);
+        ST_W2(dst0, 0, 1, dst, stride);
     }
 }
 
@@ -1889,7 +1890,7 @@ static void avg_width8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
         INSERT_D2_UB(tp6, tp7, dst3);
         AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0, dst1,
                     dst2, dst3);
-        ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+        ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
     } else if (4 == height) {
         LD4(src, stride, tp0, tp1, tp2, tp3);
         INSERT_D2_UB(tp0, tp1, src0);
@@ -1898,7 +1899,7 @@ static void avg_width8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
         INSERT_D2_UB(tp0, tp1, dst0);
         INSERT_D2_UB(tp2, tp3, dst1);
         AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst, stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
     }
 }
 
diff --git a/libavcodec/mips/h264dsp_init_mips.c b/libavcodec/mips/h264dsp_init_mips.c
index 1fe7f84..9cd05e0 100644
--- a/libavcodec/mips/h264dsp_init_mips.c
+++ b/libavcodec/mips/h264dsp_init_mips.c
@@ -19,129 +19,116 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264dsp_init_msa(H264DSPContext *c,
-                                     const int bit_depth,
-                                     const int chroma_format_idc)
-{
-    if (8 == bit_depth) {
-        c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_inter_msa;
-        c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_inter_msa;
-        c->h264_h_loop_filter_luma_mbaff =
-            ff_h264_h_loop_filter_luma_mbaff_msa;
-        c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_msa;
-        c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_msa;
-        c->h264_h_loop_filter_luma_mbaff_intra =
-            ff_h264_h_loop_filter_luma_mbaff_intra_msa;
-        c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_inter_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_inter_msa;
-        else
-            c->h264_h_loop_filter_chroma =
-                ff_h264_h_loop_filter_chroma422_msa;
-
-        if (chroma_format_idc > 1)
-            c->h264_h_loop_filter_chroma_mbaff =
-                ff_h264_h_loop_filter_chroma422_mbaff_msa;
-
-        c->h264_v_loop_filter_chroma_intra =
-            ff_h264_v_lpf_chroma_intra_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_h_loop_filter_chroma_intra =
-                ff_h264_h_lpf_chroma_intra_msa;
-
-        /* Weighted MC */
-        c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_msa;
-        c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_msa;
-        c->weight_h264_pixels_tab[2] = ff_weight_h264_pixels4_8_msa;
-
-        c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_msa;
-        c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_msa;
-        c->biweight_h264_pixels_tab[2] = ff_biweight_h264_pixels4_8_msa;
-
-        c->h264_idct_add = ff_h264_idct_add_msa;
-        c->h264_idct8_add = ff_h264_idct8_addblk_msa;
-        c->h264_idct_dc_add = ff_h264_idct4x4_addblk_dc_msa;
-        c->h264_idct8_dc_add = ff_h264_idct8_dc_addblk_msa;
-        c->h264_idct_add16 = ff_h264_idct_add16_msa;
-        c->h264_idct8_add4 = ff_h264_idct8_add4_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_idct_add8 = ff_h264_idct_add8_msa;
-        else
-            c->h264_idct_add8 = ff_h264_idct_add8_422_msa;
-
-        c->h264_idct_add16intra = ff_h264_idct_add16_intra_msa;
-        c->h264_luma_dc_dequant_idct = ff_h264_deq_idct_luma_dc_msa;
-    }  // if (8 == bit_depth)
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static av_cold void h264dsp_init_mmi(H264DSPContext * c, const int bit_depth,
-        const int chroma_format_idc)
+av_cold void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
+                                  const int chroma_format_idc)
 {
-    if (bit_depth == 8) {
-        c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_mmi;
-        c->h264_idct_add = ff_h264_idct_add_8_mmi;
-        c->h264_idct8_add = ff_h264_idct8_add_8_mmi;
-        c->h264_idct_dc_add = ff_h264_idct_dc_add_8_mmi;
-        c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_mmi;
-        c->h264_idct_add16 = ff_h264_idct_add16_8_mmi;
-        c->h264_idct_add16intra = ff_h264_idct_add16intra_8_mmi;
-        c->h264_idct8_add4 = ff_h264_idct8_add4_8_mmi;
-
-        if (chroma_format_idc <= 1)
-            c->h264_idct_add8 = ff_h264_idct_add8_8_mmi;
-        else
-            c->h264_idct_add8 = ff_h264_idct_add8_422_8_mmi;
-
-        c->h264_luma_dc_dequant_idct = ff_h264_luma_dc_dequant_idct_8_mmi;
-
-        if (chroma_format_idc <= 1)
-            c->h264_chroma_dc_dequant_idct =
-                ff_h264_chroma_dc_dequant_idct_8_mmi;
-        else
-            c->h264_chroma_dc_dequant_idct =
-                ff_h264_chroma422_dc_dequant_idct_8_mmi;
-
-        c->weight_h264_pixels_tab[0] = ff_h264_weight_pixels16_8_mmi;
-        c->weight_h264_pixels_tab[1] = ff_h264_weight_pixels8_8_mmi;
-        c->weight_h264_pixels_tab[2] = ff_h264_weight_pixels4_8_mmi;
-
-        c->biweight_h264_pixels_tab[0] = ff_h264_biweight_pixels16_8_mmi;
-        c->biweight_h264_pixels_tab[1] = ff_h264_biweight_pixels8_8_mmi;
-        c->biweight_h264_pixels_tab[2] = ff_h264_biweight_pixels4_8_mmi;
-
-        c->h264_v_loop_filter_chroma       = ff_deblock_v_chroma_8_mmi;
-        c->h264_v_loop_filter_chroma_intra = ff_deblock_v_chroma_intra_8_mmi;
-
-        if (chroma_format_idc <= 1) {
-            c->h264_h_loop_filter_chroma =
-                ff_deblock_h_chroma_8_mmi;
-            c->h264_h_loop_filter_chroma_intra =
-                ff_deblock_h_chroma_intra_8_mmi;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_mmi;
+            c->h264_idct_add = ff_h264_idct_add_8_mmi;
+            c->h264_idct8_add = ff_h264_idct8_add_8_mmi;
+            c->h264_idct_dc_add = ff_h264_idct_dc_add_8_mmi;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_mmi;
+            c->h264_idct_add16 = ff_h264_idct_add16_8_mmi;
+            c->h264_idct_add16intra = ff_h264_idct_add16intra_8_mmi;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_8_mmi;
+
+            if (chroma_format_idc <= 1)
+                c->h264_idct_add8 = ff_h264_idct_add8_8_mmi;
+            else
+                c->h264_idct_add8 = ff_h264_idct_add8_422_8_mmi;
+
+            c->h264_luma_dc_dequant_idct = ff_h264_luma_dc_dequant_idct_8_mmi;
+
+            if (chroma_format_idc <= 1)
+                c->h264_chroma_dc_dequant_idct =
+                    ff_h264_chroma_dc_dequant_idct_8_mmi;
+            else
+                c->h264_chroma_dc_dequant_idct =
+                    ff_h264_chroma422_dc_dequant_idct_8_mmi;
+
+            c->weight_h264_pixels_tab[0] = ff_h264_weight_pixels16_8_mmi;
+            c->weight_h264_pixels_tab[1] = ff_h264_weight_pixels8_8_mmi;
+            c->weight_h264_pixels_tab[2] = ff_h264_weight_pixels4_8_mmi;
+
+            c->biweight_h264_pixels_tab[0] = ff_h264_biweight_pixels16_8_mmi;
+            c->biweight_h264_pixels_tab[1] = ff_h264_biweight_pixels8_8_mmi;
+            c->biweight_h264_pixels_tab[2] = ff_h264_biweight_pixels4_8_mmi;
+
+            c->h264_v_loop_filter_chroma       = ff_deblock_v_chroma_8_mmi;
+            c->h264_v_loop_filter_chroma_intra = ff_deblock_v_chroma_intra_8_mmi;
+
+            if (chroma_format_idc <= 1) {
+                c->h264_h_loop_filter_chroma =
+                    ff_deblock_h_chroma_8_mmi;
+                c->h264_h_loop_filter_chroma_intra =
+                    ff_deblock_h_chroma_intra_8_mmi;
+            }
+
+            c->h264_v_loop_filter_luma = ff_deblock_v_luma_8_mmi;
+            c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
+            c->h264_h_loop_filter_luma = ff_deblock_h_luma_8_mmi;
+            c->h264_h_loop_filter_luma_intra = ff_deblock_h_luma_intra_8_mmi;
         }
-
-        c->h264_v_loop_filter_luma = ff_deblock_v_luma_8_mmi;
-        c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
-        c->h264_h_loop_filter_luma = ff_deblock_h_luma_8_mmi;
-        c->h264_h_loop_filter_luma_intra = ff_deblock_h_luma_intra_8_mmi;
     }
-}
-#endif /* HAVE_MMI */
 
-av_cold void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
-                                  const int chroma_format_idc)
-{
-#if HAVE_MSA
-    h264dsp_init_msa(c, bit_depth, chroma_format_idc);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    h264dsp_init_mmi(c, bit_depth, chroma_format_idc);
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_inter_msa;
+            c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_inter_msa;
+            c->h264_h_loop_filter_luma_mbaff =
+                ff_h264_h_loop_filter_luma_mbaff_msa;
+            c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_msa;
+            c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_msa;
+            c->h264_h_loop_filter_luma_mbaff_intra =
+                ff_h264_h_loop_filter_luma_mbaff_intra_msa;
+            c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_inter_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_inter_msa;
+            else
+                c->h264_h_loop_filter_chroma =
+                    ff_h264_h_loop_filter_chroma422_msa;
+
+            if (chroma_format_idc > 1)
+                c->h264_h_loop_filter_chroma_mbaff =
+                    ff_h264_h_loop_filter_chroma422_mbaff_msa;
+
+            c->h264_v_loop_filter_chroma_intra =
+                ff_h264_v_lpf_chroma_intra_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_h_loop_filter_chroma_intra =
+                    ff_h264_h_lpf_chroma_intra_msa;
+
+            /* Weighted MC */
+            c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_msa;
+            c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_msa;
+            c->weight_h264_pixels_tab[2] = ff_weight_h264_pixels4_8_msa;
+
+            c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_msa;
+            c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_msa;
+            c->biweight_h264_pixels_tab[2] = ff_biweight_h264_pixels4_8_msa;
+
+            c->h264_idct_add = ff_h264_idct_add_msa;
+            c->h264_idct8_add = ff_h264_idct8_addblk_msa;
+            c->h264_idct_dc_add = ff_h264_idct4x4_addblk_dc_msa;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_addblk_msa;
+            c->h264_idct_add16 = ff_h264_idct_add16_msa;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_idct_add8 = ff_h264_idct_add8_msa;
+            else
+                c->h264_idct_add8 = ff_h264_idct_add8_422_msa;
+
+            c->h264_idct_add16intra = ff_h264_idct_add16_intra_msa;
+            c->h264_luma_dc_dequant_idct = ff_h264_deq_idct_luma_dc_msa;
+        }
+    }
 }
diff --git a/libavcodec/mips/h264dsp_mips.h b/libavcodec/mips/h264dsp_mips.h
index 21b7de0..35e16c4 100644
--- a/libavcodec/mips/h264dsp_mips.h
+++ b/libavcodec/mips/h264dsp_mips.h
@@ -25,21 +25,21 @@
 #include "libavcodec/h264dec.h"
 #include "constants.h"
 
-void ff_h264_h_lpf_luma_inter_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_luma_inter_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta, int8_t *tc0);
-void ff_h264_h_lpf_chroma_inter_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_chroma_inter_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta, int8_t *tc0);
-void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src, ptrdiff_t stride,
                                          int32_t alpha, int32_t beta,
                                          int8_t *tc0);
-void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, ptrdiff_t stride,
                                                int32_t alpha, int32_t beta,
                                                int8_t *tc0);
-void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src, ptrdiff_t stride,
                                           int32_t alpha, int32_t beta,
                                           int8_t *tc0);
 
@@ -67,15 +67,15 @@ void ff_h264_idct8_add4_msa(uint8_t *dst, const int *blk_offset,
                             int16_t *blk, int dst_stride,
                             const uint8_t nnzc[15 * 8]);
 
-void ff_h264_h_lpf_luma_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta);
-void ff_h264_v_lpf_luma_intra_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta);
-void ff_h264_h_lpf_chroma_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta);
-void ff_h264_v_lpf_chroma_intra_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta);
-void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, ptrdiff_t stride,
                                                 int alpha, int beta);
 
 void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
@@ -357,23 +357,23 @@ void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
 
 void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v8_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v8_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
 
 void ff_put_h264_qpel16_mc00_mmi(uint8_t *dst, const uint8_t *src,
diff --git a/libavcodec/mips/h264dsp_mmi.c b/libavcodec/mips/h264dsp_mmi.c
index ac65a20..173e191 100644
--- a/libavcodec/mips/h264dsp_mmi.c
+++ b/libavcodec/mips/h264dsp_mmi.c
@@ -38,6 +38,9 @@ void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
         MMI_LDC1(%[ftmp2], %[src], 0x08)
         MMI_LDC1(%[ftmp3], %[src], 0x10)
         MMI_LDC1(%[ftmp4], %[src], 0x18)
+        /* memset(src, 0, 32); */
+        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[src])            \n\t"
+        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[src])            \n\t"
         MMI_ULWC1(%[ftmp5], %[dst0], 0x00)
         MMI_ULWC1(%[ftmp6], %[dst1], 0x00)
         MMI_ULWC1(%[ftmp7], %[dst2], 0x00)
@@ -58,11 +61,6 @@ void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
         MMI_SWC1(%[ftmp2], %[dst1], 0x00)
         MMI_SWC1(%[ftmp3], %[dst2], 0x00)
         MMI_SWC1(%[ftmp4], %[dst3], 0x00)
-
-        /* memset(src, 0, 32); */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[src])            \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[src])            \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -85,15 +83,19 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
     DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
-        "dli        %[tmp0],    0x01                                    \n\t"
         MMI_LDC1(%[ftmp0], %[block], 0x00)
-        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x08)
-        "dli        %[tmp0],    0x06                                    \n\t"
         MMI_LDC1(%[ftmp2], %[block], 0x10)
+        MMI_LDC1(%[ftmp3], %[block], 0x18)
+        /* memset(block, 0, 32) */
+        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "gssqc1     %[ftmp4],   %[ftmp4],       0x00(%[block])          \n\t"
+        "gssqc1     %[ftmp4],   %[ftmp4],       0x10(%[block])          \n\t"
+        "dli        %[tmp0],    0x01                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "dli        %[tmp0],    0x06                                    \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         "psrah      %[ftmp4],   %[ftmp1],       %[ftmp8]                \n\t"
-        MMI_LDC1(%[ftmp3], %[block], 0x18)
         "psrah      %[ftmp5],   %[ftmp3],       %[ftmp8]                \n\t"
         "psubh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
         "paddh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
@@ -121,15 +123,11 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp10],  %[ftmp3],       %[ftmp1]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
         "paddh      %[ftmp11],  %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "psubh      %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
-        MMI_SDC1(%[ftmp7], %[block], 0x00)
-        MMI_SDC1(%[ftmp7], %[block], 0x08)
-        MMI_SDC1(%[ftmp7], %[block], 0x10)
-        MMI_SDC1(%[ftmp7], %[block], 0x18)
         MMI_ULWC1(%[ftmp2], %[dst], 0x00)
-        "psrah      %[ftmp3],   %[ftmp10],      %[ftmp9]                \n\t"
         MMI_LWXC1(%[ftmp0], %[dst], %[stride], 0x00)
+        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp10],      %[ftmp9]                \n\t"
         "psrah      %[ftmp4],   %[ftmp11],      %[ftmp9]                \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
         "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
@@ -153,11 +151,6 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         MMI_SWC1(%[ftmp2], %[dst], 0x00)
         "packushb   %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
-
-        /* memset(block, 0, 32) */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[block])          \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -184,7 +177,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
     __asm__ volatile (
         "lhu        %[tmp0],    0x00(%[block])                          \n\t"
-        PTR_ADDI   "$29,        $29,            -0x20                   \n\t"
+        PTR_ADDI   "$sp,        $sp,            -0x20                   \n\t"
         PTR_ADDIU  "%[tmp0],    %[tmp0],        0x20                    \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x10)
         "sh         %[tmp0],    0x00(%[block])                          \n\t"
@@ -261,8 +254,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "punpckhwd  %[ftmp3],   %[ftmp6],       %[ftmp0]                \n\t"
         "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp0], %[block], 0x00)
-        MMI_SDC1(%[ftmp7], $29, 0x00)
-        MMI_SDC1(%[ftmp1], $29, 0x10)
+        MMI_SDC1(%[ftmp7], $sp, 0x00)
+        MMI_SDC1(%[ftmp1], $sp, 0x10)
         "dmfc1      %[tmp1],    %[ftmp6]                                \n\t"
         "dmfc1      %[tmp3],    %[ftmp3]                                \n\t"
         "punpckhhw  %[ftmp3],   %[ftmp5],       %[ftmp2]                \n\t"
@@ -273,8 +266,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "punpcklwd  %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
         "punpckhwd  %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
         "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
-        MMI_SDC1(%[ftmp5], $29, 0x08)
-        MMI_SDC1(%[ftmp0], $29, 0x18)
+        MMI_SDC1(%[ftmp5], $sp, 0x08)
+        MMI_SDC1(%[ftmp0], $sp, 0x18)
         "dmfc1      %[tmp2],    %[ftmp3]                                \n\t"
         "dmfc1      %[tmp4],    %[ftmp4]                                \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x18)
@@ -366,7 +359,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         PTR_ADDIU  "%[addr0],   %[dst],         0x04                    \n\t"
         "mov.d      %[ftmp7],   %[ftmp10]                               \n\t"
         "dmtc1      %[tmp3],    %[ftmp6]                                \n\t"
-        MMI_LDC1(%[ftmp1], $29, 0x10)
+        MMI_LDC1(%[ftmp1], $sp, 0x10)
         "dmtc1      %[tmp1],    %[ftmp3]                                \n\t"
         "mov.d      %[ftmp4],   %[ftmp1]                                \n\t"
         "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
@@ -399,7 +392,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "psrah      %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
         "paddh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
         "psubh      %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
-        MMI_LDC1(%[ftmp3], $29, 0x00)
+        MMI_LDC1(%[ftmp3], $sp, 0x00)
         "dmtc1      %[tmp5],    %[ftmp7]                                \n\t"
         "paddh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
@@ -421,9 +414,9 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
         "paddh      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
-        MMI_SDC1(%[ftmp3], $29, 0x00)
+        MMI_SDC1(%[ftmp3], $sp, 0x00)
         "psubh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
-        MMI_SDC1(%[ftmp0], $29, 0x10)
+        MMI_SDC1(%[ftmp0], $sp, 0x10)
         "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
         "xor        %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
         MMI_SDC1(%[ftmp2], %[block], 0x00)
@@ -470,8 +463,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
         MMI_SWC1(%[ftmp3], %[dst], 0x00)
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
-        MMI_LDC1(%[ftmp5], $29, 0x00)
-        MMI_LDC1(%[ftmp4], $29, 0x10)
+        MMI_LDC1(%[ftmp5], $sp, 0x00)
+        MMI_LDC1(%[ftmp4], $sp, 0x10)
         "dmtc1      %[tmp1],    %[ftmp6]                                \n\t"
         PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
         PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
@@ -503,7 +496,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
         "dmtc1      %[tmp4],    %[ftmp1]                                \n\t"
         "dmtc1      %[tmp2],    %[ftmp6]                                \n\t"
-        MMI_LDC1(%[ftmp4], $29, 0x18)
+        MMI_LDC1(%[ftmp4], $sp, 0x18)
         "mov.d      %[ftmp5],   %[ftmp4]                                \n\t"
         "psrah      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
         "psrah      %[ftmp7],   %[ftmp11],      %[ftmp8]                \n\t"
@@ -535,7 +528,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "psrah      %[ftmp7],   %[ftmp6],       %[ftmp8]                \n\t"
         "paddh      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
         "psubh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
-        MMI_LDC1(%[ftmp6], $29, 0x08)
+        MMI_LDC1(%[ftmp6], $sp, 0x08)
         "dmtc1      %[tmp6],    %[ftmp3]                                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
         "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
@@ -557,9 +550,9 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
         "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
-        MMI_SDC1(%[ftmp6], $29, 0x08)
+        MMI_SDC1(%[ftmp6], $sp, 0x08)
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
-        MMI_SDC1(%[ftmp7], $29, 0x18)
+        MMI_SDC1(%[ftmp7], $sp, 0x18)
         "dmfc1      %[tmp2],    %[ftmp0]                                \n\t"
         "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_ULWC1(%[ftmp6], %[addr0], 0x00)
@@ -588,8 +581,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
         MMI_SWC1(%[ftmp6], %[addr0], 0x00)
         MMI_SWXC1(%[ftmp7], %[addr0], %[stride], 0x00)
-        MMI_LDC1(%[ftmp2], $29, 0x08)
-        MMI_LDC1(%[ftmp5], $29, 0x18)
+        MMI_LDC1(%[ftmp2], $sp, 0x08)
+        MMI_LDC1(%[ftmp5], $sp, 0x18)
         PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
         "dmtc1      %[tmp2],    %[ftmp1]                                \n\t"
         PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
@@ -619,18 +612,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
         MMI_SWC1(%[ftmp6], %[addr0], 0x00)
         MMI_SWXC1(%[ftmp7], %[addr0], %[stride], 0x00)
-        PTR_ADDIU  "$29,        $29,            0x20                    \n\t"
-
-        /* memset(block, 0, 128) */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x20(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x30(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x40(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x50(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x60(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x70(%[block])          \n\t"
+        PTR_ADDIU  "$sp,        $sp,            0x20                    \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -648,7 +630,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
           [addr0]"=&r"(addr[0])
         : [dst]"r"(dst),                    [block]"r"(block),
           [stride]"r"((mips_reg)stride)
-        : "$29","memory"
+        : "memory"
     );
 
 }
@@ -1451,7 +1433,7 @@ void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
     }
 }
 
-void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v8_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     double ftmp[12];
@@ -1579,7 +1561,7 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
     );
 }
 
-static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     DECLARE_ALIGNED(8, const uint64_t, stack[0x0a]);
@@ -1889,7 +1871,7 @@ void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
     );
 }
 
-void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     double ftmp[9];
@@ -1967,7 +1949,7 @@ void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
     );
 }
 
-void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     double ftmp[11];
@@ -2107,7 +2089,7 @@ void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
     );
 }
 
-void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     double ftmp[11];
@@ -2240,7 +2222,7 @@ void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
     );
 }
 
-void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     if ((tc0[0] & tc0[1]) >= 0)
@@ -2249,14 +2231,14 @@ void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         ff_deblock_v8_luma_8_mmi(pix + 8, stride, alpha, beta, tc0 + 2);
 }
 
-void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     deblock_v8_luma_intra_8_mmi(pix + 0, stride, alpha, beta);
     deblock_v8_luma_intra_8_mmi(pix + 8, stride, alpha, beta);
 }
 
-void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     DECLARE_ALIGNED(8, const uint64_t, stack[0x0d]);
@@ -2475,7 +2457,7 @@ void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
     );
 }
 
-void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     DECLARE_ALIGNED(8, const uint64_t, ptmp[0x11]);
diff --git a/libavcodec/mips/h264dsp_msa.c b/libavcodec/mips/h264dsp_msa.c
index e50f5ca..a8c3f3c 100644
--- a/libavcodec/mips/h264dsp_msa.c
+++ b/libavcodec/mips/h264dsp_msa.c
@@ -21,7 +21,7 @@
 #include "libavutil/mips/generic_macros_msa.h"
 #include "h264dsp_mips.h"
 
-static void avc_wgt_4x2_msa(uint8_t *data, int32_t stride,
+static void avc_wgt_4x2_msa(uint8_t *data, ptrdiff_t stride,
                             int32_t log2_denom, int32_t src_weight,
                             int32_t offset_in)
 {
@@ -45,11 +45,12 @@ static void avc_wgt_4x2_msa(uint8_t *data, int32_t stride,
     tmp0 = __msa_srlr_h(tmp0, denom);
     tmp0 = (v8i16) __msa_sat_u_h((v8u16) tmp0, 7);
     src0 = (v16u8) __msa_pckev_b((v16i8) tmp0, (v16i8) tmp0);
-    ST4x2_UB(src0, data, stride);
+    ST_W2(src0, 0, 1, data, stride);
 }
 
-static void avc_wgt_4x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_4x4_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t tp0, tp1, tp2, tp3, offset_val;
     v16u8 src0 = { 0 };
@@ -71,11 +72,12 @@ static void avc_wgt_4x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     tmp1 = __msa_srlr_h(tmp1, denom);
     SAT_UH2_SH(tmp0, tmp1, 7);
     src0 = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(src0, src0, 0, 1, 2, 3, data, stride);
+    ST_W4(src0, 0, 1, 2, 3, data, stride);
 }
 
-static void avc_wgt_4x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_4x8_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t tp0, tp1, tp2, tp3, offset_val;
     v16u8 src0 = { 0 }, src1 = { 0 };
@@ -102,11 +104,12 @@ static void avc_wgt_4x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     SRLR_H4_SH(tmp0, tmp1, tmp2, tmp3, denom);
     SAT_UH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, src0, src1);
-    ST4x8_UB(src0, src1, data, stride);
+    ST_W8(src0, src1, 0, 1, 2, 3, 0, 1, 2, 3, data, stride);
 }
 
-static void avc_wgt_8x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_8x4_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t offset_val;
     uint64_t tp0, tp1, tp2, tp3;
@@ -133,10 +136,10 @@ static void avc_wgt_8x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     SRLR_H4_SH(tmp0, tmp1, tmp2, tmp3, denom);
     SAT_UH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, src0, src1);
-    ST8x4_UB(src0, src1, data, stride);
+    ST_D4(src0, src1, 0, 1, 0, 1, data, stride);
 }
 
-static void avc_wgt_8x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
+static void avc_wgt_8x8_msa(uint8_t *data, ptrdiff_t stride, int32_t log2_denom,
                             int32_t src_weight, int32_t offset_in)
 {
     uint32_t offset_val;
@@ -175,11 +178,12 @@ static void avc_wgt_8x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     SAT_UH8_SH(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, 7);
     PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, src0, src1,
                 src2, src3);
-    ST8x8_UB(src0, src1, src2, src3, data, stride);
+    ST_D8(src0, src1, src2, src3, 0, 1, 0, 1, 0, 1, 0, 1, data, stride);
 }
 
-static void avc_wgt_8x16_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                             int32_t src_weight, int32_t offset_in)
+static void avc_wgt_8x16_msa(uint8_t *data, ptrdiff_t stride,
+                             int32_t log2_denom, int32_t src_weight,
+                             int32_t offset_in)
 {
     uint32_t offset_val, cnt;
     uint64_t tp0, tp1, tp2, tp3;
@@ -218,12 +222,12 @@ static void avc_wgt_8x16_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
         SAT_UH8_SH(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, 7);
         PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, src0, src1,
                     src2, src3);
-        ST8x8_UB(src0, src1, src2, src3, data, stride);
+        ST_D8(src0, src1, src2, src3, 0, 1, 0, 1, 0, 1, 0, 1, data, stride);
         data += 8 * stride;
     }
 }
 
-static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -253,10 +257,10 @@ static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     tmp0 = __msa_maxi_s_h(tmp0, 0);
     tmp0 = __msa_min_s_h(max255, tmp0);
     dst0 = (v16u8) __msa_pckev_b((v16i8) tmp0, (v16i8) tmp0);
-    ST4x2_UB(dst0, dst, stride);
+    ST_W2(dst0, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -287,10 +291,10 @@ static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     tmp1 >>= denom;
     CLIP_SH2_0_255(tmp0, tmp1);
     dst0 = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
-static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -327,10 +331,10 @@ static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRA_4V(tmp0, tmp1, tmp2, tmp3, denom);
     CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, dst0, dst1);
-    ST4x8_UB(dst0, dst1, dst, stride);
+    ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
-static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -365,10 +369,10 @@ static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     SRA_4V(tmp0, tmp1, tmp2, tmp3, denom);
     CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -413,14 +417,13 @@ static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     tmp7 = __msa_dpadd_s_h(offset, wgt, vec7);
     SRA_4V(tmp0, tmp1, tmp2, tmp3, denom);
     SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
-    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
+    CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, dst0, dst1);
     PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                                int32_t log2_denom, int32_t src_weight,
                                int32_t dst_weight, int32_t offset_in)
 {
@@ -475,11 +478,10 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 
         SRA_4V(temp0, temp1, temp2, temp3, denom);
         SRA_4V(temp4, temp5, temp6, temp7, denom);
-        CLIP_SH4_0_255(temp0, temp1, temp2, temp3);
-        CLIP_SH4_0_255(temp4, temp5, temp6, temp7);
+        CLIP_SH8_0_255(temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7);
         PCKEV_B4_UB(temp1, temp0, temp3, temp2, temp5, temp4, temp7, temp6,
                     dst0, dst1, dst2, dst3);
-        ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+        ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
         dst += 8 * stride;
     }
 }
@@ -531,7 +533,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     temp = p1_or_q1_org_in << 1;                              \
     clip3 = clip3 - temp;                                     \
     clip3 = __msa_ave_s_h(p2_or_q2_org_in, clip3);            \
-    clip3 = CLIP_SH(clip3, negate_tc_in, tc_in);              \
+    CLIP_SH(clip3, negate_tc_in, tc_in);                      \
     p1_or_q1_out = p1_or_q1_org_in + clip3;                   \
 }
 
@@ -549,7 +551,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     delta = q0_sub_p0 + p1_sub_q1;                              \
     delta >>= 3;                                                \
                                                                 \
-    delta = CLIP_SH(delta, negate_threshold_in, threshold_in);  \
+    CLIP_SH(delta, negate_threshold_in, threshold_in);          \
                                                                 \
     p0_or_q0_out = p0_or_q0_org_in + delta;                     \
     q0_or_p0_out = q0_or_p0_org_in - delta;                     \
@@ -598,7 +600,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     delta = q0_sub_p0 + p1_sub_q1;                                       \
     delta = __msa_srari_h(delta, 3);                                     \
                                                                          \
-    delta = CLIP_SH(delta, -tc, tc);                                     \
+    CLIP_SH(delta, -tc, tc);                                             \
                                                                          \
     ILVR_B2_SH(zeros, src1, zeros, src2, res0_r, res1_r);                \
                                                                          \
@@ -620,7 +622,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
                                                              \
     out0 = (v16u8) __msa_ilvr_b((v16i8) in1, (v16i8) in0);   \
     out1 = (v16u8) __msa_sldi_b(zero_m, (v16i8) out0, 2);    \
-    SLDI_B2_0_UB(out1, out2, out2, out3, 2);                 \
+    SLDI_B2_UB(zero_m, out1, zero_m, out2, 2, out2, out3);   \
 }
 
 #define AVC_LPF_H_2BYTE_CHROMA_422(src, stride, tc_val, alpha, beta, res)  \
@@ -662,7 +664,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     q0_sub_p0 <<= 2;                                                       \
     delta = q0_sub_p0 + p1_sub_q1;                                         \
     delta = __msa_srari_h(delta, 3);                                       \
-    delta = CLIP_SH(delta, -tc, tc);                                       \
+    CLIP_SH(delta, -tc, tc);                                               \
                                                                            \
     ILVR_B2_SH(zeros, src1, zeros, src2, res0_r, res1_r);                  \
                                                                            \
@@ -681,7 +683,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 static void avc_loopfilter_luma_intra_edge_hor_msa(uint8_t *data,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t img_width)
+                                                   ptrdiff_t img_width)
 {
     v16u8 p0_asub_q0, p1_asub_p0, q1_asub_q0;
     v16u8 is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -814,7 +816,7 @@ static void avc_loopfilter_luma_intra_edge_hor_msa(uint8_t *data,
 static void avc_loopfilter_luma_intra_edge_ver_msa(uint8_t *data,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t img_width)
+                                                   ptrdiff_t img_width)
 {
     uint8_t *src = data - 4;
     v16u8 alpha, beta, p0_asub_q0;
@@ -955,23 +957,24 @@ static void avc_loopfilter_luma_intra_edge_ver_msa(uint8_t *data,
         ILVRL_H2_SH(tp3, tp2, tmp6, tmp7);
 
         src = data - 3;
-        ST4x4_UB(tmp3, tmp3, 0, 1, 2, 3, src, img_width);
-        ST2x4_UB(tmp2, 0, src + 4, img_width);
+        ST_W4(tmp3, 0, 1, 2, 3, src, img_width);
+        ST_H4(tmp2, 0, 1, 2, 3, src + 4, img_width);
         src += 4 * img_width;
-        ST4x4_UB(tmp4, tmp4, 0, 1, 2, 3, src, img_width);
-        ST2x4_UB(tmp2, 4, src + 4, img_width);
+        ST_W4(tmp4, 0, 1, 2, 3, src, img_width);
+        ST_H4(tmp2, 4, 5, 6, 7, src + 4, img_width);
         src += 4 * img_width;
 
-        ST4x4_UB(tmp6, tmp6, 0, 1, 2, 3, src, img_width);
-        ST2x4_UB(tmp5, 0, src + 4, img_width);
+        ST_W4(tmp6, 0, 1, 2, 3, src, img_width);
+        ST_H4(tmp5, 0, 1, 2, 3, src + 4, img_width);
         src += 4 * img_width;
-        ST4x4_UB(tmp7, tmp7, 0, 1, 2, 3, src, img_width);
-        ST2x4_UB(tmp5, 4, src + 4, img_width);
+        ST_W4(tmp7, 0, 1, 2, 3, src, img_width);
+        ST_H4(tmp5, 4, 5, 6, 7, src + 4, img_width);
     }
     }
 }
 
-static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src,
+                                                   ptrdiff_t stride,
                                                    int32_t alpha_in,
                                                    int32_t beta_in)
 {
@@ -1025,7 +1028,8 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
 
     ILVR_W2_SB(tmp2, tmp0, tmp3, tmp1, src6, src3);
     ILVL_W2_SB(tmp2, tmp0, tmp3, tmp1, src1, src5);
-    SLDI_B4_0_SB(src6, src1, src3, src5, src0, src2, src4, src7, 8);
+    SLDI_B4_SB(zeros, src6, zeros, src1, zeros, src3, zeros, src5,
+               8, src0, src2, src4, src7);
 
     p0_asub_q0 = __msa_asub_u_b((v16u8) src2, (v16u8) src3);
     p1_asub_p0 = __msa_asub_u_b((v16u8) src1, (v16u8) src2);
@@ -1116,10 +1120,10 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
     ILVRL_H2_SH(zeros, dst2_x, tmp2, tmp3);
 
     ILVR_W2_UB(tmp2, tmp0, tmp3, tmp1, dst0, dst4);
-    SLDI_B2_0_UB(dst0, dst4, dst1, dst5, 8);
+    SLDI_B2_UB(zeros, dst0, zeros, dst4, 8, dst1, dst5);
     dst2_x = (v16u8) __msa_ilvl_w((v4i32) tmp2, (v4i32) tmp0);
     dst2_y = (v16u8) __msa_ilvl_w((v4i32) tmp3, (v4i32) tmp1);
-    SLDI_B2_0_UB(dst2_x, dst2_y, dst3_x, dst3_y, 8);
+    SLDI_B2_UB(zeros, dst2_x, zeros, dst2_y, 8, dst3_x, dst3_y);
 
     out0 = __msa_copy_u_w((v4i32) dst0, 0);
     out1 = __msa_copy_u_h((v8i16) dst0, 2);
@@ -1172,7 +1176,7 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
 static void avc_loopfilter_cb_or_cr_intra_edge_hor_msa(uint8_t *data_cb_or_cr,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v16u8 alpha, beta;
     v16u8 is_less_than;
@@ -1221,7 +1225,7 @@ static void avc_loopfilter_cb_or_cr_intra_edge_hor_msa(uint8_t *data_cb_or_cr,
 static void avc_loopfilter_cb_or_cr_intra_edge_ver_msa(uint8_t *data_cb_or_cr,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v8i16 tmp1;
     v16u8 alpha, beta, is_less_than;
@@ -1274,9 +1278,9 @@ static void avc_loopfilter_cb_or_cr_intra_edge_ver_msa(uint8_t *data_cb_or_cr,
         tmp1 = (v8i16) __msa_ilvr_b((v16i8) q0_or_p0_org, (v16i8) p0_or_q0_org);
 
         data_cb_or_cr -= 1;
-        ST2x4_UB(tmp1, 0, data_cb_or_cr, img_width);
+        ST_H4(tmp1, 0, 1, 2, 3, data_cb_or_cr, img_width);
         data_cb_or_cr += 4 * img_width;
-        ST2x4_UB(tmp1, 4, data_cb_or_cr, img_width);
+        ST_H4(tmp1, 4, 5, 6, 7, data_cb_or_cr, img_width);
     }
 }
 
@@ -1287,7 +1291,7 @@ static void avc_loopfilter_luma_inter_edge_ver_msa(uint8_t *data,
                                                    uint8_t tc2, uint8_t tc3,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t img_width)
+                                                   ptrdiff_t img_width)
 {
     v16u8 tmp_vec, bs = { 0 };
 
@@ -1567,7 +1571,7 @@ static void avc_loopfilter_luma_inter_edge_hor_msa(uint8_t *data,
                                                    uint8_t tc2, uint8_t tc3,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t image_width)
+                                                   ptrdiff_t image_width)
 {
     v16u8 tmp_vec;
     v16u8 bs = { 0 };
@@ -1716,7 +1720,7 @@ static void avc_loopfilter_luma_inter_edge_hor_msa(uint8_t *data,
     }
 }
 
-static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
+static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, ptrdiff_t stride,
                                              int32_t alpha_in, int32_t beta_in,
                                              int8_t *tc0)
 {
@@ -1741,7 +1745,7 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
     v8i16 tc, tc_orig_r, tc_plus1;
     v16u8 is_tc_orig1, is_tc_orig2, tc_orig = { 0 };
     v8i16 p0_ilvr_q0, p0_add_q0, q0_sub_p0, p1_sub_q1;
-    v8u16 src2_r, src3_r;
+    v8i16 src2_r, src3_r;
     v8i16 p2_r, p1_r, q2_r, q1_r;
     v16u8 p2, q2, p0, q0;
     v4i32 dst0, dst1;
@@ -1839,8 +1843,8 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
     tc_orig_r = (v8i16) __msa_ilvr_b(zeros, (v16i8) tc_orig);
     tc = tc_orig_r;
 
-    p2_r = CLIP_SH(p2_r, -tc_orig_r, tc_orig_r);
-    q2_r = CLIP_SH(q2_r, -tc_orig_r, tc_orig_r);
+    CLIP_SH(p2_r, -tc_orig_r, tc_orig_r);
+    CLIP_SH(q2_r, -tc_orig_r, tc_orig_r);
 
     p2_r += p1_r;
     q2_r += q1_r;
@@ -1872,14 +1876,13 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
                                               (v16i8) is_less_than_beta2);
     tc = (v8i16) __msa_bmnz_v((v16u8) tc, (v16u8) tc_plus1, is_less_than_beta2);
 
-    q0_sub_p0 = CLIP_SH(q0_sub_p0, -tc, tc);
+    CLIP_SH(q0_sub_p0, -tc, tc);
 
-    ILVR_B2_UH(zeros, src2, zeros, src3, src2_r, src3_r);
+    ILVR_B2_SH(zeros, src2, zeros, src3, src2_r, src3_r);
     src2_r += q0_sub_p0;
     src3_r -= q0_sub_p0;
 
-    src2_r = (v8u16) CLIP_SH_0_255(src2_r);
-    src3_r = (v8u16) CLIP_SH_0_255(src3_r);
+    CLIP_SH2_0_255(src2_r, src3_r);
 
     PCKEV_B2_UB(src2_r, src2_r, src3_r, src3_r, p0, q0);
 
@@ -1943,7 +1946,7 @@ static void avc_loopfilter_cb_or_cr_inter_edge_hor_msa(uint8_t *data,
                                                        uint8_t tc2, uint8_t tc3,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v16u8 alpha, beta;
     v8i16 tmp_vec;
@@ -2029,7 +2032,7 @@ static void avc_loopfilter_cb_or_cr_inter_edge_ver_msa(uint8_t *data,
                                                        uint8_t tc2, uint8_t tc3,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     uint8_t *src;
     v16u8 alpha, beta;
@@ -2110,14 +2113,14 @@ static void avc_loopfilter_cb_or_cr_inter_edge_ver_msa(uint8_t *data,
             q0_org = __msa_bmnz_v(q0_org, q0, is_less_than);
             tmp1 = (v8i16) __msa_ilvr_b((v16i8) q0_org, (v16i8) p0_org);
             src = data - 1;
-            ST2x4_UB(tmp1, 0, src, img_width);
+            ST_H4(tmp1, 0, 1, 2, 3, src, img_width);
             src += 4 * img_width;
-            ST2x4_UB(tmp1, 4, src, img_width);
+            ST_H4(tmp1, 4, 5, 6, 7, src, img_width);
         }
     }
 }
 
-static void avc_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_chroma422_msa(uint8_t *src, ptrdiff_t stride,
                                             int32_t alpha_in, int32_t beta_in,
                                             int8_t *tc0)
 {
@@ -2136,12 +2139,13 @@ static void avc_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
         }
 
         AVC_LPF_H_CHROMA_422(src, stride, tc_val, alpha, beta, res);
-        ST2x4_UB(res, 0, (src - 1), stride);
+        ST_H4(res, 0, 1, 2, 3, (src - 1), stride);
         src += (4 * stride);
     }
 }
 
-static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
+                                                  ptrdiff_t stride,
                                                   int32_t alpha_in,
                                                   int32_t beta_in,
                                                   int8_t *tc0)
@@ -2173,7 +2177,7 @@ static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
     }
 }
 
-void ff_h264_h_lpf_luma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta, int8_t *tc)
 {
     uint8_t bs0 = 1;
@@ -2195,7 +2199,7 @@ void ff_h264_h_lpf_luma_inter_msa(uint8_t *data, int img_width,
                                            alpha, beta, img_width);
 }
 
-void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta, int8_t *tc)
 {
 
@@ -2218,7 +2222,7 @@ void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, int img_width,
                                            alpha, beta, img_width);
 }
 
-void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta, int8_t *tc)
 {
     uint8_t bs0 = 1;
@@ -2240,7 +2244,7 @@ void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, int img_width,
                                                alpha, beta, img_width);
 }
 
-void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta, int8_t *tc)
 {
     uint8_t bs0 = 1;
@@ -2262,40 +2266,40 @@ void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, int img_width,
                                                alpha, beta, img_width);
 }
 
-void ff_h264_h_lpf_luma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta)
 {
     avc_loopfilter_luma_intra_edge_ver_msa(data, (uint8_t) alpha,
                                            (uint8_t) beta,
-                                           (unsigned int) img_width);
+                                           img_width);
 }
 
-void ff_h264_v_lpf_luma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta)
 {
     avc_loopfilter_luma_intra_edge_hor_msa(data, (uint8_t) alpha,
                                            (uint8_t) beta,
-                                           (unsigned int) img_width);
+                                           img_width);
 }
 
-void ff_h264_h_lpf_chroma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta)
 {
     avc_loopfilter_cb_or_cr_intra_edge_ver_msa(data, (uint8_t) alpha,
                                                (uint8_t) beta,
-                                               (unsigned int) img_width);
+                                               img_width);
 }
 
-void ff_h264_v_lpf_chroma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta)
 {
     avc_loopfilter_cb_or_cr_intra_edge_hor_msa(data, (uint8_t) alpha,
                                                (uint8_t) beta,
-                                               (unsigned int) img_width);
+                                               img_width);
 }
 
 void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src,
-                                         int32_t ystride,
+                                         ptrdiff_t ystride,
                                          int32_t alpha, int32_t beta,
                                          int8_t *tc0)
 {
@@ -2303,7 +2307,7 @@ void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
-                                               int32_t ystride,
+                                               ptrdiff_t ystride,
                                                int32_t alpha,
                                                int32_t beta,
                                                int8_t *tc0)
@@ -2312,7 +2316,7 @@ void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src,
-                                          int32_t ystride,
+                                          ptrdiff_t ystride,
                                           int32_t alpha,
                                           int32_t beta,
                                           int8_t *tc0)
@@ -2321,7 +2325,7 @@ void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src,
-                                                int32_t ystride,
+                                                ptrdiff_t ystride,
                                                 int32_t alpha,
                                                 int32_t beta)
 {
@@ -2509,10 +2513,8 @@ void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
     SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
     SRA_4V(tmp8, tmp9, tmp10, tmp11, denom);
     SRA_4V(tmp12, tmp13, tmp14, tmp15, denom);
-    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
-    CLIP_SH4_0_255(tmp8, tmp9, tmp10, tmp11);
-    CLIP_SH4_0_255(tmp12, tmp13, tmp14, tmp15);
+    CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    CLIP_SH8_0_255(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15);
     PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, dst0, dst1,
                 dst2, dst3);
     PCKEV_B4_UB(tmp9, tmp8, tmp11, tmp10, tmp13, tmp12, tmp15, tmp14, dst4,
@@ -2553,10 +2555,8 @@ void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
         SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
         SRA_4V(tmp8, tmp9, tmp10, tmp11, denom);
         SRA_4V(tmp12, tmp13, tmp14, tmp15, denom);
-        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
-        CLIP_SH4_0_255(tmp8, tmp9, tmp10, tmp11);
-        CLIP_SH4_0_255(tmp12, tmp13, tmp14, tmp15);
+        CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+        CLIP_SH8_0_255(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15);
         PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, dst0, dst1,
                     dst2, dst3);
         PCKEV_B4_UB(tmp9, tmp8, tmp11, tmp10, tmp13, tmp12, tmp15, tmp14, dst4,
diff --git a/libavcodec/mips/h264idct_msa.c b/libavcodec/mips/h264idct_msa.c
index 1e1a5c8..fbf7795 100644
--- a/libavcodec/mips/h264idct_msa.c
+++ b/libavcodec/mips/h264idct_msa.c
@@ -233,13 +233,10 @@ static void avc_idct8_addblk_msa(uint8_t *dst, int16_t *src, int32_t dst_stride)
          res0, res1, res2, res3);
     ADD4(res4, tmp4, res5, tmp5, res6, tmp6, res7, tmp7,
          res4, res5, res6, res7);
-    CLIP_SH4_0_255(res0, res1, res2, res3);
-    CLIP_SH4_0_255(res4, res5, res6, res7);
+    CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
     PCKEV_B4_SB(res1, res0, res3, res2, res5, res4, res7, res6,
                 dst0, dst1, dst2, dst3);
-    ST8x4_UB(dst0, dst1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x4_UB(dst2, dst3, dst, dst_stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride)
 }
 
 static void avc_idct8_dc_addblk_msa(uint8_t *dst, int16_t *src,
@@ -265,13 +262,11 @@ static void avc_idct8_dc_addblk_msa(uint8_t *dst, int16_t *src,
          dst0_r, dst1_r, dst2_r, dst3_r);
     ADD4(dst4_r, dc, dst5_r, dc, dst6_r, dc, dst7_r, dc,
          dst4_r, dst5_r, dst6_r, dst7_r);
-    CLIP_SH4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
-    CLIP_SH4_0_255(dst4_r, dst5_r, dst6_r, dst7_r);
+    CLIP_SH8_0_255(dst0_r, dst1_r, dst2_r, dst3_r,
+                   dst4_r, dst5_r, dst6_r, dst7_r);
     PCKEV_B4_SB(dst1_r, dst0_r, dst3_r, dst2_r, dst5_r, dst4_r, dst7_r, dst6_r,
                 dst0, dst1, dst2, dst3);
-    ST8x4_UB(dst0, dst1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x4_UB(dst2, dst3, dst, dst_stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride)
 }
 
 void ff_h264_idct_add_msa(uint8_t *dst, int16_t *src, int32_t dst_stride)
@@ -340,7 +335,7 @@ void ff_h264_idct4x4_addblk_dc_msa(uint8_t *dst, int16_t *src,
     ADD2(pred_r, input_dc, pred_l, input_dc, pred_r, pred_l);
     CLIP_SH2_0_255(pred_r, pred_l);
     out = __msa_pckev_b((v16i8) pred_l, (v16i8) pred_r);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_h264_idct8_dc_addblk_msa(uint8_t *dst, int16_t *src,
diff --git a/libavcodec/mips/h264pred_init_mips.c b/libavcodec/mips/h264pred_init_mips.c
index c33d8f7..0fd9bb7 100644
--- a/libavcodec/mips/h264pred_init_mips.c
+++ b/libavcodec/mips/h264pred_init_mips.c
@@ -19,137 +19,121 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "h264dsp_mips.h"
 #include "h264pred_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264_pred_init_msa(H264PredContext *h, int codec_id,
-                                       const int bit_depth,
-                                       const int chroma_format_idc)
+av_cold void ff_h264_pred_init_mips(H264PredContext *h, int codec_id,
+                                    int bit_depth,
+                                    const int chroma_format_idc)
 {
-    if (8 == bit_depth) {
-        if (chroma_format_idc == 1) {
-            h->pred8x8[VERT_PRED8x8] = ff_h264_intra_pred_vert_8x8_msa;
-            h->pred8x8[HOR_PRED8x8] = ff_h264_intra_pred_horiz_8x8_msa;
-        }
+    int cpu_flags = av_get_cpu_flags();
 
-        if (codec_id != AV_CODEC_ID_VP7 && codec_id != AV_CODEC_ID_VP8) {
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
             if (chroma_format_idc == 1) {
-                h->pred8x8[PLANE_PRED8x8] = ff_h264_intra_predict_plane_8x8_msa;
-            }
-        }
-        if (codec_id != AV_CODEC_ID_RV40 && codec_id != AV_CODEC_ID_VP7
-            && codec_id != AV_CODEC_ID_VP8) {
-            if (chroma_format_idc == 1) {
-                h->pred8x8[DC_PRED8x8] = ff_h264_intra_predict_dc_4blk_8x8_msa;
-                h->pred8x8[LEFT_DC_PRED8x8] =
-                    ff_h264_intra_predict_hor_dc_8x8_msa;
-                h->pred8x8[TOP_DC_PRED8x8] =
-                    ff_h264_intra_predict_vert_dc_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_L0T_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_l0t_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_0LT_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_0lt_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_L00_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_l00_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_0L0_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_0l0_8x8_msa;
-            }
-        } else {
-            if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
-                h->pred8x8[7] = ff_vp8_pred8x8_127_dc_8_msa;
-                h->pred8x8[8] = ff_vp8_pred8x8_129_dc_8_msa;
+                h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x8_vertical_8_mmi;
+                h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x8_horizontal_8_mmi;
+            } else {
+                h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x16_vertical_8_mmi;
+                h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x16_horizontal_8_mmi;
             }
-        }
 
-        if (chroma_format_idc == 1) {
-            h->pred8x8[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_8x8_msa;
-        }
+            h->pred16x16[DC_PRED8x8             ] = ff_pred16x16_dc_8_mmi;
+            h->pred16x16[VERT_PRED8x8           ] = ff_pred16x16_vertical_8_mmi;
+            h->pred16x16[HOR_PRED8x8            ] = ff_pred16x16_horizontal_8_mmi;
+            h->pred8x8l [TOP_DC_PRED            ] = ff_pred8x8l_top_dc_8_mmi;
+            h->pred8x8l [DC_PRED                ] = ff_pred8x8l_dc_8_mmi;
 
-        h->pred16x16[DC_PRED8x8] = ff_h264_intra_pred_dc_16x16_msa;
-        h->pred16x16[VERT_PRED8x8] = ff_h264_intra_pred_vert_16x16_msa;
-        h->pred16x16[HOR_PRED8x8] = ff_h264_intra_pred_horiz_16x16_msa;
+    #if ARCH_MIPS64
+            switch (codec_id) {
+            case AV_CODEC_ID_SVQ3:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_svq3_8_mmi;
+                break;
+            case AV_CODEC_ID_RV40:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_rv40_8_mmi;
+                break;
+            case AV_CODEC_ID_VP7:
+            case AV_CODEC_ID_VP8:
+                break;
+            default:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_h264_8_mmi;
+                break;
+            }
+    #endif
 
-        switch (codec_id) {
-        case AV_CODEC_ID_SVQ3:
-            ;
-            break;
-        case AV_CODEC_ID_RV40:
-            ;
-            break;
-        case AV_CODEC_ID_VP7:
-        case AV_CODEC_ID_VP8:
-            h->pred16x16[7] = ff_vp8_pred16x16_127_dc_8_msa;
-            h->pred16x16[8] = ff_vp8_pred16x16_129_dc_8_msa;
-            break;
-        default:
-            h->pred16x16[PLANE_PRED8x8] =
-                ff_h264_intra_predict_plane_16x16_msa;
-            break;
+            if (codec_id == AV_CODEC_ID_SVQ3 || codec_id == AV_CODEC_ID_H264) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[TOP_DC_PRED8x8   ] = ff_pred8x8_top_dc_8_mmi;
+                    h->pred8x8[DC_PRED8x8       ] = ff_pred8x8_dc_8_mmi;
+                }
+            }
         }
-
-        h->pred16x16[LEFT_DC_PRED8x8] = ff_h264_intra_pred_dc_left_16x16_msa;
-        h->pred16x16[TOP_DC_PRED8x8] = ff_h264_intra_pred_dc_top_16x16_msa;
-        h->pred16x16[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_16x16_msa;
     }
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static av_cold void h264_pred_init_mmi(H264PredContext *h, int codec_id,
-        const int bit_depth, const int chroma_format_idc)
-{
-    if (bit_depth == 8) {
-        if (chroma_format_idc == 1) {
-            h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x8_vertical_8_mmi;
-            h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x8_horizontal_8_mmi;
-        } else {
-            h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x16_vertical_8_mmi;
-            h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x16_horizontal_8_mmi;
-        }
 
-        h->pred16x16[DC_PRED8x8             ] = ff_pred16x16_dc_8_mmi;
-        h->pred16x16[VERT_PRED8x8           ] = ff_pred16x16_vertical_8_mmi;
-        h->pred16x16[HOR_PRED8x8            ] = ff_pred16x16_horizontal_8_mmi;
-        h->pred8x8l [TOP_DC_PRED            ] = ff_pred8x8l_top_dc_8_mmi;
-        h->pred8x8l [DC_PRED                ] = ff_pred8x8l_dc_8_mmi;
+    if (have_msa(cpu_flags)) {
+        if (8 == bit_depth) {
+            if (chroma_format_idc == 1) {
+                h->pred8x8[VERT_PRED8x8] = ff_h264_intra_pred_vert_8x8_msa;
+                h->pred8x8[HOR_PRED8x8] = ff_h264_intra_pred_horiz_8x8_msa;
+            }
 
-#if ARCH_MIPS64
-        switch (codec_id) {
-        case AV_CODEC_ID_SVQ3:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_svq3_8_mmi;
-            break;
-        case AV_CODEC_ID_RV40:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_rv40_8_mmi;
-            break;
-        case AV_CODEC_ID_VP7:
-        case AV_CODEC_ID_VP8:
-            break;
-        default:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_h264_8_mmi;
-            break;
-        }
-#endif
+            if (codec_id != AV_CODEC_ID_VP7 && codec_id != AV_CODEC_ID_VP8) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[PLANE_PRED8x8] = ff_h264_intra_predict_plane_8x8_msa;
+                }
+            }
+            if (codec_id != AV_CODEC_ID_RV40 && codec_id != AV_CODEC_ID_VP7
+                && codec_id != AV_CODEC_ID_VP8) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[DC_PRED8x8] = ff_h264_intra_predict_dc_4blk_8x8_msa;
+                    h->pred8x8[LEFT_DC_PRED8x8] =
+                        ff_h264_intra_predict_hor_dc_8x8_msa;
+                    h->pred8x8[TOP_DC_PRED8x8] =
+                        ff_h264_intra_predict_vert_dc_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_L0T_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_l0t_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_0LT_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_0lt_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_L00_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_l00_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_0L0_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_0l0_8x8_msa;
+                }
+            } else {
+                if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
+                    h->pred8x8[7] = ff_vp8_pred8x8_127_dc_8_msa;
+                    h->pred8x8[8] = ff_vp8_pred8x8_129_dc_8_msa;
+                }
+            }
 
-        if (codec_id == AV_CODEC_ID_SVQ3 || codec_id == AV_CODEC_ID_H264) {
             if (chroma_format_idc == 1) {
-                h->pred8x8[TOP_DC_PRED8x8   ] = ff_pred8x8_top_dc_8_mmi;
-                h->pred8x8[DC_PRED8x8       ] = ff_pred8x8_dc_8_mmi;
+                h->pred8x8[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_8x8_msa;
+            }
+
+            h->pred16x16[DC_PRED8x8] = ff_h264_intra_pred_dc_16x16_msa;
+            h->pred16x16[VERT_PRED8x8] = ff_h264_intra_pred_vert_16x16_msa;
+            h->pred16x16[HOR_PRED8x8] = ff_h264_intra_pred_horiz_16x16_msa;
+
+            switch (codec_id) {
+            case AV_CODEC_ID_SVQ3:
+            case AV_CODEC_ID_RV40:
+                break;
+            case AV_CODEC_ID_VP7:
+            case AV_CODEC_ID_VP8:
+                h->pred16x16[7] = ff_vp8_pred16x16_127_dc_8_msa;
+                h->pred16x16[8] = ff_vp8_pred16x16_129_dc_8_msa;
+                break;
+            default:
+                h->pred16x16[PLANE_PRED8x8] =
+                    ff_h264_intra_predict_plane_16x16_msa;
+                break;
             }
+
+            h->pred16x16[LEFT_DC_PRED8x8] = ff_h264_intra_pred_dc_left_16x16_msa;
+            h->pred16x16[TOP_DC_PRED8x8] = ff_h264_intra_pred_dc_top_16x16_msa;
+            h->pred16x16[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_16x16_msa;
         }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_h264_pred_init_mips(H264PredContext *h, int codec_id,
-                                    int bit_depth,
-                                    const int chroma_format_idc)
-{
-#if HAVE_MSA
-    h264_pred_init_msa(h, codec_id, bit_depth, chroma_format_idc);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    h264_pred_init_mmi(h, codec_id, bit_depth, chroma_format_idc);
-#endif /* HAVE_MMI */
-}
diff --git a/libavcodec/mips/h264pred_mmi.c b/libavcodec/mips/h264pred_mmi.c
index f4fe091..0209c2e 100644
--- a/libavcodec/mips/h264pred_mmi.c
+++ b/libavcodec/mips/h264pred_mmi.c
@@ -178,7 +178,9 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
 
         "1:                                                             \n\t"
         "bnez       %[has_topright],            2f                      \n\t"
-        "pinsrh_3   %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "dli        %[tmp0],    0xa4                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
 
         "2:                                                             \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
@@ -370,7 +372,9 @@ void ff_pred8x8l_vertical_8_mmi(uint8_t *src, int has_topleft,
 
         "1:                                                             \n\t"
         "bnez       %[has_topright],            2f                      \n\t"
-        "pinsrh_3   %[ftmp11],  %[ftmp11],      %[ftmp9]                \n\t"
+        "dli        %[tmp0],    0xa4                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp11],  %[ftmp11],      %[ftmp1]                \n\t"
 
         "2:                                                             \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
diff --git a/libavcodec/mips/h264qpel_init_mips.c b/libavcodec/mips/h264qpel_init_mips.c
index 92219f8..ea839f0 100644
--- a/libavcodec/mips/h264qpel_init_mips.c
+++ b/libavcodec/mips/h264qpel_init_mips.c
@@ -19,231 +19,221 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264qpel_init_msa(H264QpelContext *c, int bit_depth)
+av_cold void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth)
 {
-    if (8 == bit_depth) {
-        c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_msa;
-        c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_msa;
-        c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_msa;
-        c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_msa;
-        c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_msa;
-        c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_msa;
-        c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_msa;
-        c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_msa;
-        c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_msa;
-        c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_msa;
-        c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_msa;
-        c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_msa;
-        c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_msa;
-        c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_msa;
-        c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_msa;
-        c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_msa;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_mmi;
 
-        c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_msa;
-        c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_msa;
-        c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_msa;
-        c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_msa;
-        c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_msa;
-        c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_msa;
-        c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_msa;
-        c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_msa;
-        c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_msa;
-        c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_msa;
-        c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_msa;
-        c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_msa;
-        c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_msa;
-        c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_msa;
-        c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_msa;
-        c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_msa;
+            c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_mmi;
 
-        c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_msa;
-        c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_msa;
-        c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_msa;
-        c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_msa;
-        c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_msa;
-        c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_msa;
-        c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_msa;
-        c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_msa;
-        c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_msa;
-        c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_msa;
-        c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_msa;
-        c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_msa;
-        c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_msa;
-        c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_msa;
-        c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_msa;
+            c->put_h264_qpel_pixels_tab[2][0] = ff_put_h264_qpel4_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_mmi;
+        }
     }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void h264qpel_init_mmi(H264QpelContext *c, int bit_depth)
-{
-    if (8 == bit_depth) {
-        c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_mmi;
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_msa;
+            c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_msa;
+            c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_msa;
+            c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_msa;
+            c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_msa;
+            c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_msa;
+            c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_msa;
+            c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_msa;
+            c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_msa;
+            c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_msa;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_msa;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_msa;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_msa;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_msa;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_msa;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_msa;
 
-        c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_mmi;
+            c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_msa;
+            c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_msa;
+            c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_msa;
+            c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_msa;
+            c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_msa;
+            c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_msa;
+            c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_msa;
+            c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_msa;
+            c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_msa;
+            c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_msa;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_msa;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_msa;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_msa;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_msa;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_msa;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_msa;
 
-        c->put_h264_qpel_pixels_tab[2][0] = ff_put_h264_qpel4_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_mmi;
+            c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_msa;
+            c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_msa;
+            c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_msa;
+            c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_msa;
+            c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_msa;
+            c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_msa;
+            c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_msa;
+            c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_msa;
+            c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_msa;
+            c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_msa;
+            c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_msa;
+            c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_msa;
+            c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_msa;
+            c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_msa;
+            c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_msa;
+        }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth)
-{
-#if HAVE_MSA
-    h264qpel_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    h264qpel_init_mmi(c, bit_depth);
-#endif /* HAVE_MMI */
-}
diff --git a/libavcodec/mips/h264qpel_msa.c b/libavcodec/mips/h264qpel_msa.c
index 9c779bd..e435c18 100644
--- a/libavcodec/mips/h264qpel_msa.c
+++ b/libavcodec/mips/h264qpel_msa.c
@@ -149,7 +149,7 @@ static void avc_luma_hv_qrt_4x4_msa(const uint8_t *src_x, const uint8_t *src_y,
 
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_luma_hv_qrt_8x8_msa(const uint8_t *src_x, const uint8_t *src_y,
@@ -220,7 +220,7 @@ static void avc_luma_hv_qrt_8x8_msa(const uint8_t *src_x, const uint8_t *src_y,
     SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src_y, stride, src_vt9, src_vt10, src_vt11, src_vt12);
@@ -256,8 +256,7 @@ static void avc_luma_hv_qrt_8x8_msa(const uint8_t *src_x, const uint8_t *src_y,
     SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-    ST8x4_UB(out0, out1, dst, stride);
-    dst += (4 * stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_luma_hv_qrt_16x16_msa(const uint8_t *src_x,
@@ -337,7 +336,7 @@ static void avc_luma_hv_qrt_16x16_msa(const uint8_t *src_x,
             SAT_SH4_SH(out0, out1, out2, out3, 7);
             tmp0 = PCKEV_XORI128_UB(out0, out1);
             tmp1 = PCKEV_XORI128_UB(out2, out3);
-            ST8x4_UB(tmp0, tmp1, dst, stride);
+            ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             src_vt0 = src_vt4;
@@ -419,7 +418,7 @@ static void avc_luma_hv_qrt_and_aver_dst_4x4_msa(const uint8_t *src_x,
     res = PCKEV_XORI128_UB(res0, res1);
     dst0 = __msa_aver_u_b(res, dst0);
 
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 static void avc_luma_hv_qrt_and_aver_dst_8x8_msa(const uint8_t *src_x,
@@ -498,7 +497,7 @@ static void avc_luma_hv_qrt_and_aver_dst_8x8_msa(const uint8_t *src_x,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src_y, stride, src_vt9, src_vt10, src_vt11, src_vt12);
@@ -539,8 +538,7 @@ static void avc_luma_hv_qrt_and_aver_dst_8x8_msa(const uint8_t *src_x,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
-    dst += (4 * stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
 static void avc_luma_hv_qrt_and_aver_dst_16x16_msa(const uint8_t *src_x,
@@ -627,7 +625,7 @@ static void avc_luma_hv_qrt_and_aver_dst_16x16_msa(const uint8_t *src_x,
             tmp0 = PCKEV_XORI128_UB(out0, out1);
             tmp1 = PCKEV_XORI128_UB(out2, out3);
             AVER_UB2_UB(tmp0, dst0, tmp1, dst1, dst0, dst1);
-            ST8x4_UB(dst0, dst1, dst, stride);
+            ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             src_vt0 = src_vt4;
@@ -723,7 +721,7 @@ void ff_avg_h264_qpel8_mc00_msa(uint8_t *dst, const uint8_t *src,
     AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0, dst1,
                 dst2, dst3);
 
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc00_msa(uint8_t *dst, const uint8_t *src,
@@ -739,7 +737,7 @@ void ff_avg_h264_qpel4_mc00_msa(uint8_t *dst, const uint8_t *src,
 
     dst0 = __msa_aver_u_b(src0, dst0);
 
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
@@ -792,8 +790,8 @@ void ff_put_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 2);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 2);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 2,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -860,8 +858,8 @@ void ff_put_h264_qpel16_mc30_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 3);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 3);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 3,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -913,10 +911,10 @@ void ff_put_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 2);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 2,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -930,7 +928,7 @@ void ff_put_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     tmp2 = __msa_aver_s_b(tmp2, src4);
     tmp3 = __msa_aver_s_b(tmp3, src5);
     XORI_B4_128_SB(tmp0, tmp1, tmp2, tmp3);
-    ST8x8_UB(tmp0, tmp1, tmp2, tmp3, dst, stride);
+    ST_D8(tmp0, tmp1, tmp2, tmp3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
@@ -968,10 +966,10 @@ void ff_put_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 3);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 3,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -985,7 +983,7 @@ void ff_put_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     tmp2 = __msa_aver_s_b(tmp2, src4);
     tmp3 = __msa_aver_s_b(tmp3, src5);
     XORI_B4_128_SB(tmp0, tmp1, tmp2, tmp3);
-    ST8x8_UB(tmp0, tmp1, tmp2, tmp3, dst, stride);
+    ST_D8(tmp0, tmp1, tmp2, tmp3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
@@ -1009,14 +1007,14 @@ void ff_put_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(res0, res1, 5);
     SAT_SH2_SH(res0, res1, 7);
     res = __msa_pckev_b((v16i8) res1, (v16i8) res0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
     res = __msa_aver_s_b(res, src0);
     res = (v16i8) __msa_xori_b((v16u8) res, 128);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
@@ -1040,14 +1038,14 @@ void ff_put_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(res0, res1, 5);
     SAT_SH2_SH(res0, res1, 7);
     res = __msa_pckev_b((v16i8) res1, (v16i8) res0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
     res = __msa_aver_s_b(res, src0);
     res = (v16i8) __msa_xori_b((v16u8) res, 128);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc20_msa(uint8_t *dst, const uint8_t *src,
@@ -1153,7 +1151,7 @@ void ff_put_h264_qpel8_mc20_msa(uint8_t *dst, const uint8_t *src,
     out1 = PCKEV_XORI128_UB(res2, res3);
     out2 = PCKEV_XORI128_UB(res4, res5);
     out3 = PCKEV_XORI128_UB(res6, res7);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc20_msa(uint8_t *dst, const uint8_t *src,
@@ -1178,7 +1176,7 @@ void ff_put_h264_qpel4_mc20_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(res0, res1, 5);
     SAT_SH2_SH(res0, res1, 7);
     out = PCKEV_XORI128_UB(res0, res1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc01_msa(uint8_t *dst, const uint8_t *src,
@@ -1378,7 +1376,7 @@ void ff_put_h264_qpel8_mc01_msa(uint8_t *dst, const uint8_t *src,
     out2 = __msa_aver_s_b(out2, tmp2);
     out3 = __msa_aver_s_b(out3, tmp3);
     XORI_B4_128_SB(out0, out1, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel8_mc03_msa(uint8_t *dst, const uint8_t *src,
@@ -1431,7 +1429,7 @@ void ff_put_h264_qpel8_mc03_msa(uint8_t *dst, const uint8_t *src,
     out2 = __msa_aver_s_b(out2, tmp2);
     out3 = __msa_aver_s_b(out3, tmp3);
     XORI_B4_128_SB(out0, out1, out2, out3);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc01_msa(uint8_t *dst, const uint8_t *src,
@@ -1472,7 +1470,7 @@ void ff_put_h264_qpel4_mc01_msa(uint8_t *dst, const uint8_t *src,
     src54_r = (v16i8) __msa_insve_w((v4i32) src4, 1, (v4i32) src5);
     src32_r = (v16i8) __msa_insve_d((v2i64) src32_r, 1, (v2i64) src54_r);
     out = __msa_aver_u_b(out, (v16u8) src32_r);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc03_msa(uint8_t *dst, const uint8_t *src,
@@ -1513,7 +1511,7 @@ void ff_put_h264_qpel4_mc03_msa(uint8_t *dst, const uint8_t *src,
     src54_r = (v16i8) __msa_insve_w((v4i32) src5, 1, (v4i32) src6);
     src32_r = (v16i8) __msa_insve_d((v2i64) src32_r, 1, (v2i64) src54_r);
     out = __msa_aver_u_b(out, (v16u8) src32_r);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc11_msa(uint8_t *dst, const uint8_t *src,
@@ -1691,7 +1689,7 @@ void ff_put_h264_qpel16_mc21_msa(uint8_t *dst, const uint8_t *src,
 
             out0 = PCKEV_XORI128_UB(dst0, dst1);
             out1 = PCKEV_XORI128_UB(dst2, dst3);
-            ST8x4_UB(out0, out1, dst, stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             hz_out0 = hz_out4;
@@ -1804,7 +1802,7 @@ void ff_put_h264_qpel16_mc23_msa(uint8_t *dst, const uint8_t *src,
 
             out0 = PCKEV_XORI128_UB(dst0, dst1);
             out1 = PCKEV_XORI128_UB(dst2, dst3);
-            ST8x4_UB(out0, out1, dst, stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             hz_out0 = hz_out4;
@@ -1905,7 +1903,7 @@ void ff_put_h264_qpel8_mc21_msa(uint8_t *dst, const uint8_t *src,
 
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src9, src10, src11, src12);
@@ -1951,7 +1949,7 @@ void ff_put_h264_qpel8_mc21_msa(uint8_t *dst, const uint8_t *src,
 
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
@@ -2040,7 +2038,7 @@ void ff_put_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
 
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src9, src10, src11, src12);
@@ -2086,7 +2084,7 @@ void ff_put_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
 
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc21_msa(uint8_t *dst, const uint8_t *src,
@@ -2150,7 +2148,7 @@ void ff_put_h264_qpel4_mc21_msa(uint8_t *dst, const uint8_t *src,
     dst1 = __msa_aver_s_h(dst1, hz_out4);
 
     res = PCKEV_XORI128_UB(dst0, dst1);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc23_msa(uint8_t *dst, const uint8_t *src,
@@ -2215,7 +2213,7 @@ void ff_put_h264_qpel4_mc23_msa(uint8_t *dst, const uint8_t *src,
     dst1 = __msa_aver_s_h(dst1, hz_out1);
 
     res = PCKEV_XORI128_UB(dst0, dst1);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc02_msa(uint8_t *dst, const uint8_t *src,
@@ -2332,7 +2330,7 @@ void ff_put_h264_qpel8_mc02_msa(uint8_t *dst, const uint8_t *src,
     out1 = PCKEV_XORI128_UB(out2_r, out3_r);
     out2 = PCKEV_XORI128_UB(out4_r, out5_r);
     out3 = PCKEV_XORI128_UB(out6_r, out7_r);
-    ST8x8_UB(out0, out1, out2, out3, dst, stride);
+    ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc02_msa(uint8_t *dst, const uint8_t *src,
@@ -2369,7 +2367,7 @@ void ff_put_h264_qpel4_mc02_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(out10, out32, 5);
     SAT_SH2_SH(out10, out32, 7);
     out = PCKEV_XORI128_UB(out10, out32);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc12_msa(uint8_t *dst, const uint8_t *src,
@@ -2601,7 +2599,7 @@ void ff_put_h264_qpel8_mc12_msa(uint8_t *dst, const uint8_t *src,
         dst0 = __msa_aver_s_h(dst2, dst0);
         dst1 = __msa_aver_s_h(dst3, dst1);
         out = PCKEV_XORI128_UB(dst0, dst1);
-        ST8x2_UB(out, dst, stride);
+        ST_D2(out, 0, 1, dst, stride);
         dst += (2 * stride);
 
         src0 = src2;
@@ -2677,7 +2675,7 @@ void ff_put_h264_qpel8_mc32_msa(uint8_t *dst, const uint8_t *src,
         dst0 = __msa_aver_s_h(dst2, dst0);
         dst1 = __msa_aver_s_h(dst3, dst1);
         out = PCKEV_XORI128_UB(dst0, dst1);
-        ST8x2_UB(out, dst, stride);
+        ST_D2(out, 0, 1, dst, stride);
         dst += (2 * stride);
 
         src0 = src2;
@@ -2777,7 +2775,7 @@ void ff_put_h264_qpel4_mc12_msa(uint8_t *dst, const uint8_t *src,
 
     PCKEV_H2_SH(hz_res1, hz_res0, hz_res3, hz_res2, dst0, dst2);
     out = PCKEV_XORI128_UB(dst0, dst2);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc32_msa(uint8_t *dst, const uint8_t *src,
@@ -2873,7 +2871,7 @@ void ff_put_h264_qpel4_mc32_msa(uint8_t *dst, const uint8_t *src,
 
     PCKEV_H2_SH(hz_res1, hz_res0, hz_res3, hz_res2, dst0, dst2);
     out = PCKEV_XORI128_UB(dst0, dst2);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_put_h264_qpel16_mc22_msa(uint8_t *dst, const uint8_t *src,
@@ -2961,7 +2959,7 @@ void ff_put_h264_qpel16_mc22_msa(uint8_t *dst, const uint8_t *src,
 
             out0 = PCKEV_XORI128_UB(dst0, dst1);
             out1 = PCKEV_XORI128_UB(dst2, dst3);
-            ST8x4_UB(out0, out1, dst, stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             hz_out0 = hz_out4;
@@ -3049,7 +3047,7 @@ void ff_put_h264_qpel8_mc22_msa(uint8_t *dst, const uint8_t *src,
     dst3 = __msa_pckev_h((v8i16) tmp1, (v8i16) tmp0);
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src0, src1, src2, src3);
@@ -3086,7 +3084,7 @@ void ff_put_h264_qpel8_mc22_msa(uint8_t *dst, const uint8_t *src,
     dst3 = __msa_pckev_h((v8i16) tmp1, (v8i16) tmp0);
     out0 = PCKEV_XORI128_UB(dst0, dst1);
     out1 = PCKEV_XORI128_UB(dst2, dst3);
-    ST8x4_UB(out0, out1, dst, stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_put_h264_qpel4_mc22_msa(uint8_t *dst, const uint8_t *src,
@@ -3141,7 +3139,7 @@ void ff_put_h264_qpel4_mc22_msa(uint8_t *dst, const uint8_t *src,
                           filt2);
     dst1 = __msa_pckev_h((v8i16) tmp1, (v8i16) tmp0);
     res = PCKEV_XORI128_UB(dst0, dst1);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
@@ -3196,8 +3194,8 @@ void ff_avg_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 2);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 2);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 2,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -3268,8 +3266,8 @@ void ff_avg_h264_qpel16_mc30_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 3);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 3);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 3,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -3325,10 +3323,10 @@ void ff_avg_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 2);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 2,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -3350,7 +3348,7 @@ void ff_avg_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     INSERT_D2_UB(tp2, tp3, dst3);
     AVER_UB2_UB(tmp0, dst0, tmp1, dst1, dst0, dst1);
     AVER_UB2_UB(tmp2, dst2, tmp3, dst3, dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
@@ -3390,10 +3388,10 @@ void ff_avg_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 3);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 3,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -3415,7 +3413,7 @@ void ff_avg_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     INSERT_D2_UB(tp2, tp3, dst3);
     AVER_UB2_UB(tmp0, dst0, tmp1, dst1, dst0, dst1);
     AVER_UB2_UB(tmp2, dst2, tmp3, dst3, dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
@@ -3441,8 +3439,8 @@ void ff_avg_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(out0, out1, 5);
     SAT_SH2_SH(out0, out1, 7);
     res = __msa_pckev_b((v16i8) out1, (v16i8) out0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
@@ -3451,7 +3449,7 @@ void ff_avg_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
     LW4(dst, stride, tp0, tp1, tp2, tp3);
     INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
     dst0 = __msa_aver_u_b((v16u8) res, dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
@@ -3477,8 +3475,8 @@ void ff_avg_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(out0, out1, 5);
     SAT_SH2_SH(out0, out1, 7);
     res = __msa_pckev_b((v16i8) out1, (v16i8) out0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
@@ -3487,7 +3485,7 @@ void ff_avg_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
     LW4(dst, stride, tp0, tp1, tp2, tp3);
     INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
     dst0 = __msa_aver_u_b((v16u8) res, dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc20_msa(uint8_t *dst, const uint8_t *src,
@@ -3608,7 +3606,7 @@ void ff_avg_h264_qpel8_mc20_msa(uint8_t *dst, const uint8_t *src,
     INSERT_D2_UB(tp2, tp3, out7);
     AVER_UB2_UB(out0, out2, out1, out3, out0, out1);
     AVER_UB2_UB(out4, out6, out5, out7, out4, out5);
-    ST8x8_UB(out0, out1, out4, out5, dst, stride);
+    ST_D8(out0, out1, out4, out5, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc20_msa(uint8_t *dst, const uint8_t *src,
@@ -3637,7 +3635,7 @@ void ff_avg_h264_qpel4_mc20_msa(uint8_t *dst, const uint8_t *src,
     LW4(dst, stride, tp0, tp1, tp2, tp3);
     INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
     res = __msa_aver_u_b(res, dst0);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc01_msa(uint8_t *dst, const uint8_t *src,
@@ -3856,7 +3854,7 @@ void ff_avg_h264_qpel8_mc01_msa(uint8_t *dst, const uint8_t *src,
     XORI_B4_128_SB(out0, out1, out2, out3);
     AVER_UB4_UB(out0, dst0, out1, dst1, out2, dst2, out3, dst3, dst0, dst1,
                 dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel8_mc03_msa(uint8_t *dst, const uint8_t *src,
@@ -3922,7 +3920,7 @@ void ff_avg_h264_qpel8_mc03_msa(uint8_t *dst, const uint8_t *src,
     XORI_B4_128_SB(out0, out1, out2, out3);
     AVER_UB4_UB(out0, dst0, out1, dst1, out2, dst2, out3, dst3, dst0, dst1,
                 dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc01_msa(uint8_t *dst, const uint8_t *src,
@@ -3967,7 +3965,7 @@ void ff_avg_h264_qpel4_mc01_msa(uint8_t *dst, const uint8_t *src,
     res = PCKEV_XORI128_UB(out10, out32);
     res = __msa_aver_u_b(res, (v16u8) src32_r);
     dst0 = __msa_aver_u_b(res, dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc03_msa(uint8_t *dst, const uint8_t *src,
@@ -4013,7 +4011,7 @@ void ff_avg_h264_qpel4_mc03_msa(uint8_t *dst, const uint8_t *src,
     src32_r = (v16i8) __msa_insve_d((v2i64) src32_r, 1, (v2i64) src54_r);
     res = __msa_aver_u_b(res, (v16u8) src32_r);
     dst0 = __msa_aver_u_b(res, dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc11_msa(uint8_t *dst, const uint8_t *src,
@@ -4196,7 +4194,7 @@ void ff_avg_h264_qpel16_mc21_msa(uint8_t *dst, const uint8_t *src,
 
             out0 = PCKEV_XORI128_UB(tmp0, tmp1);
             dst0 = __msa_aver_u_b(out0, dst0);
-            ST8x2_UB(dst0, dst, stride);
+            ST_D2(dst0, 0, 1, dst, stride);
             dst += (2 * stride);
 
             LD_SB2(src, stride, src7, src8);
@@ -4232,7 +4230,7 @@ void ff_avg_h264_qpel16_mc21_msa(uint8_t *dst, const uint8_t *src,
 
             out1 = PCKEV_XORI128_UB(tmp2, tmp3);
             dst1 = __msa_aver_u_b(out1, dst1);
-            ST8x2_UB(dst1, dst, stride);
+            ST_D2(dst1, 0, 1, dst, stride);
             dst += (2 * stride);
 
             hz_out0 = hz_out4;
@@ -4326,7 +4324,7 @@ void ff_avg_h264_qpel16_mc23_msa(uint8_t *dst, const uint8_t *src,
             INSERT_D2_UB(tp0, tp1, dst0);
             out0 = PCKEV_XORI128_UB(tmp0, tmp1);
             dst0 = __msa_aver_u_b(out0, dst0);
-            ST8x2_UB(dst0, dst, stride);
+            ST_D2(dst0, 0, 1, dst, stride);
             dst += (2 * stride);
 
             LD_SB2(src, stride, src7, src8);
@@ -4361,7 +4359,7 @@ void ff_avg_h264_qpel16_mc23_msa(uint8_t *dst, const uint8_t *src,
             INSERT_D2_UB(tp2, tp3, dst1);
             out1 = PCKEV_XORI128_UB(tmp2, tmp3);
             dst1 = __msa_aver_u_b(out1, dst1);
-            ST8x2_UB(dst1, dst, stride);
+            ST_D2(dst1, 0, 1, dst, stride);
             dst += (2 * stride);
 
             hz_out0 = hz_out4;
@@ -4468,7 +4466,7 @@ void ff_avg_h264_qpel8_mc21_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src9, src10, src11, src12);
@@ -4519,7 +4517,7 @@ void ff_avg_h264_qpel8_mc21_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
@@ -4614,7 +4612,7 @@ void ff_avg_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src9, src10, src11, src12);
@@ -4665,7 +4663,7 @@ void ff_avg_h264_qpel8_mc23_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc21_msa(uint8_t *dst, const uint8_t *src,
@@ -4732,7 +4730,7 @@ void ff_avg_h264_qpel4_mc21_msa(uint8_t *dst, const uint8_t *src,
     INSERT_W4_UB(tp0, tp1, tp2, tp3, out);
     res = PCKEV_XORI128_UB(dst0, dst1);
     res = __msa_aver_u_b(res, out);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc23_msa(uint8_t *dst, const uint8_t *src,
@@ -4800,7 +4798,7 @@ void ff_avg_h264_qpel4_mc23_msa(uint8_t *dst, const uint8_t *src,
     INSERT_W4_UB(tp0, tp1, tp2, tp3, out);
     res = PCKEV_XORI128_UB(dst0, dst1);
     res = __msa_aver_u_b(res, out);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc02_msa(uint8_t *dst, const uint8_t *src,
@@ -4936,7 +4934,7 @@ void ff_avg_h264_qpel8_mc02_msa(uint8_t *dst, const uint8_t *src,
     out3 = PCKEV_XORI128_UB(out6_r, out7_r);
     AVER_UB4_UB(out0, dst0, out1, dst1, out2, dst2, out3, dst3, dst0, dst1,
                 dst2, dst3);
-    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);
+    ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc02_msa(uint8_t *dst, const uint8_t *src,
@@ -4977,7 +4975,7 @@ void ff_avg_h264_qpel4_mc02_msa(uint8_t *dst, const uint8_t *src,
     INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
     res = PCKEV_XORI128_UB(out10, out32);
     dst0 = __msa_aver_u_b(res, dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc12_msa(uint8_t *dst, const uint8_t *src,
@@ -5217,7 +5215,7 @@ void ff_avg_h264_qpel8_mc12_msa(uint8_t *dst, const uint8_t *src,
         tmp1 = __msa_aver_s_h(tmp3, tmp1);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
         out = __msa_aver_u_b(out, dst0);
-        ST8x2_UB(out, dst, stride);
+        ST_D2(out, 0, 1, dst, stride);
         dst += (2 * stride);
 
         src0 = src2;
@@ -5297,7 +5295,7 @@ void ff_avg_h264_qpel8_mc32_msa(uint8_t *dst, const uint8_t *src,
         tmp1 = __msa_aver_s_h(tmp3, tmp1);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
         out = __msa_aver_u_b(out, dst0);
-        ST8x2_UB(out, dst, stride);
+        ST_D2(out, 0, 1, dst, stride);
         dst += (2 * stride);
 
         src0 = src2;
@@ -5401,7 +5399,7 @@ void ff_avg_h264_qpel4_mc12_msa(uint8_t *dst, const uint8_t *src,
     PCKEV_H2_SH(hz_res1, hz_res0, hz_res3, hz_res2, dst0, dst2);
     out = PCKEV_XORI128_UB(dst0, dst2);
     out = __msa_aver_u_b(out, dstv);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc32_msa(uint8_t *dst, const uint8_t *src,
@@ -5500,7 +5498,7 @@ void ff_avg_h264_qpel4_mc32_msa(uint8_t *dst, const uint8_t *src,
     PCKEV_H2_SH(hz_res1, hz_res0, hz_res3, hz_res2, dst0, dst2);
     out = PCKEV_XORI128_UB(dst0, dst2);
     out = __msa_aver_u_b(out, dstv);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, stride);
+    ST_W4(out, 0, 1, 2, 3, dst, stride);
 }
 
 void ff_avg_h264_qpel16_mc22_msa(uint8_t *dst, const uint8_t *src,
@@ -5592,7 +5590,7 @@ void ff_avg_h264_qpel16_mc22_msa(uint8_t *dst, const uint8_t *src,
             out0 = PCKEV_XORI128_UB(res0, res1);
             out1 = PCKEV_XORI128_UB(res2, res3);
             AVER_UB2_UB(out0, dst0, out1, dst1, out0, out1);
-            ST8x4_UB(out0, out1, dst, stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst, stride);
             dst += (4 * stride);
 
             hz_out0 = hz_out4;
@@ -5685,7 +5683,7 @@ void ff_avg_h264_qpel8_mc22_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(res0, res1);
     out1 = PCKEV_XORI128_UB(res2, res3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
     dst += (4 * stride);
 
     LD_SB4(src, stride, src0, src1, src2, src3);
@@ -5726,7 +5724,7 @@ void ff_avg_h264_qpel8_mc22_msa(uint8_t *dst, const uint8_t *src,
     out0 = PCKEV_XORI128_UB(res0, res1);
     out1 = PCKEV_XORI128_UB(res2, res3);
     AVER_UB2_UB(out0, dst0, out1, dst1, dst0, dst1);
-    ST8x4_UB(dst0, dst1, dst, stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
 void ff_avg_h264_qpel4_mc22_msa(uint8_t *dst, const uint8_t *src,
@@ -5785,5 +5783,5 @@ void ff_avg_h264_qpel4_mc22_msa(uint8_t *dst, const uint8_t *src,
     INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
     res = PCKEV_XORI128_UB(res0, res1);
     res = __msa_aver_u_b(res, dst0);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);
+    ST_W4(res, 0, 1, 2, 3, dst, stride);
 }
diff --git a/libavcodec/mips/hevc_idct_msa.c b/libavcodec/mips/hevc_idct_msa.c
index 0943119..5ab6acd 100644
--- a/libavcodec/mips/hevc_idct_msa.c
+++ b/libavcodec/mips/hevc_idct_msa.c
@@ -727,7 +727,7 @@ static void hevc_addblk_4x4_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     ADD2(dst_r0, in0, dst_l0, in1, dst_r0, dst_l0);
     CLIP_SH2_0_255(dst_r0, dst_l0);
     dst_vec = (v4i32) __msa_pckev_b((v16i8) dst_l0, (v16i8) dst_r0);
-    ST4x4_UB(dst_vec, dst_vec, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst_vec, 0, 1, 2, 3, dst, stride);
 }
 
 static void hevc_addblk_8x8_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
@@ -752,8 +752,7 @@ static void hevc_addblk_8x8_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
          dst_r0, dst_l0, dst_r1, dst_l1);
     CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
     PCKEV_B2_SH(dst_l0, dst_r0, dst_l1, dst_r1, dst_r0, dst_r1);
-    ST8x4_UB(dst_r0, dst_r1, dst, stride);
-    dst += (4 * stride);
+    ST_D4(dst_r0, dst_r1, 0, 1, 0, 1, dst, stride);
 
     LD4(temp_dst, stride, dst0, dst1, dst2, dst3);
     INSERT_D2_SD(dst0, dst1, dst_vec0);
@@ -764,7 +763,7 @@ static void hevc_addblk_8x8_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
          dst_r0, dst_l0, dst_r1, dst_l1);
     CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
     PCKEV_B2_SH(dst_l0, dst_r0, dst_l1, dst_r1, dst_r0, dst_r1);
-    ST8x4_UB(dst_r0, dst_r1, dst, stride);
+    ST_D4(dst_r0, dst_r1, 0, 1, 0, 1, dst + 4 * stride, stride);
 }
 
 static void hevc_addblk_16x16_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
@@ -804,8 +803,9 @@ static void hevc_addblk_16x16_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
         LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
         coeffs += 64;
 
-        CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-        CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+        CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                       dst_r2, dst_l2, dst_r3, dst_l3);
+
         PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                     dst_r3, dst0, dst1, dst2, dst3);
         ST_UB4(dst0, dst1, dst2, dst3, dst, stride);
@@ -826,8 +826,8 @@ static void hevc_addblk_16x16_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     dst_r3 += in6;
     dst_l3 += in7;
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB4(dst0, dst1, dst2, dst3, dst, stride);
@@ -874,8 +874,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
         LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
         coeffs += 64;
 
-        CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-        CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+        CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                       dst_r2, dst_l2, dst_r3, dst_l3);
         PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                     dst_r3, dst0, dst1, dst2, dst3);
         ST_UB2(dst0, dst1, dst, 16);
@@ -906,8 +906,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     LD_SH4(coeffs, 16, in0, in2, in4, in6);
     LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB2(dst0, dst1, dst, 16);
@@ -929,8 +929,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     dst_r3 += in6;
     dst_l3 += in7;
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB2(dst0, dst1, dst, 16);
diff --git a/libavcodec/mips/hevc_lpf_sao_msa.c b/libavcodec/mips/hevc_lpf_sao_msa.c
index adcafde..26663dd 100644
--- a/libavcodec/mips/hevc_lpf_sao_msa.c
+++ b/libavcodec/mips/hevc_lpf_sao_msa.c
@@ -140,19 +140,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -165,19 +165,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -199,11 +199,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             dst_val0 = __msa_copy_u_d((v2i64) dst2, 0);
             dst_val1 = __msa_copy_u_d((v2i64) dst2, 1);
 
-            ST8x4_UB(dst0, dst1, p2, stride);
-            p2 += (4 * stride);
-            SD(dst_val0, p2);
-            p2 += stride;
-            SD(dst_val1, p2);
+            ST_D4(dst0, dst1, 0, 1, 0, 1, p2, stride);
+            SD(dst_val0, p2 + 4 * stride);
+            SD(dst_val1, p2 + 5 * stride);
             /* strong filter ends */
         } else if (flag0 == flag1) { /* weak only */
             /* weak filter */
@@ -220,15 +218,15 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -254,9 +252,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -264,9 +262,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -288,7 +286,7 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             dst1 = __msa_bmz_v(dst1, dst3, (v16u8) cmp3);
 
             p2 += stride;
-            ST8x4_UB(dst0, dst1, p2, stride);
+            ST_D4(dst0, dst1, 0, 1, 0, 1, p2, stride);
             /* weak filter ends */
         } else { /* strong + weak */
             /* strong filter */
@@ -300,19 +298,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -325,19 +323,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -364,15 +362,15 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -396,9 +394,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -406,9 +404,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -442,11 +440,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             dst_val0 = __msa_copy_u_d((v2i64) dst2, 0);
             dst_val1 = __msa_copy_u_d((v2i64) dst2, 1);
 
-            ST8x4_UB(dst0, dst1, p2, stride);
-            p2 += (4 * stride);
-            SD(dst_val0, p2);
-            p2 += stride;
-            SD(dst_val1, p2);
+            ST_D4(dst0, dst1, 0, 1, 0, 1, p2, stride);
+            SD(dst_val0, p2 + 4 * stride);
+            SD(dst_val1, p2 + 5 * stride);
         }
     }
 }
@@ -565,19 +561,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -589,19 +585,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -624,14 +620,14 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            CLIP_SH(delta0, tc_neg, tc_pos);
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -653,9 +649,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -663,9 +659,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -730,19 +726,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -754,19 +750,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -789,15 +785,15 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -819,9 +815,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -829,9 +825,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
             delta1 = (v8i16) __msa_bmz_v((v16u8) delta1, (v16u8) p1_src,
@@ -959,15 +955,15 @@ static void hevc_loopfilter_chroma_hor_msa(uint8_t *src, int32_t stride,
         temp0 <<= 2;
         temp0 += temp1;
         delta = __msa_srari_h((v8i16) temp0, 3);
-        delta = CLIP_SH(delta, tc_neg, tc_pos);
+        CLIP_SH(delta, tc_neg, tc_pos);
 
         temp0 = (v8i16) ((v8i16) p0 + delta);
-        temp0 = CLIP_SH_0_255(temp0);
+        CLIP_SH_0_255(temp0);
         temp0 = (v8i16) __msa_bmz_v((v16u8) temp0, (v16u8) p0,
                                     (v16u8) p_is_pcm_vec);
 
         temp1 = (v8i16) ((v8i16) q0 - delta);
-        temp1 = CLIP_SH_0_255(temp1);
+        CLIP_SH_0_255(temp1);
         temp1 = (v8i16) __msa_bmz_v((v16u8) temp1, (v16u8) q0,
                                     (v16u8) q_is_pcm_vec);
 
@@ -976,7 +972,7 @@ static void hevc_loopfilter_chroma_hor_msa(uint8_t *src, int32_t stride,
         temp1 = (v8i16) __msa_bmnz_v((v16u8) temp1, (v16u8) q0, (v16u8) tc_pos);
 
         temp0 = (v8i16) __msa_pckev_b((v16i8) temp1, (v16i8) temp0);
-        ST8x2_UB(temp0, p0_ptr, stride);
+        ST_D2(temp0, 0, 1, p0_ptr, stride);
     }
 }
 
@@ -1018,15 +1014,15 @@ static void hevc_loopfilter_chroma_ver_msa(uint8_t *src, int32_t stride,
         temp0 <<= 2;
         temp0 += temp1;
         delta = __msa_srari_h((v8i16) temp0, 3);
-        delta = CLIP_SH(delta, tc_neg, tc_pos);
+        CLIP_SH(delta, tc_neg, tc_pos);
 
         temp0 = (v8i16) ((v8i16) p0 + delta);
-        temp0 = CLIP_SH_0_255(temp0);
+        CLIP_SH_0_255(temp0);
         temp0 = (v8i16) __msa_bmz_v((v16u8) temp0, (v16u8) p0,
                                     (v16u8) p_is_pcm_vec);
 
         temp1 = (v8i16) ((v8i16) q0 - delta);
-        temp1 = CLIP_SH_0_255(temp1);
+        CLIP_SH_0_255(temp1);
         temp1 = (v8i16) __msa_bmz_v((v16u8) temp1, (v16u8) q0,
                                     (v16u8) q_is_pcm_vec);
 
@@ -1037,9 +1033,7 @@ static void hevc_loopfilter_chroma_ver_msa(uint8_t *src, int32_t stride,
         temp0 = (v8i16) __msa_ilvev_b((v16i8) temp1, (v16i8) temp0);
 
         src += 1;
-        ST2x4_UB(temp0, 0, src, stride);
-        src += (4 * stride);
-        ST2x4_UB(temp0, 4, src, stride);
+        ST_H8(temp0, 0, 1, 2, 3, 4, 5, 6, 7, src, stride);
     }
 }
 
@@ -1087,7 +1081,7 @@ static void hevc_sao_band_filter_4width_msa(uint8_t *dst, int32_t dst_stride,
         LD_UB4(src, src_stride, src0, src1, src2, src3);
 
         /* store results */
-        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 
@@ -1102,7 +1096,7 @@ static void hevc_sao_band_filter_4width_msa(uint8_t *dst, int32_t dst_stride,
     dst0 = (v16i8) __msa_xori_b((v16u8) dst0, 128);
 
     /* store results */
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_sao_band_filter_8width_msa(uint8_t *dst, int32_t dst_stride,
@@ -1153,7 +1147,7 @@ static void hevc_sao_band_filter_8width_msa(uint8_t *dst, int32_t dst_stride,
         XORI_B2_128_SB(dst0, dst1);
 
         /* store results */
-        ST8x4_UB(dst0, dst1, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
         dst += dst_stride << 2;
     }
 
@@ -1173,7 +1167,7 @@ static void hevc_sao_band_filter_8width_msa(uint8_t *dst, int32_t dst_stride,
     XORI_B2_128_SB(dst0, dst1);
 
     /* store results */
-    ST8x4_UB(dst0, dst1, dst, dst_stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_sao_band_filter_16multiple_msa(uint8_t *dst,
@@ -1363,6 +1357,7 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
     v16u8 cmp_minus10, diff_minus10, diff_minus11;
     v16u8 src0, src1, dst0, src_minus10, src_minus11, src_plus10, src_plus11;
     v16i8 offset, sao_offset = LD_SB(sao_offset_val);
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src -= 1;
@@ -1373,8 +1368,8 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src += (src_stride << 1);
 
-        SLDI_B2_0_UB(src_minus10, src_minus11, src0, src1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_plus10, src_plus11, 2);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 1, src0, src1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_plus10, src_plus11);
 
         PCKEV_D2_UB(src_minus11, src_minus10, src_plus11, src_plus10,
                     src_minus10, src_plus10);
@@ -1410,8 +1405,8 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_UB(src_minus10, src_minus11, src0, src1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_plus10, src_plus11, 2);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 1, src0, src1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_plus10, src_plus11);
 
     PCKEV_D2_UB(src_minus11, src_minus10, src_plus11, src_plus10, src_minus10,
                 src_plus10);
@@ -1479,14 +1474,12 @@ static void hevc_sao_edge_filter_0degree_16multiple_msa(uint8_t *dst,
             dst_ptr = dst + v_cnt;
             LD_UB4(src_minus1, src_stride, src10, src11, src12, src13);
 
-            SLDI_B2_SB(src10, src11, src_minus10, src_minus11, src_zero0,
-                       src_zero1, 1);
-            SLDI_B2_SB(src12, src13, src_minus12, src_minus13, src_zero2,
-                       src_zero3, 1);
-            SLDI_B2_SB(src10, src11, src_minus10, src_minus11, src_plus10,
-                       src_plus11, 2);
-            SLDI_B2_SB(src12, src13, src_minus12, src_minus13, src_plus12,
-                       src_plus13, 2);
+            SLDI_B4_SB(src10, src_minus10, src11, src_minus11,
+                       src12, src_minus12, src13, src_minus13, 1,
+                       src_zero0, src_zero1, src_zero2, src_zero3);
+            SLDI_B4_SB(src10, src_minus10, src11, src_minus11,
+                       src12, src_minus12, src13, src_minus13, 2,
+                       src_plus10, src_plus11, src_plus12, src_plus13);
 
             cmp_minus10 = ((v16u8) src_zero0 == src_minus10);
             cmp_plus10 = ((v16u8) src_zero0 == (v16u8) src_plus10);
@@ -1886,6 +1879,7 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
     v16u8 src_minus11, src10, src11;
     v16i8 src_plus0, src_zero0, src_plus1, src_zero1, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
 
@@ -1898,8 +1892,8 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_SB(src10, src11, src_plus0, src_plus1, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus0, src_plus1);
 
         ILVR_B2_UB(src_plus0, src_minus10, src_plus1, src_minus11, src_minus10,
                    src_minus11);
@@ -1944,8 +1938,8 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_SB(src10, src11, src_plus0, src_plus1, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus0, src_plus1);
 
     ILVR_B2_UB(src_plus0, src_minus10, src_plus1, src_minus11, src_minus10,
                src_minus11);
@@ -1998,6 +1992,7 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
     v16u8 src_minus10, src10, src_minus11, src11;
     v16i8 src_zero0, src_plus10, src_zero1, src_plus11, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2009,8 +2004,8 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_SB(src10, src11, src_plus10, src_plus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus10, src_plus11);
 
         ILVR_B2_UB(src_plus10, src_minus10, src_plus11, src_minus11,
                    src_minus10, src_minus11);
@@ -2054,8 +2049,8 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_SB(src10, src11, src_plus10, src_plus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus10, src_plus11);
     ILVR_B2_UB(src_plus10, src_minus10, src_plus11, src_minus11, src_minus10,
                src_minus11);
     ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
@@ -2136,12 +2131,11 @@ static void hevc_sao_edge_filter_45degree_16multiple_msa(uint8_t *dst,
             src_plus13 = LD_UB(src + 1 + v_cnt + (src_stride << 2));
             src_orig += 16;
 
-            SLDI_B2_SB(src10, src11, src_minus11, src_minus12, src_zero0,
-                       src_zero1, 1);
-            SLDI_B2_SB(src12, src13, src_minus13, src_minus14, src_zero2,
-                       src_zero3, 1);
-            SLDI_B2_SB(src11, src12, src_minus12, src_minus13, src_plus10,
-                       src_plus11, 2);
+            SLDI_B4_SB(src10, src_minus11, src11, src_minus12,
+                       src12, src_minus13, src13, src_minus14, 1,
+                       src_zero0, src_zero1, src_zero2, src_zero3);
+            SLDI_B2_SB(src11, src_minus12, src12, src_minus13, 2, src_plus10,
+                       src_plus11);
 
             src_plus12 = __msa_sldi_b((v16i8) src13, (v16i8) src_minus14, 2);
 
@@ -2234,6 +2228,7 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
     v16u8 cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
     v16u8 src_minus10, src10, src_minus11, src11;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2245,8 +2240,8 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
 
         ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                    src_minus11);
@@ -2292,8 +2287,8 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
 
     ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                src_minus11);
@@ -2348,6 +2343,7 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
     v16u8 src_minus10, src10, src_minus11, src11;
     v16i8 src_zero0, src_zero1, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2359,8 +2355,8 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
         ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                    src_minus11);
         ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
@@ -2404,8 +2400,8 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
     ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                src_minus11);
     ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
diff --git a/libavcodec/mips/hevc_mc_bi_msa.c b/libavcodec/mips/hevc_mc_bi_msa.c
index b555517..c6c8d27 100644
--- a/libavcodec/mips/hevc_mc_bi_msa.c
+++ b/libavcodec/mips/hevc_mc_bi_msa.c
@@ -48,7 +48,7 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
 {                                                                  \
     ADDS_SH2_SH(vec0, in0, vec1, in1, out0, out1);                 \
     SRARI_H2_SH(out0, out1, rnd_val);                              \
-    CLIP_SH2_0_255_MAX_SATU(out0, out1);                           \
+    CLIP_SH2_0_255(out0, out1);                                    \
 }
 
 #define HEVC_BI_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, vec0, vec1, vec2,    \
@@ -83,10 +83,10 @@ static void hevc_bi_copy_4w_msa(uint8_t *src0_ptr,
         dst0 <<= 6;
         dst0 += in0;
         dst0 = __msa_srari_h(dst0, 7);
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+        CLIP_SH_0_255(dst0);
 
         dst0 = (v8i16) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
-        ST4x2_UB(dst0, dst, dst_stride);
+        ST_W2(dst0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         LW4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_SB(tp0, tp1, tp2, tp3, src0);
@@ -97,7 +97,7 @@ static void hevc_bi_copy_4w_msa(uint8_t *src0_ptr,
         SLLI_2V(dst0, dst1, 6);
         HEVC_BI_RND_CLIP2_MAX_SATU(in0, in1, dst0, dst1, 7, dst0, dst1);
         dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
     } else if (0 == height % 8) {
         for (loop_cnt = (height >> 3); loop_cnt--;) {
             LW4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
@@ -120,7 +120,7 @@ static void hevc_bi_copy_4w_msa(uint8_t *src0_ptr,
             HEVC_BI_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, dst0, dst1, dst2,
                                        dst3, 7, dst0, dst1, dst2, dst3);
             PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-            ST4x8_UB(dst0, dst1, dst, dst_stride);
+            ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
             dst += (8 * dst_stride);
         }
     }
@@ -165,9 +165,15 @@ static void hevc_bi_copy_6w_msa(uint8_t *src0_ptr,
                                    7, dst4, dst5, dst6, dst7);
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-        ST6x4_UB(out0, out1, dst, dst_stride);
+        ST_W2(out0, 0, 2, dst, dst_stride);
+        ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
-        ST6x4_UB(out2, out3, dst, dst_stride);
+        ST_W2(out2, 0, 2, dst, dst_stride);
+        ST_H2(out2, 2, 6, dst + 4, dst_stride);
+        ST_W2(out3, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -195,7 +201,7 @@ static void hevc_bi_copy_8w_msa(uint8_t *src0_ptr,
         SLLI_2V(dst0, dst1, 6);
         HEVC_BI_RND_CLIP2_MAX_SATU(in0, in1, dst0, dst1, 7, dst0, dst1);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST8x2_UB(out0, dst, dst_stride);
+        ST_D2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
         INSERT_D2_SB(tp0, tp1, src0);
@@ -207,7 +213,7 @@ static void hevc_bi_copy_8w_msa(uint8_t *src0_ptr,
         HEVC_BI_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
                                    7, dst0, dst1, dst2, dst3);
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     } else if (6 == height) {
         LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
         src0_ptr += 4 * src_stride;
@@ -225,9 +231,8 @@ static void hevc_bi_copy_8w_msa(uint8_t *src0_ptr,
                                    7, dst0, dst1, dst2, dst3);
         HEVC_BI_RND_CLIP2_MAX_SATU(in4, in5, dst4, dst5, 7, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST8x2_UB(out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
     } else if (0 == height % 8) {
         uint32_t loop_cnt;
 
@@ -255,7 +260,7 @@ static void hevc_bi_copy_8w_msa(uint8_t *src0_ptr,
                                        dst7, 7, dst4, dst5, dst6, dst7);
             PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
             PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-            ST8x8_UB(out0, out1, out2, out3, dst, dst_stride);
+            ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
             dst += (8 * dst_stride);
         }
     }
@@ -294,7 +299,8 @@ static void hevc_bi_copy_12w_msa(uint8_t *src0_ptr,
                                    7, dst0, dst1, dst2, dst3);
         HEVC_BI_RND_CLIP2_MAX_SATU(in4, in5, dst4, dst5, 7, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST12x4_UB(out0, out1, out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -378,7 +384,7 @@ static void hevc_bi_copy_24w_msa(uint8_t *src0_ptr,
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
         PCKEV_B3_UB(dst7, dst6, dst9, dst8, dst11, dst10, out3, out4, out5);
         ST_UB4(out0, out1, out3, out4, dst, dst_stride);
-        ST8x4_UB(out2, out5, dst + 16, dst_stride);
+        ST_D4(out2, out5, 0, 1, 0, 1, dst + 16, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -588,7 +594,7 @@ static void hevc_hz_bi_8t_4w_msa(uint8_t *src0_ptr,
                           dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST4x8_UB(dst0, dst1, dst, dst_stride);
+        ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -656,7 +662,7 @@ static void hevc_hz_bi_8t_8w_msa(uint8_t *src0_ptr,
                           dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -733,7 +739,7 @@ static void hevc_hz_bi_8t_12w_msa(uint8_t *src0_ptr,
         HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
         dst2 = __msa_adds_s_h(in2, dst2);
         dst2 = __msa_srari_h(dst2, 7);
-        dst2 = CLIP_SH_0_255(dst2);
+        CLIP_SH_0_255(dst2);
         PCKEV_B2_SH(dst1, dst0, dst2, dst2, dst0, dst1);
 
         tmp2 = __msa_copy_s_d((v2i64) dst0, 0);
@@ -882,7 +888,7 @@ static void hevc_hz_bi_8t_24w_msa(uint8_t *src0_ptr,
         HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
         dst2 = __msa_adds_s_h(dst2, in2);
         dst2 = __msa_srari_h(dst2, 7);
-        dst2 = CLIP_SH_0_255(dst2);
+        CLIP_SH_0_255(dst2);
 
         PCKEV_B2_SB(dst1, dst0, dst2, dst2, tmp0, tmp1);
         dst_val0 = __msa_copy_u_d((v2i64) tmp1, 0);
@@ -1242,7 +1248,7 @@ static void hevc_vt_bi_8t_4w_msa(uint8_t *src0_ptr,
                           dst10, dst32, dst54, dst76);
 
         PCKEV_B2_SH(dst32, dst10, dst76, dst54, dst10, dst54);
-        ST4x8_UB(dst10, dst54, dst, dst_stride);
+        ST_W8(dst10, dst54, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2110 = src10998;
@@ -1316,7 +1322,7 @@ static void hevc_vt_bi_8t_8w_msa(uint8_t *src0_ptr,
                           dst0_r, dst1_r, dst2_r, dst3_r);
 
         PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
-        ST8x4_UB(dst0_r, dst1_r, dst, dst_stride);
+        ST_D4(dst0_r, dst1_r, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1420,7 +1426,8 @@ static void hevc_vt_bi_8t_12w_msa(uint8_t *src0_ptr,
 
         PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         dst0_l = (v8i16) __msa_pckev_b((v16i8) dst1_l, (v16i8) dst0_l);
-        ST12x4_UB(dst0_r, dst1_r, dst0_l, dst, dst_stride);
+        ST_D4(dst0_r, dst1_r, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(dst0_l, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1719,9 +1726,9 @@ static void hevc_hv_bi_8t_4w_msa(uint8_t *src0_ptr,
         ADDS_SH2_SH(out0, in0, out1, in1, out0, out1);
         ADDS_SH2_SH(out0, const_vec, out1, const_vec, out0, out1);
         SRARI_H2_SH(out0, out1, 7);
-        CLIP_SH2_0_255_MAX_SATU(out0, out1);
+        CLIP_SH2_0_255(out0, out1);
         out = (v16u8) __msa_pckev_b((v16i8) out1, (v16i8) out0);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10 = dst54;
@@ -1847,9 +1854,9 @@ static void hevc_hv_bi_8t_8multx1mult_msa(uint8_t *src0_ptr,
             tmp = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
             ADDS_SH2_SH(tmp, in0, tmp, const_vec, tmp, tmp);
             tmp = __msa_srari_h(tmp, 7);
-            tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+            CLIP_SH_0_255(tmp);
             out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
-            ST8x1_UB(out, dst_tmp);
+            ST_D1(out, 0, dst_tmp);
             dst_tmp += dst_stride;
 
             dst0 = dst1;
@@ -1993,9 +2000,9 @@ static void hevc_hv_bi_8t_12w_msa(uint8_t *src0_ptr,
         tmp = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
         ADDS_SH2_SH(tmp, in0, tmp, const_vec, tmp, tmp);
         tmp = __msa_srari_h(tmp, 7);
-        tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+        CLIP_SH_0_255(tmp);
         out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
-        ST8x1_UB(out, dst_tmp);
+        ST_D1(out, 0, dst_tmp);
         dst_tmp += dst_stride;
 
         dst0 = dst1;
@@ -2081,9 +2088,9 @@ static void hevc_hv_bi_8t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH2_SH(out0, in0, out1, in1, out0, out1);
         ADDS_SH2_SH(out0, const_vec, out1, const_vec, out0, out1);
         SRARI_H2_SH(out0, out1, 7);
-        CLIP_SH2_0_255_MAX_SATU(out0, out1);
+        CLIP_SH2_0_255(out0, out1);
         out = (v16u8) __msa_pckev_b((v16i8) out1, (v16i8) out0);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10 = dst54;
@@ -2208,10 +2215,10 @@ static void hevc_hz_bi_4t_4x2_msa(uint8_t *src0_ptr,
 
     tmp0 = __msa_adds_s_h(tmp0, in0);
     tmp0 = __msa_srari_h(tmp0, 7);
-    tmp0 = CLIP_SH_0_255(tmp0);
+    CLIP_SH_0_255(tmp0);
     dst0 = __msa_pckev_b((v16i8) tmp0, (v16i8) tmp0);
 
-    ST4x2_UB(dst0, dst, dst_stride);
+    ST_W2(dst0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_bi_4t_4x4_msa(uint8_t *src0_ptr,
@@ -2257,7 +2264,7 @@ static void hevc_hz_bi_4t_4x4_msa(uint8_t *src0_ptr,
     HEVC_BI_RND_CLIP2(in0, in1, tmp0, tmp1, 7, tmp0, tmp1);
     dst0 = __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
 
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hz_bi_4t_4x8multiple_msa(uint8_t *src0_ptr,
@@ -2318,7 +2325,7 @@ static void hevc_hz_bi_4t_4x8multiple_msa(uint8_t *src0_ptr,
                           tmp0, tmp1, tmp2, tmp3, 7, tmp0, tmp1, tmp2, tmp3);
 
         PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, dst0, dst1);
-        ST4x8_UB(dst0, dst1, dst, dst_stride);
+        ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -2398,7 +2405,10 @@ static void hevc_hz_bi_4t_6w_msa(uint8_t *src0_ptr,
                           dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST6x4_UB(dst0, dst1, dst, dst_stride);
+        ST_W2(dst0, 0, 2, dst, dst_stride);
+        ST_H2(dst0, 2, 6, dst + 4, dst_stride);
+        ST_W2(dst1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(dst1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2443,7 +2453,7 @@ static void hevc_hz_bi_4t_8x2_msa(uint8_t *src0_ptr,
     HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
 
     dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST8x2_UB(dst0, dst, dst_stride);
+    ST_D2(dst0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_bi_4t_8x6_msa(uint8_t *src0_ptr,
@@ -2506,9 +2516,8 @@ static void hevc_hz_bi_4t_8x6_msa(uint8_t *src0_ptr,
 
     PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
     dst2 = (v8i16) __msa_pckev_b((v16i8) dst5, (v16i8) dst4);
-    ST8x4_UB(dst0, dst1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(dst2, dst, dst_stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(dst2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hz_bi_4t_8x4multiple_msa(uint8_t *src0_ptr,
@@ -2564,7 +2573,7 @@ static void hevc_hz_bi_4t_8x4multiple_msa(uint8_t *src0_ptr,
                           dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2659,7 +2668,8 @@ static void hevc_hz_bi_4t_12w_msa(uint8_t *src0_ptr,
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
         dst2 = (v8i16) __msa_pckev_b((v16i8) dst5, (v16i8) dst4);
-        ST12x4_UB(dst0, dst1, dst2, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(dst2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2825,7 +2835,7 @@ static void hevc_hz_bi_4t_24w_msa(uint8_t *src0_ptr,
                           dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst_tmp, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
     }
 }
@@ -2933,10 +2943,10 @@ static void hevc_vt_bi_4t_4x2_msa(uint8_t *src0_ptr,
     DPADD_SB2_SH(src2110, src4332, filt0, filt1, dst10, dst10);
     dst10 = __msa_adds_s_h(dst10, in0);
     dst10 = __msa_srari_h(dst10, 7);
-    dst10 = CLIP_SH_0_255(dst10);
+    CLIP_SH_0_255(dst10);
 
     dst10 = (v8i16) __msa_pckev_b((v16i8) dst10, (v16i8) dst10);
-    ST4x2_UB(dst10, dst, dst_stride);
+    ST_W2(dst10, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_bi_4t_4x4_msa(uint8_t *src0_ptr,
@@ -2985,7 +2995,7 @@ static void hevc_vt_bi_4t_4x4_msa(uint8_t *src0_ptr,
     HEVC_BI_RND_CLIP2(in0, in1, dst10, dst32, 7, dst10, dst32);
 
     dst10 = (v8i16) __msa_pckev_b((v16i8) dst32, (v16i8) dst10);
-    ST4x4_UB(dst10, dst10, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(dst10, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_vt_bi_4t_4x8multiple_msa(uint8_t *src0_ptr,
@@ -3056,7 +3066,7 @@ static void hevc_vt_bi_4t_4x8multiple_msa(uint8_t *src0_ptr,
                           dst10, dst32, dst54, dst76);
 
         PCKEV_B2_SH(dst32, dst10, dst76, dst54, dst10, dst54);
-        ST4x8_UB(dst10, dst54, dst, dst_stride);
+        ST_W8(dst10, dst54, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -3147,7 +3157,10 @@ static void hevc_vt_bi_4t_6w_msa(uint8_t *src0_ptr,
                       dst0_r, dst1_r, dst2_r, dst3_r);
 
     PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
-    ST6x4_UB(dst0_r, dst1_r, dst, dst_stride);
+    ST_W2(dst0_r, 0, 2, dst, dst_stride);
+    ST_H2(dst0_r, 2, 6, dst + 4, dst_stride);
+    ST_W2(dst1_r, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(dst1_r, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
 
     LD_SH4(src1_ptr, src2_stride, in0, in1, in2, in3);
@@ -3171,7 +3184,10 @@ static void hevc_vt_bi_4t_6w_msa(uint8_t *src0_ptr,
                       dst0_r, dst1_r, dst2_r, dst3_r);
 
     PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
-    ST6x4_UB(dst0_r, dst1_r, dst, dst_stride);
+    ST_W2(dst0_r, 0, 2, dst, dst_stride);
+    ST_H2(dst0_r, 2, 6, dst + 4, dst_stride);
+    ST_W2(dst1_r, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(dst1_r, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
 }
 
@@ -3216,7 +3232,7 @@ static void hevc_vt_bi_4t_8x2_msa(uint8_t *src0_ptr,
     HEVC_BI_RND_CLIP2(in0, in1, dst0_r, dst1_r, 7, dst0_r, dst1_r);
     dst0_r = (v8i16) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
 
-    ST8x2_UB(dst0_r, dst, dst_stride);
+    ST_D2(dst0_r, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_bi_4t_8x6_msa(uint8_t *src0_ptr,
@@ -3275,9 +3291,8 @@ static void hevc_vt_bi_4t_8x6_msa(uint8_t *src0_ptr,
 
     PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
     dst2_r = (v8i16) __msa_pckev_b((v16i8) dst5_r, (v16i8) dst4_r);
-    ST8x4_UB(dst0_r, dst1_r, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(dst2_r, dst, dst_stride);
+    ST_D4(dst0_r, dst1_r, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(dst2_r, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_vt_bi_4t_8x4multiple_msa(uint8_t *src0_ptr,
@@ -3337,7 +3352,7 @@ static void hevc_vt_bi_4t_8x4multiple_msa(uint8_t *src0_ptr,
                           dst0_r, dst1_r, dst2_r, dst3_r);
 
         PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
-        ST8x4_UB(dst0_r, dst1_r, dst, dst_stride);
+        ST_D4(dst0_r, dst1_r, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -3436,7 +3451,8 @@ static void hevc_vt_bi_4t_12w_msa(uint8_t *src0_ptr,
 
         PCKEV_B2_SH(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         dst0_l = (v8i16) __msa_pckev_b((v16i8) dst1_l, (v16i8) dst0_l);
-        ST12x4_UB(dst0_r, dst1_r, dst0_l, dst, dst_stride);
+        ST_D4(dst0_r, dst1_r, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(dst0_l, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src2 = src6;
@@ -3610,7 +3626,7 @@ static void hevc_vt_bi_4t_24w_msa(uint8_t *src0_ptr,
         PCKEV_B2_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         dst2_r = (v8i16) __msa_pckev_b((v16i8) dst3_r, (v16i8) dst2_r);
         ST_SH2(dst0_r, dst1_r, dst, dst_stride);
-        ST8x2_UB(dst2_r, dst + 16, dst_stride);
+        ST_D2(dst2_r, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
 
         /* 16width */
@@ -3650,7 +3666,7 @@ static void hevc_vt_bi_4t_24w_msa(uint8_t *src0_ptr,
         PCKEV_B2_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
         dst2_r = (v8i16) __msa_pckev_b((v16i8) dst3_r, (v16i8) dst2_r);
         ST_SH2(dst0_r, dst1_r, dst, dst_stride);
-        ST8x2_UB(dst2_r, dst + 16, dst_stride);
+        ST_D2(dst2_r, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -3827,9 +3843,9 @@ static void hevc_hv_bi_4t_4x2_msa(uint8_t *src0_ptr,
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
     tmp = __msa_adds_s_h(tmp, in0);
     tmp = __msa_srari_h(tmp, 7);
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+    CLIP_SH_0_255(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_bi_4t_4x4_msa(uint8_t *src0_ptr,
@@ -3903,9 +3919,9 @@ static void hevc_hv_bi_4t_4x4_msa(uint8_t *src0_ptr,
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
     ADDS_SH2_SH(tmp0, in0, tmp1, in1, tmp0, tmp1);
     SRARI_H2_SH(tmp0, tmp1, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hv_bi_4t_4multx8mult_msa(uint8_t *src0_ptr,
@@ -4016,9 +4032,9 @@ static void hevc_hv_bi_4t_4multx8mult_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1,
                     tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
@@ -4184,9 +4200,9 @@ static void hevc_hv_bi_4t_6w_msa(uint8_t *src0_ptr,
     ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1, tmp2,
                 tmp3);
     SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST4x8_UB(out0, out1, dst, dst_stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 
     LW4(src1_ptr + 4, src2_stride, tpw0, tpw1, tpw2, tpw3);
     src1_ptr += (4 * src2_stride);
@@ -4196,11 +4212,9 @@ static void hevc_hv_bi_4t_6w_msa(uint8_t *src0_ptr,
     ADDS_SH2_SH(in4, const_vec, in5, const_vec, in4, in5);
     ADDS_SH2_SH(in4, tmp4, in5, tmp5, tmp4, tmp5);
     SRARI_H2_SH(tmp4, tmp5, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH2_0_255(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-    ST2x4_UB(out2, 0, dst + 4, dst_stride);
-    dst += 4 * dst_stride;
-    ST2x4_UB(out2, 4, dst + 4, dst_stride);
+    ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
 
 static void hevc_hv_bi_4t_8x2_msa(uint8_t *src0_ptr,
@@ -4272,9 +4286,9 @@ static void hevc_hv_bi_4t_8x2_msa(uint8_t *src0_ptr,
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
     ADDS_SH2_SH(in0, tmp0, in1, tmp1, tmp0, tmp1);
     SRARI_H2_SH(tmp0, tmp1, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_bi_4t_8multx4_msa(uint8_t *src0_ptr,
@@ -4366,9 +4380,9 @@ static void hevc_hv_bi_4t_8multx4_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
     }
 }
@@ -4481,13 +4495,12 @@ static void hevc_hv_bi_4t_8x6_msa(uint8_t *src0_ptr,
     ADDS_SH2_SH(in4, tmp4, in5, tmp5, tmp4, tmp5);
     SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     SRARI_H2_SH(tmp4, tmp5, 7);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH2_0_255(tmp4, tmp5);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hv_bi_4t_8multx4mult_msa(uint8_t *src0_ptr,
@@ -4597,9 +4610,9 @@ static void hevc_hv_bi_4t_8multx4mult_msa(uint8_t *src0_ptr,
             ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                         tmp0, tmp1, tmp2, tmp3);
             SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+            CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-            ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
 
             dst10_r = dst54_r;
@@ -4747,9 +4760,9 @@ static void hevc_hv_bi_4t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -4833,9 +4846,9 @@ static void hevc_hv_bi_4t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
diff --git a/libavcodec/mips/hevc_mc_biw_msa.c b/libavcodec/mips/hevc_mc_biw_msa.c
index ea65f00..f775ea8 100644
--- a/libavcodec/mips/hevc_mc_biw_msa.c
+++ b/libavcodec/mips/hevc_mc_biw_msa.c
@@ -66,7 +66,7 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     out1_l = __msa_dpadd_s_w(offset, (v8i16) out1_l, (v8i16) wgt);   \
     SRAR_W4_SW(out0_r, out1_r, out0_l, out1_l, rnd);                 \
     PCKEV_H2_SH(out0_l, out0_r, out1_l, out1_r, out0, out1);         \
-    CLIP_SH2_0_255_MAX_SATU(out0, out1);                             \
+    CLIP_SH2_0_255(out0, out1);                                      \
 }
 
 #define HEVC_BIW_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, vec0, vec1, vec2,  \
@@ -124,9 +124,9 @@ static void hevc_biwgt_copy_4w_msa(uint8_t *src0_ptr,
         dst0_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_l, weight_vec);
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
         dst0 = (v8i16) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+        CLIP_SH_0_255(dst0);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
-        ST4x2_UB(out0, dst, dst_stride);
+        ST_W2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         LW4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_SB(tp0, tp1, tp2, tp3, src0);
@@ -138,7 +138,7 @@ static void hevc_biwgt_copy_4w_msa(uint8_t *src0_ptr,
         HEVC_BIW_RND_CLIP2_MAX_SATU(dst0, dst1, in0, in1, weight_vec, rnd_vec,
                                     offset_vec, dst0, dst1);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
     } else if (0 == height % 8) {
         for (loop_cnt = (height >> 3); loop_cnt--;) {
             LW4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
@@ -162,7 +162,7 @@ static void hevc_biwgt_copy_4w_msa(uint8_t *src0_ptr,
                                         in3, weight_vec, rnd_vec, offset_vec,
                                         dst0, dst1, dst2, dst3);
             PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-            ST4x8_UB(out0, out1, dst, dst_stride);
+            ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
             dst += (8 * dst_stride);
         }
     }
@@ -214,7 +214,10 @@ static void hevc_biwgt_copy_6w_msa(uint8_t *src0_ptr,
                                     weight_vec, rnd_vec, offset_vec,
                                     dst0, dst1, dst2, dst3);
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST6x4_UB(out0, out1, dst, dst_stride);
+        ST_W2(out0, 0, 2, dst, dst_stride);
+        ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -261,7 +264,7 @@ static void hevc_biwgt_copy_8w_msa(uint8_t *src0_ptr,
                            dst0, dst1);
 
         out0 = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST8x2_UB(out0, dst, dst_stride);
+        ST_D2(out0, 0, 1, dst, dst_stride);
     } else if (6 == height) {
         LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
         src0_ptr += 4 * src_stride;
@@ -281,9 +284,8 @@ static void hevc_biwgt_copy_8w_msa(uint8_t *src0_ptr,
         HEVC_BIW_RND_CLIP2_MAX_SATU(dst4, dst5, in4, in5, weight_vec, rnd_vec,
                                     offset_vec, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST8x2_UB(out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
     } else if (0 == height % 4) {
         uint32_t loop_cnt;
 
@@ -302,7 +304,7 @@ static void hevc_biwgt_copy_8w_msa(uint8_t *src0_ptr,
                                         in3, weight_vec, rnd_vec, offset_vec,
                                         dst0, dst1, dst2, dst3);
             PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-            ST8x4_UB(out0, out1, dst, dst_stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
             dst += (4 * dst_stride);
         }
     }
@@ -361,7 +363,8 @@ static void hevc_biwgt_copy_12w_msa(uint8_t *src0_ptr,
         HEVC_BIW_RND_CLIP2_MAX_SATU(dst4, dst5, in4, in5, weight_vec, rnd_vec,
                                     offset_vec, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST12x4_UB(out0, out1, out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -480,7 +483,7 @@ static void hevc_biwgt_copy_24w_msa(uint8_t *src0_ptr,
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
         PCKEV_B3_UB(dst7, dst6, dst9, dst8, dst11, dst10, out3, out4, out5);
         ST_UB4(out0, out1, out3, out4, dst, dst_stride);
-        ST8x4_UB(out2, out5, dst + 16, dst_stride);
+        ST_D4(out2, out5, 0, 1, 0, 1, dst + 16, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -720,7 +723,7 @@ static void hevc_hz_biwgt_8t_4w_msa(uint8_t *src0_ptr,
                            out0, out1);
 
         out0 = (v8i16) __msa_pckev_b((v16i8) out1, (v16i8) out0);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -800,7 +803,7 @@ static void hevc_hz_biwgt_8t_8w_msa(uint8_t *src0_ptr,
                            out0, out1, out2, out3);
 
         PCKEV_B2_SH(out1, out0, out3, out2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -876,7 +879,7 @@ static void hevc_hz_biwgt_8t_12w_msa(uint8_t *src0_ptr,
                            weight_vec, rnd_vec, offset_vec, out0, out1, out2,
                            out3);
         PCKEV_B2_SH(out1, out0, out3, out2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         LD_SB4(src0_ptr + 8, src_stride, src0, src1, src2, src3);
         src0_ptr += (4 * src_stride);
@@ -895,7 +898,7 @@ static void hevc_hz_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         HEVC_BIW_RND_CLIP2(dst0, dst1, in0, in1, weight_vec, rnd_vec,
                            offset_vec, out0, out1);
         out0 = (v8i16) __msa_pckev_b((v16i8) out1, (v16i8) out0);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_W4(out0, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -1066,8 +1069,8 @@ static void hevc_hz_biwgt_8t_24w_msa(uint8_t *src0_ptr,
         dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l,
                                  (v8i16) weight_vec);
         SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-        dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-        out2 = CLIP_SH_0_255(dst2_r);
+        out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
+        CLIP_SH_0_255(out2);
 
         LD_SB2(src0_ptr, 16, src0, src1);
         src0_ptr += src_stride;
@@ -1097,8 +1100,8 @@ static void hevc_hz_biwgt_8t_24w_msa(uint8_t *src0_ptr,
     dst2_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_r, (v8i16) weight_vec);
     dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-    dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-    out2 = CLIP_SH_0_255(dst2_r);
+    out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
+    CLIP_SH_0_255(out2);
     PCKEV_B2_SH(out1, out0, out2, out2, out0, out2);
     dst_val0 = __msa_copy_u_d((v2i64) out2, 0);
     ST_SH(out0, dst);
@@ -1483,7 +1486,7 @@ static void hevc_vt_biwgt_8t_4w_msa(uint8_t *src0_ptr,
                            out0, out1, out2, out3);
 
         PCKEV_B2_SH(out1, out0, out3, out2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2110 = src10998;
@@ -1568,7 +1571,7 @@ static void hevc_vt_biwgt_8t_8w_msa(uint8_t *src0_ptr,
                            out0, out1, out2, out3);
 
         PCKEV_B2_SH(out1, out0, out3, out2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1671,11 +1674,11 @@ static void hevc_vt_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l,
                                  (v8i16) weight_vec);
         SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-        dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-        out2 = CLIP_SH_0_255(dst2_r);
+        out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
+        CLIP_SH_0_255(out2);
         PCKEV_B2_SH(out1, out0, out2, out2, out0, out2);
-        ST8x2_UB(out0, dst, dst_stride);
-        ST4x2_UB(out2, dst + 8, dst_stride);
+        ST_D2(out0, 0, 1, dst, dst_stride);
+        ST_W2(out2, 0, 1, dst + 8, dst_stride);
         dst += (2 * dst_stride);
 
         src10_r = src32_r;
@@ -2045,10 +2048,10 @@ static void hevc_hv_biwgt_8t_4w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10 = dst54;
@@ -2223,10 +2226,10 @@ static void hevc_hv_biwgt_8t_8multx2mult_msa(uint8_t *src0_ptr,
             dst1_r = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
             dst1_l = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
             SRAR_W4_SW(dst0_l, dst0_r, dst1_l, dst1_r, rnd_vec);
-            CLIP_SW4_0_255_MAX_SATU(dst0_l, dst0_r, dst1_l, dst1_r);
+            CLIP_SW4_0_255(dst0_l, dst0_r, dst1_l, dst1_r);
             PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
             out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-            ST8x2_UB(out, dst_tmp, dst_stride);
+            ST_D2(out, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (2 * dst_stride);
 
             dst0 = dst2;
@@ -2409,10 +2412,10 @@ static void hevc_hv_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst1, dst0, dst3, dst2, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst1, dst0, dst3, dst2);
+        CLIP_SW4_0_255(dst1, dst0, dst3, dst2);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-        ST8x2_UB(out, dst_tmp, dst_stride);
+        ST_D2(out, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (2 * dst_stride);
 
         dsth0 = dsth2;
@@ -2500,10 +2503,10 @@ static void hevc_hv_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10 = dst54;
@@ -2680,10 +2683,10 @@ static void hevc_hz_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst0_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_r, (v8i16) weight_vec);
     dst0_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
-    dst0_r = (v4i32) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-    out0 = CLIP_SH_0_255(dst0_r);
+    out0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
+    CLIP_SH_0_255(out0);
     out0 = (v8i16) __msa_pckev_b((v16i8) out0, (v16i8) out0);
-    ST4x2_UB(out0, dst, dst_stride);
+    ST_W2(out0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
@@ -2743,7 +2746,7 @@ static void hevc_hz_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
                        dst0, dst1);
 
     dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hz_biwgt_4t_4x8multiple_msa(uint8_t *src0_ptr,
@@ -2816,7 +2819,7 @@ static void hevc_hz_biwgt_4t_4x8multiple_msa(uint8_t *src0_ptr,
                            dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST4x8_UB(dst0, dst1, dst, dst_stride);
+        ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -2918,7 +2921,10 @@ static void hevc_hz_biwgt_4t_6w_msa(uint8_t *src0_ptr,
                            dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST6x4_UB(dst0, dst1, dst, dst_stride);
+        ST_W2(dst0, 0, 2, dst, dst_stride);
+        ST_H2(dst0, 2, 6, dst + 4, dst_stride);
+        ST_W2(dst1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(dst1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2976,7 +2982,7 @@ static void hevc_hz_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
                        dst0, dst1);
 
     dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST8x2_UB(dst0, dst, dst_stride);
+    ST_D2(dst0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
@@ -3049,9 +3055,8 @@ static void hevc_hz_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
 
     PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
     dst3 = (v8i16) __msa_pckev_b((v16i8) dst5, (v16i8) dst4);
-    ST8x4_UB(dst0, dst1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(dst3, dst, dst_stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(dst3, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hz_biwgt_4t_8x4multiple_msa(uint8_t *src0_ptr,
@@ -3119,7 +3124,7 @@ static void hevc_hz_biwgt_4t_8x4multiple_msa(uint8_t *src0_ptr,
                            dst0, dst1, dst2, dst3);
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -3235,7 +3240,8 @@ static void hevc_hz_biwgt_4t_12w_msa(uint8_t *src0_ptr,
 
         PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
         dst3 = (v8i16) __msa_pckev_b((v16i8) dst5, (v16i8) dst4);
-        ST12x4_UB(dst0, dst1, dst3, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(dst3, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -3411,7 +3417,7 @@ static void hevc_hz_biwgt_4t_24w_msa(uint8_t *src0_ptr,
                            dst0, dst1);
 
         dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST8x2_UB(dst0, (dst + 16), dst_stride);
+        ST_D2(dst0, 0, 1, (dst + 16), dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -3548,10 +3554,10 @@ static void hevc_vt_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst10_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst10_r, (v8i16) weight_vec);
     dst10_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst10_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst10_r, dst10_l, rnd_vec);
-    dst10_r = (v4i32) __msa_pckev_h((v8i16) dst10_l, (v8i16) dst10_r);
-    out = CLIP_SH_0_255(dst10_r);
+    out = __msa_pckev_h((v8i16) dst10_l, (v8i16) dst10_r);
+    CLIP_SH_0_255(out);
     out = (v8i16) __msa_pckev_b((v16i8) out, (v16i8) out);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
@@ -3617,7 +3623,7 @@ static void hevc_vt_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
                        dst10, dst32);
 
     dst10 = (v8i16) __msa_pckev_b((v16i8) dst32, (v16i8) dst10);
-    ST4x4_UB(dst10, dst10, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(dst10, 0, 1, 2, 3, dst, dst_stride);
     dst += (4 * dst_stride);
 }
 
@@ -3702,7 +3708,7 @@ static void hevc_vt_biwgt_4t_4x8multiple_msa(uint8_t *src0_ptr,
                            dst10, dst32, dst54, dst76);
 
         PCKEV_B2_SH(dst32, dst10, dst76, dst54, dst10, dst32);
-        ST4x8_UB(dst10, dst32, dst, dst_stride);
+        ST_W8(dst10, dst32, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -3807,7 +3813,10 @@ static void hevc_vt_biwgt_4t_6w_msa(uint8_t *src0_ptr,
                            tmp0, tmp1, tmp2, tmp3);
 
         PCKEV_B2_SH(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-        ST6x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_W2(tmp0, 0, 2, dst, dst_stride);
+        ST_H2(tmp0, 2, 6, dst + 4, dst_stride);
+        ST_W2(tmp1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(tmp1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -3866,7 +3875,7 @@ static void hevc_vt_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
                        tmp0, tmp1);
 
     tmp0 = (v8i16) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST8x2_UB(tmp0, dst, dst_stride);
+    ST_D2(tmp0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
@@ -3936,9 +3945,8 @@ static void hevc_vt_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
 
     PCKEV_B2_SH(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
     tmp3 = (v8i16) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-    ST8x4_UB(tmp0, tmp1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(tmp3, dst, dst_stride);
+    ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(tmp3, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_vt_biwgt_4t_8x4multiple_msa(uint8_t *src0_ptr,
@@ -4010,7 +4018,7 @@ static void hevc_vt_biwgt_4t_8x4multiple_msa(uint8_t *src0_ptr,
                            tmp0, tmp1, tmp2, tmp3);
 
         PCKEV_B2_SH(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -4132,7 +4140,8 @@ static void hevc_vt_biwgt_4t_12w_msa(uint8_t *src0_ptr,
 
         PCKEV_B2_SH(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
         tmp2 = (v8i16) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-        ST12x4_UB(tmp0, tmp1, tmp2, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(tmp2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -4323,7 +4332,7 @@ static void hevc_vt_biwgt_4t_24w_msa(uint8_t *src0_ptr,
         /* 8width */
         tmp2 = (v8i16) __msa_pckev_b((v16i8) tmp3, (v16i8) tmp2);
         ST_SH2(tmp0, tmp1, dst, dst_stride);
-        ST8x2_UB(tmp2, dst + 16, dst_stride);
+        ST_D2(tmp2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
 
         /* 16width */
@@ -4363,7 +4372,7 @@ static void hevc_vt_biwgt_4t_24w_msa(uint8_t *src0_ptr,
         /* 8width */
         tmp2 = (v8i16) __msa_pckev_b((v16i8) tmp3, (v16i8) tmp2);
         ST_SH2(tmp0, tmp1, dst, dst_stride);
-        ST8x2_UB(tmp2, dst + 16, dst_stride);
+        ST_D2(tmp2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -4566,9 +4575,9 @@ static void hevc_hv_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst1 = __msa_dpadd_s_w(offset_vec, tmp1, weight_vec);
     SRAR_W2_SW(dst0, dst1, rnd_vec);
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+    CLIP_SH_0_255(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
@@ -4663,9 +4672,9 @@ static void hevc_hv_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
     dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hv_biwgt_4t_4multx8mult_msa(uint8_t *src0_ptr,
@@ -4801,9 +4810,9 @@ static void hevc_hv_biwgt_4t_4multx8mult_msa(uint8_t *src0_ptr,
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                     tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
@@ -4999,9 +5008,9 @@ static void hevc_hv_biwgt_4t_6w_msa(uint8_t *src0_ptr,
     SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
     PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                 tmp2, tmp3);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST4x8_UB(out0, out1, dst, dst_stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 
     PCKEV_H2_SW(dst1_l, dst0_l, dst3_l, dst2_l, dst4, dst5);
 
@@ -5021,11 +5030,9 @@ static void hevc_hv_biwgt_4t_6w_msa(uint8_t *src0_ptr,
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp4, tmp5);
 
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH2_0_255(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-    ST2x4_UB(out2, 0, dst + 4, dst_stride);
-    dst += 4 * dst_stride;
-    ST2x4_UB(out2, 4, dst + 4, dst_stride);
+    ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
 
 static void hevc_hv_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
@@ -5119,9 +5126,9 @@ static void hevc_hv_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
     dst1_l = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_biwgt_4t_8multx4_msa(uint8_t *src0_ptr,
@@ -5241,9 +5248,9 @@ static void hevc_hv_biwgt_4t_8multx4_msa(uint8_t *src0_ptr,
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
     }
 }
@@ -5380,7 +5387,7 @@ static void hevc_hv_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
     SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
     PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                 tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
 
     PCKEV_H2_SW(dst4_l, dst4_r, dst5_l, dst5_r, dst0, dst1);
@@ -5392,11 +5399,10 @@ static void hevc_hv_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
     dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp4, tmp5);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH2_0_255(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hv_biwgt_4t_8multx4mult_msa(uint8_t *src0_ptr,
@@ -5531,9 +5537,9 @@ static void hevc_hv_biwgt_4t_8multx4mult_msa(uint8_t *src0_ptr,
             SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
             PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                         tmp0, tmp1, tmp2, tmp3);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+            CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-            ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
 
             dst10_r = dst54_r;
@@ -5718,9 +5724,9 @@ static void hevc_hv_biwgt_4t_12w_msa(uint8_t *src0_ptr,
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -5814,9 +5820,9 @@ static void hevc_hv_biwgt_4t_12w_msa(uint8_t *src0_ptr,
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
diff --git a/libavcodec/mips/hevc_mc_uni_msa.c b/libavcodec/mips/hevc_mc_uni_msa.c
index 740c970..36e6552 100644
--- a/libavcodec/mips/hevc_mc_uni_msa.c
+++ b/libavcodec/mips/hevc_mc_uni_msa.c
@@ -309,7 +309,7 @@ static void common_hz_8t_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(out0, out1, 6);
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_8t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -344,10 +344,9 @@ static void common_hz_8t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_8t_4x16_msa(uint8_t *src, int32_t src_stride,
@@ -382,11 +381,10 @@ static void common_hz_8t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
+    dst += (8 * dst_stride);
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
     XORI_B4_128_SB(src0, src1, src2, src3);
@@ -402,10 +400,9 @@ static void common_hz_8t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_8t_4w_msa(uint8_t *src, int32_t src_stride,
@@ -468,7 +465,7 @@ static void common_hz_8t_8w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -546,8 +543,8 @@ static void common_hz_8t_12w_msa(uint8_t *src, int32_t src_stride,
         tmp1 = PCKEV_XORI128_UB(out2, out3);
         tmp2 = PCKEV_XORI128_UB(out4, out5);
 
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
-        ST4x4_UB(tmp2, tmp2, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(tmp2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -670,7 +667,7 @@ static void common_hz_8t_24w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0, out8, out2, out9, 7);
         SAT_SH2_SH(out1, out3, 7);
         out = PCKEV_XORI128_UB(out8, out9);
-        ST8x2_UB(out, dst + 16, dst_stride);
+        ST_D2(out, 0, 1, dst + 16, dst_stride);
         out = PCKEV_XORI128_UB(out0, out1);
         ST_UB(out, dst);
         dst += dst_stride;
@@ -965,10 +962,8 @@ static void common_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH2_SH(out54, out76, 7);
         out0 = PCKEV_XORI128_UB(out10, out32);
         out1 = PCKEV_XORI128_UB(out54, out76);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST4x4_UB(out1, out1, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+        dst += (8 * dst_stride);
 
         src2110 = src10998;
         src4332 = src12111110;
@@ -1019,7 +1014,7 @@ static void common_vt_8t_8w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
         tmp0 = PCKEV_XORI128_UB(out0_r, out1_r);
         tmp1 = PCKEV_XORI128_UB(out2_r, out3_r);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1458,10 +1453,8 @@ static void hevc_hv_uni_8t_4w_msa(uint8_t *src,
         PCKEV_H2_SW(dst5_r, dst4_r, dst7_r, dst6_r, dst4_r, dst5_r);
         out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
         out1 = PCKEV_XORI128_UB(dst4_r, dst5_r);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST4x4_UB(out1, out1, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+        dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
         dst32_r = dst1110_r;
@@ -1595,7 +1588,7 @@ static void hevc_hv_uni_8t_8multx2mult_msa(uint8_t *src,
 
             PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst0, dst1);
             out = PCKEV_XORI128_UB(dst0, dst1);
-            ST8x2_UB(out, dst_tmp, dst_stride);
+            ST_D2(out, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (2 * dst_stride);
 
             dst0 = dst2;
@@ -1741,7 +1734,7 @@ static void hevc_hv_uni_8t_12w_msa(uint8_t *src,
 
         PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst0, dst1);
         out0 = PCKEV_XORI128_UB(dst0, dst1);
-        ST8x2_UB(out0, dst_tmp, dst_stride);
+        ST_D2(out0, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (2 * dst_stride);
 
         dst0 = dst2;
@@ -1845,10 +1838,8 @@ static void hevc_hv_uni_8t_12w_msa(uint8_t *src,
         PCKEV_H2_SW(dst5_r, dst4_r, dst7_r, dst6_r, dst4_r, dst5_r);
         out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
         out1 = PCKEV_XORI128_UB(dst4_r, dst5_r);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST4x4_UB(out1, out1, 0, 1, 2, 3, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+        dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
         dst32_r = dst1110_r;
@@ -1944,7 +1935,7 @@ static void common_hz_4t_4x2_msa(uint8_t *src, int32_t src_stride,
     res0 = __msa_srari_h(res0, 6);
     res0 = __msa_sat_s_h(res0, 7);
     out = PCKEV_XORI128_UB(res0, res0);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void common_hz_4t_4x4_msa(uint8_t *src, int32_t src_stride,
@@ -1971,7 +1962,7 @@ static void common_hz_4t_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(out0, out1, 6);
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_4t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -2004,10 +1995,9 @@ static void common_hz_4t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
@@ -2038,11 +2028,10 @@ static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
+    dst += (8 * dst_stride);
 
     LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
     src += (8 * src_stride);
@@ -2054,10 +2043,9 @@ static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 6);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_4t_4w_msa(uint8_t *src, int32_t src_stride,
@@ -2102,7 +2090,10 @@ static void common_hz_4t_6w_msa(uint8_t *src, int32_t src_stride,
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out4 = PCKEV_XORI128_UB(out0, out1);
     out5 = PCKEV_XORI128_UB(out2, out3);
-    ST6x4_UB(out4, out5, dst, dst_stride);
+    ST_W2(out4, 0, 2, dst, dst_stride);
+    ST_H2(out4, 2, 6, dst + 4, dst_stride);
+    ST_W2(out5, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out5, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
@@ -2115,8 +2106,10 @@ static void common_hz_4t_6w_msa(uint8_t *src, int32_t src_stride,
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out4 = PCKEV_XORI128_UB(out0, out1);
     out5 = PCKEV_XORI128_UB(out2, out3);
-    ST6x4_UB(out4, out5, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W2(out4, 0, 2, dst, dst_stride);
+    ST_H2(out4, 2, 6, dst + 4, dst_stride);
+    ST_W2(out5, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out5, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
 }
 
 static void common_hz_4t_8x2mult_msa(uint8_t *src, int32_t src_stride,
@@ -2148,7 +2141,7 @@ static void common_hz_4t_8x2mult_msa(uint8_t *src, int32_t src_stride,
         SRARI_H2_SH(vec0, vec1, 6);
         SAT_SH2_SH(vec0, vec1, 7);
         out = PCKEV_XORI128_UB(vec0, vec1);
-        ST8x2_UB(out, dst, dst_stride);
+        ST_D2(out, 0, 1, dst, dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -2182,7 +2175,7 @@ static void common_hz_4t_8x4mult_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2235,7 +2228,7 @@ static void common_hz_4t_12w_msa(uint8_t *src, int32_t src_stride,
         SRARI_H2_SH(out0, out1, 6);
         SAT_SH2_SH(out0, out1, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
-        ST4x4_UB(tmp0, tmp0, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_W4(tmp0, 0, 1, 2, 3, dst + 8, dst_stride);
 
         VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec4, vec5);
         VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec6, vec7);
@@ -2249,7 +2242,7 @@ static void common_hz_4t_12w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out2, out3, out4, out5, 7);
         tmp0 = PCKEV_XORI128_UB(out2, out3);
         tmp1 = PCKEV_XORI128_UB(out4, out5);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2395,7 +2388,7 @@ static void common_hz_4t_24w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst1, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst1, dst_stride);
         dst1 += (4 * dst_stride);
     }
 }
@@ -2496,7 +2489,7 @@ static void common_vt_4t_4x2_msa(uint8_t *src, int32_t src_stride,
     out10 = __msa_srari_h(out10, 6);
     out10 = __msa_sat_s_h(out10, 7);
     out = PCKEV_XORI128_UB(out10, out10);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void common_vt_4t_4x4multiple_msa(uint8_t *src, int32_t src_stride,
@@ -2540,7 +2533,7 @@ static void common_vt_4t_4x4multiple_msa(uint8_t *src, int32_t src_stride,
         SRARI_H2_SH(out10, out32, 6);
         SAT_SH2_SH(out10, out32, 7);
         out = PCKEV_XORI128_UB(out10, out32);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2596,7 +2589,10 @@ static void common_vt_4t_6w_msa(uint8_t *src, int32_t src_stride,
     SAT_SH4_SH(dst0_r, dst1_r, dst2_r, dst3_r, 7);
     out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
     out1 = PCKEV_XORI128_UB(dst2_r, dst3_r);
-    ST6x4_UB(out0, out1, dst, dst_stride);
+    ST_W2(out0, 0, 2, dst, dst_stride);
+    ST_H2(out0, 2, 6, dst + 4, dst_stride);
+    ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
 
     LD_SB2(src, src_stride, src3, src4);
@@ -2619,7 +2615,10 @@ static void common_vt_4t_6w_msa(uint8_t *src, int32_t src_stride,
     SAT_SH4_SH(dst0_r, dst1_r, dst2_r, dst3_r, 7);
     out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
     out1 = PCKEV_XORI128_UB(dst2_r, dst3_r);
-    ST6x4_UB(out0, out1, dst, dst_stride);
+    ST_W2(out0, 0, 2, dst, dst_stride);
+    ST_H2(out0, 2, 6, dst + 4, dst_stride);
+    ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
 }
 
 static void common_vt_4t_8x2_msa(uint8_t *src, int32_t src_stride,
@@ -2645,7 +2644,7 @@ static void common_vt_4t_8x2_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(tmp0, tmp1, 6);
     SAT_SH2_SH(tmp0, tmp1, 7);
     out = PCKEV_XORI128_UB(tmp0, tmp1);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void common_vt_4t_8x6_msa(uint8_t *src, int32_t src_stride,
@@ -2737,7 +2736,7 @@ static void common_vt_4t_8x4mult_msa(uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
         tmp0 = PCKEV_XORI128_UB(out0_r, out1_r);
         tmp1 = PCKEV_XORI128_UB(out2_r, out3_r);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src98_r;
@@ -2811,9 +2810,9 @@ static void common_vt_4t_12w_msa(uint8_t *src, int32_t src_stride,
         SAT_SH2_SH(dst0_l, dst1_l, 7);
         out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
         out1 = PCKEV_XORI128_UB(dst2_r, dst3_r);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         out0 = PCKEV_XORI128_UB(dst0_l, dst1_l);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_W4(out0, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src2 = src6;
@@ -2982,12 +2981,12 @@ static void common_vt_4t_24w_msa(uint8_t *src, int32_t src_stride,
         out = PCKEV_XORI128_UB(out0_r, out0_l);
         ST_UB(out, dst);
         out = PCKEV_XORI128_UB(out2_r, out2_r);
-        ST8x1_UB(out, dst + 16);
+        ST_D1(out, 0, dst + 16);
         dst += dst_stride;
         out = PCKEV_XORI128_UB(out1_r, out1_l);
         ST_UB(out, dst);
         out = PCKEV_XORI128_UB(out3_r, out3_r);
-        ST8x1_UB(out, dst + 16);
+        ST_D1(out, 0, dst + 16);
         dst += dst_stride;
     }
 }
@@ -3137,7 +3136,7 @@ static void hevc_hv_uni_4t_4x2_msa(uint8_t *src,
     tmp = __msa_srari_h(tmp, 6);
     tmp = __msa_sat_s_h(tmp, 7);
     out = PCKEV_XORI128_UB(tmp, tmp);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_uni_4t_4x4_msa(uint8_t *src,
@@ -3196,7 +3195,7 @@ static void hevc_hv_uni_4t_4x4_msa(uint8_t *src,
     SRARI_H2_SH(tmp0, tmp1, 6);
     SAT_SH2_SH(tmp0, tmp1, 7);
     out = PCKEV_XORI128_UB(tmp0, tmp1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hv_uni_4t_4multx8mult_msa(uint8_t *src,
@@ -3288,7 +3287,7 @@ static void hevc_hv_uni_4t_4multx8mult_msa(uint8_t *src,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
@@ -3432,10 +3431,8 @@ static void hevc_hv_uni_4t_6w_msa(uint8_t *src,
     out0 = PCKEV_XORI128_UB(tmp0, tmp1);
     out1 = PCKEV_XORI128_UB(tmp2, tmp3);
     out2 = PCKEV_XORI128_UB(tmp4, tmp5);
-    ST4x8_UB(out0, out1, dst, dst_stride);
-    ST2x4_UB(out2, 0, dst + 4, dst_stride);
-    dst += 4 * dst_stride;
-    ST2x4_UB(out2, 4, dst + 4, dst_stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+    ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
 
 static void hevc_hv_uni_4t_8x2_msa(uint8_t *src,
@@ -3497,7 +3494,7 @@ static void hevc_hv_uni_4t_8x2_msa(uint8_t *src,
     SRARI_H2_SH(out0_r, out1_r, 6);
     SAT_SH2_SH(out0_r, out1_r, 7);
     out = PCKEV_XORI128_UB(out0_r, out1_r);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_uni_4t_8multx4_msa(uint8_t *src,
@@ -3580,7 +3577,7 @@ static void hevc_hv_uni_4t_8multx4_msa(uint8_t *src,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
     }
 }
@@ -3684,9 +3681,8 @@ static void hevc_hv_uni_4t_8x6_msa(uint8_t *src,
     out1 = PCKEV_XORI128_UB(out2_r, out3_r);
     out2 = PCKEV_XORI128_UB(out4_r, out5_r);
 
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hv_uni_4t_8multx4mult_msa(uint8_t *src,
@@ -3788,7 +3784,7 @@ static void hevc_hv_uni_4t_8multx4mult_msa(uint8_t *src,
             SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
             out0 = PCKEV_XORI128_UB(out0_r, out1_r);
             out1 = PCKEV_XORI128_UB(out2_r, out3_r);
-            ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
 
             dst10_r = dst54_r;
@@ -3919,7 +3915,7 @@ static void hevc_hv_uni_4t_12w_msa(uint8_t *src,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -3985,7 +3981,7 @@ static void hevc_hv_uni_4t_12w_msa(uint8_t *src,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
diff --git a/libavcodec/mips/hevc_mc_uniw_msa.c b/libavcodec/mips/hevc_mc_uniw_msa.c
index f9ecb41..1a8c251 100644
--- a/libavcodec/mips/hevc_mc_uniw_msa.c
+++ b/libavcodec/mips/hevc_mc_uniw_msa.c
@@ -41,7 +41,7 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     SRAR_W4_SW(in0_r_m, in1_r_m, in0_l_m, in1_l_m, rnd_w);                    \
     PCKEV_H2_SH(in0_l_m, in0_r_m, in1_l_m, in1_r_m, out0_h, out1_h);          \
     ADDS_SH2_SH(out0_h, offset_h, out1_h, offset_h, out0_h, out1_h);          \
-    CLIP_SH2_0_255_MAX_SATU(out0_h, out1_h);                                  \
+    CLIP_SH2_0_255(out0_h, out1_h);                                           \
 }
 
 #define HEVC_UNIW_RND_CLIP4_MAX_SATU_H(in0_h, in1_h, in2_h, in3_h, wgt_w,  \
@@ -88,9 +88,9 @@ static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
         dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
         dst0 += offset_vec;
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+        CLIP_SH_0_255(dst0);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
-        ST4x2_UB(out0, dst, dst_stride);
+        ST_W2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         LW4(src, src_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_SB(tp0, tp1, tp2, tp3, src0);
@@ -99,7 +99,7 @@ static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
         HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec,
                                        rnd_vec, dst0, dst1);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST4x4_UB(out0, out0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
     } else if (0 == (height % 8)) {
         for (loop_cnt = (height >> 3); loop_cnt--;) {
             LW4(src, src_stride, tp0, tp1, tp2, tp3);
@@ -115,7 +115,7 @@ static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
                                            offset_vec, rnd_vec, dst0, dst1,
                                            dst2, dst3);
             PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-            ST4x8_UB(out0, out1, dst, dst_stride);
+            ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
             dst += 8 * dst_stride;
         }
     }
@@ -170,9 +170,15 @@ static void hevc_uniwgt_copy_6w_msa(uint8_t *src,
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
 
-        ST6x4_UB(out0, out1, dst, dst_stride);
+        ST_W2(out0, 0, 2, dst, dst_stride);
+        ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
-        ST6x4_UB(out2, out3, dst, dst_stride);
+        ST_W2(out2, 0, 2, dst, dst_stride);
+        ST_H2(out2, 2, 6, dst + 4, dst_stride);
+        ST_W2(out3, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -207,7 +213,7 @@ static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
         HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec,
                                        rnd_vec, dst0, dst1);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-        ST8x2_UB(out0, dst, dst_stride);
+        ST_D2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         LD4(src, src_stride, tp0, tp1, tp2, tp3);
         INSERT_D2_SB(tp0, tp1, src0);
@@ -219,7 +225,7 @@ static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
                                        offset_vec, rnd_vec, dst0, dst1, dst2,
                                        dst3);
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     } else if (6 == height) {
         LD4(src, src_stride, tp0, tp1, tp2, tp3);
         src += 4 * src_stride;
@@ -238,9 +244,8 @@ static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
         HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
                                        rnd_vec, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
-        ST8x2_UB(out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
     } else if (0 == height % 8) {
         for (loop_cnt = (height >> 3); loop_cnt--;) {
             LD4(src, src_stride, tp0, tp1, tp2, tp3);
@@ -266,10 +271,9 @@ static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
                                            dst6, dst7);
             PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
             PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-            ST8x4_UB(out0, out1, dst, dst_stride);
-            dst += (4 * dst_stride);
-            ST8x4_UB(out2, out3, dst, dst_stride);
-            dst += (4 * dst_stride);
+            ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1,
+                  dst, dst_stride);
+            dst += (8 * dst_stride);
         }
     }
 }
@@ -313,7 +317,8 @@ static void hevc_uniwgt_copy_12w_msa(uint8_t *src,
                                        rnd_vec, dst4, dst5);
 
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST12x4_UB(out0, out1, out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -409,7 +414,7 @@ static void hevc_uniwgt_copy_24w_msa(uint8_t *src,
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
         PCKEV_B3_UB(dst7, dst6, dst9, dst8, dst11, dst10, out3, out4, out5);
         ST_UB4(out0, out1, out3, out4, dst, dst_stride);
-        ST8x4_UB(out2, out5, dst + 16, dst_stride);
+        ST_D4(out2, out5, 0, 1, 0, 1, dst + 16, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -651,7 +656,7 @@ static void hevc_hz_uniwgt_8t_4w_msa(uint8_t *src,
                                        dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -729,7 +734,7 @@ static void hevc_hz_uniwgt_8t_8w_msa(uint8_t *src,
                                        dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -822,8 +827,8 @@ static void hevc_hz_uniwgt_8t_12w_msa(uint8_t *src,
                                        rnd_vec, dst4, dst5);
 
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        ST4x4_UB(out2, out2, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -994,7 +999,7 @@ static void hevc_hz_uniwgt_8t_24w_msa(uint8_t *src,
 
         PCKEV_B3_UB(dst1, dst0, dst4, dst3, dst5, dst2, out0, out1, out2);
         ST_UB2(out0, out1, dst, dst_stride);
-        ST8x2_UB(out2, dst + 16, dst_stride);
+        ST_D2(out2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -1368,7 +1373,7 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
                                        dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2110 = src10998;
@@ -1444,7 +1449,7 @@ static void hevc_vt_uniwgt_8t_8w_msa(uint8_t *src,
                                        dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1543,8 +1548,8 @@ static void hevc_vt_uniwgt_8t_12w_msa(uint8_t *src,
                                        rnd_vec, dst4, dst5);
 
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        ST4x4_UB(out2, out2, 0, 1, 2, 3, dst + 8, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1858,10 +1863,10 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
         SRAR_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, rnd_vec);
         ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
         ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
-        CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst2_r, dst3_r);
+        CLIP_SW4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -2009,11 +2014,11 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
             SRAR_W4_SW(dst0_r, dst1_r, dst0_l, dst1_l, rnd_vec);
             ADD2(dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
             ADD2(dst1_r, offset_vec, dst1_l, offset_vec, dst1_r, dst1_l);
-            CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst0_l, dst1_l);
+            CLIP_SW4_0_255(dst0_r, dst1_r, dst0_l, dst1_l);
 
             PCKEV_H2_SW(dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
             dst0_r = (v4i32) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
-            ST8x2_UB(dst0_r, dst_tmp, dst_stride);
+            ST_D2(dst0_r, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (2 * dst_stride);
 
             dst10_r = dst32_r;
@@ -2160,10 +2165,10 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
         MUL2(dst0_r, weight_vec, dst0_l, weight_vec, dst0_r, dst0_l);
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
         ADD2(dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
-        CLIP_SW2_0_255_MAX_SATU(dst0_r, dst0_l);
+        CLIP_SW2_0_255(dst0_r, dst0_l);
         dst0_r = (v4i32) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst0_r, (v16i8) dst0_r);
-        ST8x1_UB(out, dst_tmp);
+        ST_D1(out, 0, dst_tmp);
         dst_tmp += dst_stride;
 
         dst0 = dst1;
@@ -2241,10 +2246,10 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
         SRAR_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, rnd_vec);
         ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
         ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
-        CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst2_r, dst3_r);
+        CLIP_SW4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -2389,9 +2394,9 @@ static void hevc_hz_uniwgt_4t_4x2_msa(uint8_t *src,
     SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
     dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
     dst0 = __msa_adds_s_h(dst0, offset_vec);
-    dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+    CLIP_SH_0_255(dst0);
     out = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 }
 
@@ -2448,7 +2453,7 @@ static void hevc_hz_uniwgt_4t_4x4_msa(uint8_t *src,
                                    dst0, dst1);
 
     out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     dst += (4 * dst_stride);
 }
 
@@ -2515,7 +2520,7 @@ static void hevc_hz_uniwgt_4t_4x8multiple_msa(uint8_t *src,
                                        dst0, dst1, dst2, dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -2613,9 +2618,15 @@ static void hevc_hz_uniwgt_4t_6w_msa(uint8_t *src,
 
     PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
     PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-    ST6x4_UB(out0, out1, dst, dst_stride);
+    ST_W2(out0, 0, 2, dst, dst_stride);
+    ST_H2(out0, 2, 6, dst + 4, dst_stride);
+    ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
-    ST6x4_UB(out2, out3, dst, dst_stride);
+    ST_W2(out2, 0, 2, dst, dst_stride);
+    ST_H2(out2, 2, 6, dst + 4, dst_stride);
+    ST_W2(out3, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
 }
 
 static void hevc_hz_uniwgt_4t_8x2_msa(uint8_t *src,
@@ -2670,7 +2681,7 @@ static void hevc_hz_uniwgt_4t_8x2_msa(uint8_t *src,
                                    dst0, dst1);
 
     out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_uniwgt_4t_8x4_msa(uint8_t *src,
@@ -2727,7 +2738,7 @@ static void hevc_hz_uniwgt_4t_8x4_msa(uint8_t *src,
                                    dst0, dst1, dst2, dst3);
 
     PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_uniwgt_4t_8x6_msa(uint8_t *src,
@@ -2796,9 +2807,8 @@ static void hevc_hz_uniwgt_4t_8x6_msa(uint8_t *src,
                                    dst4, dst5);
 
     PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hz_uniwgt_4t_8x8multiple_msa(uint8_t *src,
@@ -2876,7 +2886,7 @@ static void hevc_hz_uniwgt_4t_8x8multiple_msa(uint8_t *src,
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-        ST8x8_UB(out0, out1, out2, out3, dst, dst_stride);
+        ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -2981,7 +2991,8 @@ static void hevc_hz_uniwgt_4t_12w_msa(uint8_t *src,
                                        rnd_vec, dst4, dst5);
 
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST12x4_UB(out0, out1, out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -3142,7 +3153,7 @@ static void hevc_hz_uniwgt_4t_24w_msa(uint8_t *src,
 
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
         ST_UB2(out0, out1, dst, dst_stride);
-        ST8x2_UB(out2, dst + 16, dst_stride);
+        ST_D2(out2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
     }
 }
@@ -3284,9 +3295,9 @@ static void hevc_vt_uniwgt_4t_4x2_msa(uint8_t *src,
     SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
     dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
     dst0 = __msa_adds_s_h(dst0, offset_vec);
-    dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+    CLIP_SH_0_255(dst0);
     out = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_4x4_msa(uint8_t *src,
@@ -3340,7 +3351,7 @@ static void hevc_vt_uniwgt_4t_4x4_msa(uint8_t *src,
                                    dst0, dst1);
 
     out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_4x8multiple_msa(uint8_t *src,
@@ -3411,7 +3422,7 @@ static void hevc_vt_uniwgt_4t_4x8multiple_msa(uint8_t *src,
                                        dst0, dst1, dst2, dst3);
 
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2 = src10;
@@ -3509,9 +3520,15 @@ static void hevc_vt_uniwgt_4t_6w_msa(uint8_t *src,
 
     PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
     PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-    ST6x4_UB(out0, out1, dst, dst_stride);
+    ST_W2(out0, 0, 2, dst, dst_stride);
+    ST_H2(out0, 2, 6, dst + 4, dst_stride);
+    ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
     dst += (4 * dst_stride);
-    ST6x4_UB(out2, out3, dst, dst_stride);
+    ST_W2(out2, 0, 2, dst, dst_stride);
+    ST_H2(out2, 2, 6, dst + 4, dst_stride);
+    ST_W2(out3, 0, 2, dst + 2 * dst_stride, dst_stride);
+    ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_8x2_msa(uint8_t *src,
@@ -3562,7 +3579,7 @@ static void hevc_vt_uniwgt_4t_8x2_msa(uint8_t *src,
                                    dst0, dst1);
 
     out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_8x4_msa(uint8_t *src,
@@ -3617,7 +3634,7 @@ static void hevc_vt_uniwgt_4t_8x4_msa(uint8_t *src,
                                    offset_vec, rnd_vec, dst0, dst1, dst2,
                                    dst3);
     PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_8x6_msa(uint8_t *src,
@@ -3679,9 +3696,8 @@ static void hevc_vt_uniwgt_4t_8x6_msa(uint8_t *src,
     HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec, rnd_vec,
                                    dst4, dst5);
     PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_vt_uniwgt_4t_8x8mult_msa(uint8_t *src,
@@ -3754,7 +3770,7 @@ static void hevc_vt_uniwgt_4t_8x8mult_msa(uint8_t *src,
                                        dst7);
         PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
-        ST8x8_UB(out0, out1, out2, out3, dst, dst_stride);
+        ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2 = src10;
@@ -3861,7 +3877,8 @@ static void hevc_vt_uniwgt_4t_12w_msa(uint8_t *src,
         HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
                                        rnd_vec, dst4, dst5);
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST12x4_UB(out0, out1, out2, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         ILVRL_B2_SB(src7, src6, src76_r, src76_l);
@@ -3882,7 +3899,8 @@ static void hevc_vt_uniwgt_4t_12w_msa(uint8_t *src,
         HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst10, dst11, weight_vec, offset_vec,
                                        rnd_vec, dst10, dst11);
         PCKEV_B3_UB(dst7, dst6, dst9, dst8, dst11, dst10, out3, out4, out5);
-        ST12x4_UB(out3, out4, out5, dst, dst_stride);
+        ST_D4(out3, out4, 0, 1, 0, 1, dst, dst_stride);
+        ST_W4(out5, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src2 = src10;
@@ -4062,7 +4080,7 @@ static void hevc_vt_uniwgt_4t_24w_msa(uint8_t *src,
                     out2, out3);
         PCKEV_B2_UB(dst9, dst8, dst11, dst10, out4, out5);
         ST_UB4(out0, out1, out2, out3, dst, dst_stride);
-        ST8x4_UB(out4, out5, dst + 16, dst_stride);
+        ST_D4(out4, out5, 0, 1, 0, 1, dst + 16, dst_stride);
         dst += (4 * dst_stride);
 
         src2 = src6;
@@ -4229,9 +4247,9 @@ static void hevc_hv_uniwgt_4t_4x2_msa(uint8_t *src,
     SRAR_W2_SW(dst0, dst1, rnd_vec);
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
     tmp += offset_vec;
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+    CLIP_SH_0_255(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
-    ST4x2_UB(out, dst, dst_stride);
+    ST_W2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_uniwgt_4t_4x4_msa(uint8_t *src,
@@ -4298,9 +4316,9 @@ static void hevc_hv_uniwgt_4t_4x4_msa(uint8_t *src,
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
     ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void hevc_hv_uniwgt_4t_4multx8mult_msa(uint8_t *src,
@@ -4399,9 +4417,9 @@ static void hevc_hv_uniwgt_4t_4multx8mult_msa(uint8_t *src,
                     tmp2, tmp3);
         ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
         ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
@@ -4556,13 +4574,11 @@ static void hevc_hv_uniwgt_4t_6w_msa(uint8_t *src,
     ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
     ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
     ADD2(tmp4, offset_vec, tmp5, offset_vec, tmp4, tmp5);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH2_0_255(tmp4, tmp5);
     PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
-    ST4x8_UB(out0, out1, dst, dst_stride);
-    ST2x4_UB(out2, 0, dst + 4, dst_stride);
-    dst += 4 * dst_stride;
-    ST2x4_UB(out2, 4, dst + 4, dst_stride);
+    ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+    ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
 
 static void hevc_hv_uniwgt_4t_8x2_msa(uint8_t *src,
@@ -4636,9 +4652,9 @@ static void hevc_hv_uniwgt_4t_8x2_msa(uint8_t *src,
     SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
     ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST8x2_UB(out, dst, dst_stride);
+    ST_D2(out, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_uniwgt_4t_8multx4_msa(uint8_t *src,
@@ -4729,9 +4745,9 @@ static void hevc_hv_uniwgt_4t_8multx4_msa(uint8_t *src,
                     dst3_r, tmp0, tmp1, tmp2, tmp3);
         ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
         ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
     }
 }
@@ -4845,12 +4861,11 @@ static void hevc_hv_uniwgt_4t_8x6_msa(uint8_t *src,
     ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
     ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
     ADD2(tmp4, offset_vec, tmp5, offset_vec, tmp4, tmp5);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH2_0_255(tmp4, tmp5);
     PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST8x2_UB(out2, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
+    ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
@@ -4958,9 +4973,9 @@ static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
                         dst3_r, tmp0, tmp1, tmp2, tmp3);
             ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
             ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+            CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-            ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+            ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
 
             dst10_r = dst54_r;
@@ -5105,9 +5120,9 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
                     dst3_r, tmp0, tmp1, tmp2, tmp3);
         ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
         ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst_tmp, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -5172,9 +5187,9 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
                     tmp2, tmp3);
         ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
         ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST4x8_UB(out0, out1, dst, dst_stride);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
diff --git a/libavcodec/mips/hevcdsp_init_mips.c b/libavcodec/mips/hevcdsp_init_mips.c
index 776d13e..eb261e5 100644
--- a/libavcodec/mips/hevcdsp_init_mips.c
+++ b/libavcodec/mips/hevcdsp_init_mips.c
@@ -18,437 +18,500 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavcodec/mips/hevcdsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void hevc_dsp_init_msa(HEVCDSPContext *c,
-                                      const int bit_depth)
+void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth)
 {
-    if (8 == bit_depth) {
-        c->put_hevc_qpel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
-        c->put_hevc_qpel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
-        c->put_hevc_qpel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
-        c->put_hevc_qpel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
-        c->put_hevc_qpel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
-        c->put_hevc_qpel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
-        c->put_hevc_qpel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
-        c->put_hevc_qpel[8][0][0] = ff_hevc_put_hevc_pel_pixels48_8_msa;
-        c->put_hevc_qpel[9][0][0] = ff_hevc_put_hevc_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_msa;
-        c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_msa;
-        c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_msa;
-        c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_msa;
-        c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_msa;
-        c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_msa;
-        c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_msa;
-        c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_msa;
-
-        c->put_hevc_qpel[1][1][0] = ff_hevc_put_hevc_qpel_v4_8_msa;
-        c->put_hevc_qpel[3][1][0] = ff_hevc_put_hevc_qpel_v8_8_msa;
-        c->put_hevc_qpel[4][1][0] = ff_hevc_put_hevc_qpel_v12_8_msa;
-        c->put_hevc_qpel[5][1][0] = ff_hevc_put_hevc_qpel_v16_8_msa;
-        c->put_hevc_qpel[6][1][0] = ff_hevc_put_hevc_qpel_v24_8_msa;
-        c->put_hevc_qpel[7][1][0] = ff_hevc_put_hevc_qpel_v32_8_msa;
-        c->put_hevc_qpel[8][1][0] = ff_hevc_put_hevc_qpel_v48_8_msa;
-        c->put_hevc_qpel[9][1][0] = ff_hevc_put_hevc_qpel_v64_8_msa;
-
-        c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_msa;
-        c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_msa;
-        c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_msa;
-        c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_msa;
-        c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_msa;
-        c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_msa;
-        c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_msa;
-        c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_msa;
-
-        c->put_hevc_epel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
-        c->put_hevc_epel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
-        c->put_hevc_epel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
-        c->put_hevc_epel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
-        c->put_hevc_epel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
-        c->put_hevc_epel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
-        c->put_hevc_epel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
-
-        c->put_hevc_epel[1][0][1] = ff_hevc_put_hevc_epel_h4_8_msa;
-        c->put_hevc_epel[2][0][1] = ff_hevc_put_hevc_epel_h6_8_msa;
-        c->put_hevc_epel[3][0][1] = ff_hevc_put_hevc_epel_h8_8_msa;
-        c->put_hevc_epel[4][0][1] = ff_hevc_put_hevc_epel_h12_8_msa;
-        c->put_hevc_epel[5][0][1] = ff_hevc_put_hevc_epel_h16_8_msa;
-        c->put_hevc_epel[6][0][1] = ff_hevc_put_hevc_epel_h24_8_msa;
-        c->put_hevc_epel[7][0][1] = ff_hevc_put_hevc_epel_h32_8_msa;
-
-        c->put_hevc_epel[1][1][0] = ff_hevc_put_hevc_epel_v4_8_msa;
-        c->put_hevc_epel[2][1][0] = ff_hevc_put_hevc_epel_v6_8_msa;
-        c->put_hevc_epel[3][1][0] = ff_hevc_put_hevc_epel_v8_8_msa;
-        c->put_hevc_epel[4][1][0] = ff_hevc_put_hevc_epel_v12_8_msa;
-        c->put_hevc_epel[5][1][0] = ff_hevc_put_hevc_epel_v16_8_msa;
-        c->put_hevc_epel[6][1][0] = ff_hevc_put_hevc_epel_v24_8_msa;
-        c->put_hevc_epel[7][1][0] = ff_hevc_put_hevc_epel_v32_8_msa;
-
-        c->put_hevc_epel[1][1][1] = ff_hevc_put_hevc_epel_hv4_8_msa;
-        c->put_hevc_epel[2][1][1] = ff_hevc_put_hevc_epel_hv6_8_msa;
-        c->put_hevc_epel[3][1][1] = ff_hevc_put_hevc_epel_hv8_8_msa;
-        c->put_hevc_epel[4][1][1] = ff_hevc_put_hevc_epel_hv12_8_msa;
-        c->put_hevc_epel[5][1][1] = ff_hevc_put_hevc_epel_hv16_8_msa;
-        c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_msa;
-        c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
-        c->put_hevc_qpel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
-        c->put_hevc_qpel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
-        c->put_hevc_qpel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
-        c->put_hevc_qpel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
-        c->put_hevc_qpel_uni[8][0][0] = ff_hevc_put_hevc_uni_pel_pixels48_8_msa;
-        c->put_hevc_qpel_uni[9][0][0] = ff_hevc_put_hevc_uni_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_uni[1][0][1] = ff_hevc_put_hevc_uni_qpel_h4_8_msa;
-        c->put_hevc_qpel_uni[3][0][1] = ff_hevc_put_hevc_uni_qpel_h8_8_msa;
-        c->put_hevc_qpel_uni[4][0][1] = ff_hevc_put_hevc_uni_qpel_h12_8_msa;
-        c->put_hevc_qpel_uni[5][0][1] = ff_hevc_put_hevc_uni_qpel_h16_8_msa;
-        c->put_hevc_qpel_uni[6][0][1] = ff_hevc_put_hevc_uni_qpel_h24_8_msa;
-        c->put_hevc_qpel_uni[7][0][1] = ff_hevc_put_hevc_uni_qpel_h32_8_msa;
-        c->put_hevc_qpel_uni[8][0][1] = ff_hevc_put_hevc_uni_qpel_h48_8_msa;
-        c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_uni[1][1][0] = ff_hevc_put_hevc_uni_qpel_v4_8_msa;
-        c->put_hevc_qpel_uni[3][1][0] = ff_hevc_put_hevc_uni_qpel_v8_8_msa;
-        c->put_hevc_qpel_uni[4][1][0] = ff_hevc_put_hevc_uni_qpel_v12_8_msa;
-        c->put_hevc_qpel_uni[5][1][0] = ff_hevc_put_hevc_uni_qpel_v16_8_msa;
-        c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_msa;
-        c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_msa;
-        c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_msa;
-        c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_uni_qpel_hv4_8_msa;
-        c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_msa;
-        c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_uni_qpel_hv12_8_msa;
-        c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_msa;
-        c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_msa;
-        c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_msa;
-        c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_msa;
-        c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
-        c->put_hevc_epel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
-        c->put_hevc_epel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
-        c->put_hevc_epel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
-        c->put_hevc_epel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_uni[1][0][1] = ff_hevc_put_hevc_uni_epel_h4_8_msa;
-        c->put_hevc_epel_uni[2][0][1] = ff_hevc_put_hevc_uni_epel_h6_8_msa;
-        c->put_hevc_epel_uni[3][0][1] = ff_hevc_put_hevc_uni_epel_h8_8_msa;
-        c->put_hevc_epel_uni[4][0][1] = ff_hevc_put_hevc_uni_epel_h12_8_msa;
-        c->put_hevc_epel_uni[5][0][1] = ff_hevc_put_hevc_uni_epel_h16_8_msa;
-        c->put_hevc_epel_uni[6][0][1] = ff_hevc_put_hevc_uni_epel_h24_8_msa;
-        c->put_hevc_epel_uni[7][0][1] = ff_hevc_put_hevc_uni_epel_h32_8_msa;
-
-        c->put_hevc_epel_uni[1][1][0] = ff_hevc_put_hevc_uni_epel_v4_8_msa;
-        c->put_hevc_epel_uni[2][1][0] = ff_hevc_put_hevc_uni_epel_v6_8_msa;
-        c->put_hevc_epel_uni[3][1][0] = ff_hevc_put_hevc_uni_epel_v8_8_msa;
-        c->put_hevc_epel_uni[4][1][0] = ff_hevc_put_hevc_uni_epel_v12_8_msa;
-        c->put_hevc_epel_uni[5][1][0] = ff_hevc_put_hevc_uni_epel_v16_8_msa;
-        c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_msa;
-        c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_msa;
-
-        c->put_hevc_epel_uni[1][1][1] = ff_hevc_put_hevc_uni_epel_hv4_8_msa;
-        c->put_hevc_epel_uni[2][1][1] = ff_hevc_put_hevc_uni_epel_hv6_8_msa;
-        c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_msa;
-        c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_msa;
-        c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_msa;
-        c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_msa;
-        c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
-        c->put_hevc_qpel_uni_w[3][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
-        c->put_hevc_qpel_uni_w[4][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
-        c->put_hevc_qpel_uni_w[5][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
-        c->put_hevc_qpel_uni_w[6][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
-        c->put_hevc_qpel_uni_w[7][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
-        c->put_hevc_qpel_uni_w[8][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels48_8_msa;
-        c->put_hevc_qpel_uni_w[9][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_qpel_h4_8_msa;
-        c->put_hevc_qpel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_qpel_h8_8_msa;
-        c->put_hevc_qpel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_qpel_h12_8_msa;
-        c->put_hevc_qpel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_qpel_h16_8_msa;
-        c->put_hevc_qpel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_qpel_h24_8_msa;
-        c->put_hevc_qpel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_qpel_h32_8_msa;
-        c->put_hevc_qpel_uni_w[8][0][1] = ff_hevc_put_hevc_uni_w_qpel_h48_8_msa;
-        c->put_hevc_qpel_uni_w[9][0][1] = ff_hevc_put_hevc_uni_w_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_qpel_v4_8_msa;
-        c->put_hevc_qpel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_qpel_v8_8_msa;
-        c->put_hevc_qpel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_qpel_v12_8_msa;
-        c->put_hevc_qpel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_qpel_v16_8_msa;
-        c->put_hevc_qpel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_qpel_v24_8_msa;
-        c->put_hevc_qpel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_qpel_v32_8_msa;
-        c->put_hevc_qpel_uni_w[8][1][0] = ff_hevc_put_hevc_uni_w_qpel_v48_8_msa;
-        c->put_hevc_qpel_uni_w[9][1][0] = ff_hevc_put_hevc_uni_w_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv4_8_msa;
-        c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_msa;
-        c->put_hevc_qpel_uni_w[4][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv12_8_msa;
-        c->put_hevc_qpel_uni_w[5][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv16_8_msa;
-        c->put_hevc_qpel_uni_w[6][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv24_8_msa;
-        c->put_hevc_qpel_uni_w[7][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv32_8_msa;
-        c->put_hevc_qpel_uni_w[8][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv48_8_msa;
-        c->put_hevc_qpel_uni_w[9][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_uni_w[1][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
-        c->put_hevc_epel_uni_w[2][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels6_8_msa;
-        c->put_hevc_epel_uni_w[3][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
-        c->put_hevc_epel_uni_w[4][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
-        c->put_hevc_epel_uni_w[5][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
-        c->put_hevc_epel_uni_w[6][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
-        c->put_hevc_epel_uni_w[7][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_epel_h4_8_msa;
-        c->put_hevc_epel_uni_w[2][0][1] = ff_hevc_put_hevc_uni_w_epel_h6_8_msa;
-        c->put_hevc_epel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_epel_h8_8_msa;
-        c->put_hevc_epel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_epel_h12_8_msa;
-        c->put_hevc_epel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_epel_h16_8_msa;
-        c->put_hevc_epel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_epel_h24_8_msa;
-        c->put_hevc_epel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_epel_h32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_epel_v4_8_msa;
-        c->put_hevc_epel_uni_w[2][1][0] = ff_hevc_put_hevc_uni_w_epel_v6_8_msa;
-        c->put_hevc_epel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_epel_v8_8_msa;
-        c->put_hevc_epel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_epel_v12_8_msa;
-        c->put_hevc_epel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_epel_v16_8_msa;
-        c->put_hevc_epel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_epel_v24_8_msa;
-        c->put_hevc_epel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_epel_v32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_epel_hv4_8_msa;
-        c->put_hevc_epel_uni_w[2][1][1] = ff_hevc_put_hevc_uni_w_epel_hv6_8_msa;
-        c->put_hevc_epel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_epel_hv8_8_msa;
-        c->put_hevc_epel_uni_w[4][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv12_8_msa;
-        c->put_hevc_epel_uni_w[5][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv16_8_msa;
-        c->put_hevc_epel_uni_w[6][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv24_8_msa;
-        c->put_hevc_epel_uni_w[7][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
-        c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
-        c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
-        c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
-        c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
-        c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
-        c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_bi_pel_pixels48_8_msa;
-        c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_bi_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_bi_qpel_h4_8_msa;
-        c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_bi_qpel_h8_8_msa;
-        c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_bi_qpel_h12_8_msa;
-        c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_bi_qpel_h16_8_msa;
-        c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_bi_qpel_h24_8_msa;
-        c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_bi_qpel_h32_8_msa;
-        c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_bi_qpel_h48_8_msa;
-        c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_bi_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_bi[1][1][0] = ff_hevc_put_hevc_bi_qpel_v4_8_msa;
-        c->put_hevc_qpel_bi[3][1][0] = ff_hevc_put_hevc_bi_qpel_v8_8_msa;
-        c->put_hevc_qpel_bi[4][1][0] = ff_hevc_put_hevc_bi_qpel_v12_8_msa;
-        c->put_hevc_qpel_bi[5][1][0] = ff_hevc_put_hevc_bi_qpel_v16_8_msa;
-        c->put_hevc_qpel_bi[6][1][0] = ff_hevc_put_hevc_bi_qpel_v24_8_msa;
-        c->put_hevc_qpel_bi[7][1][0] = ff_hevc_put_hevc_bi_qpel_v32_8_msa;
-        c->put_hevc_qpel_bi[8][1][0] = ff_hevc_put_hevc_bi_qpel_v48_8_msa;
-        c->put_hevc_qpel_bi[9][1][0] = ff_hevc_put_hevc_bi_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_bi_qpel_hv4_8_msa;
-        c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_bi_qpel_hv8_8_msa;
-        c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_bi_qpel_hv12_8_msa;
-        c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_bi_qpel_hv16_8_msa;
-        c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_bi_qpel_hv24_8_msa;
-        c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_bi_qpel_hv32_8_msa;
-        c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_bi_qpel_hv48_8_msa;
-        c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_bi_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
-        c->put_hevc_epel_bi[2][0][0] = ff_hevc_put_hevc_bi_pel_pixels6_8_msa;
-        c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
-        c->put_hevc_epel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
-        c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
-        c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
-        c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_bi[1][0][1] = ff_hevc_put_hevc_bi_epel_h4_8_msa;
-        c->put_hevc_epel_bi[2][0][1] = ff_hevc_put_hevc_bi_epel_h6_8_msa;
-        c->put_hevc_epel_bi[3][0][1] = ff_hevc_put_hevc_bi_epel_h8_8_msa;
-        c->put_hevc_epel_bi[4][0][1] = ff_hevc_put_hevc_bi_epel_h12_8_msa;
-        c->put_hevc_epel_bi[5][0][1] = ff_hevc_put_hevc_bi_epel_h16_8_msa;
-        c->put_hevc_epel_bi[6][0][1] = ff_hevc_put_hevc_bi_epel_h24_8_msa;
-        c->put_hevc_epel_bi[7][0][1] = ff_hevc_put_hevc_bi_epel_h32_8_msa;
-
-        c->put_hevc_epel_bi[1][1][0] = ff_hevc_put_hevc_bi_epel_v4_8_msa;
-        c->put_hevc_epel_bi[2][1][0] = ff_hevc_put_hevc_bi_epel_v6_8_msa;
-        c->put_hevc_epel_bi[3][1][0] = ff_hevc_put_hevc_bi_epel_v8_8_msa;
-        c->put_hevc_epel_bi[4][1][0] = ff_hevc_put_hevc_bi_epel_v12_8_msa;
-        c->put_hevc_epel_bi[5][1][0] = ff_hevc_put_hevc_bi_epel_v16_8_msa;
-        c->put_hevc_epel_bi[6][1][0] = ff_hevc_put_hevc_bi_epel_v24_8_msa;
-        c->put_hevc_epel_bi[7][1][0] = ff_hevc_put_hevc_bi_epel_v32_8_msa;
-
-        c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_bi_epel_hv4_8_msa;
-        c->put_hevc_epel_bi[2][1][1] = ff_hevc_put_hevc_bi_epel_hv6_8_msa;
-        c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_bi_epel_hv8_8_msa;
-        c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_bi_epel_hv12_8_msa;
-        c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_bi_epel_hv16_8_msa;
-        c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_msa;
-        c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
-        c->put_hevc_qpel_bi_w[3][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
-        c->put_hevc_qpel_bi_w[4][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
-        c->put_hevc_qpel_bi_w[5][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
-        c->put_hevc_qpel_bi_w[6][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
-        c->put_hevc_qpel_bi_w[7][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
-        c->put_hevc_qpel_bi_w[8][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels48_8_msa;
-        c->put_hevc_qpel_bi_w[9][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_qpel_h4_8_msa;
-        c->put_hevc_qpel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_qpel_h8_8_msa;
-        c->put_hevc_qpel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_qpel_h12_8_msa;
-        c->put_hevc_qpel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_qpel_h16_8_msa;
-        c->put_hevc_qpel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_qpel_h24_8_msa;
-        c->put_hevc_qpel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_qpel_h32_8_msa;
-        c->put_hevc_qpel_bi_w[8][0][1] = ff_hevc_put_hevc_bi_w_qpel_h48_8_msa;
-        c->put_hevc_qpel_bi_w[9][0][1] = ff_hevc_put_hevc_bi_w_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_qpel_v4_8_msa;
-        c->put_hevc_qpel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_qpel_v8_8_msa;
-        c->put_hevc_qpel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_qpel_v12_8_msa;
-        c->put_hevc_qpel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_qpel_v16_8_msa;
-        c->put_hevc_qpel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_qpel_v24_8_msa;
-        c->put_hevc_qpel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_qpel_v32_8_msa;
-        c->put_hevc_qpel_bi_w[8][1][0] = ff_hevc_put_hevc_bi_w_qpel_v48_8_msa;
-        c->put_hevc_qpel_bi_w[9][1][0] = ff_hevc_put_hevc_bi_w_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv4_8_msa;
-        c->put_hevc_qpel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv8_8_msa;
-        c->put_hevc_qpel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv12_8_msa;
-        c->put_hevc_qpel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv16_8_msa;
-        c->put_hevc_qpel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv24_8_msa;
-        c->put_hevc_qpel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv32_8_msa;
-        c->put_hevc_qpel_bi_w[8][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv48_8_msa;
-        c->put_hevc_qpel_bi_w[9][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_bi_w[1][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
-        c->put_hevc_epel_bi_w[2][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels6_8_msa;
-        c->put_hevc_epel_bi_w[3][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
-        c->put_hevc_epel_bi_w[4][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
-        c->put_hevc_epel_bi_w[5][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
-        c->put_hevc_epel_bi_w[6][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
-        c->put_hevc_epel_bi_w[7][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_epel_h4_8_msa;
-        c->put_hevc_epel_bi_w[2][0][1] = ff_hevc_put_hevc_bi_w_epel_h6_8_msa;
-        c->put_hevc_epel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_epel_h8_8_msa;
-        c->put_hevc_epel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_epel_h12_8_msa;
-        c->put_hevc_epel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_epel_h16_8_msa;
-        c->put_hevc_epel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_epel_h24_8_msa;
-        c->put_hevc_epel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_epel_h32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_epel_v4_8_msa;
-        c->put_hevc_epel_bi_w[2][1][0] = ff_hevc_put_hevc_bi_w_epel_v6_8_msa;
-        c->put_hevc_epel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_epel_v8_8_msa;
-        c->put_hevc_epel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_epel_v12_8_msa;
-        c->put_hevc_epel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_epel_v16_8_msa;
-        c->put_hevc_epel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_epel_v24_8_msa;
-        c->put_hevc_epel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_epel_v32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_epel_hv4_8_msa;
-        c->put_hevc_epel_bi_w[2][1][1] = ff_hevc_put_hevc_bi_w_epel_hv6_8_msa;
-        c->put_hevc_epel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_epel_hv8_8_msa;
-        c->put_hevc_epel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_epel_hv12_8_msa;
-        c->put_hevc_epel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_epel_hv16_8_msa;
-        c->put_hevc_epel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_epel_hv24_8_msa;
-        c->put_hevc_epel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_epel_hv32_8_msa;
-
-        c->sao_band_filter[0] =
-        c->sao_band_filter[1] =
-        c->sao_band_filter[2] =
-        c->sao_band_filter[3] =
-        c->sao_band_filter[4] = ff_hevc_sao_band_filter_0_8_msa;
-
-        c->sao_edge_filter[0] =
-        c->sao_edge_filter[1] =
-        c->sao_edge_filter[2] =
-        c->sao_edge_filter[3] =
-        c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_msa;
-
-        c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_msa;
-        c->hevc_v_loop_filter_luma = ff_hevc_loop_filter_luma_v_8_msa;
-
-        c->hevc_h_loop_filter_chroma = ff_hevc_loop_filter_chroma_h_8_msa;
-        c->hevc_v_loop_filter_chroma = ff_hevc_loop_filter_chroma_v_8_msa;
-
-        c->hevc_h_loop_filter_luma_c = ff_hevc_loop_filter_luma_h_8_msa;
-        c->hevc_v_loop_filter_luma_c = ff_hevc_loop_filter_luma_v_8_msa;
-
-        c->hevc_h_loop_filter_chroma_c =
-            ff_hevc_loop_filter_chroma_h_8_msa;
-        c->hevc_v_loop_filter_chroma_c =
-            ff_hevc_loop_filter_chroma_v_8_msa;
-
-        c->idct[0] = ff_hevc_idct_4x4_msa;
-        c->idct[1] = ff_hevc_idct_8x8_msa;
-        c->idct[2] = ff_hevc_idct_16x16_msa;
-        c->idct[3] = ff_hevc_idct_32x32_msa;
-        c->idct_dc[0] = ff_hevc_idct_dc_4x4_msa;
-        c->idct_dc[1] = ff_hevc_idct_dc_8x8_msa;
-        c->idct_dc[2] = ff_hevc_idct_dc_16x16_msa;
-        c->idct_dc[3] = ff_hevc_idct_dc_32x32_msa;
-        c->add_residual[0] = ff_hevc_addblk_4x4_msa;
-        c->add_residual[1] = ff_hevc_addblk_8x8_msa;
-        c->add_residual[2] = ff_hevc_addblk_16x16_msa;
-        c->add_residual[3] = ff_hevc_addblk_32x32_msa;
-        c->transform_4x4_luma = ff_hevc_idct_luma_4x4_msa;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_mmi;
+            c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_mmi;
+            c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_mmi;
+            c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_mmi;
+            c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_mmi;
+            c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_mmi;
+            c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_mmi;
+            c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_mmi;
+
+            c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_mmi;
+            c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_mmi;
+            c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_mmi;
+            c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_mmi;
+            c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_mmi;
+            c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_mmi;
+            c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_mmi;
+            c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_mmi;
+
+            c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_qpel_bi_h4_8_mmi;
+            c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_qpel_bi_h8_8_mmi;
+            c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_qpel_bi_h12_8_mmi;
+            c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_qpel_bi_h16_8_mmi;
+            c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_qpel_bi_h24_8_mmi;
+            c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_qpel_bi_h32_8_mmi;
+            c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_qpel_bi_h48_8_mmi;
+            c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_qpel_bi_h64_8_mmi;
+
+            c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_qpel_bi_hv4_8_mmi;
+            c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_qpel_bi_hv8_8_mmi;
+            c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_qpel_bi_hv12_8_mmi;
+            c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_qpel_bi_hv16_8_mmi;
+            c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_qpel_bi_hv24_8_mmi;
+            c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_qpel_bi_hv32_8_mmi;
+            c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_qpel_bi_hv48_8_mmi;
+            c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_qpel_bi_hv64_8_mmi;
+
+            c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
+            c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
+            c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
+            c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
+            c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_pel_bi_pixels48_8_mmi;
+            c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_pel_bi_pixels64_8_mmi;
+
+            c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
+            c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
+            c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
+            c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
+
+            c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_epel_bi_hv4_8_mmi;
+            c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_epel_bi_hv8_8_mmi;
+            c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_epel_bi_hv12_8_mmi;
+            c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_epel_bi_hv16_8_mmi;
+            c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_epel_bi_hv24_8_mmi;
+            c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_epel_bi_hv32_8_mmi;
+
+            c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_qpel_uni_hv4_8_mmi;
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_qpel_uni_hv8_8_mmi;
+            c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_qpel_uni_hv12_8_mmi;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_qpel_uni_hv16_8_mmi;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_qpel_uni_hv24_8_mmi;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_qpel_uni_hv32_8_mmi;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_qpel_uni_hv48_8_mmi;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_qpel_uni_hv64_8_mmi;
+        }
     }
-}
-#endif  // #if HAVE_MSA
 
-void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth)
-{
-#if HAVE_MSA
-    hevc_dsp_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_hevc_qpel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
+            c->put_hevc_qpel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
+            c->put_hevc_qpel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
+            c->put_hevc_qpel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
+            c->put_hevc_qpel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
+            c->put_hevc_qpel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
+            c->put_hevc_qpel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
+            c->put_hevc_qpel[8][0][0] = ff_hevc_put_hevc_pel_pixels48_8_msa;
+            c->put_hevc_qpel[9][0][0] = ff_hevc_put_hevc_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_msa;
+            c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_msa;
+            c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_msa;
+            c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_msa;
+            c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_msa;
+            c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_msa;
+            c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_msa;
+            c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_msa;
+
+            c->put_hevc_qpel[1][1][0] = ff_hevc_put_hevc_qpel_v4_8_msa;
+            c->put_hevc_qpel[3][1][0] = ff_hevc_put_hevc_qpel_v8_8_msa;
+            c->put_hevc_qpel[4][1][0] = ff_hevc_put_hevc_qpel_v12_8_msa;
+            c->put_hevc_qpel[5][1][0] = ff_hevc_put_hevc_qpel_v16_8_msa;
+            c->put_hevc_qpel[6][1][0] = ff_hevc_put_hevc_qpel_v24_8_msa;
+            c->put_hevc_qpel[7][1][0] = ff_hevc_put_hevc_qpel_v32_8_msa;
+            c->put_hevc_qpel[8][1][0] = ff_hevc_put_hevc_qpel_v48_8_msa;
+            c->put_hevc_qpel[9][1][0] = ff_hevc_put_hevc_qpel_v64_8_msa;
+
+            c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_msa;
+            c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_msa;
+            c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_msa;
+            c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_msa;
+            c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_msa;
+            c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_msa;
+            c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_msa;
+            c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_msa;
+
+            c->put_hevc_epel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
+            c->put_hevc_epel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
+            c->put_hevc_epel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
+            c->put_hevc_epel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
+            c->put_hevc_epel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
+            c->put_hevc_epel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
+            c->put_hevc_epel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
+
+            c->put_hevc_epel[1][0][1] = ff_hevc_put_hevc_epel_h4_8_msa;
+            c->put_hevc_epel[2][0][1] = ff_hevc_put_hevc_epel_h6_8_msa;
+            c->put_hevc_epel[3][0][1] = ff_hevc_put_hevc_epel_h8_8_msa;
+            c->put_hevc_epel[4][0][1] = ff_hevc_put_hevc_epel_h12_8_msa;
+            c->put_hevc_epel[5][0][1] = ff_hevc_put_hevc_epel_h16_8_msa;
+            c->put_hevc_epel[6][0][1] = ff_hevc_put_hevc_epel_h24_8_msa;
+            c->put_hevc_epel[7][0][1] = ff_hevc_put_hevc_epel_h32_8_msa;
+
+            c->put_hevc_epel[1][1][0] = ff_hevc_put_hevc_epel_v4_8_msa;
+            c->put_hevc_epel[2][1][0] = ff_hevc_put_hevc_epel_v6_8_msa;
+            c->put_hevc_epel[3][1][0] = ff_hevc_put_hevc_epel_v8_8_msa;
+            c->put_hevc_epel[4][1][0] = ff_hevc_put_hevc_epel_v12_8_msa;
+            c->put_hevc_epel[5][1][0] = ff_hevc_put_hevc_epel_v16_8_msa;
+            c->put_hevc_epel[6][1][0] = ff_hevc_put_hevc_epel_v24_8_msa;
+            c->put_hevc_epel[7][1][0] = ff_hevc_put_hevc_epel_v32_8_msa;
+
+            c->put_hevc_epel[1][1][1] = ff_hevc_put_hevc_epel_hv4_8_msa;
+            c->put_hevc_epel[2][1][1] = ff_hevc_put_hevc_epel_hv6_8_msa;
+            c->put_hevc_epel[3][1][1] = ff_hevc_put_hevc_epel_hv8_8_msa;
+            c->put_hevc_epel[4][1][1] = ff_hevc_put_hevc_epel_hv12_8_msa;
+            c->put_hevc_epel[5][1][1] = ff_hevc_put_hevc_epel_hv16_8_msa;
+            c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_msa;
+            c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
+            c->put_hevc_qpel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
+            c->put_hevc_qpel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
+            c->put_hevc_qpel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
+            c->put_hevc_qpel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
+            c->put_hevc_qpel_uni[8][0][0] = ff_hevc_put_hevc_uni_pel_pixels48_8_msa;
+            c->put_hevc_qpel_uni[9][0][0] = ff_hevc_put_hevc_uni_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_uni[1][0][1] = ff_hevc_put_hevc_uni_qpel_h4_8_msa;
+            c->put_hevc_qpel_uni[3][0][1] = ff_hevc_put_hevc_uni_qpel_h8_8_msa;
+            c->put_hevc_qpel_uni[4][0][1] = ff_hevc_put_hevc_uni_qpel_h12_8_msa;
+            c->put_hevc_qpel_uni[5][0][1] = ff_hevc_put_hevc_uni_qpel_h16_8_msa;
+            c->put_hevc_qpel_uni[6][0][1] = ff_hevc_put_hevc_uni_qpel_h24_8_msa;
+            c->put_hevc_qpel_uni[7][0][1] = ff_hevc_put_hevc_uni_qpel_h32_8_msa;
+            c->put_hevc_qpel_uni[8][0][1] = ff_hevc_put_hevc_uni_qpel_h48_8_msa;
+            c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_uni[1][1][0] = ff_hevc_put_hevc_uni_qpel_v4_8_msa;
+            c->put_hevc_qpel_uni[3][1][0] = ff_hevc_put_hevc_uni_qpel_v8_8_msa;
+            c->put_hevc_qpel_uni[4][1][0] = ff_hevc_put_hevc_uni_qpel_v12_8_msa;
+            c->put_hevc_qpel_uni[5][1][0] = ff_hevc_put_hevc_uni_qpel_v16_8_msa;
+            c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_msa;
+            c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_msa;
+            c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_msa;
+            c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_uni_qpel_hv4_8_msa;
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_msa;
+            c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_uni_qpel_hv12_8_msa;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_msa;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_msa;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_msa;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_msa;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
+            c->put_hevc_epel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
+            c->put_hevc_epel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
+            c->put_hevc_epel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
+            c->put_hevc_epel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_uni[1][0][1] = ff_hevc_put_hevc_uni_epel_h4_8_msa;
+            c->put_hevc_epel_uni[2][0][1] = ff_hevc_put_hevc_uni_epel_h6_8_msa;
+            c->put_hevc_epel_uni[3][0][1] = ff_hevc_put_hevc_uni_epel_h8_8_msa;
+            c->put_hevc_epel_uni[4][0][1] = ff_hevc_put_hevc_uni_epel_h12_8_msa;
+            c->put_hevc_epel_uni[5][0][1] = ff_hevc_put_hevc_uni_epel_h16_8_msa;
+            c->put_hevc_epel_uni[6][0][1] = ff_hevc_put_hevc_uni_epel_h24_8_msa;
+            c->put_hevc_epel_uni[7][0][1] = ff_hevc_put_hevc_uni_epel_h32_8_msa;
+
+            c->put_hevc_epel_uni[1][1][0] = ff_hevc_put_hevc_uni_epel_v4_8_msa;
+            c->put_hevc_epel_uni[2][1][0] = ff_hevc_put_hevc_uni_epel_v6_8_msa;
+            c->put_hevc_epel_uni[3][1][0] = ff_hevc_put_hevc_uni_epel_v8_8_msa;
+            c->put_hevc_epel_uni[4][1][0] = ff_hevc_put_hevc_uni_epel_v12_8_msa;
+            c->put_hevc_epel_uni[5][1][0] = ff_hevc_put_hevc_uni_epel_v16_8_msa;
+            c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_msa;
+            c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_msa;
+
+            c->put_hevc_epel_uni[1][1][1] = ff_hevc_put_hevc_uni_epel_hv4_8_msa;
+            c->put_hevc_epel_uni[2][1][1] = ff_hevc_put_hevc_uni_epel_hv6_8_msa;
+            c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_msa;
+            c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_msa;
+            c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_msa;
+            c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_msa;
+            c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
+            c->put_hevc_qpel_uni_w[3][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
+            c->put_hevc_qpel_uni_w[4][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
+            c->put_hevc_qpel_uni_w[5][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
+            c->put_hevc_qpel_uni_w[6][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
+            c->put_hevc_qpel_uni_w[7][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
+            c->put_hevc_qpel_uni_w[8][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels48_8_msa;
+            c->put_hevc_qpel_uni_w[9][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_qpel_h4_8_msa;
+            c->put_hevc_qpel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_qpel_h8_8_msa;
+            c->put_hevc_qpel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_qpel_h12_8_msa;
+            c->put_hevc_qpel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_qpel_h16_8_msa;
+            c->put_hevc_qpel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_qpel_h24_8_msa;
+            c->put_hevc_qpel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_qpel_h32_8_msa;
+            c->put_hevc_qpel_uni_w[8][0][1] = ff_hevc_put_hevc_uni_w_qpel_h48_8_msa;
+            c->put_hevc_qpel_uni_w[9][0][1] = ff_hevc_put_hevc_uni_w_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_qpel_v4_8_msa;
+            c->put_hevc_qpel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_qpel_v8_8_msa;
+            c->put_hevc_qpel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_qpel_v12_8_msa;
+            c->put_hevc_qpel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_qpel_v16_8_msa;
+            c->put_hevc_qpel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_qpel_v24_8_msa;
+            c->put_hevc_qpel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_qpel_v32_8_msa;
+            c->put_hevc_qpel_uni_w[8][1][0] = ff_hevc_put_hevc_uni_w_qpel_v48_8_msa;
+            c->put_hevc_qpel_uni_w[9][1][0] = ff_hevc_put_hevc_uni_w_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv4_8_msa;
+            c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_msa;
+            c->put_hevc_qpel_uni_w[4][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv12_8_msa;
+            c->put_hevc_qpel_uni_w[5][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv16_8_msa;
+            c->put_hevc_qpel_uni_w[6][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv24_8_msa;
+            c->put_hevc_qpel_uni_w[7][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv32_8_msa;
+            c->put_hevc_qpel_uni_w[8][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv48_8_msa;
+            c->put_hevc_qpel_uni_w[9][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_uni_w[1][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
+            c->put_hevc_epel_uni_w[2][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels6_8_msa;
+            c->put_hevc_epel_uni_w[3][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
+            c->put_hevc_epel_uni_w[4][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
+            c->put_hevc_epel_uni_w[5][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
+            c->put_hevc_epel_uni_w[6][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
+            c->put_hevc_epel_uni_w[7][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_epel_h4_8_msa;
+            c->put_hevc_epel_uni_w[2][0][1] = ff_hevc_put_hevc_uni_w_epel_h6_8_msa;
+            c->put_hevc_epel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_epel_h8_8_msa;
+            c->put_hevc_epel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_epel_h12_8_msa;
+            c->put_hevc_epel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_epel_h16_8_msa;
+            c->put_hevc_epel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_epel_h24_8_msa;
+            c->put_hevc_epel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_epel_h32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_epel_v4_8_msa;
+            c->put_hevc_epel_uni_w[2][1][0] = ff_hevc_put_hevc_uni_w_epel_v6_8_msa;
+            c->put_hevc_epel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_epel_v8_8_msa;
+            c->put_hevc_epel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_epel_v12_8_msa;
+            c->put_hevc_epel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_epel_v16_8_msa;
+            c->put_hevc_epel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_epel_v24_8_msa;
+            c->put_hevc_epel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_epel_v32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_epel_hv4_8_msa;
+            c->put_hevc_epel_uni_w[2][1][1] = ff_hevc_put_hevc_uni_w_epel_hv6_8_msa;
+            c->put_hevc_epel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_epel_hv8_8_msa;
+            c->put_hevc_epel_uni_w[4][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv12_8_msa;
+            c->put_hevc_epel_uni_w[5][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv16_8_msa;
+            c->put_hevc_epel_uni_w[6][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv24_8_msa;
+            c->put_hevc_epel_uni_w[7][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
+            c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
+            c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
+            c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
+            c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
+            c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
+            c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_bi_pel_pixels48_8_msa;
+            c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_bi_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_bi_qpel_h4_8_msa;
+            c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_bi_qpel_h8_8_msa;
+            c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_bi_qpel_h12_8_msa;
+            c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_bi_qpel_h16_8_msa;
+            c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_bi_qpel_h24_8_msa;
+            c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_bi_qpel_h32_8_msa;
+            c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_bi_qpel_h48_8_msa;
+            c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_bi_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_bi[1][1][0] = ff_hevc_put_hevc_bi_qpel_v4_8_msa;
+            c->put_hevc_qpel_bi[3][1][0] = ff_hevc_put_hevc_bi_qpel_v8_8_msa;
+            c->put_hevc_qpel_bi[4][1][0] = ff_hevc_put_hevc_bi_qpel_v12_8_msa;
+            c->put_hevc_qpel_bi[5][1][0] = ff_hevc_put_hevc_bi_qpel_v16_8_msa;
+            c->put_hevc_qpel_bi[6][1][0] = ff_hevc_put_hevc_bi_qpel_v24_8_msa;
+            c->put_hevc_qpel_bi[7][1][0] = ff_hevc_put_hevc_bi_qpel_v32_8_msa;
+            c->put_hevc_qpel_bi[8][1][0] = ff_hevc_put_hevc_bi_qpel_v48_8_msa;
+            c->put_hevc_qpel_bi[9][1][0] = ff_hevc_put_hevc_bi_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_bi_qpel_hv4_8_msa;
+            c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_bi_qpel_hv8_8_msa;
+            c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_bi_qpel_hv12_8_msa;
+            c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_bi_qpel_hv16_8_msa;
+            c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_bi_qpel_hv24_8_msa;
+            c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_bi_qpel_hv32_8_msa;
+            c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_bi_qpel_hv48_8_msa;
+            c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_bi_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
+            c->put_hevc_epel_bi[2][0][0] = ff_hevc_put_hevc_bi_pel_pixels6_8_msa;
+            c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
+            c->put_hevc_epel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
+            c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
+            c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
+            c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_bi[1][0][1] = ff_hevc_put_hevc_bi_epel_h4_8_msa;
+            c->put_hevc_epel_bi[2][0][1] = ff_hevc_put_hevc_bi_epel_h6_8_msa;
+            c->put_hevc_epel_bi[3][0][1] = ff_hevc_put_hevc_bi_epel_h8_8_msa;
+            c->put_hevc_epel_bi[4][0][1] = ff_hevc_put_hevc_bi_epel_h12_8_msa;
+            c->put_hevc_epel_bi[5][0][1] = ff_hevc_put_hevc_bi_epel_h16_8_msa;
+            c->put_hevc_epel_bi[6][0][1] = ff_hevc_put_hevc_bi_epel_h24_8_msa;
+            c->put_hevc_epel_bi[7][0][1] = ff_hevc_put_hevc_bi_epel_h32_8_msa;
+
+            c->put_hevc_epel_bi[1][1][0] = ff_hevc_put_hevc_bi_epel_v4_8_msa;
+            c->put_hevc_epel_bi[2][1][0] = ff_hevc_put_hevc_bi_epel_v6_8_msa;
+            c->put_hevc_epel_bi[3][1][0] = ff_hevc_put_hevc_bi_epel_v8_8_msa;
+            c->put_hevc_epel_bi[4][1][0] = ff_hevc_put_hevc_bi_epel_v12_8_msa;
+            c->put_hevc_epel_bi[5][1][0] = ff_hevc_put_hevc_bi_epel_v16_8_msa;
+            c->put_hevc_epel_bi[6][1][0] = ff_hevc_put_hevc_bi_epel_v24_8_msa;
+            c->put_hevc_epel_bi[7][1][0] = ff_hevc_put_hevc_bi_epel_v32_8_msa;
+
+            c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_bi_epel_hv4_8_msa;
+            c->put_hevc_epel_bi[2][1][1] = ff_hevc_put_hevc_bi_epel_hv6_8_msa;
+            c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_bi_epel_hv8_8_msa;
+            c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_bi_epel_hv12_8_msa;
+            c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_bi_epel_hv16_8_msa;
+            c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_msa;
+            c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
+            c->put_hevc_qpel_bi_w[3][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
+            c->put_hevc_qpel_bi_w[4][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
+            c->put_hevc_qpel_bi_w[5][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
+            c->put_hevc_qpel_bi_w[6][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
+            c->put_hevc_qpel_bi_w[7][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
+            c->put_hevc_qpel_bi_w[8][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels48_8_msa;
+            c->put_hevc_qpel_bi_w[9][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_qpel_h4_8_msa;
+            c->put_hevc_qpel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_qpel_h8_8_msa;
+            c->put_hevc_qpel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_qpel_h12_8_msa;
+            c->put_hevc_qpel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_qpel_h16_8_msa;
+            c->put_hevc_qpel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_qpel_h24_8_msa;
+            c->put_hevc_qpel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_qpel_h32_8_msa;
+            c->put_hevc_qpel_bi_w[8][0][1] = ff_hevc_put_hevc_bi_w_qpel_h48_8_msa;
+            c->put_hevc_qpel_bi_w[9][0][1] = ff_hevc_put_hevc_bi_w_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_qpel_v4_8_msa;
+            c->put_hevc_qpel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_qpel_v8_8_msa;
+            c->put_hevc_qpel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_qpel_v12_8_msa;
+            c->put_hevc_qpel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_qpel_v16_8_msa;
+            c->put_hevc_qpel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_qpel_v24_8_msa;
+            c->put_hevc_qpel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_qpel_v32_8_msa;
+            c->put_hevc_qpel_bi_w[8][1][0] = ff_hevc_put_hevc_bi_w_qpel_v48_8_msa;
+            c->put_hevc_qpel_bi_w[9][1][0] = ff_hevc_put_hevc_bi_w_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv4_8_msa;
+            c->put_hevc_qpel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv8_8_msa;
+            c->put_hevc_qpel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv12_8_msa;
+            c->put_hevc_qpel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv16_8_msa;
+            c->put_hevc_qpel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv24_8_msa;
+            c->put_hevc_qpel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv32_8_msa;
+            c->put_hevc_qpel_bi_w[8][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv48_8_msa;
+            c->put_hevc_qpel_bi_w[9][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_bi_w[1][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
+            c->put_hevc_epel_bi_w[2][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels6_8_msa;
+            c->put_hevc_epel_bi_w[3][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
+            c->put_hevc_epel_bi_w[4][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
+            c->put_hevc_epel_bi_w[5][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
+            c->put_hevc_epel_bi_w[6][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
+            c->put_hevc_epel_bi_w[7][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_epel_h4_8_msa;
+            c->put_hevc_epel_bi_w[2][0][1] = ff_hevc_put_hevc_bi_w_epel_h6_8_msa;
+            c->put_hevc_epel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_epel_h8_8_msa;
+            c->put_hevc_epel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_epel_h12_8_msa;
+            c->put_hevc_epel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_epel_h16_8_msa;
+            c->put_hevc_epel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_epel_h24_8_msa;
+            c->put_hevc_epel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_epel_h32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_epel_v4_8_msa;
+            c->put_hevc_epel_bi_w[2][1][0] = ff_hevc_put_hevc_bi_w_epel_v6_8_msa;
+            c->put_hevc_epel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_epel_v8_8_msa;
+            c->put_hevc_epel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_epel_v12_8_msa;
+            c->put_hevc_epel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_epel_v16_8_msa;
+            c->put_hevc_epel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_epel_v24_8_msa;
+            c->put_hevc_epel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_epel_v32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_epel_hv4_8_msa;
+            c->put_hevc_epel_bi_w[2][1][1] = ff_hevc_put_hevc_bi_w_epel_hv6_8_msa;
+            c->put_hevc_epel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_epel_hv8_8_msa;
+            c->put_hevc_epel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_epel_hv12_8_msa;
+            c->put_hevc_epel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_epel_hv16_8_msa;
+            c->put_hevc_epel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_epel_hv24_8_msa;
+            c->put_hevc_epel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_epel_hv32_8_msa;
+
+            c->sao_band_filter[0] =
+            c->sao_band_filter[1] =
+            c->sao_band_filter[2] =
+            c->sao_band_filter[3] =
+            c->sao_band_filter[4] = ff_hevc_sao_band_filter_0_8_msa;
+
+            c->sao_edge_filter[0] =
+            c->sao_edge_filter[1] =
+            c->sao_edge_filter[2] =
+            c->sao_edge_filter[3] =
+            c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_msa;
+
+            c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_msa;
+            c->hevc_v_loop_filter_luma = ff_hevc_loop_filter_luma_v_8_msa;
+
+            c->hevc_h_loop_filter_chroma = ff_hevc_loop_filter_chroma_h_8_msa;
+            c->hevc_v_loop_filter_chroma = ff_hevc_loop_filter_chroma_v_8_msa;
+
+            c->hevc_h_loop_filter_luma_c = ff_hevc_loop_filter_luma_h_8_msa;
+            c->hevc_v_loop_filter_luma_c = ff_hevc_loop_filter_luma_v_8_msa;
+
+            c->hevc_h_loop_filter_chroma_c =
+                ff_hevc_loop_filter_chroma_h_8_msa;
+            c->hevc_v_loop_filter_chroma_c =
+                ff_hevc_loop_filter_chroma_v_8_msa;
+
+            c->idct[0] = ff_hevc_idct_4x4_msa;
+            c->idct[1] = ff_hevc_idct_8x8_msa;
+            c->idct[2] = ff_hevc_idct_16x16_msa;
+            c->idct[3] = ff_hevc_idct_32x32_msa;
+            c->idct_dc[0] = ff_hevc_idct_dc_4x4_msa;
+            c->idct_dc[1] = ff_hevc_idct_dc_8x8_msa;
+            c->idct_dc[2] = ff_hevc_idct_dc_16x16_msa;
+            c->idct_dc[3] = ff_hevc_idct_dc_32x32_msa;
+            c->add_residual[0] = ff_hevc_addblk_4x4_msa;
+            c->add_residual[1] = ff_hevc_addblk_8x8_msa;
+            c->add_residual[2] = ff_hevc_addblk_16x16_msa;
+            c->add_residual[3] = ff_hevc_addblk_32x32_msa;
+            c->transform_4x4_luma = ff_hevc_idct_luma_4x4_msa;
+        }
+    }
 }
diff --git a/libavcodec/mips/hevcdsp_mips.h b/libavcodec/mips/hevcdsp_mips.h
index 1573d1c..c84e08d 100644
--- a/libavcodec/mips/hevcdsp_mips.h
+++ b/libavcodec/mips/hevcdsp_mips.h
@@ -479,4 +479,95 @@ void ff_hevc_addblk_32x32_msa(uint8_t *dst, int16_t *pi16Coeffs,
                               ptrdiff_t stride);
 void ff_hevc_idct_luma_4x4_msa(int16_t *pi16Coeffs);
 
+/* Loongson optimization */
+#define L_MC(PEL, DIR, WIDTH, TYPE)                                          \
+void ff_hevc_put_hevc_##PEL##_##DIR##WIDTH##_8_##TYPE(int16_t *dst,          \
+                                                      uint8_t *src,          \
+                                                      ptrdiff_t src_stride,  \
+                                                      int height,            \
+                                                      intptr_t mx,           \
+                                                      intptr_t my,           \
+                                                      int width)
+L_MC(qpel, h, 4, mmi);
+L_MC(qpel, h, 8, mmi);
+L_MC(qpel, h, 12, mmi);
+L_MC(qpel, h, 16, mmi);
+L_MC(qpel, h, 24, mmi);
+L_MC(qpel, h, 32, mmi);
+L_MC(qpel, h, 48, mmi);
+L_MC(qpel, h, 64, mmi);
+
+L_MC(qpel, hv, 4, mmi);
+L_MC(qpel, hv, 8, mmi);
+L_MC(qpel, hv, 12, mmi);
+L_MC(qpel, hv, 16, mmi);
+L_MC(qpel, hv, 24, mmi);
+L_MC(qpel, hv, 32, mmi);
+L_MC(qpel, hv, 48, mmi);
+L_MC(qpel, hv, 64, mmi);
+
+#define L_BI_MC(PEL, DIR, WIDTH, TYPE)                                          \
+void ff_hevc_put_hevc_##PEL##_bi_##DIR##WIDTH##_8_##TYPE(uint8_t *dst,          \
+                                                         ptrdiff_t dst_stride,  \
+                                                         uint8_t *src,          \
+                                                         ptrdiff_t src_stride,  \
+                                                         int16_t *src2,         \
+                                                         int height,            \
+                                                         intptr_t mx,           \
+                                                         intptr_t my,           \
+                                                         int width)
+
+L_BI_MC(pel, pixels, 8, mmi);
+L_BI_MC(pel, pixels, 16, mmi);
+L_BI_MC(pel, pixels, 24, mmi);
+L_BI_MC(pel, pixels, 32, mmi);
+L_BI_MC(pel, pixels, 48, mmi);
+L_BI_MC(pel, pixels, 64, mmi);
+
+L_BI_MC(qpel, hv, 4, mmi);
+L_BI_MC(qpel, hv, 8, mmi);
+L_BI_MC(qpel, hv, 12, mmi);
+L_BI_MC(qpel, hv, 16, mmi);
+L_BI_MC(qpel, hv, 24, mmi);
+L_BI_MC(qpel, hv, 32, mmi);
+L_BI_MC(qpel, hv, 48, mmi);
+L_BI_MC(qpel, hv, 64, mmi);
+
+L_BI_MC(qpel, h, 4, mmi);
+L_BI_MC(qpel, h, 8, mmi);
+L_BI_MC(qpel, h, 12, mmi);
+L_BI_MC(qpel, h, 16, mmi);
+L_BI_MC(qpel, h, 24, mmi);
+L_BI_MC(qpel, h, 32, mmi);
+L_BI_MC(qpel, h, 48, mmi);
+L_BI_MC(qpel, h, 64, mmi);
+
+L_BI_MC(epel, hv, 4, mmi);
+L_BI_MC(epel, hv, 8, mmi);
+L_BI_MC(epel, hv, 12, mmi);
+L_BI_MC(epel, hv, 16, mmi);
+L_BI_MC(epel, hv, 24, mmi);
+L_BI_MC(epel, hv, 32, mmi);
+#undef L_BI_MC
+
+#define L_UNI_MC(PEL, DIR, WIDTH, TYPE)                                         \
+void ff_hevc_put_hevc_##PEL##_uni_##DIR##WIDTH##_8_##TYPE(uint8_t *dst,         \
+                                                          ptrdiff_t dst_stride, \
+                                                          uint8_t *src,         \
+                                                          ptrdiff_t src_stride, \
+                                                          int height,           \
+                                                          intptr_t mx,          \
+                                                          intptr_t my,          \
+                                                          int width)
+
+L_UNI_MC(qpel, hv, 4, mmi);
+L_UNI_MC(qpel, hv, 8, mmi);
+L_UNI_MC(qpel, hv, 12, mmi);
+L_UNI_MC(qpel, hv, 16, mmi);
+L_UNI_MC(qpel, hv, 24, mmi);
+L_UNI_MC(qpel, hv, 32, mmi);
+L_UNI_MC(qpel, hv, 48, mmi);
+L_UNI_MC(qpel, hv, 64, mmi);
+#undef L_UNI_MC
+
 #endif  // #ifndef AVCODEC_MIPS_HEVCDSP_MIPS_H
diff --git a/libavcodec/mips/hevcdsp_mmi.c b/libavcodec/mips/hevcdsp_mmi.c
new file mode 100644
index 0000000..aa83e1f
--- /dev/null
+++ b/libavcodec/mips/hevcdsp_mmi.c
@@ -0,0 +1,1183 @@
+/*
+ * Copyright (c) 2019 Shiyou Yin (yinshiyou-hf@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/hevcdec.h"
+#include "libavcodec/bit_depth_template.c"
+#include "libavcodec/mips/hevcdsp_mips.h"
+#include "libavutil/mips/mmiutils.h"
+
+#define PUT_HEVC_QPEL_H(w, x_step, src_step, dst_step)                   \
+void ff_hevc_put_hevc_qpel_h##w##_8_mmi(int16_t *dst, uint8_t *_src,     \
+                                        ptrdiff_t _srcstride,            \
+                                        int height, intptr_t mx,         \
+                                        intptr_t my, int width)          \
+{                                                                        \
+    int x, y;                                                            \
+    pixel *src = (pixel*)_src - 3;                                       \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                    \
+    uint64_t ftmp[15];                                                   \
+    uint64_t rtmp[1];                                                    \
+    const int8_t *filter = ff_hevc_qpel_filters[mx - 1];                 \
+                                                                         \
+    x = x_step;                                                          \
+    y = height;                                                          \
+    __asm__ volatile(                                                    \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                              \
+        "li           %[rtmp0],      0x08                       \n\t"    \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"    \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"    \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"    \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
+                                                                         \
+        "1:                                                     \n\t"    \
+        "2:                                                     \n\t"    \
+        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp3],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp4],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp4],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp5],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp6],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp6],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp6],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],             \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])            \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"    \
+        "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
+        "gssdlc1      %[ftmp3],      0x07(%[dst])               \n\t"    \
+        "gssdrc1      %[ftmp3],      0x00(%[dst])               \n\t"    \
+                                                                         \
+        "daddi        %[x],          %[x],         -0x01        \n\t"    \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],        0x08        \n\t"    \
+        "bnez         %[x],          2b                         \n\t"    \
+                                                                         \
+        "daddi        %[y],          %[y],         -0x01        \n\t"    \
+        "li           %[x],        " #x_step "                  \n\t"    \
+        PTR_ADDIU    "%[src],        %[src],     " #src_step "  \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"    \
+        PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],        0x80        \n\t"    \
+        "bnez         %[y],          1b                         \n\t"    \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                  \
+          [ftmp10]"=&f"(ftmp[10]), [rtmp0]"=&r"(rtmp[0]),                \
+          [src]"+&r"(src), [dst]"+&r"(dst), [y]"+&r"(y),                 \
+          [x]"+&r"(x)                                                    \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                    \
+        : "memory"                                                       \
+    );                                                                   \
+}
+
+PUT_HEVC_QPEL_H(4, 1, -4, -8);
+PUT_HEVC_QPEL_H(8, 2, -8, -16);
+PUT_HEVC_QPEL_H(12, 3, -12, -24);
+PUT_HEVC_QPEL_H(16, 4, -16, -32);
+PUT_HEVC_QPEL_H(24, 6, -24, -48);
+PUT_HEVC_QPEL_H(32, 8, -32, -64);
+PUT_HEVC_QPEL_H(48, 12, -48, -96);
+PUT_HEVC_QPEL_H(64, 16, -64, -128);
+
+#define PUT_HEVC_QPEL_HV(w, x_step, src_step, dst_step)                  \
+void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
+                                     ptrdiff_t _srcstride,               \
+                                     int height, intptr_t mx,            \
+                                     intptr_t my, int width)             \
+{                                                                        \
+    int x, y;                                                            \
+    const int8_t *filter;                                                \
+    pixel *src = (pixel*)_src;                                           \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                    \
+    int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];         \
+    int16_t *tmp = tmp_array;                                            \
+    uint64_t ftmp[15];                                                   \
+    uint64_t rtmp[1];                                                    \
+                                                                         \
+    src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                        \
+    filter = ff_hevc_qpel_filters[mx - 1];                               \
+    x = x_step;                                                          \
+    y = height + QPEL_EXTRA;                                             \
+    __asm__ volatile(                                                    \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                              \
+        "li           %[rtmp0],      0x08                       \n\t"    \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"    \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"    \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"    \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
+                                                                         \
+        "1:                                                     \n\t"    \
+        "2:                                                     \n\t"    \
+        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"    \
+        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"    \
+        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp3],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp4],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp4],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp5],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        "punpcklbh    %[ftmp7],      %[ftmp6],      %[ftmp0]    \n\t"    \
+        "punpckhbh    %[ftmp8],      %[ftmp6],      %[ftmp0]    \n\t"    \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddh        %[ftmp6],      %[ftmp7],      %[ftmp8]    \n\t"    \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],             \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])            \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"    \
+        "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
+        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"    \
+        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"    \
+                                                                         \
+        "daddi        %[x],          %[x],         -0x01        \n\t"    \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"    \
+        "bnez         %[x],          2b                         \n\t"    \
+                                                                         \
+        "daddi        %[y],          %[y],         -0x01        \n\t"    \
+        "li           %[x],        " #x_step "                  \n\t"    \
+        PTR_ADDIU    "%[src],        %[src],     " #src_step "  \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #dst_step "  \n\t"    \
+        PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "bnez         %[y],          1b                         \n\t"    \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                  \
+          [ftmp10]"=&f"(ftmp[10]), [rtmp0]"=&r"(rtmp[0]),                \
+          [src]"+&r"(src), [tmp]"+&r"(tmp), [y]"+&r"(y),                 \
+          [x]"+&r"(x)                                                    \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                    \
+        : "memory"                                                       \
+    );                                                                   \
+                                                                         \
+    tmp    = tmp_array + QPEL_EXTRA_BEFORE * 4 -12;                      \
+    filter = ff_hevc_qpel_filters[my - 1];                               \
+    x = x_step;                                                          \
+    y = height;                                                          \
+    __asm__ volatile(                                                    \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                              \
+        "li           %[rtmp0],      0x08                       \n\t"    \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"    \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"    \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"    \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"    \
+        "li           %[rtmp0],      0x06                       \n\t"    \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"    \
+                                                                         \
+        "1:                                                     \n\t"    \
+        "2:                                                     \n\t"    \
+        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"    \
+        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"    \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],             \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])         \
+        TRANSPOSE_4H(%[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10],            \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])         \
+        "pmaddhw      %[ftmp11],     %[ftmp3],      %[ftmp1]    \n\t"    \
+        "pmaddhw      %[ftmp12],     %[ftmp7],      %[ftmp2]    \n\t"    \
+        "pmaddhw      %[ftmp13],     %[ftmp4],      %[ftmp1]    \n\t"    \
+        "pmaddhw      %[ftmp14],     %[ftmp8],      %[ftmp2]    \n\t"    \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"    \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"    \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp3], %[ftmp4])           \
+        "paddw        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"    \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"    \
+        "pmaddhw      %[ftmp11],     %[ftmp5],      %[ftmp1]    \n\t"    \
+        "pmaddhw      %[ftmp12],     %[ftmp9],      %[ftmp2]    \n\t"    \
+        "pmaddhw      %[ftmp13],     %[ftmp6],      %[ftmp1]    \n\t"    \
+        "pmaddhw      %[ftmp14],     %[ftmp10],     %[ftmp2]    \n\t"    \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"    \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"    \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp5], %[ftmp6])           \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"    \
+        "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
+        "gssdlc1      %[ftmp3],      0x07(%[dst])               \n\t"    \
+        "gssdrc1      %[ftmp3],      0x00(%[dst])               \n\t"    \
+                                                                         \
+        "daddi        %[x],          %[x],         -0x01        \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],        0x08        \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"    \
+        "bnez         %[x],          2b                         \n\t"    \
+                                                                         \
+        "daddi        %[y],          %[y],         -0x01        \n\t"    \
+        "li           %[x],        " #x_step "                  \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #dst_step "  \n\t"    \
+        PTR_ADDIU    "%[dst],        %[dst],        0x80        \n\t"    \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
+        "bnez         %[y],          1b                         \n\t"    \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                  \
+          [ftmp10]"=&f"(ftmp[10]), [ftmp11]"=&f"(ftmp[11]),              \
+          [ftmp12]"=&f"(ftmp[12]), [ftmp13]"=&f"(ftmp[13]),              \
+          [ftmp14]"=&f"(ftmp[14]), [rtmp0]"=&r"(rtmp[0]),                \
+          [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y),                 \
+          [x]"+&r"(x)                                                    \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                    \
+        : "memory"                                                       \
+    );                                                                   \
+}
+
+PUT_HEVC_QPEL_HV(4, 1, -4, -8);
+PUT_HEVC_QPEL_HV(8, 2, -8, -16);
+PUT_HEVC_QPEL_HV(12, 3, -12, -24);
+PUT_HEVC_QPEL_HV(16, 4, -16, -32);
+PUT_HEVC_QPEL_HV(24, 6, -24, -48);
+PUT_HEVC_QPEL_HV(32, 8, -32, -64);
+PUT_HEVC_QPEL_HV(48, 12, -48, -96);
+PUT_HEVC_QPEL_HV(64, 16, -64, -128);
+
+#define PUT_HEVC_QPEL_BI_H(w, x_step, src_step, src2_step, dst_step)    \
+void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
+                                           ptrdiff_t _dststride,        \
+                                           uint8_t *_src,               \
+                                           ptrdiff_t _srcstride,        \
+                                           int16_t *src2, int height,   \
+                                           intptr_t mx, intptr_t my,    \
+                                           int width)                   \
+{                                                                       \
+    int x, y;                                                           \
+    pixel        *src       = (pixel*)_src - 3;                         \
+    ptrdiff_t     srcstride = _srcstride / sizeof(pixel);               \
+    pixel *dst          = (pixel *)_dst;                                \
+    ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
+    const int8_t *filter    = ff_hevc_qpel_filters[mx - 1];             \
+    uint64_t ftmp[20];                                                  \
+    uint64_t rtmp[1];                                                   \
+    int shift = 7;                                                      \
+    int offset = 64;                                                    \
+                                                                        \
+    x = width >> 2;                                                     \
+    y = height;                                                         \
+    __asm__ volatile(                                                   \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+        "punpcklhw    %[offset],     %[offset],     %[offset]   \n\t"   \
+        "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp4],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp6],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])           \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp3],      %[offset]   \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
+        "li           %[rtmp0],      0x10                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
+        "punpcklhw    %[ftmp5],      %[ftmp0],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp6],      %[ftmp0],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp3],      %[ftmp0],      %[ftmp4]    \n\t"   \
+        "punpcklhw    %[ftmp4],      %[ftmp0],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp4],      %[ftmp4],      %[ftmp8]    \n\t"   \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp4]    \n\t"   \
+        "paddw        %[ftmp6],      %[ftmp6],      %[ftmp3]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[shift]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
+        "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
+        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
+        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],        0x04        \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x08        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],     " #src_step "  \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],    " #src2_step " \n\t"   \
+        PTR_ADDU     "%[src],        %[src],    %[src_stride]   \n\t"   \
+        PTR_ADDU     "%[dst],        %[dst],    %[dst_stride]   \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [ftmp11]"=&f"(ftmp[11]),             \
+          [ftmp12]"=&f"(ftmp[12]), [src2]"+&r"(src2),                   \
+          [dst]"+&r"(dst), [src]"+&r"(src), [y]"+&r"(y), [x]"=&r"(x),   \
+          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+        : [src_stride]"r"(srcstride), [dst_stride]"r"(dststride),       \
+          [filter]"r"(filter), [shift]"f"(shift)                        \
+        : "memory"                                                      \
+    );                                                                  \
+}
+
+PUT_HEVC_QPEL_BI_H(4, 1, -4, -8, -4);
+PUT_HEVC_QPEL_BI_H(8, 2, -8, -16, -8);
+PUT_HEVC_QPEL_BI_H(12, 3, -12, -24, -12);
+PUT_HEVC_QPEL_BI_H(16, 4, -16, -32, -16);
+PUT_HEVC_QPEL_BI_H(24, 6, -24, -48, -24);
+PUT_HEVC_QPEL_BI_H(32, 8, -32, -64, -32);
+PUT_HEVC_QPEL_BI_H(48, 12, -48, -96, -48);
+PUT_HEVC_QPEL_BI_H(64, 16, -64, -128, -64);
+
+#define PUT_HEVC_QPEL_BI_HV(w, x_step, src_step, src2_step, dst_step)   \
+void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
+                                            ptrdiff_t _dststride,       \
+                                            uint8_t *_src,              \
+                                            ptrdiff_t _srcstride,       \
+                                            int16_t *src2, int height,  \
+                                            intptr_t mx, intptr_t my,   \
+                                            int width)                  \
+{                                                                       \
+    int x, y;                                                           \
+    const int8_t *filter;                                               \
+    pixel *src = (pixel*)_src;                                          \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                   \
+    pixel *dst          = (pixel *)_dst;                                \
+    ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
+    int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];        \
+    int16_t *tmp = tmp_array;                                           \
+    uint64_t ftmp[20];                                                  \
+    uint64_t rtmp[1];                                                   \
+    int shift = 7;                                                      \
+    int offset = 64;                                                    \
+                                                                        \
+    src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                       \
+    filter = ff_hevc_qpel_filters[mx - 1];                              \
+    x = width >> 2;                                                     \
+    y = height + QPEL_EXTRA;                                            \
+    __asm__ volatile(                                                   \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp4],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp6],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])           \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
+        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],      " #src_step " \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #src2_step " \n\t"   \
+        PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [rtmp0]"=&r"(rtmp[0]),               \
+          [src]"+&r"(src), [tmp]"+&r"(tmp), [y]"+&r"(y),                \
+          [x]"+&r"(x)                                                   \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                   \
+        : "memory"                                                      \
+    );                                                                  \
+                                                                        \
+    tmp    = tmp_array;                                                 \
+    filter = ff_hevc_qpel_filters[my - 1];                              \
+    x = width >> 2;                                                     \
+    y = height;                                                         \
+    __asm__ volatile(                                                   \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "li           %[rtmp0],      0x06                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
+        TRANSPOSE_4H(%[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10],           \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
+        "pmaddhw      %[ftmp11],     %[ftmp3],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp12],     %[ftmp7],      %[ftmp2]    \n\t"   \
+        "pmaddhw      %[ftmp13],     %[ftmp4],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp14],     %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"   \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"   \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp3], %[ftmp4])          \
+        "paddw        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmaddhw      %[ftmp11],     %[ftmp5],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp12],     %[ftmp9],      %[ftmp2]    \n\t"   \
+        "pmaddhw      %[ftmp13],     %[ftmp6],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp14],     %[ftmp10],     %[ftmp2]    \n\t"   \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"   \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"   \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp5], %[ftmp6])          \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
+        "xor          %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
+        "li           %[rtmp0],      0x10                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
+        "punpcklhw    %[ftmp5],      %[ftmp7],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp6],      %[ftmp7],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp3],      %[ftmp7],      %[ftmp4]    \n\t"   \
+        "punpcklhw    %[ftmp4],      %[ftmp7],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp4],      %[ftmp4],      %[ftmp8]    \n\t"   \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp4]    \n\t"   \
+        "paddw        %[ftmp6],      %[ftmp6],      %[ftmp3]    \n\t"   \
+        "paddw        %[ftmp5],      %[ftmp5],      %[offset]   \n\t"   \
+        "paddw        %[ftmp6],      %[ftmp6],      %[offset]   \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[shift]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
+        "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
+        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
+        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x08        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],        0x04        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],    " #src2_step " \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #src2_step " \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"   \
+        PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [ftmp11]"=&f"(ftmp[11]),             \
+          [ftmp12]"=&f"(ftmp[12]), [ftmp13]"=&f"(ftmp[13]),             \
+          [ftmp14]"=&f"(ftmp[14]), [src2]"+&r"(src2),                   \
+          [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
+          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+        : [filter]"r"(filter), [stride]"r"(dststride),                  \
+          [shift]"f"(shift)                                             \
+        : "memory"                                                      \
+    );                                                                  \
+}
+
+PUT_HEVC_QPEL_BI_HV(4, 1, -4, -8, -4);
+PUT_HEVC_QPEL_BI_HV(8, 2, -8, -16, -8);
+PUT_HEVC_QPEL_BI_HV(12, 3, -12, -24, -12);
+PUT_HEVC_QPEL_BI_HV(16, 4, -16, -32, -16);
+PUT_HEVC_QPEL_BI_HV(24, 6, -24, -48, -24);
+PUT_HEVC_QPEL_BI_HV(32, 8, -32, -64, -32);
+PUT_HEVC_QPEL_BI_HV(48, 12, -48, -96, -48);
+PUT_HEVC_QPEL_BI_HV(64, 16, -64, -128, -64);
+
+#define PUT_HEVC_EPEL_BI_HV(w, x_step, src_step, src2_step, dst_step)   \
+void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
+                                            ptrdiff_t _dststride,       \
+                                            uint8_t *_src,              \
+                                            ptrdiff_t _srcstride,       \
+                                            int16_t *src2, int height,  \
+                                            intptr_t mx, intptr_t my,   \
+                                            int width)                  \
+{                                                                       \
+    int x, y;                                                           \
+    pixel *src = (pixel *)_src;                                         \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                   \
+    pixel *dst          = (pixel *)_dst;                                \
+    ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
+    const int8_t *filter = ff_hevc_epel_filters[mx - 1];                \
+    int16_t tmp_array[(MAX_PB_SIZE + EPEL_EXTRA) * MAX_PB_SIZE];        \
+    int16_t *tmp = tmp_array;                                           \
+    uint64_t ftmp[12];                                                  \
+    uint64_t rtmp[1];                                                   \
+    int shift = 7;                                                      \
+    int offset = 64;                                                    \
+                                                                        \
+    src -= (EPEL_EXTRA_BEFORE * srcstride + 1);                         \
+    x = width >> 2;                                                     \
+    y = height + EPEL_EXTRA;                                            \
+    __asm__ volatile(                                                   \
+        MMI_LWC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "2:                                                     \n\t"   \
+        "gslwlc1      %[ftmp2],      0x03(%[src])               \n\t"   \
+        "gslwrc1      %[ftmp2],      0x00(%[src])               \n\t"   \
+        "gslwlc1      %[ftmp3],      0x04(%[src])               \n\t"   \
+        "gslwrc1      %[ftmp3],      0x01(%[src])               \n\t"   \
+        "gslwlc1      %[ftmp4],      0x05(%[src])               \n\t"   \
+        "gslwrc1      %[ftmp4],      0x02(%[src])               \n\t"   \
+        "gslwlc1      %[ftmp5],      0x06(%[src])               \n\t"   \
+        "gslwrc1      %[ftmp5],      0x03(%[src])               \n\t"   \
+        "punpcklbh    %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp2],      %[ftmp2],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp3],      %[ftmp3],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp4],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp4],      %[ftmp4],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp5],      %[ftmp5],      %[ftmp1]    \n\t"   \
+        TRANSPOSE_4H(%[ftmp2], %[ftmp3], %[ftmp4], %[ftmp5],            \
+                     %[ftmp6], %[ftmp7], %[ftmp8], %[ftmp9])            \
+        "paddh        %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"   \
+        "paddh        %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"   \
+        "paddh        %[ftmp2],      %[ftmp2],      %[ftmp4]    \n\t"   \
+        "gssdlc1      %[ftmp2],      0x07(%[tmp])               \n\t"   \
+        "gssdrc1      %[ftmp2],      0x00(%[tmp])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],      " #src_step " \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #src2_step " \n\t"   \
+        PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [rtmp0]"=&r"(rtmp[0]),                                        \
+          [src]"+&r"(src), [tmp]"+&r"(tmp), [y]"+&r"(y),                \
+          [x]"+&r"(x)                                                   \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                   \
+        : "memory"                                                      \
+    );                                                                  \
+                                                                        \
+    tmp      = tmp_array;                                               \
+    filter = ff_hevc_epel_filters[my - 1];                              \
+    x = width >> 2;                                                     \
+    y = height;                                                         \
+    __asm__ volatile(                                                   \
+        MMI_LWC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "li           %[rtmp0],      0x06                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
+        "xor          %[ftmp2],      %[ftmp2],      %[ftmp2]    \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],       -0x180       \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])           \
+        "pmaddhw      %[ftmp7],      %[ftmp3],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp8],      %[ftmp4],      %[ftmp1]    \n\t"   \
+        TRANSPOSE_2W(%[ftmp7], %[ftmp8], %[ftmp3], %[ftmp4])            \
+        "paddw        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmaddhw      %[ftmp7],      %[ftmp5],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp8],      %[ftmp6],      %[ftmp1]    \n\t"   \
+        TRANSPOSE_2W(%[ftmp7], %[ftmp8], %[ftmp5], %[ftmp6])            \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
+        "li           %[rtmp0],      0x10                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
+        "punpcklhw    %[ftmp5],      %[ftmp2],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp6],      %[ftmp2],      %[ftmp3]    \n\t"   \
+        "punpckhhw    %[ftmp3],      %[ftmp2],      %[ftmp4]    \n\t"   \
+        "punpcklhw    %[ftmp4],      %[ftmp2],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp8]    \n\t"   \
+        "psraw        %[ftmp4],      %[ftmp4],      %[ftmp8]    \n\t"   \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp4]    \n\t"   \
+        "paddw        %[ftmp6],      %[ftmp6],      %[ftmp3]    \n\t"   \
+        "paddw        %[ftmp5],      %[ftmp5],      %[offset]   \n\t"   \
+        "paddw        %[ftmp6],      %[ftmp6],      %[offset]   \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[shift]    \n\t"   \
+        "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
+        "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp2]    \n\t"   \
+        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
+        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
+        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x08        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],        0x04        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],    " #src2_step " \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #src2_step " \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"   \
+        PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"   \
+        PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [src2]"+&r"(src2),                   \
+          [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
+          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+        : [filter]"r"(filter), [stride]"r"(dststride),                  \
+          [shift]"f"(shift)                                             \
+        : "memory"                                                      \
+    );                                                                  \
+}
+
+PUT_HEVC_EPEL_BI_HV(4, 1, -4, -8, -4);
+PUT_HEVC_EPEL_BI_HV(8, 2, -8, -16, -8);
+PUT_HEVC_EPEL_BI_HV(12, 3, -12, -24, -12);
+PUT_HEVC_EPEL_BI_HV(16, 4, -16, -32, -16);
+PUT_HEVC_EPEL_BI_HV(24, 6, -24, -48, -24);
+PUT_HEVC_EPEL_BI_HV(32, 8, -32, -64, -32);
+
+#define PUT_HEVC_PEL_BI_PIXELS(w, x_step, src_step, dst_step, src2_step)  \
+void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
+                                               ptrdiff_t _dststride,      \
+                                               uint8_t *_src,             \
+                                               ptrdiff_t _srcstride,      \
+                                               int16_t *src2, int height, \
+                                               intptr_t mx, intptr_t my,  \
+                                               int width)                 \
+{                                                                         \
+    int x, y;                                                             \
+    pixel *src          = (pixel *)_src;                                  \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                     \
+    pixel *dst          = (pixel *)_dst;                                  \
+    ptrdiff_t dststride = _dststride / sizeof(pixel);                     \
+    uint64_t ftmp[12];                                                    \
+    uint64_t rtmp[1];                                                     \
+    int shift = 7;                                                        \
+                                                                          \
+    y = height;                                                           \
+    x = width >> 3;                                                       \
+    __asm__ volatile(                                                     \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"     \
+        "li           %[rtmp0],      0x06                       \n\t"     \
+        "dmtc1        %[rtmp0],      %[ftmp1]                   \n\t"     \
+        "li           %[rtmp0],      0x10                       \n\t"     \
+        "dmtc1        %[rtmp0],      %[ftmp10]                  \n\t"     \
+        "li           %[rtmp0],      0x40                       \n\t"     \
+        "dmtc1        %[rtmp0],      %[offset]                  \n\t"     \
+        "punpcklhw    %[offset],     %[offset],     %[offset]   \n\t"     \
+        "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"     \
+                                                                          \
+        "1:                                                     \n\t"     \
+        "2:                                                     \n\t"     \
+        "gsldlc1      %[ftmp5],      0x07(%[src])               \n\t"     \
+        "gsldrc1      %[ftmp5],      0x00(%[src])               \n\t"     \
+        "gsldlc1      %[ftmp2],      0x07(%[src2])              \n\t"     \
+        "gsldrc1      %[ftmp2],      0x00(%[src2])              \n\t"     \
+        "gsldlc1      %[ftmp3],      0x0f(%[src2])              \n\t"     \
+        "gsldrc1      %[ftmp3],      0x08(%[src2])              \n\t"     \
+        "punpcklbh    %[ftmp4],      %[ftmp5],      %[ftmp0]    \n\t"     \
+        "punpckhbh    %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"     \
+        "psllh        %[ftmp4],      %[ftmp4],      %[ftmp1]    \n\t"     \
+        "psllh        %[ftmp5],      %[ftmp5],      %[ftmp1]    \n\t"     \
+        "paddh        %[ftmp4],      %[ftmp4],      %[offset]   \n\t"     \
+        "paddh        %[ftmp5],      %[ftmp5],      %[offset]   \n\t"     \
+        "punpcklhw    %[ftmp6],      %[ftmp4],      %[ftmp0]    \n\t"     \
+        "punpckhhw    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"     \
+        "punpcklhw    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"     \
+        "punpckhhw    %[ftmp9],      %[ftmp5],      %[ftmp0]    \n\t"     \
+        "punpcklhw    %[ftmp4],      %[ftmp0],      %[ftmp3]    \n\t"     \
+        "punpckhhw    %[ftmp5],      %[ftmp0],      %[ftmp3]    \n\t"     \
+        "punpckhhw    %[ftmp3],      %[ftmp0],      %[ftmp2]    \n\t"     \
+        "punpcklhw    %[ftmp2],      %[ftmp0],      %[ftmp2]    \n\t"     \
+        "psraw        %[ftmp2],      %[ftmp2],      %[ftmp10]   \n\t"     \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp10]   \n\t"     \
+        "psraw        %[ftmp4],      %[ftmp4],      %[ftmp10]   \n\t"     \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp10]   \n\t"     \
+        "paddw        %[ftmp2],      %[ftmp2],      %[ftmp6]    \n\t"     \
+        "paddw        %[ftmp3],      %[ftmp3],      %[ftmp7]    \n\t"     \
+        "paddw        %[ftmp4],      %[ftmp4],      %[ftmp8]    \n\t"     \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp9]    \n\t"     \
+        "psraw        %[ftmp2],      %[ftmp2],      %[shift]    \n\t"     \
+        "psraw        %[ftmp3],      %[ftmp3],      %[shift]    \n\t"     \
+        "psraw        %[ftmp4],      %[ftmp4],      %[shift]    \n\t"     \
+        "psraw        %[ftmp5],      %[ftmp5],      %[shift]    \n\t"     \
+        "packsswh     %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"     \
+        "packsswh     %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"     \
+        "pcmpgth      %[ftmp3],      %[ftmp2],      %[ftmp0]    \n\t"     \
+        "pcmpgth      %[ftmp5],      %[ftmp4],      %[ftmp0]    \n\t"     \
+        "and          %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"     \
+        "and          %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"     \
+        "packushb     %[ftmp2],      %[ftmp2],      %[ftmp4]    \n\t"     \
+        "gssdlc1      %[ftmp2],      0x07(%[dst])               \n\t"     \
+        "gssdrc1      %[ftmp2],      0x00(%[dst])               \n\t"     \
+                                                                          \
+        "daddi        %[x],          %[x],         -0x01        \n\t"     \
+        PTR_ADDIU    "%[src],        %[src],        0x08        \n\t"     \
+        PTR_ADDIU    "%[dst],        %[dst],        0x08        \n\t"     \
+        PTR_ADDIU    "%[src2],       %[src2],       0x10        \n\t"     \
+        "bnez         %[x],          2b                         \n\t"     \
+                                                                          \
+        PTR_ADDIU    "%[src],        %[src],     " #src_step "  \n\t"     \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"     \
+        PTR_ADDIU    "%[src2],       %[src2],    " #src2_step " \n\t"     \
+        "li           %[x],        " #x_step "                  \n\t"     \
+        "daddi        %[y],          %[y],         -0x01        \n\t"     \
+        PTR_ADDU     "%[src],        %[src],       %[srcstride] \n\t"     \
+        PTR_ADDU     "%[dst],        %[dst],       %[dststride] \n\t"     \
+        PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"     \
+        "bnez         %[y],          1b                         \n\t"     \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                   \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                   \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                   \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                   \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                   \
+          [ftmp10]"=&f"(ftmp[10]), [offset]"=&f"(ftmp[11]),               \
+          [src2]"+&r"(src2), [dst]"+&r"(dst), [src]"+&r"(src),            \
+          [x]"+&r"(x), [y]"+&r"(y), [rtmp0]"=&r"(rtmp[0])                 \
+        : [dststride]"r"(dststride), [shift]"f"(shift),                   \
+          [srcstride]"r"(srcstride)                                       \
+        : "memory"                                                        \
+    );                                                                    \
+}                                                                         \
+
+PUT_HEVC_PEL_BI_PIXELS(8, 1, -8, -8, -16);
+PUT_HEVC_PEL_BI_PIXELS(16, 2, -16, -16, -32);
+PUT_HEVC_PEL_BI_PIXELS(24, 3, -24, -24, -48);
+PUT_HEVC_PEL_BI_PIXELS(32, 4, -32, -32, -64);
+PUT_HEVC_PEL_BI_PIXELS(48, 6, -48, -48, -96);
+PUT_HEVC_PEL_BI_PIXELS(64, 8, -64, -64, -128);
+
+#define PUT_HEVC_QPEL_UNI_HV(w, x_step, src_step, dst_step, tmp_step)   \
+void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
+                                             ptrdiff_t _dststride,      \
+                                             uint8_t *_src,             \
+                                             ptrdiff_t _srcstride,      \
+                                             int height,                \
+                                             intptr_t mx, intptr_t my,  \
+                                             int width)                 \
+{                                                                       \
+    int x, y;                                                           \
+    const int8_t *filter;                                               \
+    pixel *src = (pixel*)_src;                                          \
+    ptrdiff_t srcstride = _srcstride / sizeof(pixel);                   \
+    pixel *dst          = (pixel *)_dst;                                \
+    ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
+    int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];        \
+    int16_t *tmp = tmp_array;                                           \
+    uint64_t ftmp[20];                                                  \
+    uint64_t rtmp[1];                                                   \
+    int shift = 6;                                                      \
+    int offset = 32;                                                    \
+                                                                        \
+    src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                       \
+    filter = ff_hevc_qpel_filters[mx - 1];                              \
+    x = width >> 2;                                                     \
+    y = height + QPEL_EXTRA;                                            \
+    __asm__ volatile(                                                   \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
+        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp4],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp4],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        "punpcklbh    %[ftmp7],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "punpckhbh    %[ftmp8],      %[ftmp6],      %[ftmp0]    \n\t"   \
+        "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
+        "pmullh       %[ftmp8],      %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddh        %[ftmp6],      %[ftmp7],      %[ftmp8]    \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])           \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
+        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        PTR_ADDIU    "%[src],        %[src],      " #src_step " \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],      " #tmp_step " \n\t"   \
+        PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [rtmp0]"=&r"(rtmp[0]),               \
+          [src]"+&r"(src), [tmp]"+&r"(tmp), [y]"+&r"(y),                \
+          [x]"+&r"(x)                                                   \
+        : [filter]"r"(filter), [stride]"r"(srcstride)                   \
+        : "memory"                                                      \
+    );                                                                  \
+                                                                        \
+    tmp    = tmp_array;                                                 \
+    filter = ff_hevc_qpel_filters[my - 1];                              \
+    x = width >> 2;                                                     \
+    y = height;                                                         \
+    __asm__ volatile(                                                   \
+        MMI_LDC1(%[ftmp1], %[filter], 0x00)                             \
+        "li           %[rtmp0],      0x08                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpckhbh    %[ftmp2],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
+        "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
+        "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
+        "li           %[rtmp0],      0x06                       \n\t"   \
+        "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
+        "punpcklhw    %[offset],     %[offset],     %[offset]   \n\t"   \
+        "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
+                                                                        \
+        "1:                                                     \n\t"   \
+        "li           %[x],        " #x_step "                  \n\t"   \
+        "2:                                                     \n\t"   \
+        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"   \
+        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"   \
+        TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
+        TRANSPOSE_4H(%[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10],           \
+                     %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
+        "pmaddhw      %[ftmp11],     %[ftmp3],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp12],     %[ftmp7],      %[ftmp2]    \n\t"   \
+        "pmaddhw      %[ftmp13],     %[ftmp4],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp14],     %[ftmp8],      %[ftmp2]    \n\t"   \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"   \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"   \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp3], %[ftmp4])          \
+        "paddw        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
+        "psraw        %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"   \
+        "pmaddhw      %[ftmp11],     %[ftmp5],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp12],     %[ftmp9],      %[ftmp2]    \n\t"   \
+        "pmaddhw      %[ftmp13],     %[ftmp6],      %[ftmp1]    \n\t"   \
+        "pmaddhw      %[ftmp14],     %[ftmp10],     %[ftmp2]    \n\t"   \
+        "paddw        %[ftmp11],     %[ftmp11],     %[ftmp12]   \n\t"   \
+        "paddw        %[ftmp13],     %[ftmp13],     %[ftmp14]   \n\t"   \
+        TRANSPOSE_2W(%[ftmp11], %[ftmp13], %[ftmp5], %[ftmp6])          \
+        "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
+        "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
+        "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
+        "paddh        %[ftmp3],      %[ftmp3],      %[offset]   \n\t"   \
+        "psrah        %[ftmp3],      %[ftmp3],      %[shift]    \n\t"   \
+        "xor          %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
+        "pcmpgth      %[ftmp7],      %[ftmp3],      %[ftmp7]    \n\t"   \
+        "and          %[ftmp3],      %[ftmp3],      %[ftmp7]    \n\t"   \
+        "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
+        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
+        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+                                                                        \
+        "daddi        %[x],          %[x],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],        0x04        \n\t"   \
+        "bnez         %[x],          2b                         \n\t"   \
+                                                                        \
+        "daddi        %[y],          %[y],         -0x01        \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],     " #tmp_step "  \n\t"   \
+        PTR_ADDIU    "%[dst],        %[dst],     " #dst_step "  \n\t"   \
+        PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
+        PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
+        "bnez         %[y],          1b                         \n\t"   \
+        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+          [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
+          [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
+          [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
+          [ftmp10]"=&f"(ftmp[10]), [ftmp11]"=&f"(ftmp[11]),             \
+          [ftmp12]"=&f"(ftmp[12]), [ftmp13]"=&f"(ftmp[13]),             \
+          [ftmp14]"=&f"(ftmp[14]),                                      \
+          [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
+          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+        : [filter]"r"(filter), [stride]"r"(dststride),                  \
+          [shift]"f"(shift)                                             \
+        : "memory"                                                      \
+    );                                                                  \
+}
+
+PUT_HEVC_QPEL_UNI_HV(4, 1, -4, -4, -8);
+PUT_HEVC_QPEL_UNI_HV(8, 2, -8, -8, -16);
+PUT_HEVC_QPEL_UNI_HV(12, 3, -12, -12, -24);
+PUT_HEVC_QPEL_UNI_HV(16, 4, -16, -16, -32);
+PUT_HEVC_QPEL_UNI_HV(24, 6, -24, -24, -48);
+PUT_HEVC_QPEL_UNI_HV(32, 8, -32, -32, -64);
+PUT_HEVC_QPEL_UNI_HV(48, 12, -48, -48, -96);
+PUT_HEVC_QPEL_UNI_HV(64, 16, -64, -64, -128);
diff --git a/libavcodec/mips/hevcdsp_msa.c b/libavcodec/mips/hevcdsp_msa.c
index 81db62b..2c57ec8 100644
--- a/libavcodec/mips/hevcdsp_msa.c
+++ b/libavcodec/mips/hevcdsp_msa.c
@@ -44,7 +44,7 @@ static void hevc_copy_4w_msa(uint8_t *src, int32_t src_stride,
         src0 = (v16i8) __msa_ilvr_w((v4i32) src1, (v4i32) src0);
         in0 = (v8i16) __msa_ilvr_b(zero, src0);
         in0 <<= 6;
-        ST8x2_UB(in0, dst, 2 * dst_stride);
+        ST_D2(in0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
         v16i8 src0, src1, src2, src3;
         v8i16 in0, in1;
@@ -55,7 +55,7 @@ static void hevc_copy_4w_msa(uint8_t *src, int32_t src_stride,
         ILVR_B2_SH(zero, src0, zero, src1, in0, in1);
         in0 <<= 6;
         in1 <<= 6;
-        ST8x4_UB(in0, in1, dst, 2 * dst_stride);
+        ST_D4(in0, in1, 0, 1, 0, 1, dst, dst_stride);
     } else if (0 == height % 8) {
         v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
         v8i16 in0, in1, in2, in3;
@@ -71,7 +71,7 @@ static void hevc_copy_4w_msa(uint8_t *src, int32_t src_stride,
             ILVR_B4_SH(zero, src0, zero, src1, zero, src2, zero, src3,
                        in0, in1, in2, in3);
             SLLI_4V(in0, in1, in2, in3, 6);
-            ST8x8_UB(in0, in1, in2, in3, dst, 2 * dst_stride);
+            ST_D8(in0, in1, in2, in3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
             dst += (8 * dst_stride);
         }
     }
@@ -183,7 +183,7 @@ static void hevc_copy_12w_msa(uint8_t *src, int32_t src_stride,
         in0 <<= 6;
         in1 <<= 6;
         ST_SH4(in0_r, in1_r, in2_r, in3_r, dst, dst_stride);
-        ST8x4_UB(in0, in1, dst + 8, 2 * dst_stride);
+        ST_D4(in0, in1, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         ILVR_B4_SH(zero, src4, zero, src5, zero, src6, zero, src7,
@@ -194,7 +194,7 @@ static void hevc_copy_12w_msa(uint8_t *src, int32_t src_stride,
         in0 <<= 6;
         in1 <<= 6;
         ST_SH4(in0_r, in1_r, in2_r, in3_r, dst, dst_stride);
-        ST8x4_UB(in0, in1, dst + 8, 2 * dst_stride);
+        ST_D4(in0, in1, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -495,7 +495,7 @@ static void hevc_hz_8t_4w_msa(uint8_t *src, int32_t src_stride,
         DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3,
                      dst3, dst3, dst3, dst3);
 
-        ST8x8_UB(dst0, dst1, dst2, dst3, dst, 2 * dst_stride);
+        ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -1047,7 +1047,7 @@ static void hevc_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
         DPADD_SB4_SH(src8776, src10998, src12111110, src14131312,
                      filt0, filt1, filt2, filt3, dst76, dst76, dst76, dst76);
 
-        ST8x8_UB(dst10, dst32, dst54, dst76, dst, 2 * dst_stride);
+        ST_D8(dst10, dst32, dst54, dst76, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
 
         src2110 = src10998;
@@ -1191,7 +1191,7 @@ static void hevc_vt_8t_12w_msa(uint8_t *src, int32_t src_stride,
                      dst1_l, dst1_l, dst1_l, dst1_l);
 
         ST_SH4(dst0_r, dst1_r, dst2_r, dst3_r, dst, dst_stride);
-        ST8x4_UB(dst0_l, dst1_l, dst + 8, 2 * dst_stride);
+        ST_D4(dst0_l, dst1_l, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -1363,7 +1363,6 @@ static void hevc_hv_8t_4w_msa(uint8_t *src, int32_t src_stride,
                               int32_t height)
 {
     uint32_t loop_cnt;
-    int32_t dst_stride_in_bytes = 2 * dst_stride;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v8i16 filt0, filt1, filt2, filt3;
     v8i16 filt_h0, filt_h1, filt_h2, filt_h3;
@@ -1452,7 +1451,7 @@ static void hevc_hv_8t_4w_msa(uint8_t *src, int32_t src_stride,
                                 filt_h0, filt_h1, filt_h2, filt_h3);
         SRA_4V(dst0_r, dst1_r, dst2_r, dst3_r, 6);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
-        ST8x4_UB(dst0_r, dst2_r, dst, dst_stride_in_bytes);
+        ST_D4(dst0_r, dst2_r, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -1607,7 +1606,6 @@ static void hevc_hv_8t_12w_msa(uint8_t *src, int32_t src_stride,
                                int32_t height)
 {
     uint32_t loop_cnt;
-    int32_t dst_stride_in_bytes = 2 * dst_stride;
     uint8_t *src_tmp;
     int16_t *dst_tmp;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
@@ -1784,7 +1782,7 @@ static void hevc_hv_8t_12w_msa(uint8_t *src, int32_t src_stride,
                                 filt_h1, filt_h2, filt_h3);
         SRA_4V(dst0_r, dst1_r, dst2_r, dst3_r, 6);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
-        ST8x4_UB(dst0_r, dst2_r, dst, dst_stride_in_bytes);
+        ST_D4(dst0_r, dst2_r, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         dst10_r = dst54_r;
@@ -1872,7 +1870,7 @@ static void hevc_hz_4t_4x2_msa(uint8_t *src,
     dst0 = const_vec;
     DPADD_SB2_SH(vec0, vec1, filt0, filt1, dst0, dst0);
 
-    ST8x2_UB(dst0, dst, 2 * dst_stride);
+    ST_D2(dst0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_4t_4x4_msa(uint8_t *src,
@@ -1909,7 +1907,7 @@ static void hevc_hz_4t_4x4_msa(uint8_t *src,
     dst1 = const_vec;
     DPADD_SB2_SH(vec0, vec1, filt0, filt1, dst1, dst1);
 
-    ST8x4_UB(dst0, dst1, dst, 2 * dst_stride);
+    ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hz_4t_4x8multiple_msa(uint8_t *src,
@@ -1956,7 +1954,7 @@ static void hevc_hz_4t_4x8multiple_msa(uint8_t *src,
         dst3 = const_vec;
         DPADD_SB2_SH(vec0, vec1, filt0, filt1, dst3, dst3);
 
-        ST8x8_UB(dst0, dst1, dst2, dst3, dst, 2 * dst_stride);
+        ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
     }
 }
@@ -2218,7 +2216,7 @@ static void hevc_hz_4t_12w_msa(uint8_t *src,
         DPADD_SB2_SH(vec0, vec1, filt0, filt1, dst5, dst5);
 
         ST_SH4(dst0, dst1, dst2, dst3, dst, dst_stride);
-        ST8x4_UB(dst4, dst5, dst + 8, 2 * dst_stride);
+        ST_D4(dst4, dst5, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -2478,7 +2476,7 @@ static void hevc_vt_4t_4x2_msa(uint8_t *src,
     dst10 = const_vec;
     DPADD_SB2_SH(src2110, src4332, filt0, filt1, dst10, dst10);
 
-    ST8x2_UB(dst10, dst, 2 * dst_stride);
+    ST_D2(dst10, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_4t_4x4_msa(uint8_t *src,
@@ -2515,7 +2513,7 @@ static void hevc_vt_4t_4x4_msa(uint8_t *src,
     dst32 = const_vec;
     DPADD_SB2_SH(src4332, src6554, filt0, filt1, dst32, dst32);
 
-    ST8x4_UB(dst10, dst32, dst, 2 * dst_stride);
+    ST_D4(dst10, dst32, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_4t_4x8_msa(uint8_t *src,
@@ -2564,8 +2562,7 @@ static void hevc_vt_4t_4x8_msa(uint8_t *src,
     DPADD_SB2_SH(src4332, src6554, filt0, filt1, dst32, dst32);
     DPADD_SB2_SH(src6554, src8776, filt0, filt1, dst54, dst54);
     DPADD_SB2_SH(src8776, src10998, filt0, filt1, dst76, dst76);
-    ST8x8_UB(dst10, dst32, dst54, dst76, dst, 2 * dst_stride);
-    dst += (8 * dst_stride);
+    ST_D8(dst10, dst32, dst54, dst76, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_4t_4x16_msa(uint8_t *src, int32_t src_stride,
@@ -2610,7 +2607,7 @@ static void hevc_vt_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     DPADD_SB2_SH(src4332, src6554, filt0, filt1, dst32, dst32);
     DPADD_SB2_SH(src6554, src8776, filt0, filt1, dst54, dst54);
     DPADD_SB2_SH(src8776, src10998, filt0, filt1, dst76, dst76);
-    ST8x8_UB(dst10, dst32, dst54, dst76, dst, 2 * dst_stride);
+    ST_D8(dst10, dst32, dst54, dst76, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
     dst += (8 * dst_stride);
 
     src2 = src10;
@@ -2635,8 +2632,7 @@ static void hevc_vt_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     DPADD_SB2_SH(src4332, src6554, filt0, filt1, dst32, dst32);
     DPADD_SB2_SH(src6554, src8776, filt0, filt1, dst54, dst54);
     DPADD_SB2_SH(src8776, src10998, filt0, filt1, dst76, dst76);
-    ST8x8_UB(dst10, dst32, dst54, dst76, dst, 2 * dst_stride);
-    dst += (8 * dst_stride);
+    ST_D8(dst10, dst32, dst54, dst76, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hevc_vt_4t_4w_msa(uint8_t *src,
@@ -2955,7 +2951,7 @@ static void hevc_vt_4t_12w_msa(uint8_t *src,
         DPADD_SB2_SH(src4332, src6554, filt0, filt1, dst1_l, dst1_l);
 
         ST_SH4(dst0_r, dst1_r, dst2_r, dst3_r, dst, dst_stride);
-        ST8x4_UB(dst0_l, dst1_l, dst + 8, (2 * dst_stride));
+        ST_D4(dst0_l, dst1_l, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
 
         src2 = src6;
@@ -3243,7 +3239,6 @@ static void hevc_hv_4t_4x2_msa(uint8_t *src,
                                const int8_t *filter_x,
                                const int8_t *filter_y)
 {
-    int32_t dst_stride_in_bytes = 2 * dst_stride;
     v16i8 src0, src1, src2, src3, src4;
     v8i16 filt0, filt1;
     v8i16 filt_h0, filt_h1;
@@ -3288,7 +3283,7 @@ static void hevc_hv_4t_4x2_msa(uint8_t *src,
     dst0 >>= 6;
     dst1 >>= 6;
     dst0 = (v4i32) __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
-    ST8x2_UB(dst0, dst, dst_stride_in_bytes);
+    ST_D2(dst0, 0, 1, dst, dst_stride);
 }
 
 static void hevc_hv_4t_4x4_msa(uint8_t *src,
@@ -3298,7 +3293,6 @@ static void hevc_hv_4t_4x4_msa(uint8_t *src,
                                const int8_t *filter_x,
                                const int8_t *filter_y)
 {
-    int32_t dst_stride_in_bytes = 2 * dst_stride;
     v16i8 src0, src1, src2, src3, src4, src5, src6;
     v8i16 filt0, filt1;
     v8i16 filt_h0, filt_h1;
@@ -3351,7 +3345,7 @@ static void hevc_hv_4t_4x4_msa(uint8_t *src,
     dst3 = HEVC_FILT_4TAP(dst43, dst65, filt_h0, filt_h1);
     SRA_4V(dst0, dst1, dst2, dst3, 6);
     PCKEV_H2_SW(dst1, dst0, dst3, dst2, dst0, dst2);
-    ST8x4_UB(dst0, dst2, dst, dst_stride_in_bytes);
+    ST_D4(dst0, dst2, 0, 1, 0, 1, dst, dst_stride);
 }
 
 
@@ -3442,7 +3436,7 @@ static void hevc_hv_4t_4multx8mult_msa(uint8_t *src,
         SRA_4V(dst4, dst5, dst6, dst7, 6);
         PCKEV_H4_SW(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     dst0, dst1, dst2, dst3);
-        ST8x8_UB(dst0, dst1, dst2, dst3, dst, 2 * dst_stride);
+        ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
@@ -3479,7 +3473,6 @@ static void hevc_hv_4t_6w_msa(uint8_t *src,
                               const int8_t *filter_y,
                               int32_t height)
 {
-    int32_t dst_stride_in_bytes = 2 * dst_stride;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v8i16 filt0, filt1;
     v8i16 filt_h0, filt_h1;
@@ -3590,11 +3583,11 @@ static void hevc_hv_4t_6w_msa(uint8_t *src,
     PCKEV_H2_SH(dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
     PCKEV_H2_SH(dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
     PCKEV_H2_SH(dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
-    ST8x4_UB(tmp0, tmp1, dst, dst_stride_in_bytes);
-    ST4x4_UB(tmp4, tmp4, 0, 1, 2, 3, dst + 4, dst_stride_in_bytes);
+    ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
+    ST_W4(tmp4, 0, 1, 2, 3, dst + 4, dst_stride);
     dst += 4 * dst_stride;
-    ST8x4_UB(tmp2, tmp3, dst, dst_stride_in_bytes);
-    ST4x4_UB(tmp5, tmp5, 0, 1, 2, 3, dst + 4, dst_stride_in_bytes);
+    ST_D4(tmp2, tmp3, 0, 1, 0, 1, dst, dst_stride);
+    ST_W4(tmp5, 0, 1, 2, 3, dst + 4, dst_stride);
 }
 
 static void hevc_hv_4t_8x2_msa(uint8_t *src,
@@ -4164,7 +4157,7 @@ static void hevc_hv_4t_12w_msa(uint8_t *src,
         SRA_4V(tmp4, tmp5, tmp6, tmp7, 6);
         PCKEV_H4_SW(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, tmp0, tmp1,
                     tmp2, tmp3);
-        ST8x8_UB(tmp0, tmp1, tmp2, tmp3, dst, 2 * dst_stride);
+        ST_D8(tmp0, tmp1, tmp2, tmp3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
diff --git a/libavcodec/mips/hevcpred_init_mips.c b/libavcodec/mips/hevcpred_init_mips.c
index e987698..f7ecb34 100644
--- a/libavcodec/mips/hevcpred_init_mips.c
+++ b/libavcodec/mips/hevcpred_init_mips.c
@@ -18,32 +18,28 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/mips/hevcpred_mips.h"
 
-#if HAVE_MSA
-static av_cold void hevc_pred_init_msa(HEVCPredContext *c, const int bit_depth)
-{
-    if (8 == bit_depth) {
-        c->intra_pred[2] = ff_intra_pred_8_16x16_msa;
-        c->intra_pred[3] = ff_intra_pred_8_32x32_msa;
-        c->pred_planar[0] = ff_hevc_intra_pred_planar_0_msa;
-        c->pred_planar[1] = ff_hevc_intra_pred_planar_1_msa;
-        c->pred_planar[2] = ff_hevc_intra_pred_planar_2_msa;
-        c->pred_planar[3] = ff_hevc_intra_pred_planar_3_msa;
-        c->pred_dc = ff_hevc_intra_pred_dc_msa;
-        c->pred_angular[0] = ff_pred_intra_pred_angular_0_msa;
-        c->pred_angular[1] = ff_pred_intra_pred_angular_1_msa;
-        c->pred_angular[2] = ff_pred_intra_pred_angular_2_msa;
-        c->pred_angular[3] = ff_pred_intra_pred_angular_3_msa;
-    }
-}
-#endif  // #if HAVE_MSA
-
 void ff_hevc_pred_init_mips(HEVCPredContext *c, const int bit_depth)
 {
-#if HAVE_MSA
-    hevc_pred_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->intra_pred[2] = ff_intra_pred_8_16x16_msa;
+            c->intra_pred[3] = ff_intra_pred_8_32x32_msa;
+            c->pred_planar[0] = ff_hevc_intra_pred_planar_0_msa;
+            c->pred_planar[1] = ff_hevc_intra_pred_planar_1_msa;
+            c->pred_planar[2] = ff_hevc_intra_pred_planar_2_msa;
+            c->pred_planar[3] = ff_hevc_intra_pred_planar_3_msa;
+            c->pred_dc = ff_hevc_intra_pred_dc_msa;
+            c->pred_angular[0] = ff_pred_intra_pred_angular_0_msa;
+            c->pred_angular[1] = ff_pred_intra_pred_angular_1_msa;
+            c->pred_angular[2] = ff_pred_intra_pred_angular_2_msa;
+            c->pred_angular[3] = ff_pred_intra_pred_angular_3_msa;
+        }
+    }
 }
diff --git a/libavcodec/mips/hevcpred_msa.c b/libavcodec/mips/hevcpred_msa.c
index 963c64c..f53276d 100644
--- a/libavcodec/mips/hevcpred_msa.c
+++ b/libavcodec/mips/hevcpred_msa.c
@@ -83,7 +83,7 @@ static void hevc_intra_pred_vert_4x4_msa(const uint8_t *src_top,
         vec2 -= vec0;
         vec2 >>= 1;
         vec2 += vec1;
-        vec2 = CLIP_SH_0_255(vec2);
+        CLIP_SH_0_255(vec2);
 
         for (col = 0; col < 4; col++) {
             dst[stride * col] = (uint8_t) vec2[col];
@@ -122,7 +122,7 @@ static void hevc_intra_pred_vert_8x8_msa(const uint8_t *src_top,
         vec2 -= vec0;
         vec2 >>= 1;
         vec2 += vec1;
-        vec2 = CLIP_SH_0_255(vec2);
+        CLIP_SH_0_255(vec2);
 
         val0 = vec2[0];
         val1 = vec2[1];
@@ -214,7 +214,7 @@ static void hevc_intra_pred_horiz_4x4_msa(const uint8_t *src_top,
         src0_r -= src_top_val;
         src0_r >>= 1;
         src0_r += src_left_val;
-        src0_r = CLIP_SH_0_255(src0_r);
+        CLIP_SH_0_255(src0_r);
         src0 = __msa_pckev_b((v16i8) src0_r, (v16i8) src0_r);
         val0 = __msa_copy_s_w((v4i32) src0, 0);
         SW(val0, dst);
@@ -254,7 +254,7 @@ static void hevc_intra_pred_horiz_8x8_msa(const uint8_t *src_top,
         src0_r -= src_top_val;
         src0_r >>= 1;
         src0_r += src_left_val;
-        src0_r = CLIP_SH_0_255(src0_r);
+        CLIP_SH_0_255(src0_r);
         src0 = __msa_pckev_b((v16i8) src0_r, (v16i8) src0_r);
         val0 = __msa_copy_s_d((v2i64) src0, 0);
         SD(val0, dst);
@@ -589,7 +589,7 @@ static void hevc_intra_pred_plane_4x4_msa(const uint8_t *src_top,
     PCKEV_D2_SH(res1, res0, res3, res2, res0, res1);
     SRARI_H2_SH(res0, res1, 3);
     src_vec0 = __msa_pckev_b((v16i8) res1, (v16i8) res0);
-    ST4x4_UB(src_vec0, src_vec0, 0, 1, 2, 3, dst, stride);
+    ST_W4(src_vec0, 0, 1, 2, 3, dst, stride);
 }
 
 static void hevc_intra_pred_plane_8x8_msa(const uint8_t *src_top,
@@ -656,7 +656,8 @@ static void hevc_intra_pred_plane_8x8_msa(const uint8_t *src_top,
     PCKEV_B4_SB(res1, res0, res3, res2, res5, res4, res7, res6,
                 src_vec0, src_vec1, src_vec2, src_vec3);
 
-    ST8x8_UB(src_vec0, src_vec1, src_vec2, src_vec3, dst, stride);
+    ST_D8(src_vec0, src_vec1, src_vec2, src_vec3, 0, 1, 0, 1,
+          0, 1, 0, 1, dst, stride);
 }
 
 static void hevc_intra_pred_plane_16x16_msa(const uint8_t *src_top,
@@ -997,7 +998,8 @@ static void hevc_intra_pred_angular_upper_4width_msa(const uint8_t *src_top,
     ILVR_D2_SH(fact3, fact1, fact7, fact5, fact1, fact3);
     ILVR_B4_SH(zero, top0, zero, top1, zero, top2, zero, top3,
                diff0, diff2, diff4, diff6);
-    SLDI_B4_0_SH(diff0, diff2, diff4, diff6, diff1, diff3, diff5, diff7, 2);
+    SLDI_B4_SH(zero, diff0, zero, diff2, zero, diff4, zero, diff6, 2,
+               diff1, diff3, diff5, diff7);
     ILVR_D2_SH(diff2, diff0, diff6, diff4, diff0, diff2);
     ILVR_D2_SH(diff3, diff1, diff7, diff5, diff1, diff3);
     MUL2(diff1, fact0, diff3, fact2, diff1, diff3);
@@ -1007,7 +1009,7 @@ static void hevc_intra_pred_angular_upper_4width_msa(const uint8_t *src_top,
 
     SRARI_H2_SH(diff1, diff3, 5);
     dst_val0 = __msa_pckev_b((v16i8) diff3, (v16i8) diff1);
-    ST4x4_UB(dst_val0, dst_val0, 0, 1, 2, 3, dst, stride);
+    ST_W4(dst_val0, 0, 1, 2, 3, dst, stride);
 }
 
 static void hevc_intra_pred_angular_upper_8width_msa(const uint8_t *src_top,
@@ -1092,8 +1094,8 @@ static void hevc_intra_pred_angular_upper_8width_msa(const uint8_t *src_top,
         UNPCK_UB_SH(top2, diff4, diff5);
         UNPCK_UB_SH(top3, diff6, diff7);
 
-        SLDI_B2_SH(diff1, diff3, diff0, diff2, diff1, diff3, 2);
-        SLDI_B2_SH(diff5, diff7, diff4, diff6, diff5, diff7, 2);
+        SLDI_B4_SH(diff1, diff0, diff3, diff2, diff5, diff4, diff7, diff6, 2,
+                   diff1, diff3, diff5, diff7);
         MUL4(diff1, fact0, diff3, fact2, diff5, fact4, diff7, fact6,
              diff1, diff3, diff5, diff7);
 
@@ -1104,7 +1106,7 @@ static void hevc_intra_pred_angular_upper_8width_msa(const uint8_t *src_top,
 
         SRARI_H4_SH(diff1, diff3, diff5, diff7, 5);
         PCKEV_B2_UB(diff3, diff1, diff7, diff5, dst_val0, dst_val1);
-        ST8x4_UB(dst_val0, dst_val1, dst, stride);
+        ST_D4(dst_val0, dst_val1, 0, 1, 0, 1, dst, stride);
         dst += (4 * stride);
     }
 }
@@ -1185,8 +1187,8 @@ static void hevc_intra_pred_angular_upper_16width_msa(const uint8_t *src_top,
         fact6 = __msa_fill_h(fact_val3);
         fact7 = __msa_fill_h(32 - fact_val3);
 
-        SLDI_B2_UB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_UB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_UB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
@@ -1296,8 +1298,8 @@ static void hevc_intra_pred_angular_upper_32width_msa(const uint8_t *src_top,
         top2 = top1;
         top6 = top5;
 
-        SLDI_B2_UB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_UB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_UB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
@@ -1406,7 +1408,8 @@ static void hevc_intra_pred_angular_lower_4width_msa(const uint8_t *src_top,
     ILVR_D2_SH(fact3, fact1, fact7, fact5, fact1, fact3);
     ILVR_B4_SH(zero, top0, zero, top1, zero, top2, zero, top3,
                diff0, diff2, diff4, diff6);
-    SLDI_B4_0_SH(diff0, diff2, diff4, diff6, diff1, diff3, diff5, diff7, 2);
+    SLDI_B4_SH(zero, diff0, zero, diff2, zero, diff4, zero, diff6, 2,
+               diff1, diff3, diff5, diff7);
     ILVR_D2_SH(diff2, diff0, diff6, diff4, diff0, diff2);
     ILVR_D2_SH(diff3, diff1, diff7, diff5, diff1, diff3);
     MUL2(diff1, fact0, diff3, fact2, diff1, diff3);
@@ -1425,9 +1428,8 @@ static void hevc_intra_pred_angular_lower_4width_msa(const uint8_t *src_top,
     dst_val0 = __msa_pckev_b((v16i8) diff2, (v16i8) diff2);
     dst_val1 = __msa_pckod_b((v16i8) diff2, (v16i8) diff2);
 
-    ST4x2_UB(dst_val0, dst, stride);
-    dst += (2 * stride);
-    ST4x2_UB(dst_val1, dst, stride);
+    ST_W2(dst_val0, 0, 1, dst, stride);
+    ST_W2(dst_val1, 0, 1, dst + 2 * stride, stride);
 }
 
 static void hevc_intra_pred_angular_lower_8width_msa(const uint8_t *src_top,
@@ -1511,8 +1513,8 @@ static void hevc_intra_pred_angular_lower_8width_msa(const uint8_t *src_top,
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
         UNPCK_UB_SH(top3, diff6, diff7);
-        SLDI_B2_SH(diff1, diff3, diff0, diff2, diff1, diff3, 2);
-        SLDI_B2_SH(diff5, diff7, diff4, diff6, diff5, diff7, 2);
+        SLDI_B4_SH(diff1, diff0, diff3, diff2, diff5, diff4, diff7, diff6, 2,
+                   diff1, diff3, diff5, diff7);
         MUL4(diff1, fact0, diff3, fact2, diff5, fact4, diff7, fact6,
              diff1, diff3, diff5, diff7);
 
@@ -1526,7 +1528,7 @@ static void hevc_intra_pred_angular_lower_8width_msa(const uint8_t *src_top,
                     dst_val0, dst_val1, dst_val2, dst_val3);
         ILVR_B2_SH(dst_val1, dst_val0, dst_val3, dst_val2, diff0, diff1);
         ILVRL_H2_SH(diff1, diff0, diff3, diff4);
-        ST4x8_UB(diff3, diff4, dst_org, stride);
+        ST_W8(diff3, diff4, 0, 1, 2, 3, 0, 1, 2, 3, dst_org, stride);
         dst += 4;
     }
 }
@@ -1606,8 +1608,8 @@ static void hevc_intra_pred_angular_lower_16width_msa(const uint8_t *src_top,
         fact6 = __msa_fill_h(fact_val3);
         fact7 = __msa_fill_h(32 - fact_val3);
 
-        SLDI_B2_SB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_SB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_SB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
 
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
@@ -1640,9 +1642,9 @@ static void hevc_intra_pred_angular_lower_16width_msa(const uint8_t *src_top,
         ILVL_B2_SH(dst_val1, dst_val0, dst_val3, dst_val2, diff2, diff3);
         ILVRL_H2_SH(diff1, diff0, diff4, diff5);
         ILVRL_H2_SH(diff3, diff2, diff6, diff7);
-        ST4x8_UB(diff4, diff5, dst_org, stride);
+        ST_W8(diff4, diff5, 0, 1, 2, 3, 0, 1, 2, 3, dst_org, stride);
         dst_org += (8 * stride);
-        ST4x8_UB(diff6, diff7, dst_org, stride);
+        ST_W8(diff6, diff7, 0, 1, 2, 3, 0, 1, 2, 3, dst_org, stride);
         dst += 4;
     }
 }
@@ -1713,8 +1715,8 @@ static void hevc_intra_pred_angular_lower_32width_msa(const uint8_t *src_top,
         top2 = top1;
         top6 = top5;
 
-        SLDI_B2_SB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_SB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_SB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
 
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
@@ -1746,23 +1748,14 @@ static void hevc_intra_pred_angular_lower_32width_msa(const uint8_t *src_top,
         ILVRL_B2_SH(dst_val2, dst_val0, diff0, diff1);
         ILVRL_B2_SH(dst_val3, dst_val1, diff2, diff3);
 
-        ST2x4_UB(diff0, 0, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff0, 4, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff1, 0, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff1, 4, dst_org, stride);
-        dst_org += (4 * stride);
-
-        ST2x4_UB(diff2, 0, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff2, 4, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff3, 0, dst_org, stride);
-        dst_org += (4 * stride);
-        ST2x4_UB(diff3, 4, dst_org, stride);
-        dst_org += (4 * stride);
+        ST_H8(diff0, 0, 1, 2, 3, 4, 5, 6, 7, dst_org, stride)
+        dst_org += (8 * stride);
+        ST_H8(diff1, 0, 1, 2, 3, 4, 5, 6, 7, dst_org, stride)
+        dst_org += (8 * stride);
+        ST_H8(diff2, 0, 1, 2, 3, 4, 5, 6, 7, dst_org, stride)
+        dst_org += (8 * stride);
+        ST_H8(diff3, 0, 1, 2, 3, 4, 5, 6, 7, dst_org, stride)
+        dst_org += (8 * stride);
 
         dst += 2;
     }
diff --git a/libavcodec/mips/hpeldsp_init_mips.c b/libavcodec/mips/hpeldsp_init_mips.c
index 363a045..77cbe99 100644
--- a/libavcodec/mips/hpeldsp_init_mips.c
+++ b/libavcodec/mips/hpeldsp_init_mips.c
@@ -19,104 +19,94 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "../hpeldsp.h"
 #include "libavcodec/mips/hpeldsp_mips.h"
 
-#if HAVE_MSA
-static void ff_hpeldsp_init_msa(HpelDSPContext *c, int flags)
-{
-    c->put_pixels_tab[0][0] = ff_put_pixels16_msa;
-    c->put_pixels_tab[0][1] = ff_put_pixels16_x2_msa;
-    c->put_pixels_tab[0][2] = ff_put_pixels16_y2_msa;
-    c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_msa;
-
-    c->put_pixels_tab[1][0] = ff_put_pixels8_msa;
-    c->put_pixels_tab[1][1] = ff_put_pixels8_x2_msa;
-    c->put_pixels_tab[1][2] = ff_put_pixels8_y2_msa;
-    c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_msa;
-
-    c->put_pixels_tab[2][1] = ff_put_pixels4_x2_msa;
-    c->put_pixels_tab[2][2] = ff_put_pixels4_y2_msa;
-    c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_msa;
-
-    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_msa;
-    c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_msa;
-    c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_msa;
-    c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_msa;
-
-    c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_msa;
-    c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_msa;
-    c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_msa;
-    c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_msa;
-
-    c->avg_pixels_tab[0][0] = ff_avg_pixels16_msa;
-    c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_msa;
-    c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_msa;
-    c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_msa;
-
-    c->avg_pixels_tab[1][0] = ff_avg_pixels8_msa;
-    c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_msa;
-    c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_msa;
-    c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_msa;
-
-    c->avg_pixels_tab[2][0] = ff_avg_pixels4_msa;
-    c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_msa;
-    c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_msa;
-    c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_msa;
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static void ff_hpeldsp_init_mmi(HpelDSPContext *c, int flags)
-{
-    c->put_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
-    c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_mmi;
-    c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_mmi;
-    c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_mmi;
-
-    c->put_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
-    c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_mmi;
-    c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_mmi;
-    c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_mmi;
-
-    c->put_pixels_tab[2][0] = ff_put_pixels4_8_mmi;
-    c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_mmi;
-    c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_mmi;
-    c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_mmi;
-
-    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
-    c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_mmi;
-    c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_mmi;
-    c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_mmi;
-
-    c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
-    c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_mmi;
-    c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_mmi;
-    c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_mmi;
-
-    c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_mmi;
-    c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_mmi;
-    c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_mmi;
-    c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_mmi;
-
-    c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_mmi;
-    c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_mmi;
-    c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_mmi;
-    c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_mmi;
-
-    c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_mmi;
-    c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_mmi;
-    c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_mmi;
-    c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_mmi;
-}
-#endif  // #if HAVE_MMI
-
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags)
 {
-#if HAVE_MSA
-    ff_hpeldsp_init_msa(c, flags);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    ff_hpeldsp_init_mmi(c, flags);
-#endif  // #if HAVE_MMI
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->put_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+        c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_mmi;
+        c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_mmi;
+        c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_mmi;
+
+        c->put_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+        c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_mmi;
+        c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_mmi;
+        c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_mmi;
+
+        c->put_pixels_tab[2][0] = ff_put_pixels4_8_mmi;
+        c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_mmi;
+        c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_mmi;
+        c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_mmi;
+
+        c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+        c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_mmi;
+        c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_mmi;
+        c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_mmi;
+
+        c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+        c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_mmi;
+        c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_mmi;
+        c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_mmi;
+
+        c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_mmi;
+        c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_mmi;
+        c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_mmi;
+        c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_mmi;
+
+        c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_mmi;
+        c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_mmi;
+        c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_mmi;
+        c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_mmi;
+
+        c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_mmi;
+        c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_mmi;
+        c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_mmi;
+        c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        c->put_pixels_tab[0][0] = ff_put_pixels16_msa;
+        c->put_pixels_tab[0][1] = ff_put_pixels16_x2_msa;
+        c->put_pixels_tab[0][2] = ff_put_pixels16_y2_msa;
+        c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_msa;
+
+        c->put_pixels_tab[1][0] = ff_put_pixels8_msa;
+        c->put_pixels_tab[1][1] = ff_put_pixels8_x2_msa;
+        c->put_pixels_tab[1][2] = ff_put_pixels8_y2_msa;
+        c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_msa;
+
+        c->put_pixels_tab[2][1] = ff_put_pixels4_x2_msa;
+        c->put_pixels_tab[2][2] = ff_put_pixels4_y2_msa;
+        c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_msa;
+
+        c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_msa;
+        c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_msa;
+        c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_msa;
+        c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_msa;
+
+        c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_msa;
+        c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_msa;
+        c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_msa;
+        c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_msa;
+
+        c->avg_pixels_tab[0][0] = ff_avg_pixels16_msa;
+        c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_msa;
+        c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_msa;
+        c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_msa;
+
+        c->avg_pixels_tab[1][0] = ff_avg_pixels8_msa;
+        c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_msa;
+        c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_msa;
+        c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_msa;
+
+        c->avg_pixels_tab[2][0] = ff_avg_pixels4_msa;
+        c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_msa;
+        c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_msa;
+        c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_msa;
+    }
 }
diff --git a/libavcodec/mips/hpeldsp_msa.c b/libavcodec/mips/hpeldsp_msa.c
index 40a0dca..2bbe477 100644
--- a/libavcodec/mips/hpeldsp_msa.c
+++ b/libavcodec/mips/hpeldsp_msa.c
@@ -49,7 +49,7 @@
     PCKEV_B2_UB(in2, in1, in4, in3, tmp0_m, tmp1_m);                    \
     PCKEV_D2_UB(dst1, dst0, dst3, dst2, tmp2_m, tmp3_m);                \
     AVER_UB2_UB(tmp0_m, tmp2_m, tmp1_m, tmp3_m, tmp0_m, tmp1_m);        \
-    ST8x4_UB(tmp0_m, tmp1_m, pdst_m, stride);                           \
+    ST_D4(tmp0_m, tmp1_m, 0, 1, 0, 1, pdst_m, stride);                  \
 }
 
 static void common_hz_bil_4w_msa(const uint8_t *src, int32_t src_stride,
@@ -59,12 +59,13 @@ static void common_hz_bil_4w_msa(const uint8_t *src, int32_t src_stride,
     uint8_t loop_cnt;
     uint32_t out0, out1;
     v16u8 src0, src1, src0_sld1, src1_sld1, res0, res1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         LD_UB2(src, src_stride, src0, src1);
         src += (2 * src_stride);
 
-        SLDI_B2_0_UB(src0, src1, src0_sld1, src1_sld1, 1);
+        SLDI_B2_UB(zeros, src0, zeros, src1, 1, src0_sld1, src1_sld1);
         AVER_UB2_UB(src0_sld1, src0, src1_sld1, src1, res0, res1);
 
         out0 = __msa_copy_u_w((v4i32) res0, 0);
@@ -82,13 +83,14 @@ static void common_hz_bil_8w_msa(const uint8_t *src, int32_t src_stride,
 {
     uint8_t loop_cnt;
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
 
-        SLDI_B4_0_SB(src0, src1, src2, src3,
-                     src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+        SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+                   src0_sld1, src1_sld1, src2_sld1, src3_sld1);
         AVER_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                       src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
         dst += (4 * dst_stride);
@@ -125,14 +127,15 @@ static void common_hz_bil_no_rnd_8x8_msa(const uint8_t *src, int32_t src_stride,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 src0_sld1, src1_sld1, src2_sld1, src3_sld1;
     v16i8 src4_sld1, src5_sld1, src6_sld1, src7_sld1;
+    v16i8 zeros = { 0 };
 
     LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
     src += (8 * src_stride);
 
-    SLDI_B4_0_SB(src0, src1, src2, src3,
-                 src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
-    SLDI_B4_0_SB(src4, src5, src6, src7,
-                 src4_sld1, src5_sld1, src6_sld1, src7_sld1, 1);
+    SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
+    SLDI_B4_SB(zeros, src4, zeros, src5, zeros, src6, zeros, src7, 1,
+               src4_sld1, src5_sld1, src6_sld1, src7_sld1);
 
     AVE_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                  src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
@@ -145,10 +148,11 @@ static void common_hz_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
                                          uint8_t *dst, int32_t dst_stride)
 {
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
-    SLDI_B4_0_SB(src0, src1, src2, src3,
-                 src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+    SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
     AVE_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                  src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
 }
@@ -216,12 +220,13 @@ static void common_hz_bil_and_aver_dst_4w_msa(const uint8_t *src,
     v16u8 src0, src1, src0_sld1, src1_sld1, res0, res1;
     v16u8 tmp0 = { 0 };
     v16u8 tmp1 = { 0 };
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         LD_UB2(src, src_stride, src0, src1);
         src += (2 * src_stride);
 
-        SLDI_B2_0_UB(src0, src1, src0_sld1, src1_sld1, 1);
+        SLDI_B2_UB(zeros, src0, zeros, src1, 1, src0_sld1, src1_sld1);
 
         dst0 = LW(dst);
         dst1 = LW(dst + dst_stride);
@@ -247,13 +252,14 @@ static void common_hz_bil_and_aver_dst_8w_msa(const uint8_t *src,
 {
     uint8_t loop_cnt;
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
 
-        SLDI_B4_0_SB(src0, src1, src2, src3,
-                     src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+        SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+                   src0_sld1, src1_sld1, src2_sld1, src3_sld1);
 
         AVER_DST_ST8x4_UB(src0, src0_sld1, src1, src1_sld1, src2, src2_sld1,
                           src3, src3_sld1, dst, dst_stride);
@@ -529,6 +535,7 @@ static void common_hv_bil_4w_msa(const uint8_t *src, int32_t src_stride,
     v16i8 src0, src1, src2, src0_sld1, src1_sld1, src2_sld1;
     v16u8 src0_r, src1_r, src2_r, res;
     v8u16 add0, add1, add2, sum0, sum1;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -537,7 +544,8 @@ static void common_hv_bil_4w_msa(const uint8_t *src, int32_t src_stride,
         LD_SB2(src, src_stride, src1, src2);
         src += (2 * src_stride);
 
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2,
                    src0_r, src1_r, src2_r);
         HADD_UB3_UH(src0_r, src1_r, src2_r, add0, add1, add2);
@@ -565,6 +573,7 @@ static void common_hv_bil_8w_msa(const uint8_t *src, int32_t src_stride,
     v16u8 src0_r, src1_r, src2_r, src3_r, src4_r;
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -573,8 +582,9 @@ static void common_hv_bil_8w_msa(const uint8_t *src, int32_t src_stride,
         LD_SB4(src, src_stride, src1, src2, src3, src4);
         src += (4 * src_stride);
 
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-        SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
+        SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         ILVR_B2_UB(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
@@ -584,7 +594,7 @@ static void common_hv_bil_8w_msa(const uint8_t *src, int32_t src_stride,
              sum0, sum1, sum2, sum3);
         SRARI_H4_UH(sum0, sum1, sum2, sum3, 2);
         PCKEV_B2_SB(sum1, sum0, sum3, sum2, src0, src1);
-        ST8x4_UB(src0, src1, dst, dst_stride);
+        ST_D4(src0, src1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
         src0 = src4;
     }
@@ -659,15 +669,17 @@ static void common_hv_bil_no_rnd_8x8_msa(const uint8_t *src, int32_t src_stride,
     v8u16 add0, add1, add2, add3, add4, add5, add6, add7, add8;
     v8u16 sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
     v16i8 out0, out1;
+    v16i8 zeros = { 0 };
 
     LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
     src += (8 * src_stride);
     src8 = LD_UB(src);
 
-    SLDI_B4_0_UB(src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1,
-                 src3_sld1, 1);
-    SLDI_B3_0_UB(src4, src5, src6, src4_sld1, src5_sld1, src6_sld1, 1);
-    SLDI_B2_0_UB(src7, src8, src7_sld1, src8_sld1, 1);
+    SLDI_B4_UB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
+    SLDI_B3_UB(zeros, src4, zeros, src5, zeros, src6, 1, src4_sld1,
+               src5_sld1, src6_sld1);
+    SLDI_B2_UB(zeros, src7, zeros, src8, 1, src7_sld1, src8_sld1);
     ILVR_B4_UH(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src3_sld1,
                src3, src0_r, src1_r, src2_r, src3_r);
     ILVR_B3_UH(src4_sld1, src4, src5_sld1, src5, src6_sld1, src6, src4_r,
@@ -689,9 +701,9 @@ static void common_hv_bil_no_rnd_8x8_msa(const uint8_t *src, int32_t src_stride,
     SRA_4V(sum0, sum1, sum2, sum3, 2);
     SRA_4V(sum4, sum5, sum6, sum7, 2);
     PCKEV_B2_SB(sum1, sum0, sum3, sum2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     PCKEV_B2_SB(sum5, sum4, sum7, sum6, out0, out1);
-    ST8x4_UB(out0, out1, dst + 4 * dst_stride, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hv_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
@@ -703,13 +715,15 @@ static void common_hv_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
     v16i8 out0, out1;
+    v16i8 zeros = { 0 };
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
     src += (4 * src_stride);
     src4 = LD_SB(src);
 
-    SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-    SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+    SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+               src1_sld1, src2_sld1);
+    SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
     ILVR_B3_UH(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                src1_r, src2_r);
     ILVR_B2_UH(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
@@ -723,7 +737,7 @@ static void common_hv_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
 
     SRA_4V(sum0, sum1, sum2, sum3, 2);
     PCKEV_B2_SB(sum1, sum0, sum3, sum2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hv_bil_no_rnd_16x16_msa(const uint8_t *src,
@@ -918,6 +932,7 @@ static void common_hv_bil_and_aver_dst_4w_msa(const uint8_t *src,
     v16u8 src0_r, src1_r, src2_r;
     v8u16 add0, add1, add2, sum0, sum1;
     v16u8 dst0, dst1, res0, res1;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -927,7 +942,8 @@ static void common_hv_bil_and_aver_dst_4w_msa(const uint8_t *src,
         src += (2 * src_stride);
 
         LD_UB2(dst, dst_stride, dst0, dst1);
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         HADD_UB3_UH(src0_r, src1_r, src2_r, add0, add1, add2);
@@ -959,6 +975,7 @@ static void common_hv_bil_and_aver_dst_8w_msa(const uint8_t *src,
     v16u8 src0_r, src1_r, src2_r, src3_r, src4_r;
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -968,8 +985,9 @@ static void common_hv_bil_and_aver_dst_8w_msa(const uint8_t *src,
         src += (4 * src_stride);
 
         LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-        SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
+        SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         ILVR_B2_UB(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
diff --git a/libavcodec/mips/idctdsp_init_mips.c b/libavcodec/mips/idctdsp_init_mips.c
index bb33b55..23efd9e 100644
--- a/libavcodec/mips/idctdsp_init_mips.c
+++ b/libavcodec/mips/idctdsp_init_mips.c
@@ -19,56 +19,44 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "idctdsp_mips.h"
 #include "xvididct_mips.h"
 
-#if HAVE_MSA
-static av_cold void idctdsp_init_msa(IDCTDSPContext *c, AVCodecContext *avctx,
-                                     unsigned high_bit_depth)
+av_cold void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
+                          unsigned high_bit_depth)
 {
-    if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
-        (avctx->bits_per_raw_sample != 10) &&
-        (avctx->bits_per_raw_sample != 12) &&
-        (avctx->idct_algo == FF_IDCT_AUTO)) {
-                c->idct_put = ff_simple_idct_put_msa;
-                c->idct_add = ff_simple_idct_add_msa;
-                c->idct = ff_simple_idct_msa;
-                c->perm_type = FF_IDCT_PERM_NONE;
-    }
+    int cpu_flags = av_get_cpu_flags();
 
-    c->put_pixels_clamped = ff_put_pixels_clamped_msa;
-    c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_msa;
-    c->add_pixels_clamped = ff_add_pixels_clamped_msa;
-}
-#endif  // #if HAVE_MSA
+    if (have_mmi(cpu_flags)) {
+        if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
+            (avctx->bits_per_raw_sample != 10) &&
+            (avctx->bits_per_raw_sample != 12) &&
+            ((avctx->idct_algo == FF_IDCT_AUTO) || (avctx->idct_algo == FF_IDCT_SIMPLE))) {
+                    c->idct_put = ff_simple_idct_put_8_mmi;
+                    c->idct_add = ff_simple_idct_add_8_mmi;
+                    c->idct = ff_simple_idct_8_mmi;
+                    c->perm_type = FF_IDCT_PERM_NONE;
+        }
 
-#if HAVE_MMI
-static av_cold void idctdsp_init_mmi(IDCTDSPContext *c, AVCodecContext *avctx,
-        unsigned high_bit_depth)
-{
-    if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
-        (avctx->bits_per_raw_sample != 10) &&
-        (avctx->bits_per_raw_sample != 12) &&
-        ((avctx->idct_algo == FF_IDCT_AUTO) || (avctx->idct_algo == FF_IDCT_SIMPLE))) {
-                c->idct_put = ff_simple_idct_put_8_mmi;
-                c->idct_add = ff_simple_idct_add_8_mmi;
-                c->idct = ff_simple_idct_8_mmi;
-                c->perm_type = FF_IDCT_PERM_NONE;
+        c->put_pixels_clamped = ff_put_pixels_clamped_mmi;
+        c->add_pixels_clamped = ff_add_pixels_clamped_mmi;
+        c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_mmi;
     }
 
-    c->put_pixels_clamped = ff_put_pixels_clamped_mmi;
-    c->add_pixels_clamped = ff_add_pixels_clamped_mmi;
-    c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_mmi;
-}
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
+            (avctx->bits_per_raw_sample != 10) &&
+            (avctx->bits_per_raw_sample != 12) &&
+            (avctx->idct_algo == FF_IDCT_AUTO)) {
+                    c->idct_put = ff_simple_idct_put_msa;
+                    c->idct_add = ff_simple_idct_add_msa;
+                    c->idct = ff_simple_idct_msa;
+                    c->perm_type = FF_IDCT_PERM_NONE;
+        }
 
-av_cold void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
-                          unsigned high_bit_depth)
-{
-#if HAVE_MSA
-    idctdsp_init_msa(c, avctx, high_bit_depth);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    idctdsp_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
+        c->put_pixels_clamped = ff_put_pixels_clamped_msa;
+        c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_msa;
+        c->add_pixels_clamped = ff_add_pixels_clamped_msa;
+    }
 }
diff --git a/libavcodec/mips/idctdsp_msa.c b/libavcodec/mips/idctdsp_msa.c
index b29e420..b6b98dc 100644
--- a/libavcodec/mips/idctdsp_msa.c
+++ b/libavcodec/mips/idctdsp_msa.c
@@ -28,8 +28,7 @@ static void put_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
 
     LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
@@ -63,8 +62,7 @@ static void put_signed_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     in6 += 128;
     in7 += 128;
 
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
@@ -109,8 +107,7 @@ static void add_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     in6 += (v8i16) pix6;
     in7 += (v8i16) pix7;
 
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
diff --git a/libavcodec/mips/me_cmp_init_mips.c b/libavcodec/mips/me_cmp_init_mips.c
index 219a0dc..e3e33b8 100644
--- a/libavcodec/mips/me_cmp_init_mips.c
+++ b/libavcodec/mips/me_cmp_init_mips.c
@@ -18,39 +18,35 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "me_cmp_mips.h"
 
-#if HAVE_MSA
-static av_cold void me_cmp_msa(MECmpContext *c, AVCodecContext *avctx)
+av_cold void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx)
 {
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
 #if BIT_DEPTH == 8
-    c->pix_abs[0][0] = ff_pix_abs16_msa;
-    c->pix_abs[0][1] = ff_pix_abs16_x2_msa;
-    c->pix_abs[0][2] = ff_pix_abs16_y2_msa;
-    c->pix_abs[0][3] = ff_pix_abs16_xy2_msa;
-    c->pix_abs[1][0] = ff_pix_abs8_msa;
-    c->pix_abs[1][1] = ff_pix_abs8_x2_msa;
-    c->pix_abs[1][2] = ff_pix_abs8_y2_msa;
-    c->pix_abs[1][3] = ff_pix_abs8_xy2_msa;
+        c->pix_abs[0][0] = ff_pix_abs16_msa;
+        c->pix_abs[0][1] = ff_pix_abs16_x2_msa;
+        c->pix_abs[0][2] = ff_pix_abs16_y2_msa;
+        c->pix_abs[0][3] = ff_pix_abs16_xy2_msa;
+        c->pix_abs[1][0] = ff_pix_abs8_msa;
+        c->pix_abs[1][1] = ff_pix_abs8_x2_msa;
+        c->pix_abs[1][2] = ff_pix_abs8_y2_msa;
+        c->pix_abs[1][3] = ff_pix_abs8_xy2_msa;
 
-    c->hadamard8_diff[0] = ff_hadamard8_diff16_msa;
-    c->hadamard8_diff[1] = ff_hadamard8_diff8x8_msa;
+        c->hadamard8_diff[0] = ff_hadamard8_diff16_msa;
+        c->hadamard8_diff[1] = ff_hadamard8_diff8x8_msa;
 
-    c->hadamard8_diff[4] = ff_hadamard8_intra16_msa;
-    c->hadamard8_diff[5] = ff_hadamard8_intra8x8_msa;
+        c->hadamard8_diff[4] = ff_hadamard8_intra16_msa;
+        c->hadamard8_diff[5] = ff_hadamard8_intra8x8_msa;
 
-    c->sad[0] = ff_pix_abs16_msa;
-    c->sad[1] = ff_pix_abs8_msa;
-    c->sse[0] = ff_sse16_msa;
-    c->sse[1] = ff_sse8_msa;
-    c->sse[2] = ff_sse4_msa;
+        c->sad[0] = ff_pix_abs16_msa;
+        c->sad[1] = ff_pix_abs8_msa;
+        c->sse[0] = ff_sse16_msa;
+        c->sse[1] = ff_sse8_msa;
+        c->sse[2] = ff_sse4_msa;
 #endif
-}
-#endif  // #if HAVE_MSA
-
-av_cold void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx)
-{
-#if HAVE_MSA
-    me_cmp_msa(c, avctx);
-#endif  // #if HAVE_MSA
+    }
 }
diff --git a/libavcodec/mips/me_cmp_msa.c b/libavcodec/mips/me_cmp_msa.c
index 0e3165c..7cb7af0 100644
--- a/libavcodec/mips/me_cmp_msa.c
+++ b/libavcodec/mips/me_cmp_msa.c
@@ -87,8 +87,8 @@ static uint32_t sad_horiz_bilinear_filter_8width_msa(uint8_t *src,
 
         PCKEV_D2_UB(src1, src0, src3, src2, src0, src1);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref4, ref5);
-        SLDI_B2_UB(ref0, ref1, ref0, ref1, ref0, ref1, 1);
-        SLDI_B2_UB(ref2, ref3, ref2, ref3, ref2, ref3, 1);
+        SLDI_B4_UB(ref0, ref0, ref1, ref1, ref2, ref2, ref3, ref3, 1,
+                   ref0, ref1, ref2, ref3);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref0, ref1);
         AVER_UB2_UB(ref4, ref0, ref5, ref1, comp0, comp1);
         sad += SAD_UB2_UH(src0, src1, comp0, comp1);
@@ -100,8 +100,8 @@ static uint32_t sad_horiz_bilinear_filter_8width_msa(uint8_t *src,
 
         PCKEV_D2_UB(src1, src0, src3, src2, src0, src1);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref4, ref5);
-        SLDI_B2_UB(ref0, ref1, ref0, ref1, ref0, ref1, 1);
-        SLDI_B2_UB(ref2, ref3, ref2, ref3, ref2, ref3, 1);
+        SLDI_B4_UB(ref0, ref0, ref1, ref1, ref2, ref2, ref3, ref3, 1,
+                   ref0, ref1, ref2, ref3);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref0, ref1);
         AVER_UB2_UB(ref4, ref0, ref5, ref1, comp0, comp1);
         sad += SAD_UB2_UH(src0, src1, comp0, comp1);
diff --git a/libavcodec/mips/mpegaudiodsp_mips_float.c b/libavcodec/mips/mpegaudiodsp_mips_float.c
index 481b69c..ae130c7 100644
--- a/libavcodec/mips/mpegaudiodsp_mips_float.c
+++ b/libavcodec/mips/mpegaudiodsp_mips_float.c
@@ -287,9 +287,16 @@ static void ff_dct32_mips_float(float *out, const float *tab)
           val8 , val9 , val10, val11, val12, val13, val14, val15,
           val16, val17, val18, val19, val20, val21, val22, val23,
           val24, val25, val26, val27, val28, val29, val30, val31;
-    float fTmp1, fTmp2, fTmp3, fTmp4, fTmp5, fTmp6, fTmp7, fTmp8,
-          fTmp9, fTmp10, fTmp11;
+    float fTmp1, fTmp2, fTmp3, fTmp4, fTmp5, fTmp6, fTmp8, fTmp9;
+    float f1, f2, f3, f4, f5, f6, f7;
 
+    f1 = 0.50241928618815570551;
+    f2 = 0.50060299823519630134;
+    f3 = 10.19000812354805681150;
+    f4 = 5.10114861868916385802;
+    f5 = 0.67480834145500574602;
+    f6 = 0.74453627100229844977;
+    f7 = 0.50979557910415916894;
     /**
     * instructions are scheduled to minimize pipeline stall.
     */
@@ -298,149 +305,142 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "lwc1       %[fTmp2],       31*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       15*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       16*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.50241928618815570551                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.50060299823519630134                  \n\t"
-        "li.s       %[fTmp11],      10.19000812354805681150                 \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val0],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val15],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       7*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       24*4(%[tab])                            \n\t"
-        "madd.s     %[val16],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val31],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val15],       %[val15],       %[fTmp7]                \n\t"
+        "madd.s     %[val16],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val31],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val15],       %[val15],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       8*4(%[tab])                             \n\t"
         "lwc1       %[fTmp4],       23*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val31],       %[val31],       %[fTmp7]                \n\t"
+        "mul.s      %[val31],       %[val31],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       5.10114861868916385802                  \n\t"
-        "li.s       %[fTmp10],      0.67480834145500574602                  \n\t"
-        "li.s       %[fTmp11],      0.74453627100229844977                  \n\t"
         "add.s      %[val7],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val8],        %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.50979557910415916894                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val7]                 \n\t"
-        "mul.s      %[val8],        %[val8],        %[fTmp7]                \n\t"
-        "madd.s     %[val23],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val24],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
+        "mul.s      %[val8],        %[val8],        %[f4]                   \n\t"
+        "madd.s     %[val23],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val24],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
         "add.s      %[val0],        %[val0],        %[val7]                 \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val7],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val8]                 \n\t"
         "add.s      %[val8],        %[val15],       %[val8]                 \n\t"
-        "mul.s      %[val24],       %[val24],       %[fTmp7]                \n\t"
+        "mul.s      %[val24],       %[val24],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val16],       %[val23]                \n\t"
         "add.s      %[val16],       %[val16],       %[val23]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp4],       %[val31],       %[val24]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val23],       %[f7],          %[fTmp3]                \n\t"
         "add.s      %[val24],       %[val31],       %[val24]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val31],       %[f7],          %[fTmp4]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5] "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8] "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val0]  "=f" (val0),  [val7]  "=f" (val7),
-          [val8]  "=f" (val8),  [val15] "=f" (val15),
-          [val16] "=f" (val16), [val23] "=f" (val23),
-          [val24] "=f" (val24), [val31] "=f" (val31)
-        : [tab] "r" (tab)
+          [fTmp8] "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val0]  "=&f" (val0),  [val7]  "=&f" (val7),
+          [val8]  "=&f" (val8),  [val15] "=&f" (val15),
+          [val16] "=&f" (val16), [val23] "=&f" (val23),
+          [val24] "=&f" (val24), [val31] "=&f" (val31)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.64682178335999012954;
+    f2 = 0.53104259108978417447;
+    f3 = 1.48416461631416627724;
+    f4 = 0.78815462345125022473;
+    f5 = 0.55310389603444452782;
+    f6 = 1.16943993343288495515;
+    f7 = 2.56291544774150617881;
     __asm__ volatile (
         "lwc1       %[fTmp1],       3*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       28*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       12*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       19*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.64682178335999012954                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.53104259108978417447                  \n\t"
-        "li.s       %[fTmp11],      1.48416461631416627724                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val3],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val12],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       4*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       27*4(%[tab])                            \n\t"
-        "madd.s     %[val19],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val28],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val12],       %[val12],       %[fTmp7]                \n\t"
+        "madd.s     %[val19],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val28],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val12],       %[val12],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       11*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       20*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val28],       %[val28],       %[fTmp7]                \n\t"
+        "mul.s      %[val28],       %[val28],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
-        "li.s       %[fTmp7],       0.78815462345125022473                  \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.55310389603444452782                  \n\t"
-        "li.s       %[fTmp11],      1.16943993343288495515                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "add.s      %[val4],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val11],       %[fTmp5],       %[fTmp6]                \n\t"
-        "li.s       %[fTmp1],       2.56291544774150617881                  \n\t"
-        "madd.s     %[val20],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val27],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val11],       %[val11],       %[fTmp7]                \n\t"
+        "madd.s     %[val20],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val27],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "mul.s      %[val11],       %[val11],       %[f4]                   \n\t"
         "sub.s      %[fTmp2],       %[val3],        %[val4]                 \n\t"
         "add.s      %[val3],        %[val3],        %[val4]                 \n\t"
         "sub.s      %[fTmp4],       %[val19],       %[val20]                \n\t"
-        "mul.s      %[val27],       %[val27],       %[fTmp7]                \n\t"
+        "mul.s      %[val27],       %[val27],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val12],       %[val11]                \n\t"
-        "mul.s      %[val4],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val4],        %[f7],          %[fTmp2]                \n\t"
         "add.s      %[val11],       %[val12],       %[val11]                \n\t"
         "add.s      %[val19],       %[val19],       %[val20]                \n\t"
-        "mul.s      %[val20],       %[fTmp1],       %[fTmp4]                \n\t"
-        "mul.s      %[val12],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val20],       %[f7],          %[fTmp4]                \n\t"
+        "mul.s      %[val12],       %[f7],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val28],       %[val27]                \n\t"
         "add.s      %[val27],       %[val28],       %[val27]                \n\t"
-        "mul.s      %[val28],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val28],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val3]  "=f" (val3),  [val4]  "=f" (val4),
-          [val11] "=f" (val11), [val12] "=f" (val12),
-          [val19] "=f" (val19), [val20] "=f" (val20),
-          [val27] "=f" (val27), [val28] "=f" (val28)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val3]  "=&f" (val3),  [val4]  "=&f" (val4),
+          [val11] "=&f" (val11), [val12] "=&f" (val12),
+          [val19] "=&f" (val19), [val20] "=&f" (val20),
+          [val27] "=&f" (val27), [val28] "=&f" (val28)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.54119610014619698439;
     __asm__ volatile (
-        "li.s       %[fTmp1],       0.54119610014619698439                  \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val3]                 \n\t"
         "add.s      %[val0],        %[val0],        %[val3]                 \n\t"
         "sub.s      %[fTmp3],       %[val7],        %[val4]                 \n\t"
         "add.s      %[val4],        %[val7],        %[val4]                 \n\t"
         "sub.s      %[fTmp4],       %[val8],        %[val11]                \n\t"
-        "mul.s      %[val3],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val3],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val8],        %[val8],        %[val11]                \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val7],        %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val12]                \n\t"
-        "mul.s      %[val11],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val11],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val12],       %[val15],       %[val12]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f1],          %[fTmp2]                \n\t"
 
-        : [val0]  "+f" (val0),   [val3] "+f" (val3),
-          [val4]  "+f" (val4),   [val7] "+f" (val7),
-          [val8]  "+f" (val8),   [val11] "+f" (val11),
-          [val12] "+f" (val12),  [val15] "+f" (val15),
-          [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [val0]  "+&f" (val0),   [val3] "+&f" (val3),
+          [val4]  "+&f" (val4),   [val7] "+&f" (val7),
+          [val8]  "+&f" (val8),   [val11] "+&f" (val11),
+          [val12] "+&f" (val12),  [val15] "+&f" (val15),
+          [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4)
-        :
+        : [f1] "f" (f1)
     );
 
     __asm__ volatile (
@@ -449,169 +449,169 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val23],       %[val20]                \n\t"
         "add.s      %[val20],       %[val23],       %[val20]                \n\t"
         "sub.s      %[fTmp4],       %[val24],       %[val27]                \n\t"
-        "mul.s      %[val19],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val19],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val24],       %[val24],       %[val27]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val23],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val31],       %[val28]                \n\t"
-        "mul.s      %[val27],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val27],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val28],       %[val31],       %[val28]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val31],       %[f1],          %[fTmp2]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val16] "+f" (val16), [val19] "+f" (val19), [val20] "+f" (val20),
-          [val23] "+f" (val23), [val24] "+f" (val24), [val27] "+f" (val27),
-          [val28] "+f" (val28), [val31] "+f" (val31)
-        : [fTmp1] "f" (fTmp1)
+          [val16] "+&f" (val16), [val19] "+&f" (val19), [val20] "+&f" (val20),
+          [val23] "+&f" (val23), [val24] "+&f" (val24), [val27] "+&f" (val27),
+          [val28] "+&f" (val28), [val31] "+&f" (val31)
+        : [f1] "f" (f1)
     );
 
+    f1 = 0.52249861493968888062;
+    f2 = 0.50547095989754365998;
+    f3 = 3.40760841846871878570;
+    f4 = 1.72244709823833392782;
+    f5 = 0.62250412303566481615;
+    f6 = 0.83934964541552703873;
+    f7 = 0.60134488693504528054;
     __asm__ volatile (
         "lwc1       %[fTmp1],       1*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       30*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       14*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       17*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.52249861493968888062                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.50547095989754365998                  \n\t"
-        "li.s       %[fTmp11],      3.40760841846871878570                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val1],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val14],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       6*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       25*4(%[tab])                            \n\t"
-        "madd.s     %[val17],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val30],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val14],       %[val14],       %[fTmp7]                \n\t"
+        "madd.s     %[val17],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val30],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val14],       %[val14],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       9*4(%[tab])                             \n\t"
         "lwc1       %[fTmp4],       22*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val30],       %[val30],       %[fTmp7]                \n\t"
+        "mul.s      %[val30],       %[val30],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       1.72244709823833392782                  \n\t"
-        "li.s       %[fTmp10],      0.62250412303566481615                  \n\t"
-        "li.s       %[fTmp11],      0.83934964541552703873                  \n\t"
         "add.s      %[val6],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val9],        %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.60134488693504528054                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val1],        %[val6]                 \n\t"
         "add.s      %[val1],        %[val1],        %[val6]                 \n\t"
-        "mul.s      %[val9],        %[val9],        %[fTmp7]                \n\t"
-        "madd.s     %[val22],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val25],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val6],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val9],        %[val9],        %[f4]                   \n\t"
+        "madd.s     %[val22],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val25],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "mul.s      %[val6],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val14],       %[val9]                 \n\t"
         "add.s      %[val9],        %[val14],       %[val9]                 \n\t"
-        "mul.s      %[val25],       %[val25],       %[fTmp7]                \n\t"
+        "mul.s      %[val25],       %[val25],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val17],       %[val22]                \n\t"
         "add.s      %[val17],       %[val17],       %[val22]                \n\t"
-        "mul.s      %[val14],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val14],       %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val30],       %[val25]                \n\t"
-        "mul.s      %[val22],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val22],       %[f7],          %[fTmp3]                \n\t"
         "add.s      %[val25],       %[val30],       %[val25]                \n\t"
-        "mul.s      %[val30],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val30],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val1]  "=f" (val1),  [val6]  "=f" (val6),
-          [val9]  "=f" (val9),  [val14] "=f" (val14),
-          [val17] "=f" (val17), [val22] "=f" (val22),
-          [val25] "=f" (val25), [val30] "=f" (val30)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val1]  "=&f" (val1),  [val6]  "=&f" (val6),
+          [val9]  "=&f" (val9),  [val14] "=&f" (val14),
+          [val17] "=&f" (val17), [val22] "=&f" (val22),
+          [val25] "=&f" (val25), [val30] "=&f" (val30)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.56694403481635770368;
+    f2 = 0.51544730992262454697;
+    f3 = 2.05778100995341155085;
+    f4 = 1.06067768599034747134;
+    f5 = 0.58293496820613387367;
+    f6 = 0.97256823786196069369;
+    f7 = 0.89997622313641570463;
     __asm__ volatile (
         "lwc1       %[fTmp1],       2*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       29*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       13*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       18*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.56694403481635770368                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.51544730992262454697                  \n\t"
-        "li.s       %[fTmp11],      2.05778100995341155085                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val2],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val13],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       5*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       26*4(%[tab])                            \n\t"
-        "madd.s     %[val18],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val29],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val13],       %[val13],       %[fTmp7]                \n\t"
+        "madd.s     %[val18],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val29],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val13],       %[val13],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       10*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       21*4(%[tab])                            \n\t"
-        "mul.s      %[val29],       %[val29],       %[fTmp7]                \n\t"
+        "mul.s      %[val29],       %[val29],       %[f1]                   \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       1.06067768599034747134                  \n\t"
-        "li.s       %[fTmp10],      0.58293496820613387367                  \n\t"
-        "li.s       %[fTmp11],      0.97256823786196069369                  \n\t"
         "add.s      %[val5],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val10],       %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.89997622313641570463                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val2],        %[val5]                 \n\t"
-        "mul.s      %[val10],       %[val10],       %[fTmp7]                \n\t"
-        "madd.s     %[val21],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val26],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
+        "mul.s      %[val10],       %[val10],       %[f4]                   \n\t"
+        "madd.s     %[val21],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val26],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
         "add.s      %[val2],        %[val2],        %[val5]                 \n\t"
-        "mul.s      %[val5],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val5],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp3],       %[val13],       %[val10]                \n\t"
         "add.s      %[val10],       %[val13],       %[val10]                \n\t"
-        "mul.s      %[val26],       %[val26],       %[fTmp7]                \n\t"
+        "mul.s      %[val26],       %[val26],       %[f4]                   \n\t"
         "sub.s      %[fTmp4],       %[val18],       %[val21]                \n\t"
         "add.s      %[val18],       %[val18],       %[val21]                \n\t"
-        "mul.s      %[val13],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val13],       %[f7],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val29],       %[val26]                \n\t"
         "add.s      %[val26],       %[val29],       %[val26]                \n\t"
-        "mul.s      %[val21],       %[fTmp1],       %[fTmp4]                \n\t"
-        "mul.s      %[val29],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val21],       %[f7],          %[fTmp4]                \n\t"
+        "mul.s      %[val29],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val2]  "=f" (val2),  [val5]  "=f" (val5),
-          [val10] "=f" (val10), [val13] "=f" (val13),
-          [val18] "=f" (val18), [val21] "=f" (val21),
-          [val26] "=f" (val26), [val29] "=f" (val29)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val2]  "=&f" (val2),  [val5]  "=&f" (val5),
+          [val10] "=&f" (val10), [val13] "=&f" (val13),
+          [val18] "=&f" (val18), [val21] "=&f" (val21),
+          [val26] "=&f" (val26), [val29] "=&f" (val29)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 1.30656296487637652785;
     __asm__ volatile (
-        "li.s       %[fTmp1],       1.30656296487637652785                  \n\t"
         "sub.s      %[fTmp2],       %[val1],        %[val2]                 \n\t"
         "add.s      %[val1],        %[val1],        %[val2]                 \n\t"
         "sub.s      %[fTmp3],       %[val6],        %[val5]                 \n\t"
         "add.s      %[val5],        %[val6],        %[val5]                 \n\t"
         "sub.s      %[fTmp4],       %[val9],        %[val10]                \n\t"
-        "mul.s      %[val2],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val2],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val9],        %[val9],        %[val10]                \n\t"
-        "mul.s      %[val6],        %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val6],        %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val14],       %[val13]                \n\t"
-        "mul.s      %[val10],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val10],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val13],       %[val14],       %[val13]                \n\t"
-        "mul.s      %[val14],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val14],       %[f1],          %[fTmp2]                \n\t"
 
-        : [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val1]  "+f" (val1),  [val2]  "+f" (val2),
-          [val5]  "+f" (val5),  [val6]  "+f" (val6),
-          [val9]  "+f" (val9),  [val10] "+f" (val10),
-          [val13] "+f" (val13), [val14] "+f" (val14)
-        :
+          [val1]  "+&f" (val1),  [val2]  "+&f" (val2),
+          [val5]  "+&f" (val5),  [val6]  "+&f" (val6),
+          [val9]  "+&f" (val9),  [val10] "+&f" (val10),
+          [val13] "+&f" (val13), [val14] "+&f" (val14)
+        : [f1]"f"(f1)
     );
 
     __asm__ volatile (
@@ -620,39 +620,39 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val22],       %[val21]                \n\t"
         "add.s      %[val21],       %[val22],       %[val21]                \n\t"
         "sub.s      %[fTmp4],       %[val25],       %[val26]                \n\t"
-        "mul.s      %[val18],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val18],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val25],       %[val25],       %[val26]                \n\t"
-        "mul.s      %[val22],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val22],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val30],       %[val29]                \n\t"
-        "mul.s      %[val26],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val26],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val29],       %[val30],       %[val29]                \n\t"
-        "mul.s      %[val30],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val30],       %[f1],          %[fTmp2]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val17] "+f" (val17), [val18] "+f" (val18), [val21] "+f" (val21),
-          [val22] "+f" (val22), [val25] "+f" (val25), [val26] "+f" (val26),
-          [val29] "+f" (val29), [val30] "+f" (val30)
-        : [fTmp1] "f" (fTmp1)
+          [val17] "+&f" (val17), [val18] "+&f" (val18), [val21] "+&f" (val21),
+          [val22] "+&f" (val22), [val25] "+&f" (val25), [val26] "+&f" (val26),
+          [val29] "+&f" (val29), [val30] "+&f" (val30)
+        : [f1] "f" (f1)
     );
 
+    f1 = 0.70710678118654752439;
     __asm__ volatile (
-        "li.s       %[fTmp1],       0.70710678118654752439                  \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val1]                 \n\t"
         "add.s      %[val0],        %[val0],        %[val1]                 \n\t"
         "sub.s      %[fTmp3],       %[val3],        %[val2]                 \n\t"
         "add.s      %[val2],        %[val3],        %[val2]                 \n\t"
         "sub.s      %[fTmp4],       %[val4],        %[val5]                 \n\t"
-        "mul.s      %[val1],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val1],        %[f1],          %[fTmp2]                \n\t"
         "swc1       %[val0],        0(%[out])                               \n\t"
-        "mul.s      %[val3],        %[fTmp3],       %[fTmp1]                \n\t"
+        "mul.s      %[val3],        %[fTmp3],       %[f1]                   \n\t"
         "add.s      %[val4],        %[val4],        %[val5]                 \n\t"
-        "mul.s      %[val5],        %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val5],        %[f1],          %[fTmp4]                \n\t"
         "swc1       %[val1],        16*4(%[out])                            \n\t"
         "sub.s      %[fTmp2],       %[val7],        %[val6]                 \n\t"
         "add.s      %[val2],        %[val2],        %[val3]                 \n\t"
         "swc1       %[val3],        24*4(%[out])                            \n\t"
         "add.s      %[val6],        %[val7],        %[val6]                 \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val7],        %[f1],          %[fTmp2]                \n\t"
         "swc1       %[val2],        8*4(%[out])                             \n\t"
         "add.s      %[val6],        %[val6],        %[val7]                 \n\t"
         "swc1       %[val7],        28*4(%[out])                            \n\t"
@@ -663,13 +663,13 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "swc1       %[val5],        20*4(%[out])                            \n\t"
         "swc1       %[val6],        12*4(%[out])                            \n\t"
 
-        : [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val0] "+f" (val0), [val1] "+f" (val1),
-          [val2] "+f" (val2), [val3] "+f" (val3),
-          [val4] "+f" (val4), [val5] "+f" (val5),
-          [val6] "+f" (val6), [val7] "+f" (val7)
-        : [out] "r" (out)
+          [val0] "+&f" (val0), [val1] "+&f" (val1),
+          [val2] "+&f" (val2), [val3] "+&f" (val3),
+          [val4] "+&f" (val4), [val5] "+&f" (val5),
+          [val6] "+&f" (val6), [val7] "+&f" (val7)
+        : [out] "r" (out), [f1]"f"(f1)
     );
 
     __asm__ volatile (
@@ -678,14 +678,14 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val11],       %[val10]                \n\t"
         "add.s      %[val10],       %[val11],       %[val10]                \n\t"
         "sub.s      %[fTmp4],       %[val12],       %[val13]                \n\t"
-        "mul.s      %[val9],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val9],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val12],       %[val12],       %[val13]                \n\t"
-        "mul.s      %[val11],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val11],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val14]                \n\t"
-        "mul.s      %[val13],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val13],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val14],       %[val15],       %[val14]                \n\t"
         "add.s      %[val10],       %[val10],       %[val11]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val14],       %[val14],       %[val15]                \n\t"
         "add.s      %[val12],       %[val12],       %[val14]                \n\t"
         "add.s      %[val14],       %[val14],       %[val13]                \n\t"
@@ -707,10 +707,10 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "swc1       %[val15],       30*4(%[out])                            \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val8]  "+f" (val8),  [val9]  "+f" (val9),  [val10] "+f" (val10),
-          [val11] "+f" (val11), [val12] "+f" (val12), [val13] "+f" (val13),
-          [val14] "+f" (val14), [val15] "+f" (val15)
-        : [fTmp1] "f" (fTmp1), [out] "r" (out)
+          [val8]  "+&f" (val8),  [val9]  "+&f" (val9),  [val10] "+&f" (val10),
+          [val11] "+&f" (val11), [val12] "+&f" (val12), [val13] "+&f" (val13),
+          [val14] "+&f" (val14), [val15] "+&f" (val15)
+        : [f1] "f" (f1), [out] "r" (out)
     );
 
     __asm__ volatile (
@@ -719,24 +719,24 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val19],       %[val18]                \n\t"
         "add.s      %[val18],       %[val19],       %[val18]                \n\t"
         "sub.s      %[fTmp4],       %[val20],       %[val21]                \n\t"
-        "mul.s      %[val17],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val17],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val20],       %[val20],       %[val21]                \n\t"
-        "mul.s      %[val19],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val19],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val23],       %[val22]                \n\t"
-        "mul.s      %[val21],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val21],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val22],       %[val23],       %[val22]                \n\t"
         "add.s      %[val18],       %[val18],       %[val19]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val23],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val22],       %[val22],       %[val23]                \n\t"
         "add.s      %[val20],       %[val20],       %[val22]                \n\t"
         "add.s      %[val22],       %[val22],       %[val21]                \n\t"
         "add.s      %[val21],       %[val21],       %[val23]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val16] "+f" (val16), [val17] "+f" (val17), [val18] "+f" (val18),
-          [val19] "+f" (val19), [val20] "+f" (val20), [val21] "+f" (val21),
-          [val22] "+f" (val22), [val23] "+f" (val23)
-        : [fTmp1] "f" (fTmp1)
+          [val16] "+&f" (val16), [val17] "+&f" (val17), [val18] "+&f" (val18),
+          [val19] "+&f" (val19), [val20] "+&f" (val20), [val21] "+&f" (val21),
+          [val22] "+&f" (val22), [val23] "+&f" (val23)
+        : [f1] "f" (f1)
     );
 
     __asm__ volatile (
@@ -745,14 +745,14 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val27],       %[val26]                \n\t"
         "add.s      %[val26],       %[val27],       %[val26]                \n\t"
         "sub.s      %[fTmp4],       %[val28],       %[val29]                \n\t"
-        "mul.s      %[val25],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val25],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val28],       %[val28],       %[val29]                \n\t"
-        "mul.s      %[val27],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val27],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val31],       %[val30]                \n\t"
-        "mul.s      %[val29],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val29],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val30],       %[val31],       %[val30]                \n\t"
         "add.s      %[val26],       %[val26],       %[val27]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val31],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val30],       %[val30],       %[val31]                \n\t"
         "add.s      %[val28],       %[val28],       %[val30]                \n\t"
         "add.s      %[val30],       %[val30],       %[val29]                \n\t"
@@ -766,10 +766,10 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "add.s      %[val27],       %[val27],       %[val31]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val24] "+f" (val24), [val25] "+f" (val25), [val26] "+f" (val26),
-          [val27] "+f" (val27), [val28] "+f" (val28), [val29] "+f" (val29),
-          [val30] "+f" (val30), [val31] "+f" (val31)
-        : [fTmp1] "f" (fTmp1)
+          [val24] "+&f" (val24), [val25] "+&f" (val25), [val26] "+&f" (val26),
+          [val27] "+&f" (val27), [val28] "+&f" (val28), [val29] "+&f" (val29),
+          [val30] "+&f" (val30), [val31] "+&f" (val31)
+        : [f1] "f" (f1)
     );
 
     out[ 1] = val16 + val24;
@@ -797,7 +797,7 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
     /* temporary variables */
     float in1, in2, in3, in4, in5, in6;
     float out1, out2, out3, out4, out5;
-    float c1, c2, c3, c4, c5, c6, c7, c8, c9;
+    float f1, f2, f3, f4, f5, f6, f7, f8, f9;
 
     /**
     * all loops are unrolled totally, and instructions are scheduled to
@@ -881,33 +881,36 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
     );
 
     /* loop 3 */
+    f1 = 0.5;
+    f2 = 0.93969262078590838405;
+    f3 = -0.76604444311897803520;
+    f4 = -0.17364817766693034885;
+    f5 = -0.86602540378443864676;
+    f6 = 0.98480775301220805936;
+    f7 = -0.34202014332566873304;
+    f8 = 0.86602540378443864676;
+    f9 = -0.64278760968653932632;
     __asm__ volatile (
-        "li.s    %[c1],   0.5                                           \t\n"
         "lwc1    %[in1],  8*4(%[in])                                    \t\n"
         "lwc1    %[in2],  16*4(%[in])                                   \t\n"
         "lwc1    %[in3],  4*4(%[in])                                    \t\n"
         "lwc1    %[in4],  0(%[in])                                      \t\n"
         "lwc1    %[in5],  12*4(%[in])                                   \t\n"
-        "li.s    %[c2],   0.93969262078590838405                        \t\n"
         "add.s   %[t2],   %[in1],  %[in2]                               \t\n"
         "add.s   %[t0],   %[in1],  %[in3]                               \t\n"
-        "li.s    %[c3],   -0.76604444311897803520                       \t\n"
-        "madd.s  %[t3],   %[in4],  %[in5], %[c1]                        \t\n"
+        "madd.s  %[t3],   %[in4],  %[in5], %[f1]                        \t\n"
         "sub.s   %[t1],   %[in4],  %[in5]                               \t\n"
         "sub.s   %[t2],   %[t2],   %[in3]                               \t\n"
-        "mul.s   %[t0],   %[t0],   %[c2]                                \t\n"
-        "li.s    %[c4],   -0.17364817766693034885                       \t\n"
-        "li.s    %[c5],   -0.86602540378443864676                       \t\n"
-        "li.s    %[c6],   0.98480775301220805936                        \t\n"
-        "nmsub.s %[out1], %[t1],   %[t2],  %[c1]                        \t\n"
+        "mul.s   %[t0],   %[t0],   %[f2]                                \t\n"
+        "nmsub.s %[out1], %[t1],   %[t2],  %[f1]                        \t\n"
         "add.s   %[out2], %[t1],   %[t2]                                \t\n"
         "add.s   %[t2],   %[in2],  %[in3]                               \t\n"
         "sub.s   %[t1],   %[in1],  %[in2]                               \t\n"
         "sub.s   %[out3], %[t3],   %[t0]                                \t\n"
         "swc1    %[out1], 6*4(%[tmp])                                   \t\n"
         "swc1    %[out2], 16*4(%[tmp])                                  \t\n"
-        "mul.s   %[t2],   %[t2],   %[c3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c4]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f3]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f4]                                \t\n"
         "add.s   %[out1], %[t3],   %[t0]                                \t\n"
         "lwc1    %[in1],  10*4(%[in])                                   \t\n"
         "lwc1    %[in2],  14*4(%[in])                                   \t\n"
@@ -923,19 +926,16 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s   %[t2],   %[in1],  %[in3]                               \t\n"
         "sub.s   %[t3],   %[in1],  %[in2]                               \t\n"
         "swc1    %[out2], 14*4(%[tmp])                                  \t\n"
-        "li.s    %[c7],   -0.34202014332566873304                       \t\n"
         "sub.s   %[out1], %[out1], %[in3]                               \t\n"
-        "mul.s   %[t2],   %[t2],   %[c6]                                \t\n"
-        "mul.s   %[t3],   %[t3],   %[c7]                                \t\n"
-        "li.s    %[c8],   0.86602540378443864676                        \t\n"
-        "mul.s   %[t0],   %[in4],  %[c8]                                \t\n"
-        "mul.s   %[out1], %[out1], %[c5]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f6]                                \t\n"
+        "mul.s   %[t3],   %[t3],   %[f7]                                \t\n"
+        "mul.s   %[t0],   %[in4],  %[f8]                                \t\n"
+        "mul.s   %[out1], %[out1], %[f5]                                \t\n"
         "add.s   %[t1],   %[in2],  %[in3]                               \t\n"
-        "li.s    %[c9],   -0.64278760968653932632                       \t\n"
         "add.s   %[out2], %[t2],   %[t3]                                \t\n"
         "lwc1    %[in1],  9*4(%[in])                                    \t\n"
         "swc1    %[out1], 4*4(%[tmp])                                   \t\n"
-        "mul.s   %[t1],   %[t1],   %[c9]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f9]                                \t\n"
         "lwc1    %[in2],  17*4(%[in])                                   \t\n"
         "add.s   %[out2], %[out2], %[t0]                                \t\n"
         "lwc1    %[in3],  5*4(%[in])                                    \t\n"
@@ -948,21 +948,21 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "sub.s   %[out3], %[out3], %[t0]                                \t\n"
         "sub.s   %[out1], %[out1], %[t0]                                \t\n"
         "add.s   %[t0],   %[in1],  %[in3]                               \t\n"
-        "madd.s  %[t3],   %[in4],  %[in5], %[c1]                        \t\n"
+        "madd.s  %[t3],   %[in4],  %[in5], %[f1]                        \t\n"
         "sub.s   %[t2],   %[t2],   %[in3]                               \t\n"
         "swc1    %[out3], 12*4(%[tmp])                                  \t\n"
         "swc1    %[out1], 8*4(%[tmp])                                   \t\n"
         "sub.s   %[t1],   %[in4],  %[in5]                               \t\n"
-        "mul.s   %[t0],   %[t0],   %[c2]                                \t\n"
-        "nmsub.s %[out1], %[t1],   %[t2],  %[c1]                        \t\n"
+        "mul.s   %[t0],   %[t0],   %[f2]                                \t\n"
+        "nmsub.s %[out1], %[t1],   %[t2],  %[f1]                        \t\n"
         "add.s   %[out2], %[t1],   %[t2]                                \t\n"
         "add.s   %[t2],   %[in2],  %[in3]                               \t\n"
         "sub.s   %[t1],   %[in1],  %[in2]                               \t\n"
         "sub.s   %[out3], %[t3],   %[t0]                                \t\n"
         "swc1    %[out1], 7*4(%[tmp])                                   \t\n"
         "swc1    %[out2], 17*4(%[tmp])                                  \t\n"
-        "mul.s   %[t2],   %[t2],   %[c3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c4]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f3]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f4]                                \t\n"
         "add.s   %[out1], %[t3],   %[t0]                                \t\n"
         "lwc1    %[in1],  11*4(%[in])                                   \t\n"
         "lwc1    %[in2],  15*4(%[in])                                   \t\n"
@@ -978,14 +978,14 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s   %[t2],   %[in1],  %[in3]                               \t\n"
         "sub.s   %[t3],   %[in1],  %[in2]                               \t\n"
         "swc1    %[out2], 15*4(%[tmp])                                  \t\n"
-        "mul.s   %[t0],   %[in4],  %[c8]                                \t\n"
+        "mul.s   %[t0],   %[in4],  %[f8]                                \t\n"
         "sub.s   %[out3], %[out3], %[in3]                               \t\n"
-        "mul.s   %[t2],   %[t2],   %[c6]                                \t\n"
-        "mul.s   %[t3],   %[t3],   %[c7]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f6]                                \t\n"
+        "mul.s   %[t3],   %[t3],   %[f7]                                \t\n"
         "add.s   %[t1],   %[in2],  %[in3]                               \t\n"
-        "mul.s   %[out3], %[out3], %[c5]                                \t\n"
+        "mul.s   %[out3], %[out3], %[f5]                                \t\n"
         "add.s   %[out1], %[t2],   %[t3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c9]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f9]                                \t\n"
         "swc1    %[out3], 5*4(%[tmp])                                   \t\n"
         "add.s   %[out1], %[out1], %[t0]                                \t\n"
         "add.s   %[out2], %[t2],   %[t1]                                \t\n"
@@ -1000,26 +1000,29 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
           [t2] "=&f" (t2), [t3] "=&f" (t3),
           [in1] "=&f" (in1), [in2] "=&f" (in2),
           [in3] "=&f" (in3), [in4] "=&f" (in4),
-          [in5] "=&f" (in5),
-          [out1] "=&f" (out1), [out2] "=&f" (out2),
-          [out3] "=&f" (out3),
-          [c1] "=&f" (c1), [c2] "=&f" (c2),
-          [c3] "=&f" (c3), [c4] "=&f" (c4),
-          [c5] "=&f" (c5), [c6] "=&f" (c6),
-          [c7] "=&f" (c7), [c8] "=&f" (c8),
-          [c9] "=&f" (c9)
-        : [in] "r" (in), [tmp] "r" (tmp)
+          [in5] "=&f" (in5), [out1] "=&f" (out1),
+          [out2] "=&f" (out2), [out3] "=&f" (out3)
+        : [in] "r" (in), [tmp] "r" (tmp), [f1]"f"(f1), [f2]"f"(f2),
+          [f3]"f"(f3), [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6),
+          [f7]"f"(f7), [f8]"f"(f8), [f9]"f"(f9)
         : "memory"
     );
 
     /* loop 4 */
+    f1 = 0.50190991877167369479;
+    f2 = 5.73685662283492756461;
+    f3 = 0.51763809020504152469;
+    f4 = 1.93185165257813657349;
+    f5 = 0.55168895948124587824;
+    f6 = 1.18310079157624925896;
+    f7 = 0.61038729438072803416;
+    f8 = 0.87172339781054900991;
+    f9 = 0.70710678118654752439;
     __asm__ volatile (
         "lwc1   %[in1],  2*4(%[tmp])                                    \t\n"
         "lwc1   %[in2],  0(%[tmp])                                      \t\n"
         "lwc1   %[in3],  3*4(%[tmp])                                    \t\n"
         "lwc1   %[in4],  1*4(%[tmp])                                    \t\n"
-        "li.s   %[c1],   0.50190991877167369479                         \t\n"
-        "li.s   %[c2],   5.73685662283492756461                         \t\n"
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "add.s  %[s1],   %[in3], %[in4]                                 \t\n"
@@ -1027,15 +1030,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  9*4(%[win])                                    \t\n"
         "lwc1   %[in2],  4*9*4(%[buf])                                  \t\n"
         "lwc1   %[in3],  8*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f1]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f2]                                  \t\n"
         "lwc1   %[in4],  4*8*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  29*4(%[win])                                   \t\n"
         "lwc1   %[in6],  28*4(%[win])                                   \t\n"
         "add.s  %[t0],   %[s0],  %[s1]                                  \t\n"
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
-        "li.s   %[c1],   0.51763809020504152469                         \t\n"
-        "li.s   %[c2],   1.93185165257813657349                         \t\n"
         "mul.s  %[out3], %[in5], %[t0]                                  \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
@@ -1071,14 +1072,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  10*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*10*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  7*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f3]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f4]                                  \t\n"
         "add.s  %[t0],   %[s0],  %[s1]                                  \t\n"
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
         "lwc1   %[in4],  4*7*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  30*4(%[win])                                   \t\n"
         "lwc1   %[in6],  27*4(%[win])                                   \t\n"
-        "li.s   %[c1],   0.55168895948124587824                         \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
         "mul.s  %[out3], %[t0],  %[in5]                                 \t\n"
@@ -1105,7 +1105,6 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "swc1   %[out2], 32*4(%[out])                                   \t\n"
         "swc1   %[out3], 4*16*4(%[buf])                                 \t\n"
         "swc1   %[out4], 4*1*4(%[buf])                                  \t\n"
-        "li.s   %[c2],   1.18310079157624925896                         \t\n"
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "lwc1   %[in3],  11*4(%[tmp])                                   \t\n"
@@ -1115,8 +1114,8 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  11*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*11*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  6*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f5]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f6]                                  \t\n"
         "lwc1   %[in4],  4*6*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  31*4(%[win])                                   \t\n"
         "lwc1   %[in6],  26*4(%[win])                                   \t\n"
@@ -1152,15 +1151,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "lwc1   %[in4],  13*4(%[tmp])                                   \t\n"
-        "li.s   %[c1],   0.61038729438072803416                         \t\n"
-        "li.s   %[c2],   0.87172339781054900991                         \t\n"
         "add.s  %[s1],   %[in3], %[in4]                                 \t\n"
         "sub.s  %[s3],   %[in3], %[in4]                                 \t\n"
         "lwc1   %[in1],  12*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*12*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  5*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f7]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f8]                                  \t\n"
         "lwc1   %[in4],  4*5*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  32*4(%[win])                                   \t\n"
         "lwc1   %[in6],  25*4(%[win])                                   \t\n"
@@ -1168,7 +1165,6 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
         "lwc1   %[s0],   16*4(%[tmp])                                   \t\n"
         "lwc1   %[s1],   17*4(%[tmp])                                   \t\n"
-        "li.s   %[c1],   0.70710678118654752439                         \t\n"
         "mul.s  %[out3], %[t0],  %[in5]                                 \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
@@ -1186,7 +1182,7 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in5],  34*4(%[win])                                   \t\n"
         "lwc1   %[in6],  23*4(%[win])                                   \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f9]                                  \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
         "mul.s  %[out3], %[in5], %[t0]                                  \t\n"
         "mul.s  %[out4], %[in6], %[t0]                                  \t\n"
@@ -1211,18 +1207,18 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "swc1   %[out3], 4*13*4(%[buf])                                 \t\n"
         "swc1   %[out4], 4*4*4(%[buf])                                  \t\n"
 
-        : [c1] "=&f" (c1), [c2] "=&f" (c2),
-          [in1] "=&f" (in1), [in2] "=&f" (in2),
+        : [in1] "=&f" (in1), [in2] "=&f" (in2),
           [in3] "=&f" (in3), [in4] "=&f" (in4),
           [in5] "=&f" (in5), [in6] "=&f" (in6),
           [out1] "=&f" (out1), [out2] "=&f" (out2),
           [out3] "=&f" (out3), [out4] "=&f" (out4),
           [t0] "=&f" (t0), [t1] "=&f" (t1),
-          [t2] "=&f" (t2), [t3] "=&f" (t3),
           [s0] "=&f" (s0), [s1] "=&f" (s1),
           [s2] "=&f" (s2), [s3] "=&f" (s3)
         : [tmp] "r" (tmp), [win] "r" (win),
-          [buf] "r" (buf), [out] "r" (out)
+          [buf] "r" (buf), [out] "r" (out),
+          [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3), [f4]"f"(f4),
+          [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7), [f8]"f"(f8), [f9]"f"(f9)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/mpegvideo_init_mips.c b/libavcodec/mips/mpegvideo_init_mips.c
index 1918da5..bfda90b 100644
--- a/libavcodec/mips/mpegvideo_init_mips.c
+++ b/libavcodec/mips/mpegvideo_init_mips.c
@@ -18,41 +18,31 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h263dsp_mips.h"
 #include "mpegvideo_mips.h"
 
-#if HAVE_MSA
-static av_cold void dct_unquantize_init_msa(MpegEncContext *s)
+av_cold void ff_mpv_common_init_mips(MpegEncContext *s)
 {
-    s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_msa;
-    s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_msa;
-    if (!s->q_scale_type)
-        s->dct_unquantize_mpeg2_inter = ff_dct_unquantize_mpeg2_inter_msa;
-}
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
 
-#if HAVE_MMI
-static av_cold void dct_unquantize_init_mmi(MpegEncContext *s)
-{
-    s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_mmi;
-    s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_mmi;
-    s->dct_unquantize_mpeg1_intra = ff_dct_unquantize_mpeg1_intra_mmi;
-    s->dct_unquantize_mpeg1_inter = ff_dct_unquantize_mpeg1_inter_mmi;
+    if (have_mmi(cpu_flags)) {
+        s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_mmi;
+        s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_mmi;
+        s->dct_unquantize_mpeg1_intra = ff_dct_unquantize_mpeg1_intra_mmi;
+        s->dct_unquantize_mpeg1_inter = ff_dct_unquantize_mpeg1_inter_mmi;
 
-    if (!(s->avctx->flags & AV_CODEC_FLAG_BITEXACT))
-        if (!s->q_scale_type)
-            s->dct_unquantize_mpeg2_intra = ff_dct_unquantize_mpeg2_intra_mmi;
+        if (!(s->avctx->flags & AV_CODEC_FLAG_BITEXACT))
+            if (!s->q_scale_type)
+                s->dct_unquantize_mpeg2_intra = ff_dct_unquantize_mpeg2_intra_mmi;
 
-    s->denoise_dct= ff_denoise_dct_mmi;
-}
-#endif /* HAVE_MMI */
+        s->denoise_dct= ff_denoise_dct_mmi;
+    }
 
-av_cold void ff_mpv_common_init_mips(MpegEncContext *s)
-{
-#if HAVE_MSA
-    dct_unquantize_init_msa(s);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    dct_unquantize_init_mmi(s);
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_msa;
+        s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_msa;
+        if (!s->q_scale_type)
+            s->dct_unquantize_mpeg2_inter = ff_dct_unquantize_mpeg2_inter_msa;
+    }
 }
diff --git a/libavcodec/mips/mpegvideo_mmi.c b/libavcodec/mips/mpegvideo_mmi.c
index 18058e4..e4aba08 100644
--- a/libavcodec/mips/mpegvideo_mmi.c
+++ b/libavcodec/mips/mpegvideo_mmi.c
@@ -410,9 +410,9 @@ void ff_dct_unquantize_mpeg2_intra_mmi(MpegEncContext *s, int16_t *block,
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "pandn      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
         "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
-        PTR_ADDIU  "%[addr0],   %[addr0],       0x10                    \n\t"
         MMI_SDXC1(%[ftmp5], %[addr0], %[block], 0x00)
         MMI_SDXC1(%[ftmp6], %[addr0], %[block], 0x08)
+        PTR_ADDIU  "%[addr0],   %[addr0],       0x10                    \n\t"
         "blez       %[addr0],   1b                                      \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
diff --git a/libavcodec/mips/mpegvideoencdsp_init_mips.c b/libavcodec/mips/mpegvideoencdsp_init_mips.c
index 9bfe94e..71831a6 100644
--- a/libavcodec/mips/mpegvideoencdsp_init_mips.c
+++ b/libavcodec/mips/mpegvideoencdsp_init_mips.c
@@ -18,23 +18,18 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavcodec/bit_depth_template.c"
 #include "h263dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void mpegvideoencdsp_init_msa(MpegvideoEncDSPContext *c,
-                                             AVCodecContext *avctx)
-{
-#if BIT_DEPTH == 8
-    c->pix_sum = ff_pix_sum_msa;
-#endif
-}
-#endif  // #if HAVE_MSA
-
 av_cold void ff_mpegvideoencdsp_init_mips(MpegvideoEncDSPContext *c,
                                           AVCodecContext *avctx)
 {
-#if HAVE_MSA
-    mpegvideoencdsp_init_msa(c, avctx);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
+#if BIT_DEPTH == 8
+        c->pix_sum = ff_pix_sum_msa;
+#endif
+    }
 }
diff --git a/libavcodec/mips/pixblockdsp_init_mips.c b/libavcodec/mips/pixblockdsp_init_mips.c
index 1b3741e..2e2d709 100644
--- a/libavcodec/mips/pixblockdsp_init_mips.c
+++ b/libavcodec/mips/pixblockdsp_init_mips.c
@@ -19,51 +19,38 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "pixblockdsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void pixblockdsp_init_msa(PixblockDSPContext *c,
-                                         AVCodecContext *avctx,
-                                         unsigned high_bit_depth)
+void ff_pixblockdsp_init_mips(PixblockDSPContext *c, AVCodecContext *avctx,
+                              unsigned high_bit_depth)
 {
-    c->diff_pixels = ff_diff_pixels_msa;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->diff_pixels = ff_diff_pixels_mmi;
 
-    switch (avctx->bits_per_raw_sample) {
-    case 9:
-    case 10:
-    case 12:
-    case 14:
-        c->get_pixels = ff_get_pixels_16_msa;
-        break;
-    default:
-        if (avctx->bits_per_raw_sample <= 8 || avctx->codec_type !=
-            AVMEDIA_TYPE_VIDEO) {
-            c->get_pixels = ff_get_pixels_8_msa;
+        if (!high_bit_depth || avctx->codec_type != AVMEDIA_TYPE_VIDEO) {
+            c->get_pixels = ff_get_pixels_8_mmi;
         }
-        break;
     }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void pixblockdsp_init_mmi(PixblockDSPContext *c,
-        AVCodecContext *avctx, unsigned high_bit_depth)
-{
-    c->diff_pixels = ff_diff_pixels_mmi;
+    if (have_msa(cpu_flags)) {
+        c->diff_pixels = ff_diff_pixels_msa;
 
-    if (!high_bit_depth || avctx->codec_type != AVMEDIA_TYPE_VIDEO) {
-        c->get_pixels = ff_get_pixels_8_mmi;
+        switch (avctx->bits_per_raw_sample) {
+        case 9:
+        case 10:
+        case 12:
+        case 14:
+            c->get_pixels = ff_get_pixels_16_msa;
+            break;
+        default:
+            if (avctx->bits_per_raw_sample <= 8 || avctx->codec_type !=
+                AVMEDIA_TYPE_VIDEO) {
+                c->get_pixels = ff_get_pixels_8_msa;
+            }
+            break;
+        }
     }
 }
-#endif /* HAVE_MMI */
-
-void ff_pixblockdsp_init_mips(PixblockDSPContext *c, AVCodecContext *avctx,
-                              unsigned high_bit_depth)
-{
-#if HAVE_MSA
-    pixblockdsp_init_msa(c, avctx, high_bit_depth);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    pixblockdsp_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
-}
diff --git a/libavcodec/mips/qpeldsp_init_mips.c b/libavcodec/mips/qpeldsp_init_mips.c
index 140e8f8..cccf9d4 100644
--- a/libavcodec/mips/qpeldsp_init_mips.c
+++ b/libavcodec/mips/qpeldsp_init_mips.c
@@ -18,150 +18,146 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "qpeldsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void qpeldsp_init_msa(QpelDSPContext *c)
+void ff_qpeldsp_init_mips(QpelDSPContext *c)
 {
-    c->put_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
-    c->put_qpel_pixels_tab[0][1] = ff_horiz_mc_qpel_aver_src0_16width_msa;
-    c->put_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_16width_msa;
-    c->put_qpel_pixels_tab[0][3] = ff_horiz_mc_qpel_aver_src1_16width_msa;
-    c->put_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_aver_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][5] = ff_hv_mc_qpel_aver_hv_src00_16x16_msa;
-    c->put_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_aver_v_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][7] = ff_hv_mc_qpel_aver_hv_src10_16x16_msa;
-    c->put_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_16x16_msa;
-    c->put_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_aver_h_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_16x16_msa;
-    c->put_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_aver_h_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_aver_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][13] = ff_hv_mc_qpel_aver_hv_src01_16x16_msa;
-    c->put_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_aver_v_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][15] = ff_hv_mc_qpel_aver_hv_src11_16x16_msa;
+    int cpu_flags = av_get_cpu_flags();
 
-    c->put_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
-    c->put_qpel_pixels_tab[1][1] = ff_horiz_mc_qpel_aver_src0_8width_msa;
-    c->put_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_8width_msa;
-    c->put_qpel_pixels_tab[1][3] = ff_horiz_mc_qpel_aver_src1_8width_msa;
-    c->put_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_aver_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_aver_hv_src00_8x8_msa;
-    c->put_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_aver_v_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_aver_hv_src10_8x8_msa;
-    c->put_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_8x8_msa;
-    c->put_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_aver_h_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_8x8_msa;
-    c->put_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_aver_h_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_aver_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_aver_hv_src01_8x8_msa;
-    c->put_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_aver_v_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_aver_hv_src11_8x8_msa;
+    if (have_msa(cpu_flags)) {
+        c->put_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
+        c->put_qpel_pixels_tab[0][1] = ff_horiz_mc_qpel_aver_src0_16width_msa;
+        c->put_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_16width_msa;
+        c->put_qpel_pixels_tab[0][3] = ff_horiz_mc_qpel_aver_src1_16width_msa;
+        c->put_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_aver_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][5] = ff_hv_mc_qpel_aver_hv_src00_16x16_msa;
+        c->put_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_aver_v_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][7] = ff_hv_mc_qpel_aver_hv_src10_16x16_msa;
+        c->put_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_16x16_msa;
+        c->put_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_aver_h_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_16x16_msa;
+        c->put_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_aver_h_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_aver_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][13] = ff_hv_mc_qpel_aver_hv_src01_16x16_msa;
+        c->put_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_aver_v_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][15] = ff_hv_mc_qpel_aver_hv_src11_16x16_msa;
 
-    c->put_no_rnd_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][1] =
-        ff_horiz_mc_qpel_no_rnd_aver_src0_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_no_rnd_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][3] =
-        ff_horiz_mc_qpel_no_rnd_aver_src1_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][4] =
-        ff_vert_mc_qpel_no_rnd_aver_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][5] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src00_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][6] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][7] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src10_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_no_rnd_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][9] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_no_rnd_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][11] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][12] =
-        ff_vert_mc_qpel_no_rnd_aver_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][13] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src01_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][14] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][15] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src11_16x16_msa;
+        c->put_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
+        c->put_qpel_pixels_tab[1][1] = ff_horiz_mc_qpel_aver_src0_8width_msa;
+        c->put_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_8width_msa;
+        c->put_qpel_pixels_tab[1][3] = ff_horiz_mc_qpel_aver_src1_8width_msa;
+        c->put_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_aver_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_aver_hv_src00_8x8_msa;
+        c->put_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_aver_v_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_aver_hv_src10_8x8_msa;
+        c->put_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_8x8_msa;
+        c->put_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_aver_h_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_8x8_msa;
+        c->put_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_aver_h_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_aver_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_aver_hv_src01_8x8_msa;
+        c->put_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_aver_v_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_aver_hv_src11_8x8_msa;
 
-    c->put_no_rnd_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][1] =
-        ff_horiz_mc_qpel_no_rnd_aver_src0_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_no_rnd_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][3] =
-        ff_horiz_mc_qpel_no_rnd_aver_src1_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][4] =
-        ff_vert_mc_qpel_no_rnd_aver_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][5] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][6] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][7] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_no_rnd_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][9] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_no_rnd_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][11] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][12] =
-        ff_vert_mc_qpel_no_rnd_aver_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][13] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][14] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][15] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][1] =
+            ff_horiz_mc_qpel_no_rnd_aver_src0_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_no_rnd_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][3] =
+            ff_horiz_mc_qpel_no_rnd_aver_src1_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][4] =
+            ff_vert_mc_qpel_no_rnd_aver_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][5] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src00_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][6] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][7] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src10_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_no_rnd_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][9] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_no_rnd_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][11] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][12] =
+            ff_vert_mc_qpel_no_rnd_aver_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][13] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src01_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][14] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][15] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src11_16x16_msa;
 
-    c->avg_qpel_pixels_tab[0][0] = ff_avg_width16_msa;
-    c->avg_qpel_pixels_tab[0][1] =
-        ff_horiz_mc_qpel_avg_dst_aver_src0_16width_msa;
-    c->avg_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_avg_dst_16width_msa;
-    c->avg_qpel_pixels_tab[0][3] =
-        ff_horiz_mc_qpel_avg_dst_aver_src1_16width_msa;
-    c->avg_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_avg_dst_aver_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][5] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src00_16x16_msa;
-    c->avg_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][7] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src10_16x16_msa;
-    c->avg_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_avg_dst_16x16_msa;
-    c->avg_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_avg_dst_16x16_msa;
-    c->avg_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_avg_dst_aver_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][13] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src01_16x16_msa;
-    c->avg_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][15] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src11_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][1] =
+            ff_horiz_mc_qpel_no_rnd_aver_src0_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_no_rnd_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][3] =
+            ff_horiz_mc_qpel_no_rnd_aver_src1_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][4] =
+            ff_vert_mc_qpel_no_rnd_aver_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][5] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][6] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][7] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_no_rnd_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][9] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_no_rnd_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][11] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][12] =
+            ff_vert_mc_qpel_no_rnd_aver_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][13] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][14] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][15] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa;
 
-    c->avg_qpel_pixels_tab[1][0] = ff_avg_width8_msa;
-    c->avg_qpel_pixels_tab[1][1] =
-        ff_horiz_mc_qpel_avg_dst_aver_src0_8width_msa;
-    c->avg_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_avg_dst_8width_msa;
-    c->avg_qpel_pixels_tab[1][3] =
-        ff_horiz_mc_qpel_avg_dst_aver_src1_8width_msa;
-    c->avg_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_avg_dst_aver_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa;
-    c->avg_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa;
-    c->avg_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_avg_dst_8x8_msa;
-    c->avg_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_avg_dst_8x8_msa;
-    c->avg_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_avg_dst_aver_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa;
-    c->avg_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa;
-}
-#endif  // #if HAVE_MSA
+        c->avg_qpel_pixels_tab[0][0] = ff_avg_width16_msa;
+        c->avg_qpel_pixels_tab[0][1] =
+            ff_horiz_mc_qpel_avg_dst_aver_src0_16width_msa;
+        c->avg_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_avg_dst_16width_msa;
+        c->avg_qpel_pixels_tab[0][3] =
+            ff_horiz_mc_qpel_avg_dst_aver_src1_16width_msa;
+        c->avg_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_avg_dst_aver_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][5] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src00_16x16_msa;
+        c->avg_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][7] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src10_16x16_msa;
+        c->avg_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_avg_dst_16x16_msa;
+        c->avg_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_avg_dst_16x16_msa;
+        c->avg_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_avg_dst_aver_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][13] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src01_16x16_msa;
+        c->avg_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][15] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src11_16x16_msa;
 
-void ff_qpeldsp_init_mips(QpelDSPContext *c)
-{
-#if HAVE_MSA
-    qpeldsp_init_msa(c);
-#endif  // #if HAVE_MSA
+        c->avg_qpel_pixels_tab[1][0] = ff_avg_width8_msa;
+        c->avg_qpel_pixels_tab[1][1] =
+            ff_horiz_mc_qpel_avg_dst_aver_src0_8width_msa;
+        c->avg_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_avg_dst_8width_msa;
+        c->avg_qpel_pixels_tab[1][3] =
+            ff_horiz_mc_qpel_avg_dst_aver_src1_8width_msa;
+        c->avg_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_avg_dst_aver_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa;
+        c->avg_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa;
+        c->avg_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_avg_dst_8x8_msa;
+        c->avg_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_avg_dst_8x8_msa;
+        c->avg_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_avg_dst_aver_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa;
+        c->avg_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa;
+    }
 }
diff --git a/libavcodec/mips/qpeldsp_msa.c b/libavcodec/mips/qpeldsp_msa.c
index 4710b3f..c7675f1 100644
--- a/libavcodec/mips/qpeldsp_msa.c
+++ b/libavcodec/mips/qpeldsp_msa.c
@@ -96,7 +96,7 @@
     DPADD_UB2_UH(sum2_r, sum1_r, coef2, coef1, sum0_r, sum3_r);         \
     res0_r = (v8i16) (sum0_r - sum3_r);                                 \
     res0_r = __msa_srari_h(res0_r, 5);                                  \
-    res0_r = CLIP_SH_0_255(res0_r);                                     \
+    CLIP_SH_0_255(res0_r);                                              \
     out = (v16u8) __msa_pckev_b((v16i8) res0_r, (v16i8) res0_r);        \
                                                                         \
     out;                                                                \
@@ -118,7 +118,7 @@
     res0_r = (v8i16) (sum0_r - sum3_r);                                   \
     res0_r += 15;                                                         \
     res0_r >>= 5;                                                         \
-    res0_r = CLIP_SH_0_255(res0_r);                                       \
+    CLIP_SH_0_255(res0_r);                                                \
     out = (v16u8) __msa_pckev_b((v16i8) res0_r, (v16i8) res0_r);          \
                                                                           \
     out;                                                                  \
@@ -334,7 +334,7 @@ static void horiz_mc_qpel_aver_src0_8width_msa(const uint8_t *src,
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         AVER_UB2_UB(inp0, res0, inp2, res1, res0, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -409,7 +409,7 @@ static void horiz_mc_qpel_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3,
                                              mask0, mask1, mask2, mask3,
                                              const20, const6, const3);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -480,12 +480,12 @@ static void horiz_mc_qpel_aver_src1_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3,
                                              mask0, mask1, mask2, mask3,
                                              const20, const6, const3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         AVER_UB2_UB(inp0, res0, inp2, res1, res0, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -564,7 +564,7 @@ static void horiz_mc_qpel_no_rnd_aver_src0_8width_msa(const uint8_t *src,
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         res0 = __msa_ave_u_b(inp0, res0);
         res1 = __msa_ave_u_b(inp2, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -639,7 +639,7 @@ static void horiz_mc_qpel_no_rnd_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                       mask2, mask3, const20,
                                                       const6, const3);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -710,13 +710,13 @@ static void horiz_mc_qpel_no_rnd_aver_src1_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                       mask2, mask3, const20,
                                                       const6, const3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         res0 = __msa_ave_u_b(inp0, res0);
         res1 = __msa_ave_u_b(inp2, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -799,7 +799,7 @@ static void horiz_mc_qpel_avg_dst_aver_src0_8width_msa(const uint8_t *src,
         dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
         AVER_UB2_UB(inp0, res0, inp2, res1, res0, res1);
         AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -876,7 +876,7 @@ static void horiz_mc_qpel_avg_dst_8width_msa(const uint8_t *src,
         dst0 = (v16u8) __msa_insve_d((v2i64) dst0, 1, (v2i64) dst1);
         dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
         AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -948,15 +948,15 @@ static void horiz_mc_qpel_avg_dst_aver_src1_8width_msa(const uint8_t *src,
                                              mask0, mask1, mask2, mask3,
                                              const20, const6, const3);
         LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         dst0 = (v16u8) __msa_insve_d((v2i64) dst0, 1, (v2i64) dst1);
         dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
         AVER_UB2_UB(inp0, res0, inp2, res1, res0, res1);
         AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-        ST8x4_UB(res0, res1, dst, dst_stride);
+        ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -1031,8 +1031,7 @@ static void vert_mc_qpel_aver_src0_8x8_msa(const uint8_t *src,
     tmp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -1048,8 +1047,7 @@ static void vert_mc_qpel_aver_src0_8x8_msa(const uint8_t *src,
     tmp0 = (v16u8) __msa_insve_d((v2i64) inp4, 1, (v2i64) inp5);
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp6, 1, (v2i64) inp7);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_aver_src0_16x16_msa(const uint8_t *src,
@@ -1220,8 +1218,7 @@ static void vert_mc_qpel_8x8_msa(const uint8_t *src,
                                         inp3, inp2, inp1, inp0,
                                         inp4, inp5, inp6, inp7,
                                         const20, const6, const3);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -1234,8 +1231,7 @@ static void vert_mc_qpel_8x8_msa(const uint8_t *src,
                                         inp7, inp6, inp5, inp4,
                                         inp8, inp8, inp7, inp6,
                                         const20, const6, const3);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_16x16_msa(const uint8_t *src,
@@ -1405,8 +1401,7 @@ static void vert_mc_qpel_aver_src1_8x8_msa(const uint8_t *src,
     tmp0 = (v16u8) __msa_insve_d((v2i64) inp1, 1, (v2i64) inp2);
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp3, 1, (v2i64) inp4);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -1422,7 +1417,7 @@ static void vert_mc_qpel_aver_src1_8x8_msa(const uint8_t *src,
     tmp0 = (v16u8) __msa_insve_d((v2i64) inp5, 1, (v2i64) inp6);
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp7, 1, (v2i64) inp8);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_aver_src1_16x16_msa(const uint8_t *src,
@@ -1607,8 +1602,7 @@ static void vert_mc_qpel_no_rnd_aver_src0_8x8_msa(const uint8_t *src,
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     res0 = __msa_ave_u_b(res0, tmp0);
     res1 = __msa_ave_u_b(res1, tmp1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -1625,8 +1619,7 @@ static void vert_mc_qpel_no_rnd_aver_src0_8x8_msa(const uint8_t *src,
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp6, 1, (v2i64) inp7);
     res0 = __msa_ave_u_b(res0, tmp0);
     res1 = __msa_ave_u_b(res1, tmp1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_no_rnd_aver_src0_16x16_msa(const uint8_t *src,
@@ -1806,8 +1799,7 @@ static void vert_mc_qpel_no_rnd_8x8_msa(const uint8_t *src,
                                                  inp3, inp2, inp1, inp0,
                                                  inp4, inp5, inp6, inp7,
                                                  const20, const6, const3);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -1820,8 +1812,7 @@ static void vert_mc_qpel_no_rnd_8x8_msa(const uint8_t *src,
                                                  inp7, inp6, inp5, inp4,
                                                  inp8, inp8, inp7, inp6,
                                                  const20, const6, const3);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_no_rnd_16x16_msa(const uint8_t *src,
@@ -1988,8 +1979,7 @@ static void vert_mc_qpel_no_rnd_aver_src1_8x8_msa(const uint8_t *src,
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp3, 1, (v2i64) inp4);
     res0 = __msa_ave_u_b(res0, tmp0);
     res1 = __msa_ave_u_b(res1, tmp1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 
     inp8 = LD_UB(src);
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(inp4, inp3, inp2, inp1,
@@ -2006,7 +1996,7 @@ static void vert_mc_qpel_no_rnd_aver_src1_8x8_msa(const uint8_t *src,
     tmp1 = (v16u8) __msa_insve_d((v2i64) inp7, 1, (v2i64) inp8);
     res0 = __msa_ave_u_b(res0, tmp0);
     res1 = __msa_ave_u_b(res1, tmp1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
 
 static void vert_mc_qpel_no_rnd_aver_src1_16x16_msa(const uint8_t *src,
@@ -2195,7 +2185,7 @@ static void vert_mc_qpel_avg_dst_aver_src0_8x8_msa(const uint8_t *src,
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 
     inp8 = LD_UB(src);
@@ -2217,7 +2207,7 @@ static void vert_mc_qpel_avg_dst_aver_src0_8x8_msa(const uint8_t *src,
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void vert_mc_qpel_avg_dst_aver_src0_16x16_msa(const uint8_t *src,
@@ -2384,7 +2374,7 @@ static void vert_mc_qpel_avg_dst_8x8_msa(const uint8_t *src,
     dst0 = (v16u8) __msa_insve_d((v2i64) dst0, 1, (v2i64) dst1);
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 
     inp8 = LD_UB(src);
@@ -2402,8 +2392,7 @@ static void vert_mc_qpel_avg_dst_8x8_msa(const uint8_t *src,
     dst0 = (v16u8) __msa_insve_d((v2i64) dst0, 1, (v2i64) dst1);
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void vert_mc_qpel_avg_dst_16x16_msa(const uint8_t *src,
@@ -2566,7 +2555,7 @@ static void vert_mc_qpel_avg_dst_aver_src1_8x8_msa(const uint8_t *src,
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 
     inp8 = LD_UB(src);
@@ -2587,7 +2576,7 @@ static void vert_mc_qpel_avg_dst_aver_src1_8x8_msa(const uint8_t *src,
     dst2 = (v16u8) __msa_insve_d((v2i64) dst2, 1, (v2i64) dst3);
     AVER_UB2_UB(res0, tmp0, res1, tmp1, res0, res1);
     AVER_UB2_UB(dst0, res0, dst2, res1, res0, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void vert_mc_qpel_avg_dst_aver_src1_16x16_msa(const uint8_t *src,
@@ -2832,7 +2821,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -2860,7 +2849,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa(const uint8_t *src,
                                                  horiz5, horiz4, horiz3, horiz2,
                                                  horiz6, horiz7, horiz8, horiz8,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz5, (v2i64) horiz4);
@@ -2870,12 +2859,12 @@ static void hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa(const uint8_t *src,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_horiz_16x16_msa(const uint8_t *src,
@@ -2977,7 +2966,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -2999,7 +2988,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa(const uint8_t *src,
     res1 = __msa_ave_u_b(avg1, res1);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
@@ -3009,7 +2998,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz5, (v2i64) horiz4);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     res1 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
@@ -3019,7 +3008,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_horiz_src1_16x16_msa(const uint8_t *src,
@@ -3105,7 +3094,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3115,7 +3104,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3125,7 +3114,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
@@ -3137,7 +3126,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -3145,7 +3134,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -3163,7 +3152,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz3, (v2i64) horiz2);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
@@ -3173,7 +3162,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz5, (v2i64) horiz4);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     res1 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
@@ -3183,7 +3172,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_aver_h_src0_16x16_msa(const uint8_t *src,
@@ -3246,7 +3235,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa(const uint8_t *src,
 
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
@@ -3270,18 +3259,15 @@ static void hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa(const uint8_t *src,
                                                  horiz5, horiz4, horiz3, horiz2,
                                                  horiz6, horiz7, horiz8, horiz8,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
-    dst += 2 * dst_stride;
-
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += (2 * dst_stride);
+    ST_D4(res1, res0, 0, 1, 0, 1, dst, dst_stride);
+    dst += (4 * dst_stride);
 
     res1 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
                                                  horiz7, horiz8, horiz8, horiz7,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_16x16_msa(const uint8_t *src,
@@ -3337,7 +3323,7 @@ static void hv_mc_qpel_no_rnd_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     horiz6 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
@@ -3358,7 +3344,7 @@ static void hv_mc_qpel_no_rnd_8x8_msa(const uint8_t *src,
                                                  horiz5, horiz4, horiz3, horiz2,
                                                  horiz6, horiz7, horiz8, horiz8,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
 
@@ -3367,9 +3353,7 @@ static void hv_mc_qpel_no_rnd_8x8_msa(const uint8_t *src,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_aver_h_src1_16x16_msa(const uint8_t *src,
@@ -3405,7 +3389,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3415,7 +3399,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3425,7 +3409,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
@@ -3437,13 +3421,13 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -3464,7 +3448,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
                                                  horiz5, horiz4, horiz3, horiz2,
                                                  horiz6, horiz7, horiz8, horiz8,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res1 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
@@ -3472,9 +3456,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_aver_hv_src01_16x16_msa(const uint8_t *src,
@@ -3536,7 +3518,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz2, (v2i64) horiz1);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -3564,7 +3546,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa(const uint8_t *src,
                                                  horiz5, horiz4, horiz3, horiz2,
                                                  horiz6, horiz7, horiz8, horiz8,
                                                  const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz6, (v2i64) horiz5);
@@ -3575,12 +3557,12 @@ static void hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa(const uint8_t *src,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_aver_v_src1_16x16_msa(const uint8_t *src,
@@ -3638,7 +3620,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa(const uint8_t *src,
     res0 = __msa_ave_u_b(avg0, res0);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     horiz6 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
@@ -3656,7 +3638,7 @@ static void hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa(const uint8_t *src,
     horiz8 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE_1ROW(inp0, mask0, mask1,
                                                          mask2, mask3, const20,
                                                          const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res0 = APPLY_VERT_QPEL_NO_ROUND_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
@@ -3671,12 +3653,9 @@ static void hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa(const uint8_t *src,
                                                  horiz7, horiz6, horiz5, horiz4,
                                                  horiz8, horiz8, horiz7, horiz6,
                                                  const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_no_rnd_aver_hv_src11_16x16_msa(const uint8_t *src,
@@ -3712,7 +3691,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3722,7 +3701,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3733,7 +3712,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
                                                   mask2, mask3, const20,
                                                   const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
@@ -3744,7 +3723,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz2, (v2i64) horiz1);
     res0 = __msa_ave_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -3752,7 +3731,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -3764,7 +3743,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
                                                  const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz4, (v2i64) horiz3);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -3787,7 +3766,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = __msa_ave_u_b(avg0, res0);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
     res1 = __msa_ave_u_b(avg1, res1);
-    ST8x4_UB(res0, res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_horiz_src0_16x16_msa(const uint8_t *src,
@@ -3893,7 +3872,7 @@ static void hv_mc_qpel_aver_hv_src00_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -3915,7 +3894,7 @@ static void hv_mc_qpel_aver_hv_src00_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE_1ROW(inp0, mask0, mask1, mask2, mask3,
                                               const20, const6, const3);
     horiz8 = __msa_aver_u_b(inp0, res0);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
@@ -3930,11 +3909,9 @@ static void hv_mc_qpel_aver_hv_src00_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_horiz_16x16_msa(const uint8_t *src,
@@ -4034,7 +4011,7 @@ static void hv_mc_qpel_aver_v_src0_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz1, (v2i64) horiz0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4059,7 +4036,7 @@ static void hv_mc_qpel_aver_v_src0_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz5, (v2i64) horiz4);
@@ -4069,11 +4046,9 @@ static void hv_mc_qpel_aver_v_src0_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_horiz_src1_16x16_msa(const uint8_t *src,
@@ -4159,12 +4134,12 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4175,12 +4150,12 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -4197,7 +4172,7 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                         horiz3, horiz2, horiz1, horiz0,
                                         horiz4, horiz5, horiz6, horiz7,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     inp0 = LD_UB(src);
@@ -4212,7 +4187,7 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz5, (v2i64) horiz4);
@@ -4222,12 +4197,9 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz7, (v2i64) horiz6);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_h_src0_16x16_msa(const uint8_t *src,
@@ -4285,7 +4257,7 @@ static void hv_mc_qpel_aver_h_src0_8x8_msa(const uint8_t *src,
                                         horiz1, horiz0, horiz0, horiz1,
                                         horiz2, horiz3, horiz4, horiz5,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4309,7 +4281,7 @@ static void hv_mc_qpel_aver_h_src0_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res1 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
@@ -4317,9 +4289,7 @@ static void hv_mc_qpel_aver_h_src0_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_16x16_msa(const uint8_t *src,
@@ -4371,7 +4341,7 @@ static void hv_mc_qpel_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         horiz1, horiz0, horiz0, horiz1,
                                         horiz2, horiz3, horiz4, horiz5,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4389,7 +4359,7 @@ static void hv_mc_qpel_8x8_msa(const uint8_t *src, int32_t src_stride,
     horiz8 = APPLY_HORIZ_QPEL_FILTER_8BYTE_1ROW(inp0,
                                                 mask0, mask1, mask2, mask3,
                                                 const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
@@ -4402,9 +4372,7 @@ static void hv_mc_qpel_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_h_src1_16x16_msa(const uint8_t *src,
@@ -4442,12 +4410,12 @@ static void hv_mc_qpel_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4458,12 +4426,12 @@ static void hv_mc_qpel_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -4478,31 +4446,25 @@ static void hv_mc_qpel_aver_h_src1_8x8_msa(const uint8_t *src,
                                         horiz1, horiz0, horiz0, horiz1,
                                         horiz2, horiz3, horiz4, horiz5,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += (2 * dst_stride);
-
     res1 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz2, horiz1, horiz0, horiz0,
                                         horiz3, horiz4, horiz5, horiz6,
                                         horiz3, horiz2, horiz1, horiz0,
                                         horiz4, horiz5, horiz6, horiz7,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
-    dst += (2 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+    dst += (4 * dst_stride);
 
     res0 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz4, horiz3, horiz2, horiz1,
                                         horiz5, horiz6, horiz7, horiz8,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += (2 * dst_stride);
-
     res1 = APPLY_VERT_QPEL_FILTER_8BYTE(horiz6, horiz5, horiz4, horiz3,
                                         horiz7, horiz8, horiz8, horiz7,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_hv_src01_16x16_msa(const uint8_t *src,
@@ -4561,7 +4523,7 @@ static void hv_mc_qpel_aver_hv_src01_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_insve_d((v2i64) horiz1, 1, (v2i64) horiz2);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4587,7 +4549,7 @@ static void hv_mc_qpel_aver_hv_src01_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_insve_d((v2i64) horiz5, 1, (v2i64) horiz6);
@@ -4597,13 +4559,9 @@ static void hv_mc_qpel_aver_hv_src01_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-
     avg1 = (v16u8) __msa_insve_d((v2i64) horiz7, 1, (v2i64) horiz8);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
-    dst += (2 * dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_v_src1_16x16_msa(const uint8_t *src,
@@ -4660,7 +4618,7 @@ static void hv_mc_qpel_aver_v_src1_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_insve_d((v2i64) horiz1, 1, (v2i64) horiz2);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4685,7 +4643,7 @@ static void hv_mc_qpel_aver_v_src1_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
     avg0 = (v16u8) __msa_insve_d((v2i64) horiz5, 1, (v2i64) horiz6);
     res0 = __msa_aver_u_b(avg0, res0);
@@ -4695,11 +4653,9 @@ static void hv_mc_qpel_aver_v_src1_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
     avg1 = (v16u8) __msa_insve_d((v2i64) horiz7, 1, (v2i64) horiz8);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_aver_hv_src11_16x16_msa(const uint8_t *src,
@@ -4734,14 +4690,14 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1,
                                          mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4750,7 +4706,7 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
     src += (2 * src_stride);
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -4764,12 +4720,12 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -4791,7 +4747,7 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
                                         horiz5, horiz4, horiz3, horiz2,
                                         horiz6, horiz7, horiz8, horiz8,
                                         const20, const6, const3);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += 2 * dst_stride;
 
     avg0 = (v16u8) __msa_ilvr_d((v2i64) horiz6, (v2i64) horiz5);
@@ -4801,12 +4757,9 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
                                         horiz7, horiz6, horiz5, horiz4,
                                         horiz8, horiz8, horiz7, horiz6,
                                         const20, const6, const3);
-    ST8x2_UB(res0, dst, dst_stride);
-    dst += 2 * dst_stride;
-
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D4(res0, res1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_hv_src00_16x16_msa(const uint8_t *src,
@@ -4869,7 +4822,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4889,7 +4842,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -4906,7 +4859,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -4919,7 +4872,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_v_src0_16x16_msa(const uint8_t *src,
@@ -4979,7 +4932,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -4998,7 +4951,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5015,7 +4968,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5028,7 +4981,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_hv_src10_16x16_msa(const uint8_t *src,
@@ -5067,7 +5020,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
 
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5076,7 +5029,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -5084,7 +5037,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5099,7 +5052,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -5107,7 +5060,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5122,7 +5075,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5140,7 +5093,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5153,7 +5106,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_h_src0_16x16_msa(const uint8_t *src,
@@ -5214,7 +5167,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -5232,7 +5185,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5247,7 +5200,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5258,8 +5211,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
-    dst += (2 * dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_16x16_msa(const uint8_t *src, int32_t src_stride,
@@ -5324,7 +5276,7 @@ static void hv_mc_qpel_avg_dst_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5335,7 +5287,7 @@ static void hv_mc_qpel_avg_dst_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5346,7 +5298,7 @@ static void hv_mc_qpel_avg_dst_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5357,7 +5309,7 @@ static void hv_mc_qpel_avg_dst_8x8_msa(const uint8_t *src, int32_t src_stride,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_h_src1_16x16_msa(const uint8_t *src,
@@ -5395,7 +5347,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5404,7 +5356,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -5412,7 +5364,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5425,7 +5377,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
@@ -5433,7 +5385,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5446,7 +5398,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5462,7 +5414,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5473,7 +5425,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                         const20, const6, const3);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_hv_src01_16x16_msa(const uint8_t *src,
@@ -5537,7 +5489,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5557,7 +5509,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5579,7 +5531,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa(const uint8_t *src,
     LD_UB2(dst, dst_stride, dst0, dst1);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
@@ -5587,7 +5539,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa(const uint8_t *src,
     LD_UB2(dst, dst_stride, dst0, dst1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_v_src1_16x16_msa(const uint8_t *src,
@@ -5647,7 +5599,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5666,7 +5618,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5686,7 +5638,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa(const uint8_t *src,
     LD_UB2(dst, dst_stride, dst0, dst1);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     avg1 = (v16u8) __msa_ilvr_d((v2i64) horiz8, (v2i64) horiz7);
@@ -5694,7 +5646,7 @@ static void hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa(const uint8_t *src,
     LD_UB2(dst, dst_stride, dst0, dst1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void hv_mc_qpel_avg_dst_aver_hv_src11_16x16_msa(const uint8_t *src,
@@ -5732,7 +5684,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5741,14 +5693,14 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
     horiz3 = (v16u8) __msa_splati_d((v2i64) horiz2, 1);
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5762,14 +5714,14 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5783,7 +5735,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     inp0 = LD_UB(src);
@@ -5800,7 +5752,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = __msa_aver_u_b(avg0, res0);
     avg0 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res0 = __msa_aver_u_b(avg0, res0);
-    ST8x2_UB(res0, dst, dst_stride);
+    ST_D2(res0, 0, 1, dst, dst_stride);
     dst += (2 * dst_stride);
 
     LD_UB2(dst, dst_stride, dst0, dst1);
@@ -5812,7 +5764,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = __msa_aver_u_b(avg1, res1);
     avg1 = (v16u8) __msa_ilvr_d((v2i64) dst1, (v2i64) dst0);
     res1 = __msa_aver_u_b(avg1, res1);
-    ST8x2_UB(res1, dst, dst_stride);
+    ST_D2(res1, 0, 1, dst, dst_stride);
 }
 
 static void copy_8x8_msa(const uint8_t *src, int32_t src_stride,
diff --git a/libavcodec/mips/sbrdsp_mips.c b/libavcodec/mips/sbrdsp_mips.c
index 1b0a106..6c57f26 100644
--- a/libavcodec/mips/sbrdsp_mips.c
+++ b/libavcodec/mips/sbrdsp_mips.c
@@ -796,9 +796,9 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
                                  const float *q_filt, int noise,
                                  int kx, int m_max)
 {
-    int m;
+    int m, temp0, temp1;
     float *ff_table;
-    float y0,y1, temp0, temp1, temp2, temp3, temp4, temp5;
+    float y0, y1, temp2, temp3, temp4, temp5;
 
     for (m = 0; m < m_max; m++) {
 
@@ -808,14 +808,14 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
 
         __asm__ volatile(
             "lwc1   %[y0],       0(%[Y1])                                  \n\t"
-            "lwc1   %[temp1],    0(%[s_m1])                                \n\t"
+            "lwc1   %[temp3],    0(%[s_m1])                                \n\t"
             "addiu  %[noise],    %[noise],              1                  \n\t"
             "andi   %[noise],    %[noise],              0x1ff              \n\t"
             "sll    %[temp0],    %[noise],              3                  \n\t"
             PTR_ADDU "%[ff_table],%[ff_sbr_noise_table],%[temp0]           \n\t"
-            "sub.s  %[y0],       %[y0],                 %[temp1]           \n\t"
-            "mfc1   %[temp3],    %[temp1]                                  \n\t"
-            "bne    %[temp3],    $0,                    1f                 \n\t"
+            "sub.s  %[y0],       %[y0],                 %[temp3]           \n\t"
+            "mfc1   %[temp1],    %[temp3]                                  \n\t"
+            "bne    %[temp1],    $0,                    1f                 \n\t"
             "lwc1   %[y1],       4(%[Y1])                                  \n\t"
             "lwc1   %[temp2],    0(%[q_filt1])                             \n\t"
             "lwc1   %[temp4],    0(%[ff_table])                            \n\t"
@@ -826,9 +826,10 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
         "1:                                                                \n\t"
             "swc1   %[y0],       0(%[Y1])                                  \n\t"
 
-            : [temp0]"=&r"(temp0), [ff_table]"=&r"(ff_table), [y0]"=&f"(y0),
-              [y1]"=&f"(y1), [temp1]"=&f"(temp1), [temp2]"=&f"(temp2),
-              [temp3]"=&r"(temp3), [temp4]"=&f"(temp4), [temp5]"=&f"(temp5)
+            : [temp0]"=&r"(temp0), [temp1]"=&r"(temp1), [y0]"=&f"(y0),
+              [y1]"=&f"(y1), [ff_table]"=&r"(ff_table),
+              [temp2]"=&f"(temp2), [temp3]"=&f"(temp3),
+              [temp4]"=&f"(temp4), [temp5]"=&f"(temp5)
             : [ff_sbr_noise_table]"r"(ff_sbr_noise_table), [noise]"r"(noise),
               [Y1]"r"(Y1), [s_m1]"r"(s_m1), [q_filt1]"r"(q_filt1)
             : "memory"
diff --git a/libavcodec/mips/simple_idct_mmi.c b/libavcodec/mips/simple_idct_mmi.c
index 7f4bb74..73d797f 100644
--- a/libavcodec/mips/simple_idct_mmi.c
+++ b/libavcodec/mips/simple_idct_mmi.c
@@ -39,7 +39,7 @@
 #define COL_SHIFT 20
 #define DC_SHIFT 3
 
-DECLARE_ALIGNED(8, const int16_t, W_arr)[46] = {
+DECLARE_ALIGNED(16, const int16_t, W_arr)[46] = {
     W4,  W2,  W4,  W6,
     W1,  W3,  W5,  W7,
     W4,  W6, -W4, -W2,
diff --git a/libavcodec/mips/simple_idct_msa.c b/libavcodec/mips/simple_idct_msa.c
index 8a72359..4bd3dd8 100644
--- a/libavcodec/mips/simple_idct_msa.c
+++ b/libavcodec/mips/simple_idct_msa.c
@@ -336,35 +336,26 @@ static void simple_idct_put_msa(uint8_t *dst, int32_t dst_stride,
     SRA_4V(temp2_r, temp2_l, temp3_r, temp3_l, 20);
     SRA_4V(a3_r, a3_l, a2_r, a2_l, 20);
     SRA_4V(a1_r, a1_l, a0_r, a0_l, 20);
-    PCKEV_H4_SW(temp0_l, temp0_r, temp1_l, temp1_r, temp2_l, temp2_r,
-                temp3_l, temp3_r, temp0_r, temp1_r, temp2_r, temp3_r);
-    PCKEV_H4_SW(a0_l, a0_r, a1_l, a1_r, a2_l, a2_r, a3_l, a3_r,
-                a0_r, a1_r, a2_r, a3_r);
-    temp0_r = (v4i32) CLIP_SH_0_255(temp0_r);
-    temp1_r = (v4i32) CLIP_SH_0_255(temp1_r);
-    temp2_r = (v4i32) CLIP_SH_0_255(temp2_r);
-    temp3_r = (v4i32) CLIP_SH_0_255(temp3_r);
-    PCKEV_B4_SW(temp0_r, temp0_r, temp1_r, temp1_r,
-                temp2_r, temp2_r, temp3_r, temp3_r,
-                temp0_r, temp1_r, temp2_r, temp3_r);
-    tmp0 = __msa_copy_u_d((v2i64) temp0_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) temp1_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) temp2_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) temp3_r, 1);
-    SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
-    dst += 4 * dst_stride;
-    a0_r = (v4i32) CLIP_SH_0_255(a0_r);
-    a1_r = (v4i32) CLIP_SH_0_255(a1_r);
-    a2_r = (v4i32) CLIP_SH_0_255(a2_r);
-    a3_r = (v4i32) CLIP_SH_0_255(a3_r);
-    PCKEV_B4_SW(a0_r, a0_r, a1_r, a1_r,
-                a2_r, a2_r, a3_r, a3_r, a0_r, a1_r, a2_r, a3_r);
-    tmp3 = __msa_copy_u_d((v2i64) a0_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) a1_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) a2_r, 1);
-    tmp0 = __msa_copy_u_d((v2i64) a3_r, 1);
+    PCKEV_H4_SH(temp0_l, temp0_r, temp1_l, temp1_r, temp2_l, temp2_r,
+                temp3_l, temp3_r, in0, in1, in2, in3);
+    PCKEV_H4_SH(a0_l, a0_r, a1_l, a1_r, a2_l, a2_r, a3_l, a3_r,
+                in4, in5, in6, in7);
+    CLIP_SH4_0_255(in0, in1, in2, in3);
+    PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3,
+                in0, in1, in2, in3);
+    tmp0 = __msa_copy_u_d((v2i64) in0, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in1, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in2, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in3, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
-    dst += 4 * dst_stride;
+    CLIP_SH4_0_255(in4, in5, in6, in7);
+    PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7,
+                in4, in5, in6, in7);
+    tmp3 = __msa_copy_u_d((v2i64) in4, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in5, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in6, 1);
+    tmp0 = __msa_copy_u_d((v2i64) in7, 1);
+    SD4(tmp0, tmp1, tmp2, tmp3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
@@ -516,21 +507,17 @@ static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
                 temp3_l, temp3_r, temp0_r, temp1_r, temp2_r, temp3_r);
     ILVR_B4_SW(zero, in0, zero, in1, zero, in2, zero, in3,
                temp0_l, temp1_l, temp2_l, temp3_l);
-    temp0_r = (v4i32) ((v8i16) (temp0_r) + (v8i16) (temp0_l));
-    temp1_r = (v4i32) ((v8i16) (temp1_r) + (v8i16) (temp1_l));
-    temp2_r = (v4i32) ((v8i16) (temp2_r) + (v8i16) (temp2_l));
-    temp3_r = (v4i32) ((v8i16) (temp3_r) + (v8i16) (temp3_l));
-    temp0_r = (v4i32) CLIP_SH_0_255(temp0_r);
-    temp1_r = (v4i32) CLIP_SH_0_255(temp1_r);
-    temp2_r = (v4i32) CLIP_SH_0_255(temp2_r);
-    temp3_r = (v4i32) CLIP_SH_0_255(temp3_r);
-    PCKEV_B4_SW(temp0_r, temp0_r, temp1_r, temp1_r,
-                temp2_r, temp2_r, temp3_r, temp3_r,
-                temp0_r, temp1_r, temp2_r, temp3_r);
-    tmp0 = __msa_copy_u_d((v2i64) temp0_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) temp1_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) temp2_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) temp3_r, 1);
+    in0 = (v8i16) (temp0_r) + (v8i16) (temp0_l);
+    in1 = (v8i16) (temp1_r) + (v8i16) (temp1_l);
+    in2 = (v8i16) (temp2_r) + (v8i16) (temp2_l);
+    in3 = (v8i16) (temp3_r) + (v8i16) (temp3_l);
+    CLIP_SH4_0_255(in0, in1, in2, in3);
+    PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3,
+                in0, in1, in2, in3);
+    tmp0 = __msa_copy_u_d((v2i64) in0, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in1, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in2, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in3, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
 
     SRA_4V(a3_r, a3_l, a2_r, a2_l, 20);
@@ -540,20 +527,17 @@ static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
                 a0_r, a1_r, a2_r, a3_r);
     ILVR_B4_SW(zero, in4, zero, in5, zero, in6, zero, in7,
                a3_l, a2_l, a1_l, a0_l);
-    a3_r = (v4i32) ((v8i16) (a3_r) + (v8i16) (a3_l));
-    a2_r = (v4i32) ((v8i16) (a2_r) + (v8i16) (a2_l));
-    a1_r = (v4i32) ((v8i16) (a1_r) + (v8i16) (a1_l));
-    a0_r = (v4i32) ((v8i16) (a0_r) + (v8i16) (a0_l));
-    a3_r = (v4i32) CLIP_SH_0_255(a3_r);
-    a2_r = (v4i32) CLIP_SH_0_255(a2_r);
-    a1_r = (v4i32) CLIP_SH_0_255(a1_r);
-    a0_r = (v4i32) CLIP_SH_0_255(a0_r);
-    PCKEV_B4_SW(a0_r, a0_r, a1_r, a1_r,
-                a2_r, a2_r, a3_r, a3_r, a0_r, a1_r, a2_r, a3_r);
-    tmp0 = __msa_copy_u_d((v2i64) a3_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) a2_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) a1_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) a0_r, 1);
+    in4 = (v8i16) (a3_r) + (v8i16) (a3_l);
+    in5 = (v8i16) (a2_r) + (v8i16) (a2_l);
+    in6 = (v8i16) (a1_r) + (v8i16) (a1_l);
+    in7 = (v8i16) (a0_r) + (v8i16) (a0_l);
+    CLIP_SH4_0_255(in4, in5, in6, in7);
+    PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7,
+                in4, in5, in6, in7);
+    tmp0 = __msa_copy_u_d((v2i64) in4, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in5, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in6, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in7, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst + 4 * dst_stride, dst_stride);
 }
 
diff --git a/libavcodec/mips/vc1dsp_init_mips.c b/libavcodec/mips/vc1dsp_init_mips.c
index 4adc9e1..94126f3 100644
--- a/libavcodec/mips/vc1dsp_init_mips.c
+++ b/libavcodec/mips/vc1dsp_init_mips.c
@@ -18,91 +18,103 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/vc1dsp.h"
 #include "vc1dsp_mips.h"
 #include "config.h"
 
-#if HAVE_MMI
-static av_cold void vc1dsp_init_mmi(VC1DSPContext *dsp)
-{
-#if _MIPS_SIM != _ABIO32
-    dsp->vc1_inv_trans_8x8    = ff_vc1_inv_trans_8x8_mmi;
-    dsp->vc1_inv_trans_4x8    = ff_vc1_inv_trans_4x8_mmi;
-    dsp->vc1_inv_trans_8x4    = ff_vc1_inv_trans_8x4_mmi;
-#endif
-    dsp->vc1_inv_trans_4x4    = ff_vc1_inv_trans_4x4_mmi;
-    dsp->vc1_inv_trans_8x8_dc = ff_vc1_inv_trans_8x8_dc_mmi;
-    dsp->vc1_inv_trans_4x8_dc = ff_vc1_inv_trans_4x8_dc_mmi;
-    dsp->vc1_inv_trans_8x4_dc = ff_vc1_inv_trans_8x4_dc_mmi;
-    dsp->vc1_inv_trans_4x4_dc = ff_vc1_inv_trans_4x4_dc_mmi;
-
-    dsp->vc1_h_overlap        = ff_vc1_h_overlap_mmi;
-    dsp->vc1_v_overlap        = ff_vc1_v_overlap_mmi;
-    dsp->vc1_h_s_overlap      = ff_vc1_h_s_overlap_mmi;
-    dsp->vc1_v_s_overlap      = ff_vc1_v_s_overlap_mmi;
-
-    dsp->vc1_v_loop_filter4  = ff_vc1_v_loop_filter4_mmi;
-    dsp->vc1_h_loop_filter4  = ff_vc1_h_loop_filter4_mmi;
-    dsp->vc1_v_loop_filter8  = ff_vc1_v_loop_filter8_mmi;
-    dsp->vc1_h_loop_filter8  = ff_vc1_h_loop_filter8_mmi;
-    dsp->vc1_v_loop_filter16 = ff_vc1_v_loop_filter16_mmi;
-    dsp->vc1_h_loop_filter16 = ff_vc1_h_loop_filter16_mmi;
-
 #define FN_ASSIGN(OP, X, Y, INSN) \
     dsp->OP##vc1_mspel_pixels_tab[1][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##INSN; \
     dsp->OP##vc1_mspel_pixels_tab[0][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##_16##INSN
 
-    FN_ASSIGN(put_, 0, 0, _mmi);
-    FN_ASSIGN(put_, 0, 1, _mmi);
-    FN_ASSIGN(put_, 0, 2, _mmi);
-    FN_ASSIGN(put_, 0, 3, _mmi);
-
-    FN_ASSIGN(put_, 1, 0, _mmi);
-    //FN_ASSIGN(put_, 1, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 1, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 1, 3, _mmi);//FIXME
-
-    FN_ASSIGN(put_, 2, 0, _mmi);
-    //FN_ASSIGN(put_, 2, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 2, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 2, 3, _mmi);//FIXME
-
-    FN_ASSIGN(put_, 3, 0, _mmi);
-    //FN_ASSIGN(put_, 3, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 3, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 3, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 0, 0, _mmi);
-    FN_ASSIGN(avg_, 0, 1, _mmi);
-    FN_ASSIGN(avg_, 0, 2, _mmi);
-    FN_ASSIGN(avg_, 0, 3, _mmi);
-
-    FN_ASSIGN(avg_, 1, 0, _mmi);
-    //FN_ASSIGN(avg_, 1, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 1, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 1, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 2, 0, _mmi);
-    //FN_ASSIGN(avg_, 2, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 2, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 2, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 3, 0, _mmi);
-    //FN_ASSIGN(avg_, 3, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 3, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 3, 3, _mmi);//FIXME
-
-    dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = ff_put_no_rnd_vc1_chroma_mc8_mmi;
-    dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = ff_avg_no_rnd_vc1_chroma_mc8_mmi;
-    dsp->put_no_rnd_vc1_chroma_pixels_tab[1] = ff_put_no_rnd_vc1_chroma_mc4_mmi;
-    dsp->avg_no_rnd_vc1_chroma_pixels_tab[1] = ff_avg_no_rnd_vc1_chroma_mc4_mmi;
-}
-#endif /* HAVE_MMI */
-
 av_cold void ff_vc1dsp_init_mips(VC1DSPContext *dsp)
 {
-#if HAVE_MMI
-    vc1dsp_init_mmi(dsp);
-#endif /* HAVE_MMI */
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+ #if _MIPS_SIM != _ABIO32
+        dsp->vc1_inv_trans_8x8    = ff_vc1_inv_trans_8x8_mmi;
+        dsp->vc1_inv_trans_4x8    = ff_vc1_inv_trans_4x8_mmi;
+        dsp->vc1_inv_trans_8x4    = ff_vc1_inv_trans_8x4_mmi;
+#endif
+        dsp->vc1_inv_trans_4x4    = ff_vc1_inv_trans_4x4_mmi;
+        dsp->vc1_inv_trans_8x8_dc = ff_vc1_inv_trans_8x8_dc_mmi;
+        dsp->vc1_inv_trans_4x8_dc = ff_vc1_inv_trans_4x8_dc_mmi;
+        dsp->vc1_inv_trans_8x4_dc = ff_vc1_inv_trans_8x4_dc_mmi;
+        dsp->vc1_inv_trans_4x4_dc = ff_vc1_inv_trans_4x4_dc_mmi;
+
+        dsp->vc1_h_overlap        = ff_vc1_h_overlap_mmi;
+        dsp->vc1_v_overlap        = ff_vc1_v_overlap_mmi;
+        dsp->vc1_h_s_overlap      = ff_vc1_h_s_overlap_mmi;
+        dsp->vc1_v_s_overlap      = ff_vc1_v_s_overlap_mmi;
+
+        dsp->vc1_v_loop_filter4  = ff_vc1_v_loop_filter4_mmi;
+        dsp->vc1_h_loop_filter4  = ff_vc1_h_loop_filter4_mmi;
+        dsp->vc1_v_loop_filter8  = ff_vc1_v_loop_filter8_mmi;
+        dsp->vc1_h_loop_filter8  = ff_vc1_h_loop_filter8_mmi;
+        dsp->vc1_v_loop_filter16 = ff_vc1_v_loop_filter16_mmi;
+        dsp->vc1_h_loop_filter16 = ff_vc1_h_loop_filter16_mmi;
+
+        FN_ASSIGN(put_, 0, 0, _mmi);
+        FN_ASSIGN(put_, 0, 1, _mmi);
+        FN_ASSIGN(put_, 0, 2, _mmi);
+        FN_ASSIGN(put_, 0, 3, _mmi);
+
+        FN_ASSIGN(put_, 1, 0, _mmi);
+        //FN_ASSIGN(put_, 1, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 1, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 1, 3, _mmi);//FIXME
+
+        FN_ASSIGN(put_, 2, 0, _mmi);
+        //FN_ASSIGN(put_, 2, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 2, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 2, 3, _mmi);//FIXME
+
+        FN_ASSIGN(put_, 3, 0, _mmi);
+        //FN_ASSIGN(put_, 3, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 3, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 3, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 0, 0, _mmi);
+        FN_ASSIGN(avg_, 0, 1, _mmi);
+        FN_ASSIGN(avg_, 0, 2, _mmi);
+        FN_ASSIGN(avg_, 0, 3, _mmi);
+
+        FN_ASSIGN(avg_, 1, 0, _mmi);
+        //FN_ASSIGN(avg_, 1, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 1, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 1, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 2, 0, _mmi);
+        //FN_ASSIGN(avg_, 2, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 2, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 2, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 3, 0, _mmi);
+        //FN_ASSIGN(avg_, 3, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 3, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 3, 3, _mmi);//FIXME
+
+        dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = ff_put_no_rnd_vc1_chroma_mc8_mmi;
+        dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = ff_avg_no_rnd_vc1_chroma_mc8_mmi;
+        dsp->put_no_rnd_vc1_chroma_pixels_tab[1] = ff_put_no_rnd_vc1_chroma_mc4_mmi;
+        dsp->avg_no_rnd_vc1_chroma_pixels_tab[1] = ff_avg_no_rnd_vc1_chroma_mc4_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        dsp->vc1_inv_trans_8x8 = ff_vc1_inv_trans_8x8_msa;
+        dsp->vc1_inv_trans_4x8 = ff_vc1_inv_trans_4x8_msa;
+        dsp->vc1_inv_trans_8x4 = ff_vc1_inv_trans_8x4_msa;
+
+        FN_ASSIGN(put_, 1, 1, _msa);
+        FN_ASSIGN(put_, 1, 2, _msa);
+        FN_ASSIGN(put_, 1, 3, _msa);
+        FN_ASSIGN(put_, 2, 1, _msa);
+        FN_ASSIGN(put_, 2, 2, _msa);
+        FN_ASSIGN(put_, 2, 3, _msa);
+        FN_ASSIGN(put_, 3, 1, _msa);
+        FN_ASSIGN(put_, 3, 2, _msa);
+        FN_ASSIGN(put_, 3, 3, _msa);
+    }
 }
diff --git a/libavcodec/mips/vc1dsp_mips.h b/libavcodec/mips/vc1dsp_mips.h
index 0db85fa..5897dae 100644
--- a/libavcodec/mips/vc1dsp_mips.h
+++ b/libavcodec/mips/vc1dsp_mips.h
@@ -180,15 +180,38 @@ void ff_vc1_h_loop_filter16_mmi(uint8_t *src, int stride, int pq);
 
 void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_avg_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 
+void ff_vc1_inv_trans_8x8_msa(int16_t block[64]);
+void ff_vc1_inv_trans_8x4_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block);
+void ff_vc1_inv_trans_4x8_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block);
+
+#define FF_PUT_VC1_MSPEL_MC_MSA(hmode, vmode)                                 \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _msa(uint8_t *dst,              \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd); \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_msa(uint8_t *dst,           \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd);
+
+FF_PUT_VC1_MSPEL_MC_MSA(1, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(1, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(1, 3);
+
+FF_PUT_VC1_MSPEL_MC_MSA(2, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(2, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(2, 3);
+
+FF_PUT_VC1_MSPEL_MC_MSA(3, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(3, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(3, 3);
 #endif /* AVCODEC_MIPS_VC1DSP_MIPS_H */
diff --git a/libavcodec/mips/vc1dsp_mmi.c b/libavcodec/mips/vc1dsp_mmi.c
index db314de..9837868 100644
--- a/libavcodec/mips/vc1dsp_mmi.c
+++ b/libavcodec/mips/vc1dsp_mmi.c
@@ -2241,7 +2241,7 @@ DECLARE_FUNCTION(3, 3)
 
 void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
     const int A = (8 - x) * (8 - y);
     const int B =     (x) * (8 - y);
@@ -2296,7 +2296,7 @@ void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
 
 void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
     const int A = (8 - x) * (8 - y);
     const int B =     (x) * (8 - y);
@@ -2349,7 +2349,7 @@ void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
 
 void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
     const int A = (8 - x) * (8 - y);
     const int B =     (x) * (8 - y);
@@ -2407,7 +2407,7 @@ void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
 
 void ff_avg_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
     const int A = (8 - x) * (8 - y);
     const int B = (    x) * (8 - y);
diff --git a/libavcodec/mips/vc1dsp_msa.c b/libavcodec/mips/vc1dsp_msa.c
new file mode 100644
index 0000000..6e588e8
--- /dev/null
+++ b/libavcodec/mips/vc1dsp_msa.c
@@ -0,0 +1,461 @@
+/*
+ * Loongson SIMD optimized vc1dsp
+ *
+ * Copyright (c) 2019 Loongson Technology Corporation Limited
+ *                    gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vc1dsp_mips.h"
+#include "constants.h"
+#include "libavutil/mips/generic_macros_msa.h"
+
+void ff_vc1_inv_trans_8x8_msa(int16_t block[64])
+{
+    v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7;
+    v4i32 in_l0, in_l1, in_l2, in_l3, in_l4, in_l5, in_l6, in_l7;
+    v4i32 t_r1, t_r2, t_r3, t_r4, t_r5, t_r6, t_r7, t_r8;
+    v4i32 t_l1, t_l2, t_l3, t_l4, t_l5, t_l6, t_l7, t_l8;
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+    v4i32 cnst_1 = {1, 1, 1, 1};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+
+    LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
+    UNPCK_SH_SW(in0, in_r0, in_l0);
+    UNPCK_SH_SW(in1, in_r1, in_l1);
+    UNPCK_SH_SW(in2, in_r2, in_l2);
+    UNPCK_SH_SW(in3, in_r3, in_l3);
+    UNPCK_SH_SW(in4, in_r4, in_l4);
+    UNPCK_SH_SW(in5, in_r5, in_l5);
+    UNPCK_SH_SW(in6, in_r6, in_l6);
+    UNPCK_SH_SW(in7, in_r7, in_l7);
+    // First loop
+    t_r1 = cnst_12 * (in_r0 + in_r4) + cnst_4;
+    t_l1 = cnst_12 * (in_l0 + in_l4) + cnst_4;
+    t_r2 = cnst_12 * (in_r0 - in_r4) + cnst_4;
+    t_l2 = cnst_12 * (in_l0 - in_l4) + cnst_4;
+    t_r3 = cnst_16 * in_r2 + cnst_6 * in_r6;
+    t_l3 = cnst_16 * in_l2 + cnst_6 * in_l6;
+    t_r4 = cnst_6 * in_r2 - cnst_16 * in_r6;
+    t_l4 = cnst_6 * in_l2 - cnst_16 * in_l6;
+
+    ADD4(t_r1, t_r3, t_l1, t_l3, t_r2, t_r4, t_l2, t_l4, t_r5, t_l5, t_r6, t_l6);
+    SUB4(t_r2, t_r4, t_l2, t_l4, t_r1, t_r3, t_l1, t_l3, t_r7, t_l7, t_r8, t_l8);
+    t_r1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_r5 + cnst_4 * in_r7;
+    t_l1 = cnst_16 * in_l1 + cnst_15 * in_l3 + cnst_9 * in_l5 + cnst_4 * in_l7;
+    t_r2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_r5 - cnst_9 * in_r7;
+    t_l2 = cnst_15 * in_l1 - cnst_4 * in_l3 - cnst_16 * in_l5 - cnst_9 * in_l7;
+    t_r3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_r5 + cnst_15 * in_r7;
+    t_l3 = cnst_9 * in_l1 - cnst_16 * in_l3 + cnst_4 * in_l5 + cnst_15 * in_l7;
+    t_r4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_r5 - cnst_16 * in_r7;
+    t_l4 = cnst_4 * in_l1 - cnst_9 * in_l3 + cnst_15 * in_l5 - cnst_16 * in_l7;
+
+    in_r0 = (t_r5 + t_r1) >> 3;
+    in_l0 = (t_l5 + t_l1) >> 3;
+    in_r1 = (t_r6 + t_r2) >> 3;
+    in_l1 = (t_l6 + t_l2) >> 3;
+    in_r2 = (t_r7 + t_r3) >> 3;
+    in_l2 = (t_l7 + t_l3) >> 3;
+    in_r3 = (t_r8 + t_r4) >> 3;
+    in_l3 = (t_l8 + t_l4) >> 3;
+
+    in_r4 = (t_r8 - t_r4) >> 3;
+    in_l4 = (t_l8 - t_l4) >> 3;
+    in_r5 = (t_r7 - t_r3) >> 3;
+    in_l5 = (t_l7 - t_l3) >> 3;
+    in_r6 = (t_r6 - t_r2) >> 3;
+    in_l6 = (t_l6 - t_l2) >> 3;
+    in_r7 = (t_r5 - t_r1) >> 3;
+    in_l7 = (t_l5 - t_l1) >> 3;
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_l0, in_l1, in_l2, in_l3, in_l0, in_l1, in_l2, in_l3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    TRANSPOSE4x4_SW_SW(in_l4, in_l5, in_l6, in_l7, in_l4, in_l5, in_l6, in_l7);
+    // Second loop
+    t_r1 = cnst_12 * (in_r0 + in_l0) + cnst_64;
+    t_l1 = cnst_12 * (in_r4 + in_l4) + cnst_64;
+    t_r2 = cnst_12 * (in_r0 - in_l0) + cnst_64;
+    t_l2 = cnst_12 * (in_r4 - in_l4) + cnst_64;
+    t_r3 = cnst_16 * in_r2 + cnst_6 * in_l2;
+    t_l3 = cnst_16 * in_r6 + cnst_6 * in_l6;
+    t_r4 = cnst_6 * in_r2 - cnst_16 * in_l2;
+    t_l4 = cnst_6 * in_r6 - cnst_16 * in_l6;
+
+    ADD4(t_r1, t_r3, t_l1, t_l3, t_r2, t_r4, t_l2, t_l4, t_r5, t_l5, t_r6, t_l6);
+    SUB4(t_r2, t_r4, t_l2, t_l4, t_r1, t_r3, t_l1, t_l3, t_r7, t_l7, t_r8, t_l8);
+    t_r1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_l1 + cnst_4 * in_l3;
+    t_l1 = cnst_16 * in_r5 + cnst_15 * in_r7 + cnst_9 * in_l5 + cnst_4 * in_l7;
+    t_r2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_l1 - cnst_9 * in_l3;
+    t_l2 = cnst_15 * in_r5 - cnst_4 * in_r7 - cnst_16 * in_l5 - cnst_9 * in_l7;
+    t_r3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_l1 + cnst_15 * in_l3;
+    t_l3 = cnst_9 * in_r5 - cnst_16 * in_r7 + cnst_4 * in_l5 + cnst_15 * in_l7;
+    t_r4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_l1 - cnst_16 * in_l3;
+    t_l4 = cnst_4 * in_r5 - cnst_9 * in_r7 + cnst_15 * in_l5 - cnst_16 * in_l7;
+
+    in_r0 = (t_r5 + t_r1) >> 7;
+    in_l0 = (t_l5 + t_l1) >> 7;
+    in_r1 = (t_r6 + t_r2) >> 7;
+    in_l1 = (t_l6 + t_l2) >> 7;
+    in_r2 = (t_r7 + t_r3) >> 7;
+    in_l2 = (t_l7 + t_l3) >> 7;
+    in_r3 = (t_r8 + t_r4) >> 7;
+    in_l3 = (t_l8 + t_l4) >> 7;
+
+    in_r4 = (t_r8 - t_r4 + cnst_1) >> 7;
+    in_l4 = (t_l8 - t_l4 + cnst_1) >> 7;
+    in_r5 = (t_r7 - t_r3 + cnst_1) >> 7;
+    in_l5 = (t_l7 - t_l3 + cnst_1) >> 7;
+    in_r6 = (t_r6 - t_r2 + cnst_1) >> 7;
+    in_l6 = (t_l6 - t_l2 + cnst_1) >> 7;
+    in_r7 = (t_r5 - t_r1 + cnst_1) >> 7;
+    in_l7 = (t_l5 - t_l1 + cnst_1) >> 7;
+    PCKEV_H4_SH(in_l0, in_r0, in_l1, in_r1, in_l2, in_r2, in_l3, in_r3,
+                in0, in1, in2, in3);
+    PCKEV_H4_SH(in_l4, in_r4, in_l5, in_r5, in_l6, in_r6, in_l7, in_r7,
+                in4, in5, in6, in7);
+    ST_SH8(in0, in1, in2, in3, in4, in5, in6, in7, block, 8);
+}
+
+void ff_vc1_inv_trans_4x8_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
+{
+    v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7;
+    v4i32 t1, t2, t3, t4, t5, t6, t7, t8;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v16i8 zero_m = { 0 };
+    v4i32 cnst_17 = {17, 17, 17, 17};
+    v4i32 cnst_22 = {22, 22, 22, 22};
+    v4i32 cnst_10 = {10, 10, 10, 10};
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+    v4i32 cnst_1 = {1, 1, 1, 1};
+
+    LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
+    UNPCK_R_SH_SW(in0, in_r0);
+    UNPCK_R_SH_SW(in1, in_r1);
+    UNPCK_R_SH_SW(in2, in_r2);
+    UNPCK_R_SH_SW(in3, in_r3);
+    UNPCK_R_SH_SW(in4, in_r4);
+    UNPCK_R_SH_SW(in5, in_r5);
+    UNPCK_R_SH_SW(in6, in_r6);
+    UNPCK_R_SH_SW(in7, in_r7);
+    // First loop
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    t1 = cnst_17 * (in_r0 + in_r2) + cnst_4;
+    t5 = cnst_17 * (in_r4 + in_r6) + cnst_4;
+    t2 = cnst_17 * (in_r0 - in_r2) + cnst_4;
+    t6 = cnst_17 * (in_r4 - in_r6) + cnst_4;
+    t3 = cnst_22 * in_r1 + cnst_10 * in_r3;
+    t7 = cnst_22 * in_r5 + cnst_10 * in_r7;
+    t4 = cnst_22 * in_r3 - cnst_10 * in_r1;
+    t8 = cnst_22 * in_r7 - cnst_10 * in_r5;
+
+    in_r0 = (t1 + t3) >> 3;
+    in_r4 = (t5 + t7) >> 3;
+    in_r1 = (t2 - t4) >> 3;
+    in_r5 = (t6 - t8) >> 3;
+    in_r2 = (t2 + t4) >> 3;
+    in_r6 = (t6 + t8) >> 3;
+    in_r3 = (t1 - t3) >> 3;
+    in_r7 = (t5 - t7) >> 3;
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    PCKEV_H4_SH(in_r1, in_r0, in_r3, in_r2, in_r5, in_r4, in_r7, in_r6,
+                in0, in1, in2, in3);
+    ST_D8(in0, in1, in2, in3, 0, 1, 0, 1, 0, 1, 0, 1, block, 8);
+    // Second loop
+    t1 = cnst_12 * (in_r0 + in_r4) + cnst_64;
+    t2 = cnst_12 * (in_r0 - in_r4) + cnst_64;
+    t3 = cnst_16 * in_r2 + cnst_6 * in_r6;
+    t4 = cnst_6 * in_r2 - cnst_16 * in_r6;
+    t5 = t1 + t3, t6 = t2 + t4;
+    t7 = t2 - t4, t8 = t1 - t3;
+    t1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_r5 + cnst_4 * in_r7;
+    t2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_r5 - cnst_9 * in_r7;
+    t3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_r5 + cnst_15 * in_r7;
+    t4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_r5 - cnst_16 * in_r7;
+    LD_SW8(dest, linesize, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    ILVR_B8_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               zero_m, dst4, zero_m, dst5, zero_m, dst6, zero_m, dst7,
+               dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    ILVR_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    ILVR_H4_SW(zero_m, dst4, zero_m, dst5, zero_m, dst6, zero_m, dst7,
+               dst4, dst5, dst6, dst7);
+    in_r0 = (t5 + t1) >> 7;
+    in_r1 = (t6 + t2) >> 7;
+    in_r2 = (t7 + t3) >> 7;
+    in_r3 = (t8 + t4) >> 7;
+    in_r4 = (t8 - t4 + cnst_1) >> 7;
+    in_r5 = (t7 - t3 + cnst_1) >> 7;
+    in_r6 = (t6 - t2 + cnst_1) >> 7;
+    in_r7 = (t5 - t1 + cnst_1) >> 7;
+    ADD4(in_r0, dst0, in_r1, dst1, in_r2, dst2, in_r3, dst3,
+         in_r0, in_r1, in_r2, in_r3);
+    ADD4(in_r4, dst4, in_r5, dst5, in_r6, dst6, in_r7, dst7,
+         in_r4, in_r5, in_r6, in_r7);
+    CLIP_SW8_0_255(in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7);
+    PCKEV_H4_SH(in_r1, in_r0, in_r3, in_r2, in_r5, in_r4, in_r7, in_r6,
+                in0, in1, in2, in3);
+    PCKEV_B2_SH(in1, in0, in3, in2, in0, in1);
+    ST_W8(in0, in1, 0, 1, 2, 3, 0, 1, 2, 3, dest, linesize);
+}
+
+void ff_vc1_inv_trans_8x4_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
+{
+    v4i32 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 t1, t2, t3, t4, t5, t6, t7, t8;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v16i8 zero_m = { 0 };
+    v4i32 cnst_17 = {17, 17, 17, 17};
+    v4i32 cnst_22 = {22, 22, 22, 22};
+    v4i32 cnst_10 = {10, 10, 10, 10};
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+
+    LD_SW4(block, 8, t1, t2, t3, t4);
+    UNPCK_SH_SW(t1, in0, in4);
+    UNPCK_SH_SW(t2, in1, in5);
+    UNPCK_SH_SW(t3, in2, in6);
+    UNPCK_SH_SW(t4, in3, in7);
+    TRANSPOSE4x4_SW_SW(in0, in1, in2, in3, in0, in1, in2, in3);
+    TRANSPOSE4x4_SW_SW(in4, in5, in6, in7, in4, in5, in6, in7);
+    // First loop
+    t1 = cnst_12 * (in0 + in4) + cnst_4;
+    t2 = cnst_12 * (in0 - in4) + cnst_4;
+    t3 = cnst_16 * in2 + cnst_6 * in6;
+    t4 = cnst_6 * in2 - cnst_16 * in6;
+    t5 = t1 + t3, t6 = t2 + t4;
+    t7 = t2 - t4, t8 = t1 - t3;
+    t1 = cnst_16 * in1 + cnst_15 * in3 + cnst_9 * in5 + cnst_4 * in7;
+    t2 = cnst_15 * in1 - cnst_4 * in3 - cnst_16 * in5 - cnst_9 * in7;
+    t3 = cnst_9 * in1 - cnst_16 * in3 + cnst_4 * in5 + cnst_15 * in7;
+    t4 = cnst_4 * in1 - cnst_9 * in3 + cnst_15 * in5 - cnst_16 * in7;
+    in0 = (t5 + t1) >> 3;
+    in1 = (t6 + t2) >> 3;
+    in2 = (t7 + t3) >> 3;
+    in3 = (t8 + t4) >> 3;
+    in4 = (t8 - t4) >> 3;
+    in5 = (t7 - t3) >> 3;
+    in6 = (t6 - t2) >> 3;
+    in7 = (t5 - t1) >> 3;
+    TRANSPOSE4x4_SW_SW(in0, in1, in2, in3, in0, in1, in2, in3);
+    TRANSPOSE4x4_SW_SW(in4, in5, in6, in7, in4, in5, in6, in7);
+    PCKEV_H4_SW(in4, in0, in5, in1, in6, in2, in7, in3, t1, t2, t3, t4);
+    ST_SW4(t1, t2, t3, t4, block, 8);
+    // Second loop
+    LD_SW4(dest, linesize, dst0, dst1, dst2, dst3);
+    ILVR_B4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    ILVL_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst4, dst5, dst6, dst7);
+    ILVR_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    // Right part
+    t1 = cnst_17 * (in0 + in2) + cnst_64;
+    t2 = cnst_17 * (in0 - in2) + cnst_64;
+    t3 = cnst_22 * in1 + cnst_10 * in3;
+    t4 = cnst_22 * in3 - cnst_10 * in1;
+    in0 = (t1 + t3) >> 7;
+    in1 = (t2 - t4) >> 7;
+    in2 = (t2 + t4) >> 7;
+    in3 = (t1 - t3) >> 7;
+    ADD4(in0, dst0, in1, dst1, in2, dst2, in3, dst3, in0, in1, in2, in3);
+    CLIP_SW4_0_255(in0, in1, in2, in3);
+    // Left part
+    t5 = cnst_17 * (in4 + in6) + cnst_64;
+    t6 = cnst_17 * (in4 - in6) + cnst_64;
+    t7 = cnst_22 * in5 + cnst_10 * in7;
+    t8 = cnst_22 * in7 - cnst_10 * in5;
+    in4 = (t5 + t7) >> 7;
+    in5 = (t6 - t8) >> 7;
+    in6 = (t6 + t8) >> 7;
+    in7 = (t5 - t7) >> 7;
+    ADD4(in4, dst4, in5, dst5, in6, dst6, in7, dst7, in4, in5, in6, in7);
+    CLIP_SW4_0_255(in4, in5, in6, in7);
+    PCKEV_H4_SW(in4, in0, in5, in1, in6, in2, in7, in3, in0, in1, in2, in3);
+    PCKEV_B2_SW(in1, in0, in3, in2, in0, in1);
+    ST_D4(in0, in1, 0, 1, 0, 1, dest, linesize);
+}
+
+static void put_vc1_mspel_mc_h_v_msa(uint8_t *dst, const uint8_t *src,
+                                     ptrdiff_t stride, int hmode, int vmode,
+                                     int rnd)
+{
+    v8i16 in_r0, in_r1, in_r2, in_r3, in_l0, in_l1, in_l2, in_l3;
+    v8i16 t0, t1, t2, t3, t4, t5, t6, t7;
+    v8i16 t8, t9, t10, t11, t12, t13, t14, t15;
+    v8i16 cnst_para0, cnst_para1, cnst_para2, cnst_para3, cnst_r;
+    static const int para_value[][4] = {{4, 53, 18, 3},
+                                        {1, 9, 9, 1},
+                                        {3, 18, 53, 4}};
+    static const int shift_value[] = {0, 5, 1, 5};
+    int shift = (shift_value[hmode] + shift_value[vmode]) >> 1;
+    int r = (1 << (shift - 1)) + rnd - 1;
+    cnst_r = __msa_fill_h(r);
+    src -= 1, src -= stride;
+    cnst_para0 = __msa_fill_h(para_value[vmode - 1][0]);
+    cnst_para1 = __msa_fill_h(para_value[vmode - 1][1]);
+    cnst_para2 = __msa_fill_h(para_value[vmode - 1][2]);
+    cnst_para3 = __msa_fill_h(para_value[vmode - 1][3]);
+    LD_SH4(src, stride, in_l0, in_l1, in_l2, in_l3);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    UNPCK_UB_SH(in_l3, in_r3, in_l3);
+    // row 0
+    t0 = cnst_para1 * in_r1 + cnst_para2 * in_r2
+         - cnst_para0 * in_r0 - cnst_para3 * in_r3;
+    t8 = cnst_para1 * in_l1 + cnst_para2 * in_l2
+         - cnst_para0 * in_l0 - cnst_para3 * in_l3;
+    in_l0 = LD_SH(src + 4 * stride);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    // row 1
+    t1 = cnst_para1 * in_r2 + cnst_para2 * in_r3
+         - cnst_para0 * in_r1 - cnst_para3 * in_r0;
+    t9 = cnst_para1 * in_l2 + cnst_para2 * in_l3
+         - cnst_para0 * in_l1 - cnst_para3 * in_l0;
+    in_l1 = LD_SH(src + 5 * stride);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    // row 2
+    t2 = cnst_para1 * in_r3 + cnst_para2 * in_r0
+         - cnst_para0 * in_r2 - cnst_para3 * in_r1;
+    t10 = cnst_para1 * in_l3 + cnst_para2 * in_l0
+          - cnst_para0 * in_l2 - cnst_para3 * in_l1;
+    in_l2 = LD_SH(src + 6 * stride);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    // row 3
+    t3 = cnst_para1 * in_r0 + cnst_para2 * in_r1
+         - cnst_para0 * in_r3 - cnst_para3 * in_r2;
+    t11 = cnst_para1 * in_l0 + cnst_para2 * in_l1
+          - cnst_para0 * in_l3 - cnst_para3 * in_l2;
+    in_l3 = LD_SH(src + 7 * stride);
+    UNPCK_UB_SH(in_l3, in_r3, in_l3);
+    // row 4
+    t4 = cnst_para1 * in_r1 + cnst_para2 * in_r2
+         - cnst_para0 * in_r0 - cnst_para3 * in_r3;
+    t12 = cnst_para1 * in_l1 + cnst_para2 * in_l2
+          - cnst_para0 * in_l0 - cnst_para3 * in_l3;
+    in_l0 = LD_SH(src + 8 * stride);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    // row 5
+    t5 = cnst_para1 * in_r2 + cnst_para2 * in_r3
+         - cnst_para0 * in_r1 - cnst_para3 * in_r0;
+    t13 = cnst_para1 * in_l2 + cnst_para2 * in_l3
+          - cnst_para0 * in_l1 - cnst_para3 * in_l0;
+    in_l1 = LD_SH(src + 9 * stride);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    // row 6
+    t6 = cnst_para1 * in_r3 + cnst_para2 * in_r0
+         - cnst_para0 * in_r2 - cnst_para3 * in_r1;
+    t14 = cnst_para1 * in_l3 + cnst_para2 * in_l0
+          - cnst_para0 * in_l2 - cnst_para3 * in_l1;
+    in_l2 = LD_SH(src + 10 * stride);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    // row 7
+    t7 = cnst_para1 * in_r0 + cnst_para2 * in_r1
+         - cnst_para0 * in_r3 - cnst_para3 * in_r2;
+    t15 = cnst_para1 * in_l0 + cnst_para2 * in_l1
+          - cnst_para0 * in_l3 - cnst_para3 * in_l2;
+
+    ADD4(t0, cnst_r, t1, cnst_r, t2, cnst_r, t3, cnst_r, t0, t1, t2, t3);
+    ADD4(t4, cnst_r, t5, cnst_r, t6, cnst_r, t7, cnst_r, t4, t5, t6, t7);
+    ADD4(t8, cnst_r, t9, cnst_r, t10, cnst_r, t11, cnst_r,
+         t8, t9, t10, t11);
+    ADD4(t12, cnst_r, t13, cnst_r, t14, cnst_r, t15, cnst_r,
+         t12, t13, t14, t15);
+    t0 >>= shift, t1 >>= shift, t2 >>= shift, t3 >>= shift;
+    t4 >>= shift, t5 >>= shift, t6 >>= shift, t7 >>= shift;
+    t8 >>= shift, t9 >>= shift, t10 >>= shift, t11 >>= shift;
+    t12 >>= shift, t13 >>= shift, t14 >>= shift, t15 >>= shift;
+    TRANSPOSE8x8_SH_SH(t0, t1, t2, t3, t4, t5, t6, t7,
+                       t0, t1, t2, t3, t4, t5, t6, t7);
+    TRANSPOSE8x8_SH_SH(t8, t9, t10, t11, t12, t13, t14, t15,
+                       t8, t9, t10, t11, t12, t13, t14, t15);
+    cnst_para0 = __msa_fill_h(para_value[hmode - 1][0]);
+    cnst_para1 = __msa_fill_h(para_value[hmode - 1][1]);
+    cnst_para2 = __msa_fill_h(para_value[hmode - 1][2]);
+    cnst_para3 = __msa_fill_h(para_value[hmode - 1][3]);
+    r = 64 - rnd;
+    cnst_r = __msa_fill_h(r);
+    // col 0 ~ 7
+    t0 = cnst_para1 * t1 + cnst_para2 * t2 - cnst_para0 * t0 - cnst_para3 * t3;
+    t1 = cnst_para1 * t2 + cnst_para2 * t3 - cnst_para0 * t1 - cnst_para3 * t4;
+    t2 = cnst_para1 * t3 + cnst_para2 * t4 - cnst_para0 * t2 - cnst_para3 * t5;
+    t3 = cnst_para1 * t4 + cnst_para2 * t5 - cnst_para0 * t3 - cnst_para3 * t6;
+    t4 = cnst_para1 * t5 + cnst_para2 * t6 - cnst_para0 * t4 - cnst_para3 * t7;
+    t5 = cnst_para1 * t6 + cnst_para2 * t7 - cnst_para0 * t5 - cnst_para3 * t8;
+    t6 = cnst_para1 * t7 + cnst_para2 * t8 - cnst_para0 * t6 - cnst_para3 * t9;
+    t7 = cnst_para1 * t8 + cnst_para2 * t9 - cnst_para0 * t7 - cnst_para3 * t10;
+    ADD4(t0, cnst_r, t1, cnst_r, t2, cnst_r, t3, cnst_r, t0, t1, t2, t3);
+    ADD4(t4, cnst_r, t5, cnst_r, t6, cnst_r, t7, cnst_r, t4, t5, t6, t7);
+    t0 >>= 7, t1 >>= 7, t2 >>= 7, t3 >>= 7;
+    t4 >>= 7, t5 >>= 7, t6 >>= 7, t7 >>= 7;
+    TRANSPOSE8x8_SH_SH(t0, t1, t2, t3, t4, t5, t6, t7,
+                       t0, t1, t2, t3, t4, t5, t6, t7);
+    CLIP_SH8_0_255(t0, t1, t2, t3, t4, t5, t6, t7);
+    PCKEV_B4_SH(t1, t0, t3, t2, t5, t4, t7, t6, t0, t1, t2, t3);
+    ST_D8(t0, t1, t2, t3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
+}
+
+#define PUT_VC1_MSPEL_MC_MSA(hmode, vmode)                                    \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _msa(uint8_t *dst,              \
+                                                const uint8_t *src,           \
+                                                ptrdiff_t stride, int rnd)    \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+}                                                                             \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_msa(uint8_t *dst,           \
+                                                   const uint8_t *src,        \
+                                                   ptrdiff_t stride, int rnd) \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+    put_vc1_mspel_mc_h_v_msa(dst + 8, src + 8, stride, hmode, vmode, rnd);    \
+    dst += 8 * stride, src += 8 * stride;                                     \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+    put_vc1_mspel_mc_h_v_msa(dst + 8, src + 8, stride, hmode, vmode, rnd);    \
+}
+
+PUT_VC1_MSPEL_MC_MSA(1, 1);
+PUT_VC1_MSPEL_MC_MSA(1, 2);
+PUT_VC1_MSPEL_MC_MSA(1, 3);
+
+PUT_VC1_MSPEL_MC_MSA(2, 1);
+PUT_VC1_MSPEL_MC_MSA(2, 2);
+PUT_VC1_MSPEL_MC_MSA(2, 3);
+
+PUT_VC1_MSPEL_MC_MSA(3, 1);
+PUT_VC1_MSPEL_MC_MSA(3, 2);
+PUT_VC1_MSPEL_MC_MSA(3, 3);
diff --git a/libavcodec/mips/videodsp_init.c b/libavcodec/mips/videodsp_init.c
index 8170404..07c23bc 100644
--- a/libavcodec/mips/videodsp_init.c
+++ b/libavcodec/mips/videodsp_init.c
@@ -18,12 +18,12 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavutil/mips/asmdefs.h"
 #include "libavcodec/videodsp.h"
 
-#if HAVE_MSA
 static void prefetch_mips(uint8_t *mem, ptrdiff_t stride, int h)
 {
     register const uint8_t *p = mem;
@@ -41,11 +41,11 @@ static void prefetch_mips(uint8_t *mem, ptrdiff_t stride, int h)
         : [stride] "r" (stride)
     );
 }
-#endif  // #if HAVE_MSA
 
 av_cold void ff_videodsp_init_mips(VideoDSPContext *ctx, int bpc)
 {
-#if HAVE_MSA
-    ctx->prefetch = prefetch_mips;
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags))
+        ctx->prefetch = prefetch_mips;
 }
diff --git a/libavcodec/mips/vp3dsp_idct_mmi.c b/libavcodec/mips/vp3dsp_idct_mmi.c
new file mode 100644
index 0000000..c5c4cf3
--- /dev/null
+++ b/libavcodec/mips/vp3dsp_idct_mmi.c
@@ -0,0 +1,769 @@
+/*
+ * Copyright (c) 2018 gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vp3dsp_mips.h"
+#include "libavutil/intreadwrite.h"
+#include "libavutil/mips/mmiutils.h"
+#include "libavutil/common.h"
+#include "libavcodec/rnd_avg.h"
+
+#define LOAD_CONST(dst, value)                        \
+    "li     %[tmp1],      "#value"              \n\t" \
+    "dmtc1  %[tmp1],      "#dst"                \n\t" \
+    "pshufh "#dst",       "#dst",     %[ftmp10] \n\t"
+
+static void idct_row_mmi(int16_t *input)
+{
+    double ftmp[23];
+    uint64_t tmp[2];
+    __asm__ volatile (
+        "xor        %[ftmp10],      %[ftmp10],        %[ftmp10] \n\t"
+        LOAD_CONST(%[csth_1], 1)
+        "li         %[tmp0],        0x02                        \n\t"
+        "1:                                                     \n\t"
+        /* Load input */
+        "ldc1       %[ftmp0],       0x00(%[input])              \n\t"
+        "ldc1       %[ftmp1],       0x10(%[input])              \n\t"
+        "ldc1       %[ftmp2],       0x20(%[input])              \n\t"
+        "ldc1       %[ftmp3],       0x30(%[input])              \n\t"
+        "ldc1       %[ftmp4],       0x40(%[input])              \n\t"
+        "ldc1       %[ftmp5],       0x50(%[input])              \n\t"
+        "ldc1       %[ftmp6],       0x60(%[input])              \n\t"
+        "ldc1       %[ftmp7],       0x70(%[input])              \n\t"
+        LOAD_CONST(%[ftmp8], 64277)
+        LOAD_CONST(%[ftmp9], 12785)
+        "pmulhh     %[A],           %[ftmp9],         %[ftmp7]  \n\t"
+        "pcmpgth    %[C],           %[ftmp10],        %[ftmp1]  \n\t"
+        "or         %[mask],        %[C],             %[csth_1] \n\t"
+        "pmullh     %[B],           %[ftmp1],         %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],         %[B]      \n\t"
+        "pmullh     %[B],           %[B],             %[mask]   \n\t"
+        "paddh      %[A],           %[A],             %[B]      \n\t"
+        "paddh      %[A],           %[A],             %[C]      \n\t"
+        "pcmpgth    %[D],           %[ftmp10],        %[ftmp7]  \n\t"
+        "or         %[mask],        %[D],             %[csth_1] \n\t"
+        "pmullh     %[ftmp7],       %[ftmp7],         %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],         %[ftmp7]  \n\t"
+        "pmullh     %[B],           %[B],             %[mask]   \n\t"
+        "pmulhh     %[C],           %[ftmp9],         %[ftmp1]  \n\t"
+        "psubh      %[B],           %[C],             %[B]      \n\t"
+        "psubh      %[B],           %[B],             %[D]      \n\t"
+
+        LOAD_CONST(%[ftmp8], 54491)
+        LOAD_CONST(%[ftmp9], 36410)
+        "pcmpgth    %[Ad],          %[ftmp10],        %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ad],            %[csth_1] \n\t"
+        "pmullh     %[ftmp1],       %[ftmp5],         %[mask]   \n\t"
+        "pmulhuh    %[C],           %[ftmp9],         %[ftmp1]  \n\t"
+        "pmullh     %[C],           %[C],             %[mask]   \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],        %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "pmullh     %[D],           %[ftmp3],         %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp8],         %[D]      \n\t"
+        "pmullh     %[D],           %[D],             %[mask]   \n\t"
+        "paddh      %[C],           %[C],             %[D]      \n\t"
+        "paddh      %[C],           %[C],             %[Ad]     \n\t"
+        "paddh      %[C],           %[C],             %[Bd]     \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],        %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "pmullh     %[ftmp1],       %[ftmp3],         %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp9],         %[ftmp1]  \n\t"
+        "pmullh     %[D],           %[D],             %[mask]   \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ed],            %[csth_1] \n\t"
+        "pmullh     %[Ad],          %[ftmp5],         %[mask]   \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]     \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
+        "psubh      %[D],           %[Ad],            %[D]      \n\t"
+        "paddh      %[D],           %[D],             %[Ed]     \n\t"
+        "psubh      %[D],           %[D],             %[Bd]     \n\t"
+
+        LOAD_CONST(%[ftmp8], 46341)
+        "psubh      %[Ad],          %[A],             %[C]      \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]     \n\t"
+        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]     \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
+        "paddh      %[Ad],          %[Ad],            %[Bd]     \n\t"
+        "psubh      %[Bd],          %[B],             %[D]      \n\t"
+        "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]     \n\t"
+        "or         %[mask],        %[Cd],            %[csth_1] \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]   \n\t"
+        "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]     \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]   \n\t"
+        "paddh      %[Bd],          %[Bd],            %[Cd]     \n\t"
+        "paddh      %[Cd],          %[A],             %[C]      \n\t"
+        "paddh      %[Dd],          %[B],             %[D]      \n\t"
+        "paddh      %[A],           %[ftmp0],         %[ftmp4]  \n\t"
+        "pcmpgth    %[B],           %[ftmp10],        %[A]      \n\t"
+        "or         %[mask],        %[B],             %[csth_1] \n\t"
+        "pmullh     %[A],           %[A],             %[mask]   \n\t"
+        "pmulhuh    %[A],           %[ftmp8],         %[A]      \n\t"
+        "pmullh     %[A],           %[A],             %[mask]   \n\t"
+        "paddh      %[A],           %[A],             %[B]      \n\t"
+        "psubh      %[B],           %[ftmp0],         %[ftmp4]  \n\t"
+        "pcmpgth    %[C],           %[ftmp10],        %[B]      \n\t"
+        "or         %[mask],        %[C],             %[csth_1] \n\t"
+        "pmullh     %[B],           %[B],             %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],         %[B]      \n\t"
+        "pmullh     %[B],           %[B],             %[mask]   \n\t"
+        "paddh      %[B],           %[B],             %[C]      \n\t"
+
+        LOAD_CONST(%[ftmp8], 60547)
+        LOAD_CONST(%[ftmp9], 25080)
+        "pmulhh     %[C],           %[ftmp9],         %[ftmp6]  \n\t"
+        "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]  \n\t"
+        "or         %[mask],        %[D],             %[csth_1] \n\t"
+        "pmullh     %[Ed],          %[ftmp2],         %[mask]   \n\t"
+        "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]     \n\t"
+        "pmullh     %[Ed],          %[Ed],            %[mask]   \n\t"
+        "paddh      %[C],           %[C],             %[Ed]     \n\t"
+        "paddh      %[C],           %[C],             %[D]      \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]  \n\t"
+        "or         %[mask],        %[Ed],            %[csth_1] \n\t"
+        "pmullh     %[ftmp6],       %[ftmp6],         %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp8],         %[ftmp6]  \n\t"
+        "pmullh     %[D],           %[D],             %[mask]   \n\t"
+        "pmulhh     %[Gd],          %[ftmp9],         %[ftmp2]  \n\t"
+        "psubh      %[D],           %[Gd],            %[D]      \n\t"
+        "psubh      %[D],           %[D],             %[Ed]     \n\t"
+        "psubh      %[Ed],          %[A],             %[C]      \n\t"
+        "paddh      %[Gd],          %[A],             %[C]      \n\t"
+        "paddh      %[A],           %[B],             %[Ad]     \n\t"
+        "psubh      %[C],           %[B],             %[Ad]     \n\t"
+        "psubh      %[B],           %[Bd],            %[D]      \n\t"
+        "paddh      %[D],           %[Bd],            %[D]      \n\t"
+        /* Final sequence of operations over-write original inputs */
+        "paddh      %[ftmp0],       %[Gd],            %[Cd]     \n\t"
+        "paddh      %[ftmp1],       %[A],             %[D]      \n\t"
+        "psubh      %[ftmp2],       %[A],             %[D]      \n\t"
+        "paddh      %[ftmp3],       %[Ed],            %[Dd]     \n\t"
+        "psubh      %[ftmp4],       %[Ed],            %[Dd]     \n\t"
+        "paddh      %[ftmp5],       %[C],             %[B]      \n\t"
+        "psubh      %[ftmp6],       %[C],             %[B]      \n\t"
+        "psubh      %[ftmp7],       %[Gd],            %[Cd]     \n\t"
+        "sdc1       %[ftmp0],       0x00(%[input])              \n\t"
+        "sdc1       %[ftmp1],       0x10(%[input])              \n\t"
+        "sdc1       %[ftmp2],       0x20(%[input])              \n\t"
+        "sdc1       %[ftmp3],       0x30(%[input])              \n\t"
+        "sdc1       %[ftmp4],       0x40(%[input])              \n\t"
+        "sdc1       %[ftmp5],       0x50(%[input])              \n\t"
+        "sdc1       %[ftmp6],       0x60(%[input])              \n\t"
+        "sdc1       %[ftmp7],       0x70(%[input])              \n\t"
+        PTR_ADDU   "%[tmp0],        %[tmp0],          -0x01     \n\t"
+        PTR_ADDIU  "%[input],       %[input],         0x08      \n\t"
+        "bnez       %[tmp0],        1b                          \n\t"
+        : [input]"+&r"(input), [tmp0]"=&r"(tmp[0]), [tmp1]"=&r"(tmp[1]),
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]), [ftmp2]"=&f"(ftmp[2]),
+          [ftmp3]"=&f"(ftmp[3]), [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]), [ftmp8]"=&f"(ftmp[8]),
+          [ftmp9]"=&f"(ftmp[9]), [ftmp10]"=&f"(ftmp[10]), [mask]"=&f"(ftmp[11]),
+          [A]"=&f"(ftmp[12]), [B]"=&f"(ftmp[13]), [C]"=&f"(ftmp[14]),
+          [D]"=&f"(ftmp[15]), [Ad]"=&f"(ftmp[16]), [Bd]"=&f"(ftmp[17]),
+          [Cd]"=&f"(ftmp[18]), [Dd]"=&f"(ftmp[19]), [Ed]"=&f"(ftmp[20]),
+          [Gd]"=&f"(ftmp[21]), [csth_1]"=&f"(ftmp[22])
+        :
+        : "memory"
+    );
+}
+
+static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
+{
+    uint8_t temp_value[8];
+    double ftmp[23];
+    uint64_t tmp[2];
+    for (int i = 0; i < 8; ++i)
+        temp_value[i] = av_clip_uint8(128 + ((46341 * input[i << 3] + (8 << 16)) >> 20));
+    __asm__ volatile (
+        "xor        %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
+        "li         %[tmp0],        0x02                          \n\t"
+        "1:                                                       \n\t"
+        "ldc1       %[ftmp0],       0x00(%[input])                \n\t"
+        "ldc1       %[ftmp4],       0x08(%[input])                \n\t"
+        "ldc1       %[ftmp1],       0x10(%[input])                \n\t"
+        "ldc1       %[ftmp5],       0x18(%[input])                \n\t"
+        "ldc1       %[ftmp2],       0x20(%[input])                \n\t"
+        "ldc1       %[ftmp6],       0x28(%[input])                \n\t"
+        "ldc1       %[ftmp3],       0x30(%[input])                \n\t"
+        "ldc1       %[ftmp7],       0x38(%[input])                \n\t"
+        TRANSPOSE_4H(%[ftmp0], %[ftmp1], %[ftmp2], %[ftmp3],
+                     %[A], %[B], %[C], %[D])
+        TRANSPOSE_4H(%[ftmp4], %[ftmp5], %[ftmp6], %[ftmp7],
+                     %[A], %[B], %[C], %[D])
+        LOAD_CONST(%[ftmp8], 64277)
+        LOAD_CONST(%[ftmp9], 12785)
+        LOAD_CONST(%[Gd], 1)
+        "pmulhh     %[A],           %[ftmp9],           %[ftmp7]  \n\t"
+        "pcmpgth    %[C],           %[ftmp10],          %[ftmp1]  \n\t"
+        "or         %[mask],        %[C],               %[Gd]     \n\t"
+        "pmullh     %[B],           %[ftmp1],           %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],           %[B]      \n\t"
+        "pmullh     %[B],           %[B],               %[mask]   \n\t"
+        "paddh      %[A],           %[A],               %[B]      \n\t"
+        "paddh      %[A],           %[A],               %[C]      \n\t"
+        "pcmpgth    %[D],           %[ftmp10],          %[ftmp7]  \n\t"
+        "or         %[mask],        %[D],               %[Gd]     \n\t"
+        "pmullh     %[Ad],          %[ftmp7],           %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],           %[Ad]     \n\t"
+        "pmullh     %[B],           %[B],               %[mask]   \n\t"
+        "pmulhh     %[C],           %[ftmp9],           %[ftmp1]  \n\t"
+        "psubh      %[B],           %[C],               %[B]      \n\t"
+        "psubh      %[B],           %[B],               %[D]      \n\t"
+
+        LOAD_CONST(%[ftmp8], 54491)
+        LOAD_CONST(%[ftmp9], 36410)
+        "pcmpgth    %[Ad],          %[ftmp10],          %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ad],              %[Gd]     \n\t"
+        "pmullh     %[Cd],          %[ftmp5],           %[mask]   \n\t"
+        "pmulhuh    %[C],           %[ftmp9],           %[Cd]     \n\t"
+        "pmullh     %[C],           %[C],               %[mask]   \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "pmullh     %[D],           %[ftmp3],           %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp8],           %[D]      \n\t"
+        "pmullh     %[D],           %[D],               %[mask]   \n\t"
+        "paddh      %[C],           %[C],               %[D]      \n\t"
+        "paddh      %[C],           %[C],               %[Ad]     \n\t"
+        "paddh      %[C],           %[C],               %[Bd]     \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "pmullh     %[Cd],          %[ftmp3],           %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp9],           %[Cd]     \n\t"
+        "pmullh     %[D],           %[D],               %[mask]   \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],          %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ed],              %[Gd]     \n\t"
+        "pmullh     %[Ad],          %[ftmp5],           %[mask]   \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],           %[Ad]     \n\t"
+        "pmullh     %[Ad],          %[Ad],              %[mask]   \n\t"
+        "psubh      %[D],           %[Ad],              %[D]      \n\t"
+        "paddh      %[D],           %[D],               %[Ed]     \n\t"
+        "psubh      %[D],           %[D],               %[Bd]     \n\t"
+
+        LOAD_CONST(%[ftmp8], 46341)
+        "psubh      %[Ad],          %[A],             %[C]        \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]       \n\t"
+        "or         %[mask],        %[Bd],            %[Gd]       \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]       \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
+        "paddh      %[Ad],          %[Ad],            %[Bd]       \n\t"
+        "psubh      %[Bd],          %[B],             %[D]        \n\t"
+        "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]       \n\t"
+        "or         %[mask],        %[Cd],            %[Gd]       \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
+        "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]       \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
+        "paddh      %[Bd],          %[Bd],            %[Cd]       \n\t"
+        "paddh      %[Cd],          %[A],             %[C]        \n\t"
+        "paddh      %[Dd],          %[B],             %[D]        \n\t"
+
+        LOAD_CONST(%[Ed], 2056)
+        "paddh      %[A],           %[ftmp0],         %[ftmp4]    \n\t"
+        "pcmpgth    %[B],           %[ftmp10],        %[A]        \n\t"
+        "or         %[mask],        %[B],             %[Gd]       \n\t"
+        "pmullh     %[A],           %[A],             %[mask]     \n\t"
+        "pmulhuh    %[A],           %[ftmp8],         %[A]        \n\t"
+        "pmullh     %[A],           %[A],             %[mask]     \n\t"
+        "paddh      %[A],           %[A],             %[B]        \n\t"
+        "paddh      %[A],           %[A],             %[Ed]       \n\t"
+        "psubh      %[B],           %[ftmp0],         %[ftmp4]    \n\t"
+        "pcmpgth    %[C],           %[ftmp10],        %[B]        \n\t"
+        "or         %[mask],        %[C],             %[Gd]       \n\t"
+        "pmullh     %[B],           %[B],             %[mask]     \n\t"
+        "pmulhuh    %[B],           %[ftmp8],         %[B]        \n\t"
+        "pmullh     %[B],           %[B],             %[mask]     \n\t"
+        "paddh      %[B],           %[B],             %[C]        \n\t"
+        "paddh      %[B],           %[B],             %[Ed]       \n\t"
+
+        LOAD_CONST(%[ftmp8], 60547)
+        LOAD_CONST(%[ftmp9], 25080)
+        "pmulhh     %[C],           %[ftmp9],         %[ftmp6]    \n\t"
+        "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]    \n\t"
+        "or         %[mask],        %[D],             %[Gd]       \n\t"
+        "pmullh     %[Ed],          %[ftmp2],         %[mask]     \n\t"
+        "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]       \n\t"
+        "pmullh     %[Ed],          %[Ed],            %[mask]     \n\t"
+        "paddh      %[C],           %[C],             %[Ed]       \n\t"
+        "paddh      %[C],           %[C],             %[D]        \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]    \n\t"
+        "or         %[mask],        %[Ed],            %[Gd]       \n\t"
+        "pmullh     %[D],           %[ftmp6],         %[mask]     \n\t"
+        "pmulhuh    %[D],           %[ftmp8],         %[D]        \n\t"
+        "pmullh     %[D],           %[D],             %[mask]     \n\t"
+        "pmulhh     %[Gd],          %[ftmp9],         %[ftmp2]    \n\t"
+        "psubh      %[D],           %[Gd],            %[D]        \n\t"
+        "psubh      %[D],           %[D],             %[Ed]       \n\t"
+        "psubh      %[Ed],          %[A],             %[C]        \n\t"
+        "paddh      %[Gd],          %[A],             %[C]        \n\t"
+        "paddh      %[A],           %[B],             %[Ad]       \n\t"
+        "psubh      %[C],           %[B],             %[Ad]       \n\t"
+        "psubh      %[B],           %[Bd],            %[D]        \n\t"
+        "paddh      %[D],           %[Bd],            %[D]        \n\t"
+        "or         %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp3]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp4]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp5]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp6]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp7]    \n\t"
+        "pcmpeqh    %[mask],        %[mask],          %[ftmp10]   \n\t"
+        "packushb   %[mask],        %[mask],          %[ftmp10]   \n\t"
+        "li         %[tmp1],        0x04                          \n\t"
+        "dmtc1      %[tmp1],        %[ftmp8]                      \n\t"
+        "paddh      %[ftmp0],       %[Gd],            %[Cd]       \n\t"
+        "psrah      %[ftmp0],       %[ftmp0],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp1],       %[A],             %[D]        \n\t"
+        "psrah      %[ftmp1],       %[ftmp1],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp2],       %[A],             %[D]        \n\t"
+        "psrah      %[ftmp2],       %[ftmp2],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp3],       %[Ed],            %[Dd]       \n\t"
+        "psrah      %[ftmp3],       %[ftmp3],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp4],       %[Ed],            %[Dd]       \n\t"
+        "psrah      %[ftmp4],       %[ftmp4],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp5],       %[C],             %[B]        \n\t"
+        "psrah      %[ftmp5],       %[ftmp5],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp6],       %[C],             %[B]        \n\t"
+        "psrah      %[ftmp6],       %[ftmp6],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp7],       %[Gd],            %[Cd]       \n\t"
+        "psrah      %[ftmp7],       %[ftmp7],         %[ftmp8]    \n\t"
+        "pmaxsh     %[ftmp0],       %[ftmp0],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp0],       %[ftmp0],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp1],       %[ftmp1],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp1],       %[ftmp1],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp2],       %[ftmp2],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp2],       %[ftmp2],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp3],       %[ftmp3],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp3],       %[ftmp3],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp4],       %[ftmp4],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp4],       %[ftmp4],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp5],       %[ftmp5],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp5],       %[ftmp5],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp6],       %[ftmp6],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp6],       %[ftmp6],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp7],       %[ftmp7],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp7],       %[ftmp7],         %[ftmp10]   \n\t"
+
+        "lwc1       %[Ed],          0x00(%[temp_value])           \n\t"
+        "and        %[Ed],          %[Ed],            %[mask]     \n\t"
+        "paddb      %[ftmp0],       %[ftmp0],         %[Ed]       \n\t"
+        "paddb      %[ftmp1],       %[ftmp1],         %[Ed]       \n\t"
+        "paddb      %[ftmp2],       %[ftmp2],         %[Ed]       \n\t"
+        "paddb      %[ftmp3],       %[ftmp3],         %[Ed]       \n\t"
+        "paddb      %[ftmp4],       %[ftmp4],         %[Ed]       \n\t"
+        "paddb      %[ftmp5],       %[ftmp5],         %[Ed]       \n\t"
+        "paddb      %[ftmp6],       %[ftmp6],         %[Ed]       \n\t"
+        "paddb      %[ftmp7],       %[ftmp7],         %[Ed]       \n\t"
+        "swc1       %[ftmp0],       0x00(%[dst])                  \n\t"
+        PTR_ADDU   "%[tmp1],        %[dst],           %[stride]   \n\t"
+        "swc1       %[ftmp1],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp2],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp3],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp4],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp5],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp6],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp7],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDIU  "%[dst],         %[dst],           0x04        \n\t"
+        PTR_ADDIU  "%[input],       %[input],         0x40        \n\t"
+        PTR_ADDIU  "%[temp_value],  %[temp_value],    0x04        \n\t"
+        PTR_ADDIU  "%[tmp0],        %[tmp0],          -0x01       \n\t"
+        "bnez       %[tmp0],        1b                            \n\t"
+        : [dst]"+&r"(dst), [tmp0]"=&r"(tmp[0]), [tmp1]"=&r"(tmp[1]),
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]), [ftmp2]"=&f"(ftmp[2]),
+          [ftmp3]"=&f"(ftmp[3]), [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]), [ftmp8]"=&f"(ftmp[8]),
+          [ftmp9]"=&f"(ftmp[9]), [ftmp10]"=&f"(ftmp[10]), [mask]"=&f"(ftmp[11]),
+          [A]"=&f"(ftmp[12]), [B]"=&f"(ftmp[13]), [C]"=&f"(ftmp[14]),
+          [D]"=&f"(ftmp[15]), [Ad]"=&f"(ftmp[16]), [Bd]"=&f"(ftmp[17]),
+          [Cd]"=&f"(ftmp[18]), [Dd]"=&f"(ftmp[19]), [Ed]"=&f"(ftmp[20]),
+          [Gd]"=&f"(ftmp[21]), [input]"+&r"(input)
+        : [stride]"r"(stride), [temp_value]"r"(temp_value)
+        : "memory"
+    );
+}
+
+static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
+{
+    int16_t temp_value[8];
+    double ftmp[23];
+    uint64_t tmp[2];
+    for (int i = 0; i < 8; ++i)
+        temp_value[i] = (46341 * input[i << 3] + (8 << 16)) >> 20;
+    __asm__ volatile (
+        "xor        %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
+        "li         %[tmp0],        0x02                          \n\t"
+        "1:                                                       \n\t"
+        "ldc1       %[ftmp0],       0x00(%[input])                \n\t"
+        "ldc1       %[ftmp4],       0x08(%[input])                \n\t"
+        "ldc1       %[ftmp1],       0x10(%[input])                \n\t"
+        "ldc1       %[ftmp5],       0x18(%[input])                \n\t"
+        "ldc1       %[ftmp2],       0x20(%[input])                \n\t"
+        "ldc1       %[ftmp6],       0x28(%[input])                \n\t"
+        "ldc1       %[ftmp3],       0x30(%[input])                \n\t"
+        "ldc1       %[ftmp7],       0x38(%[input])                \n\t"
+        TRANSPOSE_4H(%[ftmp0], %[ftmp1], %[ftmp2], %[ftmp3],
+                     %[A], %[B], %[C], %[D])
+        TRANSPOSE_4H(%[ftmp4], %[ftmp5], %[ftmp6], %[ftmp7],
+                     %[A], %[B], %[C], %[D])
+        LOAD_CONST(%[ftmp8], 64277)
+        LOAD_CONST(%[ftmp9], 12785)
+        LOAD_CONST(%[Gd], 1)
+        "pmulhh     %[A],           %[ftmp9],           %[ftmp7]  \n\t"
+        "pcmpgth    %[C],           %[ftmp10],          %[ftmp1]  \n\t"
+        "or         %[mask],        %[C],               %[Gd]     \n\t"
+        "pmullh     %[B],           %[ftmp1],           %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],           %[B]      \n\t"
+        "pmullh     %[B],           %[B],               %[mask]   \n\t"
+        "paddh      %[A],           %[A],               %[B]      \n\t"
+        "paddh      %[A],           %[A],               %[C]      \n\t"
+        "pcmpgth    %[D],           %[ftmp10],          %[ftmp7]  \n\t"
+        "or         %[mask],        %[D],               %[Gd]     \n\t"
+        "pmullh     %[Ad],          %[ftmp7],           %[mask]   \n\t"
+        "pmulhuh    %[B],           %[ftmp8],           %[Ad]     \n\t"
+        "pmullh     %[B],           %[B],               %[mask]   \n\t"
+        "pmulhh     %[C],           %[ftmp9],           %[ftmp1]  \n\t"
+        "psubh      %[B],           %[C],               %[B]      \n\t"
+        "psubh      %[B],           %[B],               %[D]      \n\t"
+
+        LOAD_CONST(%[ftmp8], 54491)
+        LOAD_CONST(%[ftmp9], 36410)
+        "pcmpgth    %[Ad],          %[ftmp10],          %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ad],              %[Gd]     \n\t"
+        "pmullh     %[Cd],          %[ftmp5],           %[mask]   \n\t"
+        "pmulhuh    %[C],           %[ftmp9],           %[Cd]     \n\t"
+        "pmullh     %[C],           %[C],               %[mask]   \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "pmullh     %[D],           %[ftmp3],           %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp8],           %[D]      \n\t"
+        "pmullh     %[D],           %[D],               %[mask]   \n\t"
+        "paddh      %[C],           %[C],               %[D]      \n\t"
+        "paddh      %[C],           %[C],               %[Ad]     \n\t"
+        "paddh      %[C],           %[C],               %[Bd]     \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
+        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "pmullh     %[Cd],          %[ftmp3],           %[mask]   \n\t"
+        "pmulhuh    %[D],           %[ftmp9],           %[Cd]     \n\t"
+        "pmullh     %[D],           %[D],               %[mask]   \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],          %[ftmp5]  \n\t"
+        "or         %[mask],        %[Ed],              %[Gd]     \n\t"
+        "pmullh     %[Ad],          %[ftmp5],           %[mask]   \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],           %[Ad]     \n\t"
+        "pmullh     %[Ad],          %[Ad],              %[mask]   \n\t"
+        "psubh      %[D],           %[Ad],              %[D]      \n\t"
+        "paddh      %[D],           %[D],               %[Ed]     \n\t"
+        "psubh      %[D],           %[D],               %[Bd]     \n\t"
+
+        LOAD_CONST(%[ftmp8], 46341)
+        "psubh      %[Ad],          %[A],             %[C]        \n\t"
+        "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]       \n\t"
+        "or         %[mask],        %[Bd],            %[Gd]       \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
+        "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]       \n\t"
+        "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
+        "paddh      %[Ad],          %[Ad],            %[Bd]       \n\t"
+        "psubh      %[Bd],          %[B],             %[D]        \n\t"
+        "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]       \n\t"
+        "or         %[mask],        %[Cd],            %[Gd]       \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
+        "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]       \n\t"
+        "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
+        "paddh      %[Bd],          %[Bd],            %[Cd]       \n\t"
+        "paddh      %[Cd],          %[A],             %[C]        \n\t"
+        "paddh      %[Dd],          %[B],             %[D]        \n\t"
+
+        LOAD_CONST(%[Ed], 8)
+        "paddh      %[A],           %[ftmp0],         %[ftmp4]    \n\t"
+        "pcmpgth    %[B],           %[ftmp10],        %[A]        \n\t"
+        "or         %[mask],        %[B],             %[Gd]       \n\t"
+        "pmullh     %[A],           %[A],             %[mask]     \n\t"
+        "pmulhuh    %[A],           %[ftmp8],         %[A]        \n\t"
+        "pmullh     %[A],           %[A],             %[mask]     \n\t"
+        "paddh      %[A],           %[A],             %[B]        \n\t"
+        "paddh      %[A],           %[A],             %[Ed]       \n\t"
+        "psubh      %[B],           %[ftmp0],         %[ftmp4]    \n\t"
+        "pcmpgth    %[C],           %[ftmp10],        %[B]        \n\t"
+        "or         %[mask],        %[C],             %[Gd]       \n\t"
+        "pmullh     %[B],           %[B],             %[mask]     \n\t"
+        "pmulhuh    %[B],           %[ftmp8],         %[B]        \n\t"
+        "pmullh     %[B],           %[B],             %[mask]     \n\t"
+        "paddh      %[B],           %[B],             %[C]        \n\t"
+        "paddh      %[B],           %[B],             %[Ed]       \n\t"
+
+        LOAD_CONST(%[ftmp8], 60547)
+        LOAD_CONST(%[ftmp9], 25080)
+        "pmulhh     %[C],           %[ftmp9],         %[ftmp6]    \n\t"
+        "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]    \n\t"
+        "or         %[mask],        %[D],             %[Gd]       \n\t"
+        "pmullh     %[Ed],          %[ftmp2],         %[mask]     \n\t"
+        "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]       \n\t"
+        "pmullh     %[Ed],          %[Ed],            %[mask]     \n\t"
+        "paddh      %[C],           %[C],             %[Ed]       \n\t"
+        "paddh      %[C],           %[C],             %[D]        \n\t"
+        "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]    \n\t"
+        "or         %[mask],        %[Ed],            %[Gd]       \n\t"
+        "pmullh     %[D],           %[ftmp6],         %[mask]     \n\t"
+        "pmulhuh    %[D],           %[ftmp8],         %[D]        \n\t"
+        "pmullh     %[D],           %[D],             %[mask]     \n\t"
+        "pmulhh     %[Gd],          %[ftmp9],         %[ftmp2]    \n\t"
+        "psubh      %[D],           %[Gd],            %[D]        \n\t"
+        "psubh      %[D],           %[D],             %[Ed]       \n\t"
+        "psubh      %[Ed],          %[A],             %[C]        \n\t"
+        "paddh      %[Gd],          %[A],             %[C]        \n\t"
+        "paddh      %[A],           %[B],             %[Ad]       \n\t"
+        "psubh      %[C],           %[B],             %[Ad]       \n\t"
+        "psubh      %[B],           %[Bd],            %[D]        \n\t"
+        "paddh      %[D],           %[Bd],            %[D]        \n\t"
+        "or         %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp3]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp4]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp5]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp6]    \n\t"
+        "or         %[mask],        %[mask],          %[ftmp7]    \n\t"
+        "pcmpeqh    %[mask],        %[mask],          %[ftmp10]   \n\t"
+        "li         %[tmp1],        0x04                          \n\t"
+        "dmtc1      %[tmp1],        %[ftmp8]                      \n\t"
+        "paddh      %[ftmp0],       %[Gd],            %[Cd]       \n\t"
+        "psrah      %[ftmp0],       %[ftmp0],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp1],       %[A],             %[D]        \n\t"
+        "psrah      %[ftmp1],       %[ftmp1],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp2],       %[A],             %[D]        \n\t"
+        "psrah      %[ftmp2],       %[ftmp2],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp3],       %[Ed],            %[Dd]       \n\t"
+        "psrah      %[ftmp3],       %[ftmp3],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp4],       %[Ed],            %[Dd]       \n\t"
+        "psrah      %[ftmp4],       %[ftmp4],         %[ftmp8]    \n\t"
+        "paddh      %[ftmp5],       %[C],             %[B]        \n\t"
+        "psrah      %[ftmp5],       %[ftmp5],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp6],       %[C],             %[B]        \n\t"
+        "psrah      %[ftmp6],       %[ftmp6],         %[ftmp8]    \n\t"
+        "psubh      %[ftmp7],       %[Gd],            %[Cd]       \n\t"
+        "psrah      %[ftmp7],       %[ftmp7],         %[ftmp8]    \n\t"
+
+        /* Load from dst */
+        "lwc1       %[A],           0x00(%[dst])                  \n\t"
+        PTR_ADDU   "%[tmp1],        %[dst],           %[stride]   \n\t"
+        "lwc1       %[B],           0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[C],           0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[D],           0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[Ad],          0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[Bd],          0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[Cd],          0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "lwc1       %[Dd],          0x00(%[tmp1])                 \n\t"
+        "punpcklbh  %[A],           %[A],             %[ftmp10]   \n\t"
+        "punpcklbh  %[B],           %[B],             %[ftmp10]   \n\t"
+        "punpcklbh  %[C],           %[C],             %[ftmp10]   \n\t"
+        "punpcklbh  %[D],           %[D],             %[ftmp10]   \n\t"
+        "punpcklbh  %[Ad],          %[Ad],            %[ftmp10]   \n\t"
+        "punpcklbh  %[Bd],          %[Bd],            %[ftmp10]   \n\t"
+        "punpcklbh  %[Cd],          %[Cd],            %[ftmp10]   \n\t"
+        "punpcklbh  %[Dd],          %[Dd],            %[ftmp10]   \n\t"
+        "ldc1       %[Ed],          0x00(%[temp_value])           \n\t"
+        "and        %[Ed],          %[Ed],            %[mask]     \n\t"
+        "nor        %[mask],        %[mask],          %[mask]     \n\t"
+        "and        %[ftmp0],       %[ftmp0],         %[mask]     \n\t"
+        "and        %[ftmp1],       %[ftmp1],         %[mask]     \n\t"
+        "and        %[ftmp2],       %[ftmp2],         %[mask]     \n\t"
+        "and        %[ftmp3],       %[ftmp3],         %[mask]     \n\t"
+        "and        %[ftmp4],       %[ftmp4],         %[mask]     \n\t"
+        "and        %[ftmp5],       %[ftmp5],         %[mask]     \n\t"
+        "and        %[ftmp6],       %[ftmp6],         %[mask]     \n\t"
+        "and        %[ftmp7],       %[ftmp7],         %[mask]     \n\t"
+        "paddh      %[ftmp0],       %[ftmp0],         %[A]        \n\t"
+        "paddh      %[ftmp1],       %[ftmp1],         %[B]        \n\t"
+        "paddh      %[ftmp2],       %[ftmp2],         %[C]        \n\t"
+        "paddh      %[ftmp3],       %[ftmp3],         %[D]        \n\t"
+        "paddh      %[ftmp4],       %[ftmp4],         %[Ad]       \n\t"
+        "paddh      %[ftmp5],       %[ftmp5],         %[Bd]       \n\t"
+        "paddh      %[ftmp6],       %[ftmp6],         %[Cd]       \n\t"
+        "paddh      %[ftmp7],       %[ftmp7],         %[Dd]       \n\t"
+        "paddh      %[ftmp0],       %[ftmp0],         %[Ed]       \n\t"
+        "paddh      %[ftmp1],       %[ftmp1],         %[Ed]       \n\t"
+        "paddh      %[ftmp2],       %[ftmp2],         %[Ed]       \n\t"
+        "paddh      %[ftmp3],       %[ftmp3],         %[Ed]       \n\t"
+        "paddh      %[ftmp4],       %[ftmp4],         %[Ed]       \n\t"
+        "paddh      %[ftmp5],       %[ftmp5],         %[Ed]       \n\t"
+        "paddh      %[ftmp6],       %[ftmp6],         %[Ed]       \n\t"
+        "paddh      %[ftmp7],       %[ftmp7],         %[Ed]       \n\t"
+        "pmaxsh     %[ftmp0],       %[ftmp0],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp0],       %[ftmp0],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp1],       %[ftmp1],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp1],       %[ftmp1],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp2],       %[ftmp2],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp2],       %[ftmp2],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp3],       %[ftmp3],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp3],       %[ftmp3],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp4],       %[ftmp4],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp4],       %[ftmp4],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp5],       %[ftmp5],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp5],       %[ftmp5],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp6],       %[ftmp6],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp6],       %[ftmp6],         %[ftmp10]   \n\t"
+        "pmaxsh     %[ftmp7],       %[ftmp7],         %[ftmp10]   \n\t"
+        "packushb   %[ftmp7],       %[ftmp7],         %[ftmp10]   \n\t"
+        "swc1       %[ftmp0],       0x00(%[dst])                  \n\t"
+        PTR_ADDU   "%[tmp1],        %[dst],           %[stride]   \n\t"
+        "swc1       %[ftmp1],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp2],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp3],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp4],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp5],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp6],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDU   "%[tmp1],        %[tmp1],          %[stride]   \n\t"
+        "swc1       %[ftmp7],       0x00(%[tmp1])                 \n\t"
+        PTR_ADDIU  "%[dst],         %[dst],           0x04        \n\t"
+        PTR_ADDIU  "%[input],       %[input],         0x40        \n\t"
+        PTR_ADDIU  "%[temp_value],  %[temp_value],    0x08        \n\t"
+        PTR_ADDIU  "%[tmp0],        %[tmp0],          -0x01       \n\t"
+        "bnez       %[tmp0],        1b                            \n\t"
+        : [dst]"+&r"(dst), [tmp0]"=&r"(tmp[0]), [tmp1]"=&r"(tmp[1]),
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]), [ftmp2]"=&f"(ftmp[2]),
+          [ftmp3]"=&f"(ftmp[3]), [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]), [ftmp8]"=&f"(ftmp[8]),
+          [ftmp9]"=&f"(ftmp[9]), [ftmp10]"=&f"(ftmp[10]), [mask]"=&f"(ftmp[11]),
+          [A]"=&f"(ftmp[12]), [B]"=&f"(ftmp[13]), [C]"=&f"(ftmp[14]),
+          [D]"=&f"(ftmp[15]), [Ad]"=&f"(ftmp[16]), [Bd]"=&f"(ftmp[17]),
+          [Cd]"=&f"(ftmp[18]), [Dd]"=&f"(ftmp[19]), [Ed]"=&f"(ftmp[20]),
+          [Gd]"=&f"(ftmp[21]), [input]"+&r"(input)
+        : [stride]"r"(stride), [temp_value]"r"(temp_value)
+        : "memory"
+    );
+}
+static void idct_mmi(uint8_t *dst, int stride, int16_t *input, int type)
+{
+    idct_row_mmi(input);
+    if (type == 1)
+        idct_column_true_mmi(dst, stride, input);
+    else
+        idct_column_false_mmi(dst, stride, input);
+}
+
+void ff_vp3_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    idct_mmi(dest, line_size, block, 1);
+    memset(block, 0, sizeof(*block) << 6);
+}
+
+void ff_vp3_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    idct_mmi(dest, line_size, block, 2);
+    memset(block, 0, sizeof(*block) << 6);
+}
+void ff_vp3_idct_dc_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    int dc = (block[0] + 15) >> 5;
+
+    double ftmp[7];
+    uint64_t tmp;
+    __asm__ volatile (
+        "xor        %[ftmp0],     %[ftmp0],           %[ftmp0]      \n\t"
+        "mtc1       %[dc],        %[ftmp5]                          \n\t"
+        "pshufh     %[ftmp5],     %[ftmp5],           %[ftmp0]      \n\t"
+        "li         %[tmp0],      0x08                              \n\t"
+        "1:                                                         \n\t"
+        "ldc1       %[ftmp1],     0x00(%[dest])                     \n\t"
+        "punpcklbh  %[ftmp2],     %[ftmp1],           %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp3],     %[ftmp1],           %[ftmp0]      \n\t"
+        "paddh      %[ftmp4],     %[ftmp2],           %[ftmp5]      \n\t"
+        "paddh      %[ftmp6],     %[ftmp3],           %[ftmp5]      \n\t"
+        "packushb   %[ftmp4],     %[ftmp4],           %[ftmp0]      \n\t"
+        "packushb   %[ftmp6],     %[ftmp6],           %[ftmp0]      \n\t"
+        "swc1       %[ftmp4],     0x00(%[dest])                     \n\t"
+        "swc1       %[ftmp6],     0x04(%[dest])                     \n\t"
+        PTR_ADDU   "%[dest],      %[dest],            %[line_size]  \n\t"
+        PTR_ADDIU  "%[tmp0],      %[tmp0],            -0x01         \n\t"
+        "bnez       %[tmp0],      1b                                \n\t"
+        : [dest]"+&r"(dest), [block]"+&r"(block), [tmp0]"=&r"(tmp),
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]), [ftmp2]"=&f"(ftmp[2]),
+          [ftmp3]"=&f"(ftmp[3]), [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),
+          [ftmp6]"=&f"(ftmp[6])
+        : [line_size]"r"(line_size), [dc]"r"(dc)
+        : "memory"
+    );
+    block[0] = 0;
+}
+
+void ff_put_no_rnd_pixels_l2_mmi(uint8_t *dst, const uint8_t *src1,
+                                 const uint8_t *src2, ptrdiff_t stride, int h)
+{
+    if (h == 8) {
+        double ftmp[6];
+        uint64_t tmp[2];
+        __asm__ volatile (
+            "li          %[tmp0],        0x08                            \n\t"
+            "li          %[tmp1],        0xfefefefe                      \n\t"
+            "dmtc1       %[tmp1],        %[ftmp4]                        \n\t"
+            "punpcklwd   %[ftmp4],       %[ftmp4],             %[ftmp4]  \n\t"
+            "li          %[tmp1],        0x01                            \n\t"
+            "dmtc1       %[tmp1],        %[ftmp5]                        \n\t"
+            "1:                                                          \n\t"
+            "gsldlc1     %[ftmp1],       0x07(%[src1])                   \n\t"
+            "gsldrc1     %[ftmp1],       0x00(%[src1])                   \n\t"
+            "gsldlc1     %[ftmp2],       0x07(%[src2])                   \n\t"
+            "gsldrc1     %[ftmp2],       0x00(%[src2])                   \n\t"
+            "xor         %[ftmp3],       %[ftmp1],             %[ftmp2]  \n\t"
+            "and         %[ftmp3],       %[ftmp3],             %[ftmp4]  \n\t"
+            "psrlw       %[ftmp3],       %[ftmp3],             %[ftmp5]  \n\t"
+            "and         %[ftmp6],       %[ftmp1],             %[ftmp2]  \n\t"
+            "paddw       %[ftmp3],       %[ftmp3],             %[ftmp6]  \n\t"
+            "sdc1        %[ftmp3],       0x00(%[dst])                    \n\t"
+            PTR_ADDU    "%[src1],        %[src1],              %[stride] \n\t"
+            PTR_ADDU    "%[src2],        %[src2],              %[stride] \n\t"
+            PTR_ADDU    "%[dst],         %[dst],               %[stride] \n\t"
+            PTR_ADDIU   "%[tmp0],        %[tmp0],              -0x01     \n\t"
+            "bnez        %[tmp0],        1b                              \n\t"
+            : [dst]"+&r"(dst), [src1]"+&r"(src1), [src2]"+&r"(src2),
+              [ftmp1]"=&f"(ftmp[0]), [ftmp2]"=&f"(ftmp[1]), [ftmp3]"=&f"(ftmp[2]),
+              [ftmp4]"=&f"(ftmp[3]), [ftmp5]"=&f"(ftmp[4]), [ftmp6]"=&f"(ftmp[5]),
+              [tmp0]"=&r"(tmp[0]), [tmp1]"=&r"(tmp[1])
+            : [stride]"r"(stride)
+            : "memory"
+        );
+    } else {
+        int i;
+
+        for (i = 0; i < h; i++) {
+            uint32_t a, b;
+
+            a = AV_RN32(&src1[i * stride]);
+            b = AV_RN32(&src2[i * stride]);
+            AV_WN32A(&dst[i * stride], no_rnd_avg32(a, b));
+            a = AV_RN32(&src1[i * stride + 4]);
+            b = AV_RN32(&src2[i * stride + 4]);
+            AV_WN32A(&dst[i * stride + 4], no_rnd_avg32(a, b));
+        }
+    }
+}
diff --git a/libavcodec/mips/vp3dsp_idct_msa.c b/libavcodec/mips/vp3dsp_idct_msa.c
new file mode 100644
index 0000000..90c578f
--- /dev/null
+++ b/libavcodec/mips/vp3dsp_idct_msa.c
@@ -0,0 +1,598 @@
+/*
+ * Copyright (c) 2018 gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vp3dsp_mips.h"
+#include "libavutil/mips/generic_macros_msa.h"
+#include "libavutil/intreadwrite.h"
+#include "libavcodec/rnd_avg.h"
+
+static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
+{
+    v8i16 r0, r1, r2, r3, r4, r5, r6, r7, sign;
+    v4i32 r0_r, r0_l, r1_r, r1_l, r2_r, r2_l, r3_r, r3_l,
+          r4_r, r4_l, r5_r, r5_l, r6_r, r6_l, r7_r, r7_l;
+    v4i32 A, B, C, D, Ad, Bd, Cd, Dd, E, F, G, H;
+    v4i32 Ed, Gd, Add, Bdd, Fd, Hd;
+    v16u8 sign_l;
+    v16i8 d0, d1, d2, d3, d4, d5, d6, d7;
+    v4i32 c0, c1, c2, c3, c4, c5, c6, c7;
+    v4i32 f0, f1, f2, f3, f4, f5, f6, f7;
+    v4i32 sign_t;
+    v16i8 zero = {0};
+    v16i8 mask = {0, 4, 8, 12, 16, 20, 24, 28, 0, 0, 0, 0, 0, 0, 0, 0};
+    v4i32 cnst64277w = {64277, 64277, 64277, 64277};
+    v4i32 cnst60547w = {60547, 60547, 60547, 60547};
+    v4i32 cnst54491w = {54491, 54491, 54491, 54491};
+    v4i32 cnst46341w = {46341, 46341, 46341, 46341};
+    v4i32 cnst36410w = {36410, 36410, 36410, 36410};
+    v4i32 cnst25080w = {25080, 25080, 25080, 25080};
+    v4i32 cnst12785w = {12785, 12785, 12785, 12785};
+    v4i32 cnst8w = {8, 8, 8, 8};
+    v4i32 cnst2048w = {2048, 2048, 2048, 2048};
+    v4i32 cnst128w = {128, 128, 128, 128};
+
+    /* Extended input data */
+    LD_SH8(input, 8, r0, r1, r2, r3, r4, r5, r6, r7);
+    sign = __msa_clti_s_h(r0, 0);
+    r0_r = (v4i32) __msa_ilvr_h(sign, r0);
+    r0_l = (v4i32) __msa_ilvl_h(sign, r0);
+    sign = __msa_clti_s_h(r1, 0);
+    r1_r = (v4i32) __msa_ilvr_h(sign, r1);
+    r1_l = (v4i32) __msa_ilvl_h(sign, r1);
+    sign = __msa_clti_s_h(r2, 0);
+    r2_r = (v4i32) __msa_ilvr_h(sign, r2);
+    r2_l = (v4i32) __msa_ilvl_h(sign, r2);
+    sign = __msa_clti_s_h(r3, 0);
+    r3_r = (v4i32) __msa_ilvr_h(sign, r3);
+    r3_l = (v4i32) __msa_ilvl_h(sign, r3);
+    sign = __msa_clti_s_h(r4, 0);
+    r4_r = (v4i32) __msa_ilvr_h(sign, r4);
+    r4_l = (v4i32) __msa_ilvl_h(sign, r4);
+    sign = __msa_clti_s_h(r5, 0);
+    r5_r = (v4i32) __msa_ilvr_h(sign, r5);
+    r5_l = (v4i32) __msa_ilvl_h(sign, r5);
+    sign = __msa_clti_s_h(r6, 0);
+    r6_r = (v4i32) __msa_ilvr_h(sign, r6);
+    r6_l = (v4i32) __msa_ilvl_h(sign, r6);
+    sign = __msa_clti_s_h(r7, 0);
+    r7_r = (v4i32) __msa_ilvr_h(sign, r7);
+    r7_l = (v4i32) __msa_ilvl_h(sign, r7);
+
+    /* Right part */
+    A = ((r1_r * cnst64277w) >> 16) + ((r7_r * cnst12785w) >> 16);
+    B = ((r1_r * cnst12785w) >> 16) - ((r7_r * cnst64277w) >> 16);
+    C = ((r3_r * cnst54491w) >> 16) + ((r5_r * cnst36410w) >> 16);
+    D = ((r5_r * cnst54491w) >> 16) - ((r3_r * cnst36410w) >> 16);
+    Ad = ((A - C) * cnst46341w) >> 16;
+    Bd = ((B - D) * cnst46341w) >> 16;
+    Cd = A + C;
+    Dd = B + D;
+    E = ((r0_r + r4_r) * cnst46341w) >> 16;
+    F = ((r0_r - r4_r) * cnst46341w) >> 16;
+    G = ((r2_r * cnst60547w) >> 16) + ((r6_r * cnst25080w) >> 16);
+    H = ((r2_r * cnst25080w) >> 16) - ((r6_r * cnst60547w) >> 16);
+    Ed = E - G;
+    Gd = E + G;
+    Add = F + Ad;
+    Bdd = Bd - H;
+    Fd = F - Ad;
+    Hd = Bd + H;
+    r0_r = Gd + Cd;
+    r7_r = Gd - Cd;
+    r1_r = Add + Hd;
+    r2_r = Add - Hd;
+    r3_r = Ed + Dd;
+    r4_r = Ed - Dd;
+    r5_r = Fd + Bdd;
+    r6_r = Fd - Bdd;
+
+    /* Left part */
+    A = ((r1_l * cnst64277w) >> 16) + ((r7_l * cnst12785w) >> 16);
+    B = ((r1_l * cnst12785w) >> 16) - ((r7_l * cnst64277w) >> 16);
+    C = ((r3_l * cnst54491w) >> 16) + ((r5_l * cnst36410w) >> 16);
+    D = ((r5_l * cnst54491w) >> 16) - ((r3_l * cnst36410w) >> 16);
+    Ad = ((A - C) * cnst46341w) >> 16;
+    Bd = ((B - D) * cnst46341w) >> 16;
+    Cd = A + C;
+    Dd = B + D;
+    E = ((r0_l + r4_l) * cnst46341w) >> 16;
+    F = ((r0_l - r4_l) * cnst46341w) >> 16;
+    G = ((r2_l * cnst60547w) >> 16) + ((r6_l * cnst25080w) >> 16);
+    H = ((r2_l * cnst25080w) >> 16) - ((r6_l * cnst60547w) >> 16);
+    Ed = E - G;
+    Gd = E + G;
+    Add = F + Ad;
+    Bdd = Bd - H;
+    Fd = F - Ad;
+    Hd = Bd + H;
+    r0_l = Gd + Cd;
+    r7_l = Gd - Cd;
+    r1_l = Add + Hd;
+    r2_l = Add - Hd;
+    r3_l = Ed + Dd;
+    r4_l = Ed - Dd;
+    r5_l = Fd + Bdd;
+    r6_l = Fd - Bdd;
+
+    /* Row 0 to 3 */
+    TRANSPOSE4x4_SW_SW(r0_r, r1_r, r2_r, r3_r,
+                       r0_r, r1_r, r2_r, r3_r);
+    TRANSPOSE4x4_SW_SW(r0_l, r1_l, r2_l, r3_l,
+                       r0_l, r1_l, r2_l, r3_l);
+    A = ((r1_r * cnst64277w) >> 16) + ((r3_l * cnst12785w) >> 16);
+    B = ((r1_r * cnst12785w) >> 16) - ((r3_l * cnst64277w) >> 16);
+    C = ((r3_r * cnst54491w) >> 16) + ((r1_l * cnst36410w) >> 16);
+    D = ((r1_l * cnst54491w) >> 16) - ((r3_r * cnst36410w) >> 16);
+    Ad = ((A - C) * cnst46341w) >> 16;
+    Bd = ((B - D) * cnst46341w) >> 16;
+    Cd = A + C;
+    Dd = B + D;
+    E = ((r0_r + r0_l) * cnst46341w) >> 16;
+    E += cnst8w;
+    F = ((r0_r - r0_l) * cnst46341w) >> 16;
+    F += cnst8w;
+    if (type == 1) { // HACK
+        E += cnst2048w;
+        F += cnst2048w;
+    }
+    G = ((r2_r * cnst60547w) >> 16) + ((r2_l * cnst25080w) >> 16);
+    H = ((r2_r * cnst25080w) >> 16) - ((r2_l * cnst60547w) >> 16);
+    Ed = E - G;
+    Gd = E + G;
+    Add = F + Ad;
+    Bdd = Bd - H;
+    Fd = F - Ad;
+    Hd = Bd + H;
+    A = (Gd + Cd) >> 4;
+    B = (Gd - Cd) >> 4;
+    C = (Add + Hd) >> 4;
+    D = (Add - Hd) >> 4;
+    E = (Ed + Dd) >> 4;
+    F = (Ed - Dd) >> 4;
+    G = (Fd + Bdd) >> 4;
+    H = (Fd - Bdd) >> 4;
+    if (type != 1) {
+        LD_SB8(dst, stride, d0, d1, d2, d3, d4, d5, d6, d7);
+        ILVR_B4_SW(zero, d0, zero, d1, zero, d2, zero, d3,
+                   f0, f1, f2, f3);
+        ILVR_B4_SW(zero, d4, zero, d5, zero, d6, zero, d7,
+                   f4, f5, f6, f7);
+        ILVR_H4_SW(zero, f0, zero, f1, zero, f2, zero, f3,
+                   c0, c1, c2, c3);
+        ILVR_H4_SW(zero, f4, zero, f5, zero, f6, zero, f7,
+                   c4, c5, c6, c7);
+        A += c0;
+        B += c7;
+        C += c1;
+        D += c2;
+        E += c3;
+        F += c4;
+        G += c5;
+        H += c6;
+    }
+    CLIP_SW8_0_255(A, B, C, D, E, F, G, H);
+    sign_l = __msa_or_v((v16u8)r1_r, (v16u8)r2_r);
+    sign_l = __msa_or_v(sign_l, (v16u8)r3_r);
+    sign_l = __msa_or_v(sign_l, (v16u8)r0_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r1_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r2_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r3_l);
+    sign_t = __msa_ceqi_w((v4i32)sign_l, 0);
+    Add = ((r0_r * cnst46341w) + (8 << 16)) >> 20;
+    if (type == 1) {
+        Bdd = Add + cnst128w;
+        CLIP_SW_0_255(Bdd);
+        Ad = Bdd;
+        Bd = Bdd;
+        Cd = Bdd;
+        Dd = Bdd;
+        Ed = Bdd;
+        Fd = Bdd;
+        Gd = Bdd;
+        Hd = Bdd;
+    } else {
+        Ad = Add + c0;
+        Bd = Add + c1;
+        Cd = Add + c2;
+        Dd = Add + c3;
+        Ed = Add + c4;
+        Fd = Add + c5;
+        Gd = Add + c6;
+        Hd = Add + c7;
+        CLIP_SW8_0_255(Ad, Bd, Cd, Dd, Ed, Fd, Gd, Hd);
+    }
+    Ad = (v4i32)__msa_and_v((v16u8)Ad, (v16u8)sign_t);
+    Bd = (v4i32)__msa_and_v((v16u8)Bd, (v16u8)sign_t);
+    Cd = (v4i32)__msa_and_v((v16u8)Cd, (v16u8)sign_t);
+    Dd = (v4i32)__msa_and_v((v16u8)Dd, (v16u8)sign_t);
+    Ed = (v4i32)__msa_and_v((v16u8)Ed, (v16u8)sign_t);
+    Fd = (v4i32)__msa_and_v((v16u8)Fd, (v16u8)sign_t);
+    Gd = (v4i32)__msa_and_v((v16u8)Gd, (v16u8)sign_t);
+    Hd = (v4i32)__msa_and_v((v16u8)Hd, (v16u8)sign_t);
+    sign_t = __msa_ceqi_w(sign_t, 0);
+    A = (v4i32)__msa_and_v((v16u8)A, (v16u8)sign_t);
+    B = (v4i32)__msa_and_v((v16u8)B, (v16u8)sign_t);
+    C = (v4i32)__msa_and_v((v16u8)C, (v16u8)sign_t);
+    D = (v4i32)__msa_and_v((v16u8)D, (v16u8)sign_t);
+    E = (v4i32)__msa_and_v((v16u8)E, (v16u8)sign_t);
+    F = (v4i32)__msa_and_v((v16u8)F, (v16u8)sign_t);
+    G = (v4i32)__msa_and_v((v16u8)G, (v16u8)sign_t);
+    H = (v4i32)__msa_and_v((v16u8)H, (v16u8)sign_t);
+    r0_r = Ad + A;
+    r1_r = Bd + C;
+    r2_r = Cd + D;
+    r3_r = Dd + E;
+    r0_l = Ed + F;
+    r1_l = Fd + G;
+    r2_l = Gd + H;
+    r3_l = Hd + B;
+
+    /* Row 4 to 7 */
+    TRANSPOSE4x4_SW_SW(r4_r, r5_r, r6_r, r7_r,
+                       r4_r, r5_r, r6_r, r7_r);
+    TRANSPOSE4x4_SW_SW(r4_l, r5_l, r6_l, r7_l,
+                       r4_l, r5_l, r6_l, r7_l);
+    A = ((r5_r * cnst64277w) >> 16) + ((r7_l * cnst12785w) >> 16);
+    B = ((r5_r * cnst12785w) >> 16) - ((r7_l * cnst64277w) >> 16);
+    C = ((r7_r * cnst54491w) >> 16) + ((r5_l * cnst36410w) >> 16);
+    D = ((r5_l * cnst54491w) >> 16) - ((r7_r * cnst36410w) >> 16);
+    Ad = ((A - C) * cnst46341w) >> 16;
+    Bd = ((B - D) * cnst46341w) >> 16;
+    Cd = A + C;
+    Dd = B + D;
+    E = ((r4_r + r4_l) * cnst46341w) >> 16;
+    E += cnst8w;
+    F = ((r4_r - r4_l) * cnst46341w) >> 16;
+    F += cnst8w;
+    if (type == 1) { // HACK
+        E += cnst2048w;
+        F += cnst2048w;
+    }
+    G = ((r6_r * cnst60547w) >> 16) + ((r6_l * cnst25080w) >> 16);
+    H = ((r6_r * cnst25080w) >> 16) - ((r6_l * cnst60547w) >> 16);
+    Ed = E - G;
+    Gd = E + G;
+    Add = F + Ad;
+    Bdd = Bd - H;
+    Fd = F - Ad;
+    Hd = Bd + H;
+    A = (Gd + Cd) >> 4;
+    B = (Gd - Cd) >> 4;
+    C = (Add + Hd) >> 4;
+    D = (Add - Hd) >> 4;
+    E = (Ed + Dd) >> 4;
+    F = (Ed - Dd) >> 4;
+    G = (Fd + Bdd) >> 4;
+    H = (Fd - Bdd) >> 4;
+    if (type != 1) {
+        ILVL_H4_SW(zero, f0, zero, f1, zero, f2, zero, f3,
+                   c0, c1, c2, c3);
+        ILVL_H4_SW(zero, f4, zero, f5, zero, f6, zero, f7,
+                   c4, c5, c6, c7);
+        A += c0;
+        B += c7;
+        C += c1;
+        D += c2;
+        E += c3;
+        F += c4;
+        G += c5;
+        H += c6;
+    }
+    CLIP_SW8_0_255(A, B, C, D, E, F, G, H);
+    sign_l = __msa_or_v((v16u8)r5_r, (v16u8)r6_r);
+    sign_l = __msa_or_v(sign_l, (v16u8)r7_r);
+    sign_l = __msa_or_v(sign_l, (v16u8)r4_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r5_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r6_l);
+    sign_l = __msa_or_v(sign_l, (v16u8)r7_l);
+    sign_t = __msa_ceqi_w((v4i32)sign_l, 0);
+    Add = ((r4_r * cnst46341w) + (8 << 16)) >> 20;
+    if (type == 1) {
+        Bdd = Add + cnst128w;
+        CLIP_SW_0_255(Bdd);
+        Ad = Bdd;
+        Bd = Bdd;
+        Cd = Bdd;
+        Dd = Bdd;
+        Ed = Bdd;
+        Fd = Bdd;
+        Gd = Bdd;
+        Hd = Bdd;
+    } else {
+        Ad = Add + c0;
+        Bd = Add + c1;
+        Cd = Add + c2;
+        Dd = Add + c3;
+        Ed = Add + c4;
+        Fd = Add + c5;
+        Gd = Add + c6;
+        Hd = Add + c7;
+        CLIP_SW8_0_255(Ad, Bd, Cd, Dd, Ed, Fd, Gd, Hd);
+    }
+    Ad = (v4i32)__msa_and_v((v16u8)Ad, (v16u8)sign_t);
+    Bd = (v4i32)__msa_and_v((v16u8)Bd, (v16u8)sign_t);
+    Cd = (v4i32)__msa_and_v((v16u8)Cd, (v16u8)sign_t);
+    Dd = (v4i32)__msa_and_v((v16u8)Dd, (v16u8)sign_t);
+    Ed = (v4i32)__msa_and_v((v16u8)Ed, (v16u8)sign_t);
+    Fd = (v4i32)__msa_and_v((v16u8)Fd, (v16u8)sign_t);
+    Gd = (v4i32)__msa_and_v((v16u8)Gd, (v16u8)sign_t);
+    Hd = (v4i32)__msa_and_v((v16u8)Hd, (v16u8)sign_t);
+    sign_t = __msa_ceqi_w(sign_t, 0);
+    A = (v4i32)__msa_and_v((v16u8)A, (v16u8)sign_t);
+    B = (v4i32)__msa_and_v((v16u8)B, (v16u8)sign_t);
+    C = (v4i32)__msa_and_v((v16u8)C, (v16u8)sign_t);
+    D = (v4i32)__msa_and_v((v16u8)D, (v16u8)sign_t);
+    E = (v4i32)__msa_and_v((v16u8)E, (v16u8)sign_t);
+    F = (v4i32)__msa_and_v((v16u8)F, (v16u8)sign_t);
+    G = (v4i32)__msa_and_v((v16u8)G, (v16u8)sign_t);
+    H = (v4i32)__msa_and_v((v16u8)H, (v16u8)sign_t);
+    r4_r = Ad + A;
+    r5_r = Bd + C;
+    r6_r = Cd + D;
+    r7_r = Dd + E;
+    r4_l = Ed + F;
+    r5_l = Fd + G;
+    r6_l = Gd + H;
+    r7_l = Hd + B;
+    VSHF_B2_SB(r0_r, r4_r, r1_r, r5_r, mask, mask, d0, d1);
+    VSHF_B2_SB(r2_r, r6_r, r3_r, r7_r, mask, mask, d2, d3);
+    VSHF_B2_SB(r0_l, r4_l, r1_l, r5_l, mask, mask, d4, d5);
+    VSHF_B2_SB(r2_l, r6_l, r3_l, r7_l, mask, mask, d6, d7);
+
+    /* Final sequence of operations over-write original dst */
+    ST_D1(d0, 0, dst);
+    ST_D1(d1, 0, dst + stride);
+    ST_D1(d2, 0, dst + 2 * stride);
+    ST_D1(d3, 0, dst + 3 * stride);
+    ST_D1(d4, 0, dst + 4 * stride);
+    ST_D1(d5, 0, dst + 5 * stride);
+    ST_D1(d6, 0, dst + 6 * stride);
+    ST_D1(d7, 0, dst + 7 * stride);
+}
+
+void ff_vp3_idct_put_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    idct_msa(dest, line_size, block, 1);
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+void ff_vp3_idct_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    idct_msa(dest, line_size, block, 2);
+    memset(block, 0, sizeof(*block) * 64);
+}
+
+void ff_vp3_idct_dc_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
+{
+    int i = (block[0] + 15) >> 5;
+    v4i32 dc = {i, i, i, i};
+    v16i8 d0, d1, d2, d3, d4, d5, d6, d7;
+    v4i32 c0, c1, c2, c3, c4, c5, c6, c7;
+    v4i32 e0, e1, e2, e3, e4, e5, e6, e7;
+    v4i32 r0, r1, r2, r3, r4, r5, r6, r7;
+    v16i8 mask = {0, 4, 8, 12, 16, 20, 24, 28, 0, 0, 0, 0, 0, 0, 0, 0};
+    v16i8 zero = {0};
+
+    LD_SB8(dest, line_size, d0, d1, d2, d3, d4, d5, d6, d7);
+    ILVR_B4_SW(zero, d0, zero, d1, zero, d2, zero, d3,
+               c0, c1, c2, c3);
+    ILVR_B4_SW(zero, d4, zero, d5, zero, d6, zero, d7,
+               c4, c5, c6, c7);
+    /* Right part */
+    ILVR_H4_SW(zero, c0, zero, c1, zero, c2, zero, c3,
+               e0, e1, e2, e3);
+    ILVR_H4_SW(zero, c4, zero, c5, zero, c6, zero, c7,
+               e4, e5, e6, e7);
+    e0 += dc;
+    e1 += dc;
+    e2 += dc;
+    e3 += dc;
+    e4 += dc;
+    e5 += dc;
+    e6 += dc;
+    e7 += dc;
+    CLIP_SW8_0_255(e0, e1, e2, e3, e4, e5, e6, e7);
+
+    /* Left part */
+    ILVL_H4_SW(zero, c0, zero, c1, zero, c2, zero, c3,
+               r0, r1, r2, r3);
+    ILVL_H4_SW(zero, c4, zero, c5, zero, c6, zero, c7,
+               r4, r5, r6, r7);
+    r0 += dc;
+    r1 += dc;
+    r2 += dc;
+    r3 += dc;
+    r4 += dc;
+    r5 += dc;
+    r6 += dc;
+    r7 += dc;
+    CLIP_SW8_0_255(r0, r1, r2, r3, r4, r5, r6, r7);
+    VSHF_B2_SB(e0, r0, e1, r1, mask, mask, d0, d1);
+    VSHF_B2_SB(e2, r2, e3, r3, mask, mask, d2, d3);
+    VSHF_B2_SB(e4, r4, e5, r5, mask, mask, d4, d5);
+    VSHF_B2_SB(e6, r6, e7, r7, mask, mask, d6, d7);
+
+    /* Final sequence of operations over-write original dst */
+    ST_D1(d0, 0, dest);
+    ST_D1(d1, 0, dest + line_size);
+    ST_D1(d2, 0, dest + 2 * line_size);
+    ST_D1(d3, 0, dest + 3 * line_size);
+    ST_D1(d4, 0, dest + 4 * line_size);
+    ST_D1(d5, 0, dest + 5 * line_size);
+    ST_D1(d6, 0, dest + 6 * line_size);
+    ST_D1(d7, 0, dest + 7 * line_size);
+
+    block[0] = 0;
+}
+
+void ff_vp3_v_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
+                              int *bounding_values)
+{
+    int nstride = -stride;
+    v4i32 e0, e1, f0, f1, g0, g1;
+    v16i8 zero = {0};
+    v16i8 d0, d1, d2, d3;
+    v8i16 c0, c1, c2, c3;
+    v8i16 r0;
+    v8i16 cnst3h = {3, 3, 3, 3, 3, 3, 3, 3},
+          cnst4h = {4, 4, 4, 4, 4, 4, 4, 4};
+    v16i8 mask = {0, 4, 8, 12, 16, 20, 24, 28, 0, 0, 0, 0, 0, 0, 0, 0};
+    int16_t temp_16[8];
+    int temp_32[8];
+
+    LD_SB4(first_pixel + nstride * 2, stride, d0, d1, d2, d3);
+    ILVR_B4_SH(zero, d0, zero, d1, zero, d2, zero, d3,
+               c0, c1, c2, c3);
+    r0 = (c0 - c3) + (c2 - c1) * cnst3h;
+    r0 += cnst4h;
+    r0 = r0 >> 3;
+    /* Get filter_value from bounding_values one by one */
+    ST_SH(r0, temp_16);
+    for (int i = 0; i < 8; i++)
+        temp_32[i] = bounding_values[temp_16[i]];
+    LD_SW2(temp_32, 4, e0, e1);
+    ILVR_H2_SW(zero, c1, zero, c2, f0, g0);
+    ILVL_H2_SW(zero, c1, zero, c2, f1, g1);
+    f0 += e0;
+    f1 += e1;
+    g0 -= e0;
+    g1 -= e1;
+    CLIP_SW4_0_255(f0, f1, g0, g1);
+    VSHF_B2_SB(f0, f1, g0, g1, mask, mask, d1, d2);
+
+    /* Final move to first_pixel */
+    ST_D1(d1, 0, first_pixel + nstride);
+    ST_D1(d2, 0, first_pixel);
+}
+
+void ff_vp3_h_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
+                              int *bounding_values)
+{
+    v16i8 d0, d1, d2, d3, d4, d5, d6, d7;
+    v8i16 c0, c1, c2, c3, c4, c5, c6, c7;
+    v8i16 r0;
+    v4i32 e0, e1, f0, f1, g0, g1;
+    v16i8 zero = {0};
+    v8i16 cnst3h = {3, 3, 3, 3, 3, 3, 3, 3},
+          cnst4h = {4, 4, 4, 4, 4, 4, 4, 4};
+    v16i8 mask = {0, 16, 4, 20, 8, 24, 12, 28, 0, 0, 0, 0, 0, 0, 0, 0};
+    int16_t temp_16[8];
+    int temp_32[8];
+
+    LD_SB8(first_pixel - 2, stride, d0, d1, d2, d3, d4, d5, d6, d7);
+    ILVR_B4_SH(zero, d0, zero, d1, zero, d2, zero, d3,
+               c0, c1, c2, c3);
+    ILVR_B4_SH(zero, d4, zero, d5, zero, d6, zero, d7,
+               c4, c5, c6, c7);
+    TRANSPOSE8x8_SH_SH(c0, c1, c2, c3, c4, c5, c6, c7,
+                       c0, c1, c2, c3, c4, c5, c6, c7);
+    r0 = (c0 - c3) + (c2 - c1) * cnst3h;
+    r0 += cnst4h;
+    r0 = r0 >> 3;
+
+    /* Get filter_value from bounding_values one by one */
+    ST_SH(r0, temp_16);
+    for (int i = 0; i < 8; i++)
+        temp_32[i] = bounding_values[temp_16[i]];
+    LD_SW2(temp_32, 4, e0, e1);
+    ILVR_H2_SW(zero, c1, zero, c2, f0, g0);
+    ILVL_H2_SW(zero, c1, zero, c2, f1, g1);
+    f0 += e0;
+    f1 += e1;
+    g0 -= e0;
+    g1 -= e1;
+    CLIP_SW4_0_255(f0, f1, g0, g1);
+    VSHF_B2_SB(f0, g0, f1, g1, mask, mask, d1, d2);
+    /* Final move to first_pixel */
+    ST_H4(d1, 0, 1, 2, 3, first_pixel - 1, stride);
+    ST_H4(d2, 0, 1, 2, 3, first_pixel - 1 + 4 * stride, stride);
+}
+
+void ff_put_no_rnd_pixels_l2_msa(uint8_t *dst, const uint8_t *src1,
+                                 const uint8_t *src2, ptrdiff_t stride, int h)
+{
+    if (h == 8) {
+        v16i8 d0, d1, d2, d3, d4, d5, d6, d7;
+        v16i8 c0, c1, c2, c3;
+        v4i32 a0, a1, a2, a3, b0, b1, b2, b3;
+        v4i32 e0, e1, e2;
+        v4i32 f0, f1, f2;
+        v4u32 t0, t1, t2, t3;
+        v16i8 mask = {0, 1, 2, 3, 16, 17, 18, 19, 4, 5, 6, 7, 20, 21, 22, 23};
+        int32_t value = 0xfefefefe;
+        v4i32 fmask = {value, value, value, value};
+
+        LD_SB8(src1, stride, d0, d1, d2, d3, d4, d5, d6, d7);
+        VSHF_B2_SB(d0, d1, d2, d3, mask, mask, c0, c1);
+        VSHF_B2_SB(d4, d5, d6, d7, mask, mask, c2, c3);
+        a0 = (v4i32) __msa_pckev_d((v2i64)c1, (v2i64)c0);
+        a2 = (v4i32) __msa_pckod_d((v2i64)c1, (v2i64)c0);
+        a1 = (v4i32) __msa_pckev_d((v2i64)c3, (v2i64)c2);
+        a3 = (v4i32) __msa_pckod_d((v2i64)c3, (v2i64)c2);
+
+        LD_SB8(src2, stride, d0, d1, d2, d3, d4, d5, d6, d7);
+        VSHF_B2_SB(d0, d1, d2, d3, mask, mask, c0, c1);
+        VSHF_B2_SB(d4, d5, d6, d7, mask, mask, c2, c3);
+        b0 = (v4i32) __msa_pckev_d((v2i64)c1, (v2i64)c0);
+        b2 = (v4i32) __msa_pckod_d((v2i64)c1, (v2i64)c0);
+        b1 = (v4i32) __msa_pckev_d((v2i64)c3, (v2i64)c2);
+        b3 = (v4i32) __msa_pckod_d((v2i64)c3, (v2i64)c2);
+
+        e0 = (v4i32) __msa_xor_v((v16u8)a0, (v16u8)b0);
+        e0 = (v4i32) __msa_and_v((v16u8)e0, (v16u8)fmask);
+        t0 = ((v4u32)e0) >> 1;
+        e2 = (v4i32) __msa_and_v((v16u8)a0, (v16u8)b0);
+        t0 = t0 + (v4u32)e2;
+
+        e1 = (v4i32) __msa_xor_v((v16u8)a1, (v16u8)b1);
+        e1 = (v4i32) __msa_and_v((v16u8)e1, (v16u8)fmask);
+        t1 = ((v4u32)e1) >> 1;
+        e2 = (v4i32) __msa_and_v((v16u8)a1, (v16u8)b1);
+        t1 = t1 + (v4u32)e2;
+
+        f0 = (v4i32) __msa_xor_v((v16u8)a2, (v16u8)b2);
+        f0 = (v4i32) __msa_and_v((v16u8)f0, (v16u8)fmask);
+        t2 = ((v4u32)f0) >> 1;
+        f2 = (v4i32) __msa_and_v((v16u8)a2, (v16u8)b2);
+        t2 = t2 + (v4u32)f2;
+
+        f1 = (v4i32) __msa_xor_v((v16u8)a3, (v16u8)b3);
+        f1 = (v4i32) __msa_and_v((v16u8)f1, (v16u8)fmask);
+        t3 = ((v4u32)f1) >> 1;
+        f2 = (v4i32) __msa_and_v((v16u8)a3, (v16u8)b3);
+        t3 = t3 + (v4u32)f2;
+
+        ST_W8(t0, t1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
+        ST_W8(t2, t3, 0, 1, 2, 3, 0, 1, 2, 3, dst + 4, stride);
+    } else {
+        int i;
+
+        for (i = 0; i < h; i++) {
+            uint32_t a, b;
+
+            a = AV_RN32(&src1[i * stride]);
+            b = AV_RN32(&src2[i * stride]);
+            AV_WN32A(&dst[i * stride], no_rnd_avg32(a, b));
+            a = AV_RN32(&src1[i * stride + 4]);
+            b = AV_RN32(&src2[i * stride + 4]);
+            AV_WN32A(&dst[i * stride + 4], no_rnd_avg32(a, b));
+        }
+    }
+}
diff --git a/libavcodec/mips/vp3dsp_init_mips.c b/libavcodec/mips/vp3dsp_init_mips.c
new file mode 100644
index 0000000..4252ff7
--- /dev/null
+++ b/libavcodec/mips/vp3dsp_init_mips.c
@@ -0,0 +1,50 @@
+
+/*
+ * Copyright (c) 2018 gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/mips/cpu.h"
+#include "config.h"
+#include "libavutil/attributes.h"
+#include "libavcodec/avcodec.h"
+#include "libavcodec/vp3dsp.h"
+#include "vp3dsp_mips.h"
+
+av_cold void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_mmi;
+
+        c->idct_add      = ff_vp3_idct_add_mmi;
+        c->idct_put      = ff_vp3_idct_put_mmi;
+        c->idct_dc_add   = ff_vp3_idct_dc_add_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_msa;
+
+        c->idct_add      = ff_vp3_idct_add_msa;
+        c->idct_put      = ff_vp3_idct_put_msa;
+        c->idct_dc_add   = ff_vp3_idct_dc_add_msa;
+        c->v_loop_filter = ff_vp3_v_loop_filter_msa;
+        c->h_loop_filter = ff_vp3_h_loop_filter_msa;
+    }
+}
diff --git a/libavcodec/mips/vp3dsp_mips.h b/libavcodec/mips/vp3dsp_mips.h
new file mode 100644
index 0000000..4685a82
--- /dev/null
+++ b/libavcodec/mips/vp3dsp_mips.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (c) 2018 gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_MIPS_VP3DSP_MIPS_H
+#define AVCODEC_MIPS_VP3DSP_MIPS_H
+
+#include "libavcodec/vp3dsp.h"
+#include <string.h>
+
+void ff_vp3_idct_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_vp3_idct_put_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_vp3_idct_dc_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_vp3_v_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
+                              int *bounding_values);
+void ff_put_no_rnd_pixels_l2_msa(uint8_t *dst, const uint8_t *src1,
+                                 const uint8_t *src2, ptrdiff_t stride, int h);
+void ff_vp3_h_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
+                              int *bounding_values);
+
+void ff_vp3_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_vp3_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_vp3_idct_dc_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_put_no_rnd_pixels_l2_mmi(uint8_t *dst, const uint8_t *src1,
+                                 const uint8_t *src2, ptrdiff_t stride, int h);
+
+#endif /* #ifndef AVCODEC_MIPS_VP3DSP_MIPS_H */
diff --git a/libavcodec/mips/vp8_idct_msa.c b/libavcodec/mips/vp8_idct_msa.c
index 11ac9ff..ce37ca1 100644
--- a/libavcodec/mips/vp8_idct_msa.c
+++ b/libavcodec/mips/vp8_idct_msa.c
@@ -71,12 +71,10 @@ void ff_vp8_idct_add_msa(uint8_t *dst, int16_t input[16], ptrdiff_t stride)
     ILVR_H4_SW(zero, res0, zero, res1, zero, res2, zero, res3,
                res0, res1, res2, res3);
     ADD4(res0, vt0, res1, vt1, res2, vt2, res3, vt3, res0, res1, res2, res3);
-    res0 = CLIP_SW_0_255(res0);
-    res1 = CLIP_SW_0_255(res1);
-    res2 = CLIP_SW_0_255(res2);
-    res3 = CLIP_SW_0_255(res3);
+    CLIP_SW4_0_255(res0, res1, res2, res3);
     VSHF_B2_SB(res0, res1, res2, res3, mask, mask, dest0, dest1);
-    ST4x4_UB(dest0, dest1, 0, 1, 0, 1, dst, stride);
+    ST_W2(dest0, 0, 1, dst, stride);
+    ST_W2(dest1, 0, 1, dst + 2 * stride, stride);
 
     memset(input, 0, 4 * 4 * sizeof(*input));
 }
@@ -97,7 +95,8 @@ void ff_vp8_idct_dc_add_msa(uint8_t *dst, int16_t in_dc[16], ptrdiff_t stride)
     ADD4(res0, vec, res1, vec, res2, vec, res3, vec, res0, res1, res2, res3);
     CLIP_SH4_0_255(res0, res1, res2, res3);
     VSHF_B2_SB(res0, res1, res2, res3, mask, mask, dest0, dest1);
-    ST4x4_UB(dest0, dest1, 0, 1, 0, 1, dst, stride);
+    ST_W2(dest0, 0, 1, dst, stride);
+    ST_W2(dest1, 0, 1, dst + 2 * stride, stride);
 
     in_dc[0] = 0;
 }
diff --git a/libavcodec/mips/vp8_lpf_msa.c b/libavcodec/mips/vp8_lpf_msa.c
index 3590961..1b51334 100644
--- a/libavcodec/mips/vp8_lpf_msa.c
+++ b/libavcodec/mips/vp8_lpf_msa.c
@@ -540,14 +540,8 @@ void ff_vp8_h_loop_filter_simple_msa(uint8_t *src, ptrdiff_t pitch,
     ILVRL_B2_SH(q0, p0, tmp1, tmp0);
 
     src -= 1;
-    ST2x4_UB(tmp1, 0, src, pitch);
-    src += 4 * pitch;
-    ST2x4_UB(tmp1, 4, src, pitch);
-    src += 4 * pitch;
-    ST2x4_UB(tmp0, 0, src, pitch);
-    src += 4 * pitch;
-    ST2x4_UB(tmp0, 4, src, pitch);
-    src += 4 * pitch;
+    ST_H8(tmp1, 0, 1, 2, 3, 4, 5, 6, 7, src, pitch)
+    ST_H8(tmp0, 0, 1, 2, 3, 4, 5, 6, 7, src + 8 * pitch, pitch)
 }
 
 void ff_vp8_v_loop_filter8uv_inner_msa(uint8_t *src_u, uint8_t *src_v,
@@ -596,7 +590,6 @@ void ff_vp8_h_loop_filter8uv_inner_msa(uint8_t *src_u, uint8_t *src_v,
                                        ptrdiff_t pitch, int b_limit_in,
                                        int limit_in, int thresh_in)
 {
-    uint8_t *temp_src_u, *temp_src_v;
     v16u8 p3, p2, p1, p0, q3, q2, q1, q0;
     v16u8 mask, hev, flat, thresh, limit, b_limit;
     v16u8 row0, row1, row2, row3, row4, row5, row6, row7, row8;
@@ -623,15 +616,8 @@ void ff_vp8_h_loop_filter8uv_inner_msa(uint8_t *src_u, uint8_t *src_v,
     tmp1 = (v4i32) __msa_ilvl_b((v16i8) q1, (v16i8) q0);
     ILVRL_H2_SW(tmp1, tmp0, tmp4, tmp5);
 
-    temp_src_u = src_u - 2;
-    ST4x4_UB(tmp2, tmp2, 0, 1, 2, 3, temp_src_u, pitch);
-    temp_src_u += 4 * pitch;
-    ST4x4_UB(tmp3, tmp3, 0, 1, 2, 3, temp_src_u, pitch);
-
-    temp_src_v = src_v - 2;
-    ST4x4_UB(tmp4, tmp4, 0, 1, 2, 3, temp_src_v, pitch);
-    temp_src_v += 4 * pitch;
-    ST4x4_UB(tmp5, tmp5, 0, 1, 2, 3, temp_src_v, pitch);
+    ST_W8(tmp2, tmp3, 0, 1, 2, 3, 0, 1, 2, 3, src_u - 2, pitch);
+    ST_W8(tmp4, tmp5, 0, 1, 2, 3, 0, 1, 2, 3, src_v - 2, pitch);
 }
 
 void ff_vp8_v_loop_filter16_inner_msa(uint8_t *src, ptrdiff_t pitch,
@@ -684,7 +670,6 @@ void ff_vp8_h_loop_filter16_inner_msa(uint8_t *src, ptrdiff_t pitch,
     ILVRL_H2_SH(tmp1, tmp0, tmp4, tmp5);
 
     src -= 2;
-    ST4x8_UB(tmp2, tmp3, src, pitch);
-    src += (8 * pitch);
-    ST4x8_UB(tmp4, tmp5, src, pitch);
+    ST_W8(tmp2, tmp3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch)
+    ST_W8(tmp4, tmp5, 0, 1, 2, 3, 0, 1, 2, 3, src + 8 * pitch, pitch)
 }
diff --git a/libavcodec/mips/vp8_mc_msa.c b/libavcodec/mips/vp8_mc_msa.c
index 2bf0abd..a613206 100644
--- a/libavcodec/mips/vp8_mc_msa.c
+++ b/libavcodec/mips/vp8_mc_msa.c
@@ -181,7 +181,7 @@ static void common_hz_6t_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(out0, out1, 7);
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_6t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -214,10 +214,9 @@ static void common_hz_6t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 7);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 void ff_put_vp8_epel4_h6_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -263,7 +262,7 @@ void ff_put_vp8_epel8_h6_msa(uint8_t *dst, ptrdiff_t dst_stride,
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     tmp0 = PCKEV_XORI128_UB(out0, out1);
     tmp1 = PCKEV_XORI128_UB(out2, out3);
-    ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+    ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 
     for (loop_cnt = (height >> 2) - 1; loop_cnt--;) {
@@ -276,7 +275,7 @@ void ff_put_vp8_epel8_h6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -368,7 +367,7 @@ void ff_put_vp8_epel4_v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SRARI_H2_SH(out10, out32, 7);
         SAT_SH2_SH(out10, out32, 7);
         out = PCKEV_XORI128_UB(out10, out32);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src2110 = src6554;
@@ -416,7 +415,7 @@ void ff_put_vp8_epel8_v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
         tmp0 = PCKEV_XORI128_UB(out0_r, out1_r);
         tmp1 = PCKEV_XORI128_UB(out2_r, out3_r);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src76_r;
@@ -567,7 +566,7 @@ void ff_put_vp8_epel4_h6v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SRARI_H2_SH(tmp0, tmp1, 7);
         SAT_SH2_SH(tmp0, tmp1, 7);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out3 = hz_out7;
@@ -651,7 +650,7 @@ void ff_put_vp8_epel8_h6v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         vec0 = PCKEV_XORI128_UB(tmp0, tmp1);
         vec1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(vec0, vec1, dst, dst_stride);
+        ST_D4(vec0, vec1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out4 = hz_out8;
@@ -702,7 +701,7 @@ static void common_hz_4t_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(out0, out1, 7);
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_4t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -735,10 +734,9 @@ static void common_hz_4t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 7);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
@@ -769,10 +767,10 @@ static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 7);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     dst += (4 * dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     dst += (4 * dst_stride);
 
     LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
@@ -785,10 +783,10 @@ static void common_hz_4t_4x16_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 7);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     dst += (4 * dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_put_vp8_epel4_h4_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -836,7 +834,7 @@ void ff_put_vp8_epel8_h4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -932,7 +930,7 @@ void ff_put_vp8_epel4_v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SRARI_H2_SH(out10, out32, 7);
         SAT_SH2_SH(out10, out32, 7);
         out = PCKEV_XORI128_UB(out10, out32);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -974,7 +972,7 @@ void ff_put_vp8_epel8_v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
         tmp0 = PCKEV_XORI128_UB(out0_r, out1_r);
         tmp1 = PCKEV_XORI128_UB(out2_r, out3_r);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src98_r;
@@ -1093,7 +1091,7 @@ void ff_put_vp8_epel4_h4v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SRARI_H2_SH(tmp0, tmp1, 7);
         SAT_SH2_SH(tmp0, tmp1, 7);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out1 = hz_out5;
@@ -1160,7 +1158,7 @@ void ff_put_vp8_epel8_h4v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         vec0 = vec4;
@@ -1240,7 +1238,8 @@ void ff_put_vp8_epel4_h6v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH2_SH(tmp0, tmp1, 7);
         PCKEV_B2_UB(tmp0, tmp0, tmp1, tmp1, res0, res1);
         XORI_B2_128_UB(res0, res1);
-        ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+        ST_W2(res0, 0, 1, dst, dst_stride);
+        ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out1 = hz_out5;
@@ -1316,7 +1315,7 @@ void ff_put_vp8_epel8_h6v4_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         out0 = PCKEV_XORI128_UB(tmp0, tmp1);
         out1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -1391,7 +1390,7 @@ void ff_put_vp8_epel4_h4v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SRARI_H2_SH(tmp0, tmp1, 7);
         SAT_SH2_SH(tmp0, tmp1, 7);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out3 = hz_out7;
@@ -1464,7 +1463,7 @@ void ff_put_vp8_epel8_h4v6_msa(uint8_t *dst, ptrdiff_t dst_stride,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         vec0 = PCKEV_XORI128_UB(tmp0, tmp1);
         vec1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(vec0, vec1, dst, dst_stride);
+        ST_D4(vec0, vec1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out4 = hz_out8;
@@ -1509,7 +1508,8 @@ static void common_hz_2t_4x4_msa(uint8_t *src, int32_t src_stride,
     DOTP_UB2_UH(vec0, vec1, filt0, filt0, vec2, vec3);
     SRARI_H2_UH(vec2, vec3, 7);
     PCKEV_B2_UB(vec2, vec2, vec3, vec3, res0, res1);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
 }
 
 static void common_hz_2t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -1535,9 +1535,10 @@ static void common_hz_2t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(vec4, vec5, vec6, vec7, 7);
     PCKEV_B4_SB(vec4, vec4, vec5, vec5, vec6, vec6, vec7, vec7,
                 res0, res1, res2, res3);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST4x4_UB(res2, res3, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
+    ST_W2(res2, 0, 1, dst + 4 * dst_stride, dst_stride);
+    ST_W2(res3, 0, 1, dst + 6 * dst_stride, dst_stride);
 }
 
 void ff_put_vp8_bilinear4_h_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -1574,7 +1575,7 @@ static void common_hz_2t_8x4_msa(uint8_t *src, int32_t src_stride,
                 vec0, vec1, vec2, vec3);
     SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, src0, src1);
-    ST8x4_UB(src0, src1, dst, dst_stride);
+    ST_D4(src0, src1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hz_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
@@ -1604,8 +1605,7 @@ static void common_hz_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
     src += (4 * src_stride);
 
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
     VSHF_B2_UH(src0, src0, src1, src1, mask, mask, vec0, vec1);
     VSHF_B2_UH(src2, src2, src3, src3, mask, mask, vec2, vec3);
@@ -1613,8 +1613,8 @@ static void common_hz_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
                 vec0, vec1, vec2, vec3);
     SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+    dst += (8 * dst_stride);
 
     if (16 == height) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
@@ -1629,7 +1629,7 @@ static void common_hz_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
         src += (4 * src_stride);
 
         PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         VSHF_B2_UH(src0, src0, src1, src1, mask, mask, vec0, vec1);
         VSHF_B2_UH(src2, src2, src3, src3, mask, mask, vec2, vec3);
@@ -1637,7 +1637,7 @@ static void common_hz_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
                     vec0, vec1, vec2, vec3);
         SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
         PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-        ST8x4_UB(out0, out1, dst + 4 * dst_stride, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
     }
 }
 
@@ -1745,7 +1745,7 @@ static void common_vt_2t_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_UH(tmp0, tmp1, 7);
     SAT_UH2_UH(tmp0, tmp1, 7);
     src2110 = __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(src2110, src2110, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(src2110, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_vt_2t_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -1779,8 +1779,7 @@ static void common_vt_2t_4x8_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, src2110, src4332);
-    ST4x4_UB(src2110, src2110, 0, 1, 2, 3, dst, dst_stride);
-    ST4x4_UB(src4332, src4332, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
+    ST_W8(src2110, src4332, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_put_vp8_bilinear4_v_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -1817,7 +1816,7 @@ static void common_vt_2t_8x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_vt_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
@@ -1851,16 +1850,15 @@ static void common_vt_2t_8x8mult_msa(uint8_t *src, int32_t src_stride,
         SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         DOTP_UB4_UH(vec4, vec5, vec6, vec7, filt0, filt0, filt0, filt0,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        dst += (8 * dst_stride);
 
         src0 = src8;
     }
@@ -1964,7 +1962,8 @@ static void common_hv_2ht_2vt_4x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H2_UH(tmp0, tmp1, 7);
     SAT_UH2_UH(tmp0, tmp1, 7);
     PCKEV_B2_UB(tmp0, tmp0, tmp1, tmp1, res0, res1);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
 }
 
 static void common_hv_2ht_2vt_4x8_msa(uint8_t *src, int32_t src_stride,
@@ -1996,8 +1995,8 @@ static void common_hv_2ht_2vt_4x8_msa(uint8_t *src, int32_t src_stride,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     ILVEV_B2_UB(hz_out0, hz_out1, hz_out2, hz_out3, vec0, vec1);
@@ -2008,9 +2007,10 @@ static void common_hv_2ht_2vt_4x8_msa(uint8_t *src, int32_t src_stride,
     SAT_UH4_UH(vec4, vec5, vec6, vec7, 7);
     PCKEV_B4_SB(vec4, vec4, vec5, vec5, vec6, vec6, vec7, vec7,
                 res0, res1, res2, res3);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST4x4_UB(res2, res3, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
+    ST_W2(res2, 0, 1, dst + 4 * dst_stride, dst_stride);
+    ST_W2(res3, 0, 1, dst + 6 * dst_stride, dst_stride);
 }
 
 void ff_put_vp8_bilinear4_hv_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -2070,7 +2070,7 @@ static void common_hv_2ht_2vt_8x4_msa(uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hv_2ht_2vt_8x8mult_msa(uint8_t *src, int32_t src_stride,
@@ -2127,8 +2127,7 @@ static void common_hv_2ht_2vt_8x8mult_msa(uint8_t *src, int32_t src_stride,
         SRARI_H2_UH(tmp3, tmp4, 7);
         SAT_UH2_UH(tmp3, tmp4, 7);
         PCKEV_B2_SB(tmp2, tmp1, tmp4, tmp3, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         hz_out1 = HORIZ_2TAP_FILT_UH(src1, src1, mask, filt_hz, 7);
         vec0 = (v16u8) __msa_ilvev_b((v16i8) hz_out1, (v16i8) hz_out0);
@@ -2149,8 +2148,8 @@ static void common_hv_2ht_2vt_8x8mult_msa(uint8_t *src, int32_t src_stride,
         SRARI_H4_UH(tmp5, tmp6, tmp7, tmp8, 7);
         SAT_UH4_UH(tmp5, tmp6, tmp7, tmp8, 7);
         PCKEV_B2_SB(tmp6, tmp5, tmp8, tmp7, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        dst += (8 * dst_stride);
     }
 }
 
diff --git a/libavcodec/mips/vp8dsp_init_mips.c b/libavcodec/mips/vp8dsp_init_mips.c
index 3fc5f8e..92d8c79 100644
--- a/libavcodec/mips/vp8dsp_init_mips.c
+++ b/libavcodec/mips/vp8dsp_init_mips.c
@@ -24,6 +24,7 @@
  * VP8 compatible video decoder
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/vp8dsp.h"
@@ -71,132 +72,123 @@
     dsp->put_vp8_bilinear_pixels_tab[IDX][0][0] =  \
         ff_put_vp8_pixels##SIZE##_msa;
 
-#if HAVE_MSA
-static av_cold void vp8dsp_init_msa(VP8DSPContext *dsp)
-{
-    dsp->vp8_luma_dc_wht = ff_vp8_luma_dc_wht_msa;
-    dsp->vp8_idct_add = ff_vp8_idct_add_msa;
-    dsp->vp8_idct_dc_add = ff_vp8_idct_dc_add_msa;
-    dsp->vp8_idct_dc_add4y = ff_vp8_idct_dc_add4y_msa;
-    dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_msa;
-
-    VP8_MC_MIPS_FUNC(0, 16);
-    VP8_MC_MIPS_FUNC(1, 8);
-    VP8_MC_MIPS_FUNC(2, 4);
-
-    VP8_BILINEAR_MC_MIPS_FUNC(0, 16);
-    VP8_BILINEAR_MC_MIPS_FUNC(1, 8);
-    VP8_BILINEAR_MC_MIPS_FUNC(2, 4);
-
-    VP8_MC_MIPS_COPY(0, 16);
-    VP8_MC_MIPS_COPY(1, 8);
-
-    dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_msa;
-    dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_msa;
-    dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_msa;
-    dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_msa;
-
-    dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_msa;
-    dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_msa;
-    dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_msa;
-    dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_msa;
-
-    dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_msa;
-    dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_msa;
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void vp8dsp_init_mmi(VP8DSPContext *dsp)
-{
-    dsp->vp8_luma_dc_wht    = ff_vp8_luma_dc_wht_mmi;
-    dsp->vp8_luma_dc_wht_dc = ff_vp8_luma_dc_wht_dc_mmi;
-    dsp->vp8_idct_add       = ff_vp8_idct_add_mmi;
-    dsp->vp8_idct_dc_add    = ff_vp8_idct_dc_add_mmi;
-    dsp->vp8_idct_dc_add4y  = ff_vp8_idct_dc_add4y_mmi;
-    dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[0][0][1] = ff_put_vp8_epel16_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][0][2] = ff_put_vp8_epel16_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][0] = ff_put_vp8_epel16_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][1] = ff_put_vp8_epel16_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][2] = ff_put_vp8_epel16_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][0] = ff_put_vp8_epel16_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][1] = ff_put_vp8_epel16_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][2] = ff_put_vp8_epel16_h6v6_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[1][0][1] = ff_put_vp8_epel8_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][0][2] = ff_put_vp8_epel8_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][0] = ff_put_vp8_epel8_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][1] = ff_put_vp8_epel8_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][2] = ff_put_vp8_epel8_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][0] = ff_put_vp8_epel8_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][1] = ff_put_vp8_epel8_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][2] = ff_put_vp8_epel8_h6v6_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[2][0][1] = ff_put_vp8_epel4_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][0][2] = ff_put_vp8_epel4_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][0] = ff_put_vp8_epel4_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][1] = ff_put_vp8_epel4_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][2] = ff_put_vp8_epel4_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][0] = ff_put_vp8_epel4_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][1] = ff_put_vp8_epel4_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][2] = ff_put_vp8_epel4_h6v6_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[0][0][1] = ff_put_vp8_bilinear16_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][0][2] = ff_put_vp8_bilinear16_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][0] = ff_put_vp8_bilinear16_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][1] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][2] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][0] = ff_put_vp8_bilinear16_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][1] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][2] = ff_put_vp8_bilinear16_hv_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[1][0][1] = ff_put_vp8_bilinear8_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][0][2] = ff_put_vp8_bilinear8_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][0] = ff_put_vp8_bilinear8_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][1] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][2] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][0] = ff_put_vp8_bilinear8_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][1] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][2] = ff_put_vp8_bilinear8_hv_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[2][0][1] = ff_put_vp8_bilinear4_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][0][2] = ff_put_vp8_bilinear4_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][0] = ff_put_vp8_bilinear4_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][1] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][2] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][0] = ff_put_vp8_bilinear4_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][1] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][2] = ff_put_vp8_bilinear4_hv_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[0][0][0]     = ff_put_vp8_pixels16_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][0][0] = ff_put_vp8_pixels16_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[1][0][0]     = ff_put_vp8_pixels8_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][0][0] = ff_put_vp8_pixels8_mmi;
-
-    dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_mmi;
-    dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_mmi;
-    dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_mmi;
-    dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_mmi;
-
-    dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_mmi;
-    dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_mmi;
-    dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_mmi;
-    dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_mmi;
-
-    dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_mmi;
-    dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_mmi;
-}
-#endif /* HAVE_MMI */
 
 av_cold void ff_vp8dsp_init_mips(VP8DSPContext *dsp)
 {
-#if HAVE_MSA
-    vp8dsp_init_msa(dsp);
-#endif  // #if HAVE_MSA
-#if HAVE_MMI
-    vp8dsp_init_mmi(dsp);
-#endif /* HAVE_MMI */
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        dsp->vp8_luma_dc_wht    = ff_vp8_luma_dc_wht_mmi;
+        dsp->vp8_luma_dc_wht_dc = ff_vp8_luma_dc_wht_dc_mmi;
+        dsp->vp8_idct_add       = ff_vp8_idct_add_mmi;
+        dsp->vp8_idct_dc_add    = ff_vp8_idct_dc_add_mmi;
+        dsp->vp8_idct_dc_add4y  = ff_vp8_idct_dc_add4y_mmi;
+        dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[0][0][1] = ff_put_vp8_epel16_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][0][2] = ff_put_vp8_epel16_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][0] = ff_put_vp8_epel16_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][1] = ff_put_vp8_epel16_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][2] = ff_put_vp8_epel16_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][0] = ff_put_vp8_epel16_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][1] = ff_put_vp8_epel16_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][2] = ff_put_vp8_epel16_h6v6_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[1][0][1] = ff_put_vp8_epel8_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][0][2] = ff_put_vp8_epel8_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][0] = ff_put_vp8_epel8_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][1] = ff_put_vp8_epel8_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][2] = ff_put_vp8_epel8_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][0] = ff_put_vp8_epel8_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][1] = ff_put_vp8_epel8_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][2] = ff_put_vp8_epel8_h6v6_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[2][0][1] = ff_put_vp8_epel4_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][0][2] = ff_put_vp8_epel4_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][0] = ff_put_vp8_epel4_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][1] = ff_put_vp8_epel4_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][2] = ff_put_vp8_epel4_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][0] = ff_put_vp8_epel4_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][1] = ff_put_vp8_epel4_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][2] = ff_put_vp8_epel4_h6v6_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[0][0][1] = ff_put_vp8_bilinear16_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][0][2] = ff_put_vp8_bilinear16_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][0] = ff_put_vp8_bilinear16_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][1] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][2] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][0] = ff_put_vp8_bilinear16_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][1] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][2] = ff_put_vp8_bilinear16_hv_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[1][0][1] = ff_put_vp8_bilinear8_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][0][2] = ff_put_vp8_bilinear8_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][0] = ff_put_vp8_bilinear8_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][1] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][2] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][0] = ff_put_vp8_bilinear8_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][1] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][2] = ff_put_vp8_bilinear8_hv_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[2][0][1] = ff_put_vp8_bilinear4_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][0][2] = ff_put_vp8_bilinear4_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][0] = ff_put_vp8_bilinear4_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][1] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][2] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][0] = ff_put_vp8_bilinear4_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][1] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][2] = ff_put_vp8_bilinear4_hv_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[0][0][0]     = ff_put_vp8_pixels16_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][0][0] = ff_put_vp8_pixels16_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[1][0][0]     = ff_put_vp8_pixels8_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][0][0] = ff_put_vp8_pixels8_mmi;
+
+        dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_mmi;
+        dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_mmi;
+        dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_mmi;
+        dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_mmi;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_mmi;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_mmi;
+        dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_mmi;
+        dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_mmi;
+
+        dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_mmi;
+        dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        dsp->vp8_luma_dc_wht = ff_vp8_luma_dc_wht_msa;
+        dsp->vp8_idct_add = ff_vp8_idct_add_msa;
+        dsp->vp8_idct_dc_add = ff_vp8_idct_dc_add_msa;
+        dsp->vp8_idct_dc_add4y = ff_vp8_idct_dc_add4y_msa;
+        dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_msa;
+
+        VP8_MC_MIPS_FUNC(0, 16);
+        VP8_MC_MIPS_FUNC(1, 8);
+        VP8_MC_MIPS_FUNC(2, 4);
+
+        VP8_BILINEAR_MC_MIPS_FUNC(0, 16);
+        VP8_BILINEAR_MC_MIPS_FUNC(1, 8);
+        VP8_BILINEAR_MC_MIPS_FUNC(2, 4);
+
+        VP8_MC_MIPS_COPY(0, 16);
+        VP8_MC_MIPS_COPY(1, 8);
+
+        dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_msa;
+        dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_msa;
+        dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_msa;
+        dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_msa;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_msa;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_msa;
+        dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_msa;
+        dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_msa;
+
+        dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_msa;
+        dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_msa;
+    }
 }
diff --git a/libavcodec/mips/vp9_idct_msa.c b/libavcodec/mips/vp9_idct_msa.c
index bd762f2..53bfbb4 100644
--- a/libavcodec/mips/vp9_idct_msa.c
+++ b/libavcodec/mips/vp9_idct_msa.c
@@ -241,7 +241,7 @@ static const int32_t sinpi_4_9 = 15212;
          res0_m, res1_m, res2_m, res3_m);                         \
     CLIP_SH4_0_255(res0_m, res1_m, res2_m, res3_m);               \
     PCKEV_B2_SB(res1_m, res0_m, res3_m, res2_m, tmp0_m, tmp1_m);  \
-    ST8x4_UB(tmp0_m, tmp1_m, dst_m, dst_stride);                  \
+    ST_D4(tmp0_m, tmp1_m, 0, 1, 0, 1, dst_m, dst_stride);         \
 }
 
 #define VP9_IDCT4x4(in0, in1, in2, in3, out0, out1, out2, out3)       \
@@ -249,6 +249,7 @@ static const int32_t sinpi_4_9 = 15212;
     v8i16 c0_m, c1_m, c2_m, c3_m;                                     \
     v8i16 step0_m, step1_m;                                           \
     v4i32 tmp0_m, tmp1_m, tmp2_m, tmp3_m;                             \
+    v16i8 zeros = { 0 };                                              \
                                                                       \
     c0_m = VP9_SET_COSPI_PAIR(cospi_16_64, cospi_16_64);              \
     c1_m = VP9_SET_COSPI_PAIR(cospi_16_64, -cospi_16_64);             \
@@ -262,7 +263,7 @@ static const int32_t sinpi_4_9 = 15212;
     SRARI_W4_SW(tmp0_m, tmp1_m, tmp2_m, tmp3_m, VP9_DCT_CONST_BITS);  \
                                                                       \
     PCKEV_H2_SW(tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp0_m, tmp2_m);      \
-    SLDI_B2_0_SW(tmp0_m, tmp2_m, tmp1_m, tmp3_m, 8);                  \
+    SLDI_B2_SW(zeros, tmp0_m, zeros, tmp2_m, 8, tmp1_m, tmp3_m);      \
     BUTTERFLY_4((v8i16) tmp0_m, (v8i16) tmp1_m,                       \
                 (v8i16) tmp2_m, (v8i16) tmp3_m,                       \
                 out0, out1, out2, out3);                              \
@@ -364,7 +365,10 @@ static void vp9_idct4x4_colcol_addblk_msa(int16_t *input, uint8_t *dst,
     v8i16 zero = { 0 };
 
     /* load vector elements of 4x4 block */
-    LD4x4_SH(input, in0, in1, in2, in3);
+    in0 = LD_SH(input);
+    in2 = LD_SH(input + 8);
+    in1 = (v8i16) __msa_ilvl_d((v2i64) in0, (v2i64) in0);
+    in3 = (v8i16) __msa_ilvl_d((v2i64) in2, (v2i64) in2);
     ST_SH2(zero, zero, input, 8);
     /* rows */
     VP9_IDCT4x4(in0, in1, in2, in3, in0, in1, in2, in3);
@@ -383,7 +387,10 @@ static void vp9_iadst4x4_colcol_addblk_msa(int16_t *input, uint8_t *dst,
     v8i16 zero = { 0 };
 
     /* load vector elements of 4x4 block */
-    LD4x4_SH(input, in0, in1, in2, in3);
+    in0 = LD_SH(input);
+    in2 = LD_SH(input + 8);
+    in1 = (v8i16) __msa_ilvl_d((v2i64) in0, (v2i64) in0);
+    in3 = (v8i16) __msa_ilvl_d((v2i64) in2, (v2i64) in2);
     ST_SH2(zero, zero, input, 8);
     /* rows */
     VP9_IADST4x4(in0, in1, in2, in3, in0, in1, in2, in3);
@@ -402,7 +409,10 @@ static void vp9_iadst_idct_4x4_add_msa(int16_t *input, uint8_t *dst,
     v8i16 zero = { 0 };
 
     /* load vector elements of 4x4 block */
-    LD4x4_SH(input, in0, in1, in2, in3);
+    in0 = LD_SH(input);
+    in2 = LD_SH(input + 8);
+    in1 = (v8i16) __msa_ilvl_d((v2i64) in0, (v2i64) in0);
+    in3 = (v8i16) __msa_ilvl_d((v2i64) in2, (v2i64) in2);
     ST_SH2(zero, zero, input, 8);
     /* cols */
     VP9_IADST4x4(in0, in1, in2, in3, in0, in1, in2, in3);
@@ -421,7 +431,10 @@ static void vp9_idct_iadst_4x4_add_msa(int16_t *input, uint8_t *dst,
     v8i16 zero = { 0 };
 
     /* load vector elements of 4x4 block */
-    LD4x4_SH(input, in0, in1, in2, in3);
+    in0 = LD_SH(input);
+    in2 = LD_SH(input + 8);
+    in1 = (v8i16) __msa_ilvl_d((v2i64) in0, (v2i64) in0);
+    in3 = (v8i16) __msa_ilvl_d((v2i64) in2, (v2i64) in2);
     ST_SH2(zero, zero, input, 8);
     /* cols */
     VP9_IDCT4x4(in0, in1, in2, in3, in0, in1, in2, in3);
@@ -751,15 +764,15 @@ static void vp9_iadst8x8_colcol_addblk_msa(int16_t *input, uint8_t *dst,
 
     res0 = (v8i16) __msa_ilvr_b((v16i8) zero, (v16i8) dst0);
     res0 += out0;
-    res0 = CLIP_SH_0_255(res0);
+    CLIP_SH_0_255(res0);
     res0 = (v8i16) __msa_pckev_b((v16i8) res0, (v16i8) res0);
-    ST8x1_UB(res0, dst);
+    ST_D1(res0, 0, dst);
 
     res7 = (v8i16) __msa_ilvr_b((v16i8) zero, (v16i8) dst7);
     res7 += out7;
-    res7 = CLIP_SH_0_255(res7);
+    CLIP_SH_0_255(res7);
     res7 = (v8i16) __msa_pckev_b((v16i8) res7, (v16i8) res7);
-    ST8x1_UB(res7, dst + 7 * dst_stride);
+    ST_D1(res7, 0, dst + 7 * dst_stride);
 
     cnst1 = __msa_fill_h(cospi_24_64);
     cnst0 = __msa_fill_h(cospi_8_64);
@@ -782,8 +795,8 @@ static void vp9_iadst8x8_colcol_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res1, out1, res6, out6, res1, res6);
     CLIP_SH2_0_255(res1, res6);
     PCKEV_B2_SH(res1, res1, res6, res6, res1, res6);
-    ST8x1_UB(res1, dst + dst_stride);
-    ST8x1_UB(res6, dst + 6 * dst_stride);
+    ST_D1(res1, 0, dst + dst_stride);
+    ST_D1(res6, 0, dst + 6 * dst_stride);
 
     cnst0 = __msa_fill_h(cospi_16_64);
     cnst1 = -cnst0;
@@ -801,8 +814,8 @@ static void vp9_iadst8x8_colcol_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res3, out3, res4, out4, res3, res4);
     CLIP_SH2_0_255(res3, res4);
     PCKEV_B2_SH(res3, res3, res4, res4, res3, res4);
-    ST8x1_UB(res3, dst + 3 * dst_stride);
-    ST8x1_UB(res4, dst + 4 * dst_stride);
+    ST_D1(res3, 0, dst + 3 * dst_stride);
+    ST_D1(res4, 0, dst + 4 * dst_stride);
 
     out2 = VP9_DOT_SHIFT_RIGHT_PCK_H(temp2, temp3, cnst0);
     out5 = VP9_DOT_SHIFT_RIGHT_PCK_H(temp2, temp3, cnst1);
@@ -814,8 +827,8 @@ static void vp9_iadst8x8_colcol_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res2, out2, res5, out5, res2, res5);
     CLIP_SH2_0_255(res2, res5);
     PCKEV_B2_SH(res2, res2, res5, res5, res2, res5);
-    ST8x1_UB(res2, dst + 2 * dst_stride);
-    ST8x1_UB(res5, dst + 5 * dst_stride);
+    ST_D1(res2, 0, dst + 2 * dst_stride);
+    ST_D1(res5, 0, dst + 5 * dst_stride);
 }
 
 static void vp9_iadst_idct_8x8_add_msa(int16_t *input, uint8_t *dst,
@@ -1180,8 +1193,7 @@ static void vp9_idct16x16_1_add_msa(int16_t *input, uint8_t *dst,
              res3);
         ADD4(res4, vec, res5, vec, res6, vec, res7, vec, res4, res5, res6,
              res7);
-        CLIP_SH4_0_255(res0, res1, res2, res3);
-        CLIP_SH4_0_255(res4, res5, res6, res7);
+        CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
         PCKEV_B4_UB(res4, res0, res5, res1, res6, res2, res7, res3,
                     tmp0, tmp1, tmp2, tmp3);
         ST_UB4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
@@ -1354,8 +1366,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res0, out0, res1, out1, res0, res1);
     CLIP_SH2_0_255(res0, res1);
     PCKEV_B2_SH(res0, res0, res1, res1, res0, res1);
-    ST8x1_UB(res0, dst);
-    ST8x1_UB(res1, dst + 15 * dst_stride);
+    ST_D1(res0, 0, dst);
+    ST_D1(res1, 0, dst + 15 * dst_stride);
 
     k0 = VP9_SET_COSPI_PAIR(cospi_12_64, cospi_20_64);
     k1 = VP9_SET_COSPI_PAIR(-cospi_20_64, cospi_12_64);
@@ -1371,8 +1383,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res8, out8, res9, out9, res8, res9);
     CLIP_SH2_0_255(res8, res9);
     PCKEV_B2_SH(res8, res8, res9, res9, res8, res9);
-    ST8x1_UB(res8, dst + dst_stride);
-    ST8x1_UB(res9, dst + 14 * dst_stride);
+    ST_D1(res8, 0, dst + dst_stride);
+    ST_D1(res9, 0, dst + 14 * dst_stride);
 
     k0 = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);
     k1 = VP9_SET_COSPI_PAIR(cospi_24_64, -cospi_8_64);
@@ -1386,8 +1398,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res4, out4, res5, out5, res4, res5);
     CLIP_SH2_0_255(res4, res5);
     PCKEV_B2_SH(res4, res4, res5, res5, res4, res5);
-    ST8x1_UB(res4, dst + 3 * dst_stride);
-    ST8x1_UB(res5, dst + 12 * dst_stride);
+    ST_D1(res4, 0, dst + 3 * dst_stride);
+    ST_D1(res5, 0, dst + 12 * dst_stride);
 
     VP9_MADD_BF(h1, h3, h5, h7, k0, k1, k2, k0, out12, out14, out13, out15);
     out13 = -out13;
@@ -1398,8 +1410,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res12, out12, res13, out13, res12, res13);
     CLIP_SH2_0_255(res12, res13);
     PCKEV_B2_SH(res12, res12, res13, res13, res12, res13);
-    ST8x1_UB(res12, dst + 2 * dst_stride);
-    ST8x1_UB(res13, dst + 13 * dst_stride);
+    ST_D1(res12, 0, dst + 2 * dst_stride);
+    ST_D1(res13, 0, dst + 13 * dst_stride);
 
     k0 = VP9_SET_COSPI_PAIR(cospi_16_64, cospi_16_64);
     k3 = VP9_SET_COSPI_PAIR(-cospi_16_64, cospi_16_64);
@@ -1411,8 +1423,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res6, out6, res7, out7, res6, res7);
     CLIP_SH2_0_255(res6, res7);
     PCKEV_B2_SH(res6, res6, res7, res7, res6, res7);
-    ST8x1_UB(res6, dst + 4 * dst_stride);
-    ST8x1_UB(res7, dst + 11 * dst_stride);
+    ST_D1(res6, 0, dst + 4 * dst_stride);
+    ST_D1(res7, 0, dst + 11 * dst_stride);
 
     VP9_MADD_SHORT(out10, out11, k0, k3, out10, out11);
     SRARI_H2_SH(out10, out11, 6);
@@ -1422,8 +1434,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res10, out10, res11, out11, res10, res11);
     CLIP_SH2_0_255(res10, res11);
     PCKEV_B2_SH(res10, res10, res11, res11, res10, res11);
-    ST8x1_UB(res10, dst + 6 * dst_stride);
-    ST8x1_UB(res11, dst + 9 * dst_stride);
+    ST_D1(res10, 0, dst + 6 * dst_stride);
+    ST_D1(res11, 0, dst + 9 * dst_stride);
 
     k1 = VP9_SET_COSPI_PAIR(-cospi_16_64, -cospi_16_64);
     k2 = VP9_SET_COSPI_PAIR(cospi_16_64, -cospi_16_64);
@@ -1435,8 +1447,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res2, out2, res3, out3, res2, res3);
     CLIP_SH2_0_255(res2, res3);
     PCKEV_B2_SH(res2, res2, res3, res3, res2, res3);
-    ST8x1_UB(res2, dst + 7 * dst_stride);
-    ST8x1_UB(res3, dst + 8 * dst_stride);
+    ST_D1(res2, 0, dst + 7 * dst_stride);
+    ST_D1(res3, 0, dst + 8 * dst_stride);
 
     VP9_MADD_SHORT(out14, out15, k1, k2, out14, out15);
     SRARI_H2_SH(out14, out15, 6);
@@ -1446,8 +1458,8 @@ static void vp9_iadst16_1d_columns_addblk_msa(int16_t *input, uint8_t *dst,
     ADD2(res14, out14, res15, out15, res14, res15);
     CLIP_SH2_0_255(res14, res15);
     PCKEV_B2_SH(res14, res14, res15, res15, res14, res15);
-    ST8x1_UB(res14, dst + 5 * dst_stride);
-    ST8x1_UB(res15, dst + 10 * dst_stride);
+    ST_D1(res14, 0, dst + 5 * dst_stride);
+    ST_D1(res15, 0, dst + 10 * dst_stride);
 }
 
 static void vp9_iadst16x16_colcol_addblk_msa(int16_t *input, uint8_t *dst,
@@ -1969,8 +1981,7 @@ static void vp9_idct32x32_1_add_msa(int16_t *input, uint8_t *dst,
              res3);
         ADD4(res4, vec, res5, vec, res6, vec, res7, vec, res4, res5, res6,
              res7);
-        CLIP_SH4_0_255(res0, res1, res2, res3);
-        CLIP_SH4_0_255(res4, res5, res6, res7);
+        CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
         PCKEV_B4_UB(res4, res0, res5, res1, res6, res2, res7, res3,
                     tmp0, tmp1, tmp2, tmp3);
 
diff --git a/libavcodec/mips/vp9_intra_msa.c b/libavcodec/mips/vp9_intra_msa.c
index 54cf0ae..97cf212 100644
--- a/libavcodec/mips/vp9_intra_msa.c
+++ b/libavcodec/mips/vp9_intra_msa.c
@@ -378,7 +378,8 @@ void ff_tm_4x4_msa(uint8_t *dst, ptrdiff_t dst_stride,
     IPRED_SUBS_UH2_UH(src_top_left, src_top_left, vec2, vec3);
     SAT_UH4_UH(vec0, vec1, vec2, vec3, 7);
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, tmp0, tmp1);
-    ST4x4_UB(tmp0, tmp1, 0, 2, 0, 2, dst, dst_stride);
+    ST_W2(tmp0, 0, 2, dst, dst_stride);
+    ST_W2(tmp1, 0, 2, dst + 2 * dst_stride, dst_stride);
 }
 
 void ff_tm_8x8_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -409,7 +410,7 @@ void ff_tm_8x8_msa(uint8_t *dst, ptrdiff_t dst_stride,
         IPRED_SUBS_UH2_UH(src_top_left, src_top_left, vec2, vec3);
         SAT_UH4_UH(vec0, vec1, vec2, vec3, 7);
         PCKEV_B2_SB(vec1, vec0, vec3, vec2, tmp0, tmp1);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
diff --git a/libavcodec/mips/vp9_lpf_msa.c b/libavcodec/mips/vp9_lpf_msa.c
index c82a9e9..cbb1409 100644
--- a/libavcodec/mips/vp9_lpf_msa.c
+++ b/libavcodec/mips/vp9_lpf_msa.c
@@ -1219,9 +1219,7 @@ void ff_loop_filter_h_4_8_msa(uint8_t *src, ptrdiff_t pitch,
     ILVRL_H2_SH(vec1, vec0, vec2, vec3);
 
     src -= 2;
-    ST4x4_UB(vec2, vec2, 0, 1, 2, 3, src, pitch);
-    src += 4 * pitch;
-    ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
+    ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
 }
 
 void ff_loop_filter_h_44_16_msa(uint8_t *src, ptrdiff_t pitch,
@@ -1266,9 +1264,8 @@ void ff_loop_filter_h_44_16_msa(uint8_t *src, ptrdiff_t pitch,
 
     src -= 2;
 
-    ST4x8_UB(tmp2, tmp3, src, pitch);
-    src += (8 * pitch);
-    ST4x8_UB(tmp4, tmp5, src, pitch);
+    ST_W8(tmp2, tmp3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
+    ST_W8(tmp4, tmp5, 0, 1, 2, 3, 0, 1, 2, 3, src + 8 * pitch, pitch);
 }
 
 void ff_loop_filter_h_8_8_msa(uint8_t *src, ptrdiff_t pitch,
@@ -1313,9 +1310,7 @@ void ff_loop_filter_h_8_8_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_H2_SH(vec1, vec0, vec2, vec3);
 
         src -= 2;
-        ST4x4_UB(vec2, vec2, 0, 1, 2, 3, src, pitch);
-        src += 4 * pitch;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
     } else {
         ILVR_B8_UH(zero, p3, zero, p2, zero, p1, zero, p0, zero, q0, zero, q1,
                    zero, q2, zero, q3, p3_r, p2_r, p1_r, p0_r, q0_r, q1_r, q2_r,
@@ -1343,11 +1338,11 @@ void ff_loop_filter_h_8_8_msa(uint8_t *src, ptrdiff_t pitch,
         vec4 = (v8i16) __msa_ilvr_b((v16i8) q2, (v16i8) q1);
 
         src -= 3;
-        ST4x4_UB(vec2, vec2, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec4, 0, src + 4, pitch);
+        ST_W4(vec2, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec4, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec4, 4, src + 4, pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec4, 4, 5, 6, 7, src + 4, pitch);
     }
 }
 
@@ -1410,9 +1405,8 @@ void ff_loop_filter_h_88_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_H2_SH(vec1, vec0, vec4, vec5);
 
         src -= 2;
-        ST4x8_UB(vec2, vec3, src, pitch);
-        src += 8 * pitch;
-        ST4x8_UB(vec4, vec5, src, pitch);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
+        ST_W8(vec4, vec5, 0, 1, 2, 3, 0, 1, 2, 3, src + 8 * pitch, pitch);
     } else {
         ILVR_B8_UH(zero, p3, zero, p2, zero, p1, zero, p0, zero, q0, zero, q1,
                    zero, q2, zero, q3, p3_r, p2_r, p1_r, p0_r, q0_r, q1_r, q2_r,
@@ -1451,17 +1445,17 @@ void ff_loop_filter_h_88_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_B2_SH(q2, q1, vec2, vec5);
 
         src -= 3;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 0, src + 4, pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec4, vec4, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 4, src + 4, pitch);
+        ST_W4(vec4, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 4, 5, 6, 7, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec6, vec6, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 0, src + 4, pitch);
+        ST_W4(vec6, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec7, vec7, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 4, src + 4, pitch);
+        ST_W4(vec7, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 4, 5, 6, 7, src + 4, pitch);
     }
 }
 
@@ -1523,9 +1517,8 @@ void ff_loop_filter_h_84_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_H2_SH(vec1, vec0, vec4, vec5);
 
         src -= 2;
-        ST4x8_UB(vec2, vec3, src, pitch);
-        src += 8 * pitch;
-        ST4x8_UB(vec4, vec5, src, pitch);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
+        ST_W8(vec4, vec5, 0, 1, 2, 3, 0, 1, 2, 3, src + 8 * pitch, pitch);
     } else {
         ILVR_B8_UH(zero, p3, zero, p2, zero, p1, zero, p0, zero, q0, zero, q1,
                    zero, q2, zero, q3, p3_r, p2_r, p1_r, p0_r, q0_r, q1_r, q2_r,
@@ -1555,17 +1548,17 @@ void ff_loop_filter_h_84_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_B2_SH(q2, q1, vec2, vec5);
 
         src -= 3;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 0, src + 4, pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec4, vec4, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 4, src + 4, pitch);
+        ST_W4(vec4, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 4, 5, 6, 7, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec6, vec6, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 0, src + 4, pitch);
+        ST_W4(vec6, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec7, vec7, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 4, src + 4, pitch);
+        ST_W4(vec7, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 4, 5, 6, 7, src + 4, pitch);
     }
 }
 
@@ -1627,9 +1620,8 @@ void ff_loop_filter_h_48_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_H2_SH(vec1, vec0, vec4, vec5);
 
         src -= 2;
-        ST4x8_UB(vec2, vec3, src, pitch);
-        src += 8 * pitch;
-        ST4x8_UB(vec4, vec5, src, pitch);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src, pitch);
+        ST_W8(vec4, vec5, 0, 1, 2, 3, 0, 1, 2, 3, src + 8 * pitch, pitch);
     } else {
         ILVL_B4_UH(zero, p3, zero, p2, zero, p1, zero, p0, p3_l, p2_l, p1_l,
                    p0_l);
@@ -1661,17 +1653,17 @@ void ff_loop_filter_h_48_16_msa(uint8_t *src, ptrdiff_t pitch,
         ILVRL_B2_SH(q2, q1, vec2, vec5);
 
         src -= 3;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 0, src + 4, pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec4, vec4, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec2, 4, src + 4, pitch);
+        ST_W4(vec4, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec2, 4, 5, 6, 7, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec6, vec6, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 0, src + 4, pitch);
+        ST_W4(vec6, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 0, 1, 2, 3, src + 4, pitch);
         src += (4 * pitch);
-        ST4x4_UB(vec7, vec7, 0, 1, 2, 3, src, pitch);
-        ST2x4_UB(vec5, 4, src + 4, pitch);
+        ST_W4(vec7, 0, 1, 2, 3, src, pitch);
+        ST_H4(vec5, 4, 5, 6, 7, src + 4, pitch);
     }
 }
 
@@ -1681,6 +1673,7 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     v16u8 p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org;
     v16i8 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v16u8 p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    v16i8 zeros = { 0 };
 
     LD_UB8(input, in_pitch,
            p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org);
@@ -1694,7 +1687,7 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     ILVL_B2_SB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp7);
     ILVR_W2_UB(tmp6, tmp4, tmp7, tmp5, q0, q4);
     ILVL_W2_UB(tmp6, tmp4, tmp7, tmp5, q2, q6);
-    SLDI_B4_0_UB(q0, q2, q4, q6, q1, q3, q5, q7, 8);
+    SLDI_B4_UB(zeros, q0, zeros, q2, zeros, q4, zeros, q6, 8, q1, q3, q5, q7);
 
     ST_UB8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_pitch);
     output += (8 * out_pitch);
@@ -1811,7 +1804,7 @@ static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
     if (__msa_test_bz_v(flat)) {
         ILVR_B2_SH(p0_out, p1_out, q1_out, q0_out, vec0, vec1);
         ILVRL_H2_SH(vec1, vec0, vec2, vec3);
-        ST4x8_UB(vec2, vec3, (src_org - 2), pitch_org);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, (src_org - 2), pitch_org);
         return 1;
     } else {
         ILVR_B8_UH(zero, p3, zero, p2, zero, p1, zero, p0, zero, q0, zero, q1,
@@ -1878,11 +1871,11 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         vec2 = (v8i16) __msa_ilvr_b((v16i8) q2, (v16i8) q1);
 
         src_org -= 3;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec2, 0, (src_org + 4), pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec2, 0, 1, 2, 3, (src_org + 4), pitch);
         src_org += (4 * pitch);
-        ST4x4_UB(vec4, vec4, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec2, 4, (src_org + 4), pitch);
+        ST_W4(vec4, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec2, 4, 5, 6, 7, (src_org + 4), pitch);
 
         return 1;
     } else {
@@ -1908,7 +1901,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         p6 = __msa_bmnz_v(p6, (v16u8) r_out, flat2);
-        ST8x1_UB(p6, src);
+        ST_D1(p6, 0, src);
         src += 16;
 
         /* p5 */
@@ -1920,7 +1913,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         p5 = __msa_bmnz_v(p5, (v16u8) r_out, flat2);
-        ST8x1_UB(p5, src);
+        ST_D1(p5, 0, src);
         src += 16;
 
         /* p4 */
@@ -1932,7 +1925,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         p4 = __msa_bmnz_v(p4, (v16u8) r_out, flat2);
-        ST8x1_UB(p4, src);
+        ST_D1(p4, 0, src);
         src += 16;
 
         /* p3 */
@@ -1944,7 +1937,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         p3 = __msa_bmnz_v(p3, (v16u8) r_out, flat2);
-        ST8x1_UB(p3, src);
+        ST_D1(p3, 0, src);
         src += 16;
 
         /* p2 */
@@ -1957,7 +1950,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* p1 */
@@ -1970,7 +1963,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* p0 */
@@ -1983,7 +1976,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* q0 */
@@ -1996,7 +1989,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* q1 */
@@ -2008,7 +2001,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* q2 */
@@ -2020,7 +2013,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         filter8 = __msa_bmnz_v(filter8, (v16u8) r_out, flat2);
-        ST8x1_UB(filter8, src);
+        ST_D1(filter8, 0, src);
         src += 16;
 
         /* q3 */
@@ -2031,7 +2024,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         q3 = __msa_bmnz_v(q3, (v16u8) r_out, flat2);
-        ST8x1_UB(q3, src);
+        ST_D1(q3, 0, src);
         src += 16;
 
         /* q4 */
@@ -2042,7 +2035,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         q4 = __msa_bmnz_v(q4, (v16u8) r_out, flat2);
-        ST8x1_UB(q4, src);
+        ST_D1(q4, 0, src);
         src += 16;
 
         /* q5 */
@@ -2053,7 +2046,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         q5 = __msa_bmnz_v(q5, (v16u8) r_out, flat2);
-        ST8x1_UB(q5, src);
+        ST_D1(q5, 0, src);
         src += 16;
 
         /* q6 */
@@ -2064,7 +2057,7 @@ static int32_t vp9_vt_lpf_t16_8w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitch
         r_out = __msa_srari_h((v8i16) tmp1_r, 4);
         r_out = (v8i16) __msa_pckev_b((v16i8) r_out, (v16i8) r_out);
         q6 = __msa_bmnz_v(q6, (v16u8) r_out, flat2);
-        ST8x1_UB(q6, src);
+        ST_D1(q6, 0, src);
 
         return 0;
     }
@@ -2137,9 +2130,8 @@ static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *src, uint8_t *filter48,
         ILVRL_H2_SH(vec1, vec0, vec4, vec5);
 
         src_org -= 2;
-        ST4x8_UB(vec2, vec3, src_org, pitch);
-        src_org += 8 * pitch;
-        ST4x8_UB(vec4, vec5, src_org, pitch);
+        ST_W8(vec2, vec3, 0, 1, 2, 3, 0, 1, 2, 3, src_org, pitch);
+        ST_W8(vec4, vec5, 0, 1, 2, 3, 0, 1, 2, 3, src_org + 8 * pitch, pitch);
 
         return 1;
     } else {
@@ -2218,17 +2210,17 @@ static int32_t vp9_vt_lpf_t16_16w(uint8_t *src, uint8_t *src_org, ptrdiff_t pitc
         ILVRL_B2_SH(q2, q1, vec2, vec5);
 
         src_org -= 3;
-        ST4x4_UB(vec3, vec3, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec2, 0, (src_org + 4), pitch);
+        ST_W4(vec3, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec2, 0, 1, 2, 3, (src_org + 4), pitch);
         src_org += (4 * pitch);
-        ST4x4_UB(vec4, vec4, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec2, 4, (src_org + 4), pitch);
+        ST_W4(vec4, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec2, 4, 5, 6, 7, (src_org + 4), pitch);
         src_org += (4 * pitch);
-        ST4x4_UB(vec6, vec6, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec5, 0, (src_org + 4), pitch);
+        ST_W4(vec6, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec5, 0, 1, 2, 3, (src_org + 4), pitch);
         src_org += (4 * pitch);
-        ST4x4_UB(vec7, vec7, 0, 1, 2, 3, src_org, pitch);
-        ST2x4_UB(vec5, 4, (src_org + 4), pitch);
+        ST_W4(vec7, 0, 1, 2, 3, src_org, pitch);
+        ST_H4(vec5, 4, 5, 6, 7, (src_org + 4), pitch);
 
         return 1;
     } else {
diff --git a/libavcodec/mips/vp9_mc_mmi.c b/libavcodec/mips/vp9_mc_mmi.c
new file mode 100644
index 0000000..e7a8387
--- /dev/null
+++ b/libavcodec/mips/vp9_mc_mmi.c
@@ -0,0 +1,628 @@
+/*
+ * Copyright (c) 2019 gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp9dsp.h"
+#include "libavutil/mips/mmiutils.h"
+#include "vp9dsp_mips.h"
+
+#define GET_DATA_H_MMI                                       \
+    "pmaddhw    %[ftmp4],    %[ftmp4],   %[filter1]    \n\t" \
+    "pmaddhw    %[ftmp5],    %[ftmp5],   %[filter2]    \n\t" \
+    "paddw      %[ftmp4],    %[ftmp4],   %[ftmp5]      \n\t" \
+    "punpckhwd  %[ftmp5],    %[ftmp4],   %[ftmp0]      \n\t" \
+    "paddw      %[ftmp4],    %[ftmp4],   %[ftmp5]      \n\t" \
+    "pmaddhw    %[ftmp6],    %[ftmp6],   %[filter1]    \n\t" \
+    "pmaddhw    %[ftmp7],    %[ftmp7],   %[filter2]    \n\t" \
+    "paddw      %[ftmp6],    %[ftmp6],   %[ftmp7]      \n\t" \
+    "punpckhwd  %[ftmp7],    %[ftmp6],   %[ftmp0]      \n\t" \
+    "paddw      %[ftmp6],    %[ftmp6],   %[ftmp7]      \n\t" \
+    "punpcklwd  %[srcl],     %[ftmp4],   %[ftmp6]      \n\t" \
+    "pmaddhw    %[ftmp8],    %[ftmp8],   %[filter1]    \n\t" \
+    "pmaddhw    %[ftmp9],    %[ftmp9],   %[filter2]    \n\t" \
+    "paddw      %[ftmp8],    %[ftmp8],   %[ftmp9]      \n\t" \
+    "punpckhwd  %[ftmp9],    %[ftmp8],   %[ftmp0]      \n\t" \
+    "paddw      %[ftmp8],    %[ftmp8],   %[ftmp9]      \n\t" \
+    "pmaddhw    %[ftmp10],   %[ftmp10],  %[filter1]    \n\t" \
+    "pmaddhw    %[ftmp11],   %[ftmp11],  %[filter2]    \n\t" \
+    "paddw      %[ftmp10],   %[ftmp10],  %[ftmp11]     \n\t" \
+    "punpckhwd  %[ftmp11],   %[ftmp10],  %[ftmp0]      \n\t" \
+    "paddw      %[ftmp10],   %[ftmp10],  %[ftmp11]     \n\t" \
+    "punpcklwd  %[srch],     %[ftmp8],   %[ftmp10]     \n\t"
+
+#define GET_DATA_V_MMI                                       \
+    "punpcklhw  %[srcl],     %[ftmp4],   %[ftmp5]      \n\t" \
+    "pmaddhw    %[srcl],     %[srcl],    %[filter10]   \n\t" \
+    "punpcklhw  %[ftmp12],   %[ftmp6],   %[ftmp7]      \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter32]   \n\t" \
+    "paddw      %[srcl],     %[srcl],    %[ftmp12]     \n\t" \
+    "punpcklhw  %[ftmp12],   %[ftmp8],   %[ftmp9]      \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter54]   \n\t" \
+    "paddw      %[srcl],     %[srcl],    %[ftmp12]     \n\t" \
+    "punpcklhw  %[ftmp12],   %[ftmp10],  %[ftmp11]     \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter76]   \n\t" \
+    "paddw      %[srcl],     %[srcl],    %[ftmp12]     \n\t" \
+    "punpckhhw  %[srch],     %[ftmp4],   %[ftmp5]      \n\t" \
+    "pmaddhw    %[srch],     %[srch],    %[filter10]   \n\t" \
+    "punpckhhw  %[ftmp12],   %[ftmp6],   %[ftmp7]      \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter32]   \n\t" \
+    "paddw      %[srch],     %[srch],    %[ftmp12]     \n\t" \
+    "punpckhhw  %[ftmp12],   %[ftmp8],   %[ftmp9]      \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter54]   \n\t" \
+    "paddw      %[srch],     %[srch],    %[ftmp12]     \n\t" \
+    "punpckhhw  %[ftmp12],   %[ftmp10],  %[ftmp11]     \n\t" \
+    "pmaddhw    %[ftmp12],   %[ftmp12],  %[filter76]   \n\t" \
+    "paddw      %[srch],     %[srch],    %[ftmp12]     \n\t"
+
+static void convolve_horiz_mmi(const uint8_t *src, int32_t src_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const uint16_t *filter_x, int32_t w,
+                               int32_t h)
+{
+    double ftmp[15];
+    uint32_t tmp[2];
+    src -= 3;
+    src_stride -= w;
+    dst_stride -= w;
+    __asm__ volatile (
+        "move       %[tmp1],    %[width]                   \n\t"
+        "xor        %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
+        "gsldlc1    %[filter1], 0x03(%[filter])            \n\t"
+        "gsldrc1    %[filter1], 0x00(%[filter])            \n\t"
+        "gsldlc1    %[filter2], 0x0b(%[filter])            \n\t"
+        "gsldrc1    %[filter2], 0x08(%[filter])            \n\t"
+        "li         %[tmp0],    0x07                       \n\t"
+        "dmtc1      %[tmp0],    %[ftmp13]                  \n\t"
+        "punpcklwd  %[ftmp13],  %[ftmp13],   %[ftmp13]     \n\t"
+        "1:                                                \n\t"
+        /* Get 8 data per row */
+        "gsldlc1    %[ftmp5],   0x07(%[src])               \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src])               \n\t"
+        "gsldlc1    %[ftmp7],   0x08(%[src])               \n\t"
+        "gsldrc1    %[ftmp7],   0x01(%[src])               \n\t"
+        "gsldlc1    %[ftmp9],   0x09(%[src])               \n\t"
+        "gsldrc1    %[ftmp9],   0x02(%[src])               \n\t"
+        "gsldlc1    %[ftmp11],  0x0A(%[src])               \n\t"
+        "gsldrc1    %[ftmp11],  0x03(%[src])               \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp5],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp5],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp7],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp7],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp8],   %[ftmp9],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp9],   %[ftmp9],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp10],  %[ftmp11],   %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp11],  %[ftmp11],   %[ftmp0]      \n\t"
+        PTR_ADDIU  "%[width],   %[width],    -0x04         \n\t"
+        /* Get raw data */
+        GET_DATA_H_MMI
+        ROUND_POWER_OF_TWO_MMI(%[srcl], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        ROUND_POWER_OF_TWO_MMI(%[srch], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        "packsswh   %[srcl],    %[srcl],     %[srch]       \n\t"
+        "packushb   %[ftmp12],  %[srcl],     %[ftmp0]      \n\t"
+        "swc1       %[ftmp12],  0x00(%[dst])               \n\t"
+        PTR_ADDIU  "%[dst],     %[dst],      0x04          \n\t"
+        PTR_ADDIU  "%[src],     %[src],      0x04          \n\t"
+        /* Loop count */
+        "bnez       %[width],   1b                         \n\t"
+        "move       %[width],   %[tmp1]                    \n\t"
+        PTR_ADDU   "%[src],     %[src],      %[src_stride] \n\t"
+        PTR_ADDU   "%[dst],     %[dst],      %[dst_stride] \n\t"
+        PTR_ADDIU  "%[height],  %[height],   -0x01         \n\t"
+        "bnez       %[height],  1b                         \n\t"
+        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+          [filter1]"=&f"(ftmp[2]),  [filter2]"=&f"(ftmp[3]),
+          [ftmp0]"=&f"(ftmp[4]),    [ftmp4]"=&f"(ftmp[5]),
+          [ftmp5]"=&f"(ftmp[6]),    [ftmp6]"=&f"(ftmp[7]),
+          [ftmp7]"=&f"(ftmp[8]),    [ftmp8]"=&f"(ftmp[9]),
+          [ftmp9]"=&f"(ftmp[10]),   [ftmp10]"=&f"(ftmp[11]),
+          [ftmp11]"=&f"(ftmp[12]),  [ftmp12]"=&f"(ftmp[13]),
+          [tmp0]"=&r"(tmp[0]),      [tmp1]"=&r"(tmp[1]),
+          [src]"+&r"(src),          [width]"+&r"(w),
+          [dst]"+&r"(dst),          [height]"+&r"(h),
+          [ftmp13]"=&f"(ftmp[14])
+        : [filter]"r"(filter_x),
+          [src_stride]"r"((mips_reg)src_stride),
+          [dst_stride]"r"((mips_reg)dst_stride)
+        : "memory"
+    );
+}
+
+static void convolve_vert_mmi(const uint8_t *src, int32_t src_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int16_t *filter_y, int32_t w,
+                              int32_t h)
+{
+    double ftmp[17];
+    uint32_t tmp[1];
+    ptrdiff_t addr = src_stride;
+    src_stride -= w;
+    dst_stride -= w;
+
+    __asm__ volatile (
+        "xor        %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
+        "gsldlc1    %[ftmp4],    0x03(%[filter])           \n\t"
+        "gsldrc1    %[ftmp4],    0x00(%[filter])           \n\t"
+        "gsldlc1    %[ftmp5],    0x0b(%[filter])           \n\t"
+        "gsldrc1    %[ftmp5],    0x08(%[filter])           \n\t"
+        "punpcklwd  %[filter10], %[ftmp4],   %[ftmp4]      \n\t"
+        "punpckhwd  %[filter32], %[ftmp4],   %[ftmp4]      \n\t"
+        "punpcklwd  %[filter54], %[ftmp5],   %[ftmp5]      \n\t"
+        "punpckhwd  %[filter76], %[ftmp5],   %[ftmp5]      \n\t"
+        "li         %[tmp0],     0x07                      \n\t"
+        "dmtc1      %[tmp0],     %[ftmp13]                 \n\t"
+        "punpcklwd  %[ftmp13],   %[ftmp13],  %[ftmp13]     \n\t"
+        "1:                                                \n\t"
+        /* Get 8 data per column */
+        "gsldlc1    %[ftmp4],    0x07(%[src])              \n\t"
+        "gsldrc1    %[ftmp4],    0x00(%[src])              \n\t"
+        PTR_ADDU   "%[tmp0],     %[src],     %[addr]       \n\t"
+        "gsldlc1    %[ftmp5],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp5],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp6],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp6],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp7],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp7],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp8],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp8],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp9],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp9],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp10],   0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp10],   0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp11],   0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp11],   0x00(%[tmp0])             \n\t"
+        "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp5],    %[ftmp5],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp6],    %[ftmp6],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp7],    %[ftmp7],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp8],    %[ftmp8],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp9],    %[ftmp9],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp10],   %[ftmp10],  %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp11],   %[ftmp11],  %[ftmp0]      \n\t"
+        PTR_ADDIU  "%[width],    %[width],   -0x04         \n\t"
+        /* Get raw data */
+        GET_DATA_V_MMI
+        ROUND_POWER_OF_TWO_MMI(%[srcl], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        ROUND_POWER_OF_TWO_MMI(%[srch], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        "packsswh   %[srcl],     %[srcl],    %[srch]       \n\t"
+        "packushb   %[ftmp12],   %[srcl],    %[ftmp0]      \n\t"
+        "swc1       %[ftmp12],   0x00(%[dst])              \n\t"
+        PTR_ADDIU  "%[dst],      %[dst],      0x04         \n\t"
+        PTR_ADDIU  "%[src],      %[src],      0x04         \n\t"
+        /* Loop count */
+        "bnez       %[width],    1b                        \n\t"
+        PTR_SUBU   "%[width],    %[addr],    %[src_stride] \n\t"
+        PTR_ADDU   "%[src],      %[src],     %[src_stride] \n\t"
+        PTR_ADDU   "%[dst],      %[dst],     %[dst_stride] \n\t"
+        PTR_ADDIU  "%[height],   %[height],  -0x01         \n\t"
+        "bnez       %[height],   1b                        \n\t"
+        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+          [filter10]"=&f"(ftmp[2]), [filter32]"=&f"(ftmp[3]),
+          [filter54]"=&f"(ftmp[4]), [filter76]"=&f"(ftmp[5]),
+          [ftmp0]"=&f"(ftmp[6]),    [ftmp4]"=&f"(ftmp[7]),
+          [ftmp5]"=&f"(ftmp[8]),    [ftmp6]"=&f"(ftmp[9]),
+          [ftmp7]"=&f"(ftmp[10]),   [ftmp8]"=&f"(ftmp[11]),
+          [ftmp9]"=&f"(ftmp[12]),   [ftmp10]"=&f"(ftmp[13]),
+          [ftmp11]"=&f"(ftmp[14]),  [ftmp12]"=&f"(ftmp[15]),
+          [src]"+&r"(src),          [dst]"+&r"(dst),
+          [width]"+&r"(w),          [height]"+&r"(h),
+          [tmp0]"=&r"(tmp[0]),      [ftmp13]"=&f"(ftmp[16])
+        : [filter]"r"(filter_y),
+          [src_stride]"r"((mips_reg)src_stride),
+          [dst_stride]"r"((mips_reg)dst_stride),
+          [addr]"r"((mips_reg)addr)
+        : "memory"
+    );
+}
+
+static void convolve_avg_horiz_mmi(const uint8_t *src, int32_t src_stride,
+                                   uint8_t *dst, int32_t dst_stride,
+                                   const uint16_t *filter_x, int32_t w,
+                                   int32_t h)
+{
+    double ftmp[15];
+    uint32_t tmp[2];
+    src -= 3;
+    src_stride -= w;
+    dst_stride -= w;
+
+    __asm__ volatile (
+        "move       %[tmp1],    %[width]                   \n\t"
+        "xor        %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
+        "gsldlc1    %[filter1], 0x03(%[filter])            \n\t"
+        "gsldrc1    %[filter1], 0x00(%[filter])            \n\t"
+        "gsldlc1    %[filter2], 0x0b(%[filter])            \n\t"
+        "gsldrc1    %[filter2], 0x08(%[filter])            \n\t"
+        "li         %[tmp0],    0x07                       \n\t"
+        "dmtc1      %[tmp0],    %[ftmp13]                  \n\t"
+        "punpcklwd  %[ftmp13],  %[ftmp13],   %[ftmp13]     \n\t"
+        "1:                                                \n\t"
+        /* Get 8 data per row */
+        "gsldlc1    %[ftmp5],   0x07(%[src])               \n\t"
+        "gsldrc1    %[ftmp5],   0x00(%[src])               \n\t"
+        "gsldlc1    %[ftmp7],   0x08(%[src])               \n\t"
+        "gsldrc1    %[ftmp7],   0x01(%[src])               \n\t"
+        "gsldlc1    %[ftmp9],   0x09(%[src])               \n\t"
+        "gsldrc1    %[ftmp9],   0x02(%[src])               \n\t"
+        "gsldlc1    %[ftmp11],  0x0A(%[src])               \n\t"
+        "gsldrc1    %[ftmp11],  0x03(%[src])               \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp5],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp5],   %[ftmp5],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp6],   %[ftmp7],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp7],   %[ftmp7],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp8],   %[ftmp9],    %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp9],   %[ftmp9],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp10],  %[ftmp11],   %[ftmp0]      \n\t"
+        "punpckhbh  %[ftmp11],  %[ftmp11],   %[ftmp0]      \n\t"
+        PTR_ADDIU  "%[width],   %[width],    -0x04         \n\t"
+        /* Get raw data */
+        GET_DATA_H_MMI
+        ROUND_POWER_OF_TWO_MMI(%[srcl], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        ROUND_POWER_OF_TWO_MMI(%[srch], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        "packsswh   %[srcl],    %[srcl],     %[srch]       \n\t"
+        "packushb   %[ftmp12],  %[srcl],     %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp12],  %[ftmp12],   %[ftmp0]      \n\t"
+        "gsldlc1    %[ftmp4],   0x07(%[dst])               \n\t"
+        "gsldrc1    %[ftmp4],   0x00(%[dst])               \n\t"
+        "punpcklbh  %[ftmp4],   %[ftmp4],    %[ftmp0]      \n\t"
+        "paddh      %[ftmp12],  %[ftmp12],   %[ftmp4]      \n\t"
+        "li         %[tmp0],    0x10001                    \n\t"
+        "dmtc1      %[tmp0],    %[ftmp5]                   \n\t"
+        "punpcklhw  %[ftmp5],   %[ftmp5],    %[ftmp5]      \n\t"
+        "paddh      %[ftmp12],  %[ftmp12],   %[ftmp5]      \n\t"
+        "psrah      %[ftmp12],  %[ftmp12],   %[ftmp5]      \n\t"
+        "packushb   %[ftmp12],  %[ftmp12],   %[ftmp0]      \n\t"
+        "swc1       %[ftmp12],  0x00(%[dst])               \n\t"
+        PTR_ADDIU  "%[dst],     %[dst],      0x04          \n\t"
+        PTR_ADDIU  "%[src],     %[src],      0x04          \n\t"
+        /* Loop count */
+        "bnez       %[width],   1b                         \n\t"
+        "move       %[width],   %[tmp1]                    \n\t"
+        PTR_ADDU   "%[src],     %[src],      %[src_stride] \n\t"
+        PTR_ADDU   "%[dst],     %[dst],      %[dst_stride] \n\t"
+        PTR_ADDIU  "%[height],  %[height],   -0x01         \n\t"
+        "bnez       %[height],  1b                         \n\t"
+        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+          [filter1]"=&f"(ftmp[2]),  [filter2]"=&f"(ftmp[3]),
+          [ftmp0]"=&f"(ftmp[4]),    [ftmp4]"=&f"(ftmp[5]),
+          [ftmp5]"=&f"(ftmp[6]),    [ftmp6]"=&f"(ftmp[7]),
+          [ftmp7]"=&f"(ftmp[8]),    [ftmp8]"=&f"(ftmp[9]),
+          [ftmp9]"=&f"(ftmp[10]),   [ftmp10]"=&f"(ftmp[11]),
+          [ftmp11]"=&f"(ftmp[12]),  [ftmp12]"=&f"(ftmp[13]),
+          [tmp0]"=&r"(tmp[0]),      [tmp1]"=&r"(tmp[1]),
+          [src]"+&r"(src),          [width]"+&r"(w),
+          [dst]"+&r"(dst),          [height]"+&r"(h),
+          [ftmp13]"=&f"(ftmp[14])
+        : [filter]"r"(filter_x),
+          [src_stride]"r"((mips_reg)src_stride),
+          [dst_stride]"r"((mips_reg)dst_stride)
+        : "memory"
+    );
+}
+
+static void convolve_avg_vert_mmi(const uint8_t *src, int32_t src_stride,
+                                  uint8_t *dst, int32_t dst_stride,
+                                  const int16_t *filter_y, int32_t w,
+                                  int32_t h)
+{
+    double ftmp[17];
+    uint32_t tmp[1];
+    ptrdiff_t addr = src_stride;
+    src_stride -= w;
+    dst_stride -= w;
+
+    __asm__ volatile (
+        "xor        %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
+        "gsldlc1    %[ftmp4],    0x03(%[filter])           \n\t"
+        "gsldrc1    %[ftmp4],    0x00(%[filter])           \n\t"
+        "gsldlc1    %[ftmp5],    0x0b(%[filter])           \n\t"
+        "gsldrc1    %[ftmp5],    0x08(%[filter])           \n\t"
+        "punpcklwd  %[filter10], %[ftmp4],   %[ftmp4]      \n\t"
+        "punpckhwd  %[filter32], %[ftmp4],   %[ftmp4]      \n\t"
+        "punpcklwd  %[filter54], %[ftmp5],   %[ftmp5]      \n\t"
+        "punpckhwd  %[filter76], %[ftmp5],   %[ftmp5]      \n\t"
+        "li         %[tmp0],     0x07                      \n\t"
+        "dmtc1      %[tmp0],     %[ftmp13]                 \n\t"
+        "punpcklwd  %[ftmp13],   %[ftmp13],  %[ftmp13]     \n\t"
+        "1:                                                \n\t"
+        /* Get 8 data per column */
+        "gsldlc1    %[ftmp4],    0x07(%[src])              \n\t"
+        "gsldrc1    %[ftmp4],    0x00(%[src])              \n\t"
+        PTR_ADDU   "%[tmp0],     %[src],     %[addr]       \n\t"
+        "gsldlc1    %[ftmp5],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp5],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp6],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp6],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp7],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp7],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp8],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp8],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp9],    0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp9],    0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp10],   0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp10],   0x00(%[tmp0])             \n\t"
+        PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
+        "gsldlc1    %[ftmp11],   0x07(%[tmp0])             \n\t"
+        "gsldrc1    %[ftmp11],   0x00(%[tmp0])             \n\t"
+        "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp5],    %[ftmp5],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp6],    %[ftmp6],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp7],    %[ftmp7],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp8],    %[ftmp8],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp9],    %[ftmp9],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp10],   %[ftmp10],  %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp11],   %[ftmp11],  %[ftmp0]      \n\t"
+        PTR_ADDIU  "%[width],    %[width],   -0x04         \n\t"
+        /* Get raw data */
+        GET_DATA_V_MMI
+        ROUND_POWER_OF_TWO_MMI(%[srcl], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        ROUND_POWER_OF_TWO_MMI(%[srch], %[ftmp13], %[ftmp5],
+                               %[ftmp6], %[tmp0])
+        "packsswh   %[srcl],     %[srcl],    %[srch]       \n\t"
+        "packushb   %[ftmp12],   %[srcl],    %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp12],   %[ftmp12],  %[ftmp0]      \n\t"
+        "gsldlc1    %[ftmp4],    0x07(%[dst])              \n\t"
+        "gsldrc1    %[ftmp4],    0x00(%[dst])              \n\t"
+        "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
+        "paddh      %[ftmp12],   %[ftmp12],  %[ftmp4]      \n\t"
+        "li         %[tmp0],     0x10001                   \n\t"
+        "dmtc1      %[tmp0],     %[ftmp5]                  \n\t"
+        "punpcklhw  %[ftmp5],    %[ftmp5],   %[ftmp5]      \n\t"
+        "paddh      %[ftmp12],   %[ftmp12],  %[ftmp5]      \n\t"
+        "psrah      %[ftmp12],   %[ftmp12],  %[ftmp5]      \n\t"
+        "packushb   %[ftmp12],   %[ftmp12],  %[ftmp0]      \n\t"
+        "swc1       %[ftmp12],   0x00(%[dst])              \n\t"
+        PTR_ADDIU  "%[dst],      %[dst],     0x04          \n\t"
+        PTR_ADDIU  "%[src],      %[src],     0x04          \n\t"
+        /* Loop count */
+        "bnez       %[width],    1b                        \n\t"
+        PTR_SUBU   "%[width],    %[addr],    %[src_stride] \n\t"
+        PTR_ADDU   "%[src],      %[src],     %[src_stride] \n\t"
+        PTR_ADDU   "%[dst],      %[dst],     %[dst_stride] \n\t"
+        PTR_ADDIU  "%[height],   %[height],  -0x01         \n\t"
+        "bnez       %[height],   1b                        \n\t"
+        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+          [filter10]"=&f"(ftmp[2]), [filter32]"=&f"(ftmp[3]),
+          [filter54]"=&f"(ftmp[4]), [filter76]"=&f"(ftmp[5]),
+          [ftmp0]"=&f"(ftmp[6]),    [ftmp4]"=&f"(ftmp[7]),
+          [ftmp5]"=&f"(ftmp[8]),    [ftmp6]"=&f"(ftmp[9]),
+          [ftmp7]"=&f"(ftmp[10]),   [ftmp8]"=&f"(ftmp[11]),
+          [ftmp9]"=&f"(ftmp[12]),   [ftmp10]"=&f"(ftmp[13]),
+          [ftmp11]"=&f"(ftmp[14]),  [ftmp12]"=&f"(ftmp[15]),
+          [src]"+&r"(src),          [dst]"+&r"(dst),
+          [width]"+&r"(w),          [height]"+&r"(h),
+          [tmp0]"=&r"(tmp[0]),      [ftmp13]"=&f"(ftmp[16])
+        : [filter]"r"(filter_y),
+          [src_stride]"r"((mips_reg)src_stride),
+          [dst_stride]"r"((mips_reg)dst_stride),
+          [addr]"r"((mips_reg)addr)
+        : "memory"
+    );
+}
+
+static void convolve_avg_mmi(const uint8_t *src, int32_t src_stride,
+                             uint8_t *dst, int32_t dst_stride,
+                             int32_t w, int32_t h)
+{
+    double ftmp[4];
+    uint32_t tmp[2];
+    src_stride -= w;
+    dst_stride -= w;
+
+    __asm__ volatile (
+        "move       %[tmp1],    %[width]                  \n\t"
+        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]      \n\t"
+        "li         %[tmp0],    0x10001                   \n\t"
+        "dmtc1      %[tmp0],    %[ftmp3]                  \n\t"
+        "punpcklhw  %[ftmp3],   %[ftmp3],   %[ftmp3]      \n\t"
+        "1:                                               \n\t"
+        "gslwlc1    %[ftmp1],   0x07(%[src])              \n\t"
+        "gslwrc1    %[ftmp1],   0x00(%[src])              \n\t"
+        "gslwlc1    %[ftmp2],   0x07(%[dst])              \n\t"
+        "gslwrc1    %[ftmp2],   0x00(%[dst])              \n\t"
+        "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]      \n\t"
+        "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]      \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],   %[ftmp2]      \n\t"
+        "paddh      %[ftmp1],   %[ftmp1],   %[ftmp3]      \n\t"
+        "psrah      %[ftmp1],   %[ftmp1],   %[ftmp3]      \n\t"
+        "packushb   %[ftmp1],   %[ftmp1],   %[ftmp0]      \n\t"
+        "swc1       %[ftmp1],   0x00(%[dst])              \n\t"
+        PTR_ADDIU  "%[width],   %[width],   -0x04         \n\t"
+        PTR_ADDIU  "%[dst],     %[dst],     0x04          \n\t"
+        PTR_ADDIU  "%[src],     %[src],     0x04          \n\t"
+        "bnez       %[width],   1b                        \n\t"
+        "move       %[width],   %[tmp1]                   \n\t"
+        PTR_ADDU   "%[dst],     %[dst],     %[dst_stride] \n\t"
+        PTR_ADDU   "%[src],     %[src],     %[src_stride] \n\t"
+        PTR_ADDIU  "%[height],  %[height],  -0x01         \n\t"
+        "bnez       %[height],  1b                        \n\t"
+        : [ftmp0]"=&f"(ftmp[0]),  [ftmp1]"=&f"(ftmp[1]),
+          [ftmp2]"=&f"(ftmp[2]),  [ftmp3]"=&f"(ftmp[3]),
+          [tmp0]"=&r"(tmp[0]),    [tmp1]"=&r"(tmp[1]),
+          [src]"+&r"(src),        [dst]"+&r"(dst),
+          [width]"+&r"(w),        [height]"+&r"(h)
+        : [src_stride]"r"((mips_reg)src_stride),
+          [dst_stride]"r"((mips_reg)dst_stride)
+        : "memory"
+    );
+}
+
+static const int16_t vp9_subpel_filters_mmi[3][15][8] = {
+    [FILTER_8TAP_REGULAR] = {
+         {0, 1, -5, 126, 8, -3, 1, 0},
+         {-1, 3, -10, 122, 18, -6, 2, 0},
+         {-1, 4, -13, 118, 27, -9, 3, -1},
+         {-1, 4, -16, 112, 37, -11, 4, -1},
+         {-1, 5, -18, 105, 48, -14, 4, -1},
+         {-1, 5, -19, 97, 58, -16, 5, -1},
+         {-1, 6, -19, 88, 68, -18, 5, -1},
+         {-1, 6, -19, 78, 78, -19, 6, -1},
+         {-1, 5, -18, 68, 88, -19, 6, -1},
+         {-1, 5, -16, 58, 97, -19, 5, -1},
+         {-1, 4, -14, 48, 105, -18, 5, -1},
+         {-1, 4, -11, 37, 112, -16, 4, -1},
+         {-1, 3, -9, 27, 118, -13, 4, -1},
+         {0, 2, -6, 18, 122, -10, 3, -1},
+         {0, 1, -3, 8, 126, -5, 1, 0},
+    }, [FILTER_8TAP_SHARP] = {
+        {-1, 3, -7, 127, 8, -3, 1, 0},
+        {-2, 5, -13, 125, 17, -6, 3, -1},
+        {-3, 7, -17, 121, 27, -10, 5, -2},
+        {-4, 9, -20, 115, 37, -13, 6, -2},
+        {-4, 10, -23, 108, 48, -16, 8, -3},
+        {-4, 10, -24, 100, 59, -19, 9, -3},
+        {-4, 11, -24, 90, 70, -21, 10, -4},
+        {-4, 11, -23, 80, 80, -23, 11, -4},
+        {-4, 10, -21, 70, 90, -24, 11, -4},
+        {-3, 9, -19, 59, 100, -24, 10, -4},
+        {-3, 8, -16, 48, 108, -23, 10, -4},
+        {-2, 6, -13, 37, 115, -20, 9, -4},
+        {-2, 5, -10, 27, 121, -17, 7, -3},
+        {-1, 3, -6, 17, 125, -13, 5, -2},
+        {0, 1, -3, 8, 127, -7, 3, -1},
+    }, [FILTER_8TAP_SMOOTH] = {
+        {-3, -1, 32, 64, 38, 1, -3, 0},
+        {-2, -2, 29, 63, 41, 2, -3, 0},
+        {-2, -2, 26, 63, 43, 4, -4, 0},
+        {-2, -3, 24, 62, 46, 5, -4, 0},
+        {-2, -3, 21, 60, 49, 7, -4, 0},
+        {-1, -4, 18, 59, 51, 9, -4, 0},
+        {-1, -4, 16, 57, 53, 12, -4, -1},
+        {-1, -4, 14, 55, 55, 14, -4, -1},
+        {-1, -4, 12, 53, 57, 16, -4, -1},
+        {0, -4, 9, 51, 59, 18, -4, -1},
+        {0, -4, 7, 49, 60, 21, -3, -2},
+        {0, -4, 5, 46, 62, 24, -3, -2},
+        {0, -4, 4, 43, 63, 26, -2, -2},
+        {0, -3, 2, 41, 63, 29, -2, -2},
+        {0, -3, 1, 38, 64, 32, -1, -3},
+    }
+};
+
+#define VP9_8TAP_MIPS_MMI_FUNC(SIZE, TYPE, TYPE_IDX)                           \
+void ff_put_8tap_##TYPE##_##SIZE##h_mmi(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int16_t *filter = vp9_subpel_filters_mmi[TYPE_IDX][mx-1];            \
+                                                                               \
+    convolve_horiz_mmi(src, srcstride, dst, dststride, filter, SIZE, h);       \
+}                                                                              \
+                                                                               \
+void ff_put_8tap_##TYPE##_##SIZE##v_mmi(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int16_t *filter = vp9_subpel_filters_mmi[TYPE_IDX][my-1];            \
+                                                                               \
+    src -= (3 * srcstride);                                                    \
+    convolve_vert_mmi(src, srcstride, dst, dststride, filter, SIZE, h);        \
+}                                                                              \
+                                                                               \
+void ff_put_8tap_##TYPE##_##SIZE##hv_mmi(uint8_t *dst, ptrdiff_t dststride,    \
+                                         const uint8_t *src,                   \
+                                         ptrdiff_t srcstride,                  \
+                                         int h, int mx, int my)                \
+{                                                                              \
+    const uint16_t *hfilter = vp9_subpel_filters_mmi[TYPE_IDX][mx-1];          \
+    const uint16_t *vfilter = vp9_subpel_filters_mmi[TYPE_IDX][my-1];          \
+                                                                               \
+    int tmp_h = h + 7;                                                         \
+    uint8_t temp[64 * 71];                                                     \
+    src -= (3 * srcstride);                                                    \
+    convolve_horiz_mmi(src, srcstride, temp, 64, hfilter, SIZE, tmp_h);        \
+    convolve_vert_mmi(temp, 64, dst, dststride, vfilter, SIZE, h);             \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##TYPE##_##SIZE##h_mmi(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int16_t *filter = vp9_subpel_filters_mmi[TYPE_IDX][mx-1];            \
+                                                                               \
+    convolve_avg_horiz_mmi(src, srcstride, dst, dststride, filter, SIZE, h);   \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##TYPE##_##SIZE##v_mmi(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int16_t *filter = vp9_subpel_filters_mmi[TYPE_IDX][my-1];            \
+                                                                               \
+    src -= (3 * srcstride);                                                    \
+    convolve_avg_vert_mmi(src, srcstride, dst, dststride, filter, SIZE, h);    \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##TYPE##_##SIZE##hv_mmi(uint8_t *dst, ptrdiff_t dststride,    \
+                                         const uint8_t *src,                   \
+                                         ptrdiff_t srcstride,                  \
+                                         int h, int mx, int my)                \
+{                                                                              \
+    const uint16_t *hfilter = vp9_subpel_filters_mmi[TYPE_IDX][mx-1];          \
+    const uint16_t *vfilter = vp9_subpel_filters_mmi[TYPE_IDX][my-1];          \
+                                                                               \
+    uint8_t temp1[64 * 64];                                                    \
+    uint8_t temp2[64 * 71];                                                    \
+    int tmp_h = h + 7;                                                         \
+    src -= (3 * srcstride);                                                    \
+    convolve_horiz_mmi(src, srcstride, temp2, 64, hfilter, SIZE, tmp_h);       \
+    convolve_vert_mmi(temp2, 64, temp1, 64, vfilter, SIZE, h);                 \
+    convolve_avg_mmi(temp1, 64, dst, dststride, SIZE, h);                      \
+}
+
+VP9_8TAP_MIPS_MMI_FUNC(64, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(32, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(16, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(8, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(4, regular, FILTER_8TAP_REGULAR);
+
+VP9_8TAP_MIPS_MMI_FUNC(64, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(32, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(16, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(8, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(4, sharp, FILTER_8TAP_SHARP);
+
+VP9_8TAP_MIPS_MMI_FUNC(64, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(32, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(16, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(8, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(4, smooth, FILTER_8TAP_SMOOTH);
+
+#undef VP9_8TAP_MIPS_MMI_FUNC
diff --git a/libavcodec/mips/vp9_mc_msa.c b/libavcodec/mips/vp9_mc_msa.c
index 749e8cb..57ea425 100644
--- a/libavcodec/mips/vp9_mc_msa.c
+++ b/libavcodec/mips/vp9_mc_msa.c
@@ -153,7 +153,7 @@ static const int8_t vp9_bilinear_filters_msa[15][2] = {
                                                               \
     PCKEV_B2_UB(in1, in0, in3, in2, tmp0_m, tmp1_m);          \
     AVER_UB2_UB(tmp0_m, dst0, tmp1_m, dst1, tmp0_m, tmp1_m);  \
-    ST8x4_UB(tmp0_m, tmp1_m, pdst_m, stride);                 \
+    ST_D4(tmp0_m, tmp1_m, 0, 1, 0, 1, pdst_m, stride);        \
 }
 
 static void common_hz_8t_4x4_msa(const uint8_t *src, int32_t src_stride,
@@ -182,7 +182,7 @@ static void common_hz_8t_4x4_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H2_SH(out0, out1, 7);
     SAT_SH2_SH(out0, out1, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_8t_4x8_msa(const uint8_t *src, int32_t src_stride,
@@ -217,10 +217,9 @@ static void common_hz_8t_4x8_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H4_SH(out0, out1, out2, out3, 7);
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     out = PCKEV_XORI128_UB(out0, out1);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
     out = PCKEV_XORI128_UB(out2, out3);
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void common_hz_8t_4w_msa(const uint8_t *src, int32_t src_stride,
@@ -262,7 +261,7 @@ static void common_hz_8t_8x4_msa(const uint8_t *src, int32_t src_stride,
     SAT_SH4_SH(out0, out1, out2, out3, 7);
     tmp0 = PCKEV_XORI128_UB(out0, out1);
     tmp1 = PCKEV_XORI128_UB(out2, out3);
-    ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+    ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hz_8t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
@@ -296,7 +295,7 @@ static void common_hz_8t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0, out1, out2, out3, 7);
         tmp0 = PCKEV_XORI128_UB(out0, out1);
         tmp1 = PCKEV_XORI128_UB(out2, out3);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
 }
@@ -510,7 +509,7 @@ static void common_vt_8t_4w_msa(const uint8_t *src, int32_t src_stride,
         SRARI_H2_SH(out10, out32, 7);
         SAT_SH2_SH(out10, out32, 7);
         out = PCKEV_XORI128_UB(out10, out32);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src2110 = src6554;
@@ -562,7 +561,7 @@ static void common_vt_8t_8w_msa(const uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(out0_r, out1_r, out2_r, out3_r, 7);
         tmp0 = PCKEV_XORI128_UB(out0_r, out1_r);
         tmp1 = PCKEV_XORI128_UB(out2_r, out3_r);
-        ST8x4_UB(tmp0, tmp1, dst, dst_stride);
+        ST_D4(tmp0, tmp1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src10_r = src54_r;
@@ -796,7 +795,7 @@ static void common_hv_8ht_8vt_4w_msa(const uint8_t *src, int32_t src_stride,
                               filt_hz1, filt_hz2, filt_hz3);
     hz_out5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                               filt_hz1, filt_hz2, filt_hz3);
-    SLDI_B2_SH(hz_out2, hz_out4, hz_out0, hz_out2, hz_out1, hz_out3, 8);
+    SLDI_B2_SH(hz_out2, hz_out0, hz_out4, hz_out2, 8, hz_out1, hz_out3);
 
     filt = LD_SH(filter_vert);
     SPLATI_H4_SH(filt, 0, 1, 2, 3, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
@@ -825,7 +824,7 @@ static void common_hv_8ht_8vt_4w_msa(const uint8_t *src, int32_t src_stride,
         SRARI_H2_SH(tmp0, tmp1, 7);
         SAT_SH2_SH(tmp0, tmp1, 7);
         out = PCKEV_XORI128_UB(tmp0, tmp1);
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out5 = hz_out9;
@@ -920,7 +919,7 @@ static void common_hv_8ht_8vt_8w_msa(const uint8_t *src, int32_t src_stride,
         SAT_SH4_SH(tmp0, tmp1, tmp2, tmp3, 7);
         vec0 = PCKEV_XORI128_UB(tmp0, tmp1);
         vec1 = PCKEV_XORI128_UB(tmp2, tmp3);
-        ST8x4_UB(vec0, vec1, dst, dst_stride);
+        ST_D4(vec0, vec1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out6 = hz_out10;
@@ -1016,7 +1015,7 @@ static void common_hz_8t_and_aver_dst_4x4_msa(const uint8_t *src,
     SAT_SH2_SH(res0, res1, 7);
     res = PCKEV_XORI128_UB(res0, res1);
     res = (v16u8) __msa_aver_u_b(res, dst0);
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(res, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_8t_and_aver_dst_4x8_msa(const uint8_t *src,
@@ -1061,7 +1060,7 @@ static void common_hz_8t_and_aver_dst_4x8_msa(const uint8_t *src,
     ILVR_D2_UB(res1, res0, res3, res2, res0, res2);
     XORI_B2_128_UB(res0, res2);
     AVER_UB2_UB(res0, dst0, res2, dst1, res0, res2);
-    ST4x8_UB(res0, res2, dst, dst_stride);
+    ST_W8(res0, res2, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_8t_and_aver_dst_4w_msa(const uint8_t *src,
@@ -1348,7 +1347,7 @@ static void common_vt_8t_and_aver_dst_4w_msa(const uint8_t *src,
         out = PCKEV_XORI128_UB(out10, out32);
         out = __msa_aver_u_b(out, dst0);
 
-        ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         src2110 = src6554;
@@ -1586,7 +1585,7 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_msa(const uint8_t *src,
                               filt_hz1, filt_hz2, filt_hz3);
     hz_out5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                               filt_hz1, filt_hz2, filt_hz3);
-    SLDI_B2_SH(hz_out2, hz_out4, hz_out0, hz_out2, hz_out1, hz_out3, 8);
+    SLDI_B2_SH(hz_out2, hz_out0, hz_out4, hz_out2, 8, hz_out1, hz_out3);
 
     filt = LD_SH(filter_vert);
     SPLATI_H4_SH(filt, 0, 1, 2, 3, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
@@ -1619,7 +1618,7 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_msa(const uint8_t *src,
         SAT_SH2_SH(res0, res1, 7);
         res = PCKEV_XORI128_UB(res0, res1);
         res = (v16u8) __msa_aver_u_b(res, dst0);
-        ST4x4_UB(res, res, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(res, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
 
         hz_out5 = hz_out9;
@@ -1812,7 +1811,8 @@ static void common_hz_2t_4x4_msa(const uint8_t *src, int32_t src_stride,
     DOTP_UB2_UH(vec0, vec1, filt0, filt0, vec2, vec3);
     SRARI_H2_UH(vec2, vec3, 7);
     PCKEV_B2_UB(vec2, vec2, vec3, vec3, res0, res1);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
 }
 
 static void common_hz_2t_4x8_msa(const uint8_t *src, int32_t src_stride,
@@ -1838,9 +1838,10 @@ static void common_hz_2t_4x8_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(vec4, vec5, vec6, vec7, 7);
     PCKEV_B4_SB(vec4, vec4, vec5, vec5, vec6, vec6, vec7, vec7,
                 res0, res1, res2, res3);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST4x4_UB(res2, res3, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
+    ST_W2(res2, 0, 1, dst + 4 * dst_stride, dst_stride);
+    ST_W2(res3, 0, 1, dst + 6 * dst_stride, dst_stride);
 }
 
 void ff_put_bilin_4h_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -1877,7 +1878,7 @@ static void common_hz_2t_8x4_msa(const uint8_t *src, int32_t src_stride,
                 vec0, vec1, vec2, vec3);
     SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, src0, src1);
-    ST8x4_UB(src0, src1, dst, dst_stride);
+    ST_D4(src0, src1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hz_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
@@ -1906,8 +1907,7 @@ static void common_hz_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
     src += (4 * src_stride);
 
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
     VSHF_B2_UH(src0, src0, src1, src1, mask, mask, vec0, vec1);
     VSHF_B2_UH(src2, src2, src3, src3, mask, mask, vec2, vec3);
@@ -1915,8 +1915,8 @@ static void common_hz_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
                 vec0, vec1, vec2, vec3);
     SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
     PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
-    dst += (4 * dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+    dst += (8 * dst_stride);
 
     if (16 == height) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
@@ -1931,7 +1931,7 @@ static void common_hz_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
         src += (4 * src_stride);
 
         PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         VSHF_B2_UH(src0, src0, src1, src1, mask, mask, vec0, vec1);
         VSHF_B2_UH(src2, src2, src3, src3, mask, mask, vec2, vec3);
@@ -1939,7 +1939,7 @@ static void common_hz_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
                     vec0, vec1, vec2, vec3);
         SRARI_H4_UH(vec0, vec1, vec2, vec3, 7);
         PCKEV_B2_SB(vec1, vec0, vec3, vec2, out0, out1);
-        ST8x4_UB(out0, out1, dst + 4 * dst_stride, dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
     }
 }
 
@@ -2093,7 +2093,7 @@ void ff_put_bilin_64h_msa(uint8_t *dst, ptrdiff_t dst_stride,
         src4 = LD_SB(src + 32);
         src6 = LD_SB(src + 48);
         src7 = LD_SB(src + 56);
-        SLDI_B3_SB(src2, src4, src6, src0, src2, src4, src1, src3, src5, 8);
+        SLDI_B3_SB(src2, src0, src4, src2, src6, src4, 8, src1, src3, src5);
         src += src_stride;
 
         VSHF_B2_UB(src0, src0, src1, src1, mask, mask, vec0, vec1);
@@ -2137,7 +2137,7 @@ static void common_vt_2t_4x4_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H2_UH(tmp0, tmp1, 7);
     SAT_UH2_UH(tmp0, tmp1, 7);
     src2110 = __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
-    ST4x4_UB(src2110, src2110, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(src2110, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_vt_2t_4x8_msa(const uint8_t *src, int32_t src_stride,
@@ -2171,8 +2171,7 @@ static void common_vt_2t_4x8_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, src2110, src4332);
-    ST4x4_UB(src2110, src2110, 0, 1, 2, 3, dst, dst_stride);
-    ST4x4_UB(src4332, src4332, 0, 1, 2, 3, dst + 4 * dst_stride, dst_stride);
+    ST_W8(src2110, src4332, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_put_bilin_4v_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -2209,7 +2208,7 @@ static void common_vt_2t_8x4_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_vt_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
@@ -2243,16 +2242,15 @@ static void common_vt_2t_8x8mult_msa(const uint8_t *src, int32_t src_stride,
         SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         DOTP_UB4_UH(vec4, vec5, vec6, vec7, filt0, filt0, filt0, filt0,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
         PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        dst += (8 * dst_stride);
 
         src0 = src8;
     }
@@ -2514,7 +2512,8 @@ static void common_hv_2ht_2vt_4x4_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H2_UH(tmp0, tmp1, 7);
     SAT_UH2_UH(tmp0, tmp1, 7);
     PCKEV_B2_UB(tmp0, tmp0, tmp1, tmp1, res0, res1);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
 }
 
 static void common_hv_2ht_2vt_4x8_msa(const uint8_t *src, int32_t src_stride,
@@ -2545,8 +2544,8 @@ static void common_hv_2ht_2vt_4x8_msa(const uint8_t *src, int32_t src_stride,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     ILVEV_B2_UB(hz_out0, hz_out1, hz_out2, hz_out3, vec0, vec1);
@@ -2557,9 +2556,10 @@ static void common_hv_2ht_2vt_4x8_msa(const uint8_t *src, int32_t src_stride,
     SAT_UH4_UH(vec4, vec5, vec6, vec7, 7);
     PCKEV_B4_SB(vec4, vec4, vec5, vec5, vec6, vec6, vec7, vec7,
                 res0, res1, res2, res3);
-    ST4x4_UB(res0, res1, 0, 1, 0, 1, dst, dst_stride);
-    dst += (4 * dst_stride);
-    ST4x4_UB(res2, res3, 0, 1, 0, 1, dst, dst_stride);
+    ST_W2(res0, 0, 1, dst, dst_stride);
+    ST_W2(res1, 0, 1, dst + 2 * dst_stride, dst_stride);
+    ST_W2(res2, 0, 1, dst + 4 * dst_stride, dst_stride);
+    ST_W2(res3, 0, 1, dst + 6 * dst_stride, dst_stride);
 }
 
 void ff_put_bilin_4hv_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -2618,7 +2618,7 @@ static void common_hv_2ht_2vt_8x4_msa(const uint8_t *src, int32_t src_stride,
     SRARI_H4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_SB(tmp1, tmp0, tmp3, tmp2, out0, out1);
-    ST8x4_UB(out0, out1, dst, dst_stride);
+    ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
 static void common_hv_2ht_2vt_8x8mult_msa(const uint8_t *src, int32_t src_stride,
@@ -2674,8 +2674,7 @@ static void common_hv_2ht_2vt_8x8mult_msa(const uint8_t *src, int32_t src_stride
         SRARI_H2_UH(tmp3, tmp4, 7);
         SAT_UH2_UH(tmp3, tmp4, 7);
         PCKEV_B2_SB(tmp2, tmp1, tmp4, tmp3, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 
         hz_out1 = HORIZ_2TAP_FILT_UH(src1, src1, mask, filt_hz, 7);
         vec0 = (v16u8) __msa_ilvev_b((v16i8) hz_out1, (v16i8) hz_out0);
@@ -2696,8 +2695,8 @@ static void common_hv_2ht_2vt_8x8mult_msa(const uint8_t *src, int32_t src_stride
         SRARI_H4_UH(tmp5, tmp6, tmp7, tmp8, 7);
         SAT_UH4_UH(tmp5, tmp6, tmp7, tmp8, 7);
         PCKEV_B2_SB(tmp6, tmp5, tmp8, tmp7, out0, out1);
-        ST8x4_UB(out0, out1, dst, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D4(out0, out1, 0, 1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        dst += (8 * dst_stride);
     }
 }
 
@@ -2842,7 +2841,7 @@ static void common_hz_2t_and_aver_dst_4x4_msa(const uint8_t *src,
     res = (v16u8) __msa_pckev_b((v16i8) vec3, (v16i8) vec2);
     res = (v16u8) __msa_aver_u_b(res, dst0);
 
-    ST4x4_UB(res, res, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(res, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hz_2t_and_aver_dst_4x8_msa(const uint8_t *src,
@@ -2876,7 +2875,7 @@ static void common_hz_2t_and_aver_dst_4x8_msa(const uint8_t *src,
                 res2, res3);
     ILVR_D2_UB(res1, res0, res3, res2, res0, res2);
     AVER_UB2_UB(res0, dst0, res2, dst1, res0, res2);
-    ST4x8_UB(res0, res2, dst, dst_stride);
+    ST_W8(res0, res2, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_avg_bilin_4h_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -3147,7 +3146,7 @@ void ff_avg_bilin_64h_msa(uint8_t *dst, ptrdiff_t dst_stride,
     for (loop_cnt = height; loop_cnt--;) {
         LD_SB4(src, 16, src0, src2, src4, src6);
         src7 = LD_SB(src + 56);
-        SLDI_B3_SB(src2, src4, src6, src0, src2, src4, src1, src3, src5, 8);
+        SLDI_B3_SB(src2, src0, src4, src2, src6, src4, 8, src1, src3, src5);
         src += src_stride;
 
         VSHF_B2_UB(src0, src0, src1, src1, mask, mask, vec0, vec1);
@@ -3202,7 +3201,7 @@ static void common_vt_2t_and_aver_dst_4x4_msa(const uint8_t *src,
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     out = __msa_aver_u_b(out, dst0);
 
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_vt_2t_and_aver_dst_4x8_msa(const uint8_t *src,
@@ -3241,7 +3240,7 @@ static void common_vt_2t_and_aver_dst_4x8_msa(const uint8_t *src,
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, src2110, src4332);
     AVER_UB2_UB(src2110, dst0, src4332, dst1, src2110, src4332);
-    ST4x8_UB(src2110, src4332, dst, dst_stride);
+    ST_W8(src2110, src4332, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_avg_bilin_4v_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -3620,7 +3619,7 @@ static void common_hv_2ht_2vt_and_aver_dst_4x4_msa(const uint8_t *src,
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     out = __msa_aver_u_b(out, dst0);
 
-    ST4x4_UB(out, out, 0, 1, 2, 3, dst, dst_stride);
+    ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
 static void common_hv_2ht_2vt_and_aver_dst_4x8_msa(const uint8_t *src,
@@ -3656,8 +3655,8 @@ static void common_hv_2ht_2vt_and_aver_dst_4x8_msa(const uint8_t *src,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     LW4(dst, dst_stride, tp0, tp1, tp2, tp3);
@@ -3672,7 +3671,7 @@ static void common_hv_2ht_2vt_and_aver_dst_4x8_msa(const uint8_t *src,
     SAT_UH4_UH(tmp0, tmp1, tmp2, tmp3, 7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, res0, res1);
     AVER_UB2_UB(res0, dst0, res1, dst1, res0, res1);
-    ST4x8_UB(res0, res1, dst, dst_stride);
+    ST_W8(res0, res1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 }
 
 void ff_avg_bilin_4hv_msa(uint8_t *dst, ptrdiff_t dst_stride,
@@ -4071,14 +4070,14 @@ static void avg_width4_msa(const uint8_t *src, int32_t src_stride,
         LW4(dst + 4 * dst_stride, dst_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, dst1);
         AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);
-        ST4x8_UB(dst0, dst1, dst, dst_stride);
+        ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
     } else if (4 == height) {
         LW4(src, src_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);
         LW4(dst, dst_stride, tp0, tp1, tp2, tp3);
         INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);
         dst0 = __msa_aver_u_b(src0, dst0);
-        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);
+        ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
     }
 }
 
@@ -4109,7 +4108,7 @@ static void avg_width8_msa(const uint8_t *src, int32_t src_stride,
             INSERT_D2_UB(tp6, tp7, dst3);
             AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0,
                         dst1, dst2, dst3);
-            ST8x8_UB(dst0, dst1, dst2, dst3, dst, dst_stride);
+            ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
             dst += 8 * dst_stride;
         }
     } else if (4 == height) {
@@ -4120,7 +4119,7 @@ static void avg_width8_msa(const uint8_t *src, int32_t src_stride,
         INSERT_D2_UB(tp0, tp1, dst0);
         INSERT_D2_UB(tp2, tp3, dst1);
         AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);
-        ST8x4_UB(dst0, dst1, dst, dst_stride);
+        ST_D4(dst0, dst1, 0, 1, 0, 1, dst, dst_stride);
     }
 }
 
diff --git a/libavcodec/mips/vp9dsp_init_mips.c b/libavcodec/mips/vp9dsp_init_mips.c
index c8a4890..5a8c599 100644
--- a/libavcodec/mips/vp9dsp_init_mips.c
+++ b/libavcodec/mips/vp9dsp_init_mips.c
@@ -18,6 +18,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/common.h"
 #include "libavcodec/vp9dsp.h"
@@ -168,9 +169,58 @@ static av_cold void vp9dsp_init_msa(VP9DSPContext *dsp, int bpp)
 }
 #endif  // #if HAVE_MSA
 
+#if HAVE_MMI
+static av_cold void vp9dsp_mc_init_mmi(VP9DSPContext *dsp)
+{
+#define init_subpel1(idx1, idx2, idxh, idxv, sz, dir, type)  \
+    dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_smooth_##sz##dir##_mmi;             \
+    dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_regular_##sz##dir##_mmi;            \
+    dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_sharp_##sz##dir##_mmi;
+
+#define init_subpel2(idx, idxh, idxv, dir, type)      \
+    init_subpel1(0, idx, idxh, idxv, 64, dir, type);  \
+    init_subpel1(1, idx, idxh, idxv, 32, dir, type);  \
+    init_subpel1(2, idx, idxh, idxv, 16, dir, type);  \
+    init_subpel1(3, idx, idxh, idxv,  8, dir, type);  \
+    init_subpel1(4, idx, idxh, idxv,  4, dir, type)
+
+#define init_subpel3(idx, type)         \
+    init_subpel2(idx, 1, 1, hv, type);  \
+    init_subpel2(idx, 0, 1, v, type);   \
+    init_subpel2(idx, 1, 0, h, type)
+
+    init_subpel3(0, put);
+    init_subpel3(1, avg);
+
+#undef init_subpel1
+#undef init_subpel2
+#undef init_subpel3
+}
+
+static av_cold void vp9dsp_init_mmi(VP9DSPContext *dsp, int bpp)
+{
+    if (bpp == 8) {
+        vp9dsp_mc_init_mmi(dsp);
+    }
+}
+#endif  // #if HAVE_MMI
+
 av_cold void ff_vp9dsp_init_mips(VP9DSPContext *dsp, int bpp)
 {
+#if HAVE_MSA || HAVE_MMI
+    int cpu_flags = av_get_cpu_flags();
+#endif
+
+#if HAVE_MMI
+    if (have_mmi(cpu_flags))
+        vp9dsp_init_mmi(dsp, bpp);
+#endif
+
 #if HAVE_MSA
-    vp9dsp_init_msa(dsp, bpp);
-#endif  // #if HAVE_MSA
+    if (have_msa(cpu_flags))
+        vp9dsp_init_msa(dsp, bpp);
+#endif
 }
diff --git a/libavcodec/mips/vp9dsp_mips.h b/libavcodec/mips/vp9dsp_mips.h
index 4d73038..0b6ce7c 100644
--- a/libavcodec/mips/vp9dsp_mips.h
+++ b/libavcodec/mips/vp9dsp_mips.h
@@ -234,4 +234,54 @@ void ff_tm_16x16_msa(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
 void ff_tm_32x32_msa(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
                      const uint8_t *top);
 
+#define VP9_8TAP_MIPS_MMI_FUNC(SIZE, type, type_idx)                         \
+void ff_put_8tap_##type##_##SIZE##h_mmi(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_put_8tap_##type##_##SIZE##v_mmi(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_put_8tap_##type##_##SIZE##hv_mmi(uint8_t *dst, ptrdiff_t dststride,  \
+                                         const uint8_t *src,                 \
+                                         ptrdiff_t srcstride,                \
+                                         int h, int mx, int my);             \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##h_mmi(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##v_mmi(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##hv_mmi(uint8_t *dst, ptrdiff_t dststride,  \
+                                         const uint8_t *src,                 \
+                                         ptrdiff_t srcstride,                \
+                                         int h, int mx, int my);
+
+VP9_8TAP_MIPS_MMI_FUNC(64, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(32, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(16, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(8, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_MIPS_MMI_FUNC(4, regular, FILTER_8TAP_REGULAR);
+
+VP9_8TAP_MIPS_MMI_FUNC(64, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(32, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(16, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(8, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_MIPS_MMI_FUNC(4, sharp, FILTER_8TAP_SHARP);
+
+VP9_8TAP_MIPS_MMI_FUNC(64, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(32, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(16, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(8, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_MIPS_MMI_FUNC(4, smooth, FILTER_8TAP_SMOOTH);
+#undef VP9_8TAP_MIPS_MMI_FUNC
+
 #endif  // #ifndef AVCODEC_MIPS_VP9DSP_MIPS_H
diff --git a/libavcodec/mips/wmv2dsp_init_mips.c b/libavcodec/mips/wmv2dsp_init_mips.c
index 51dd207..af14007 100644
--- a/libavcodec/mips/wmv2dsp_init_mips.c
+++ b/libavcodec/mips/wmv2dsp_init_mips.c
@@ -18,21 +18,17 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "wmv2dsp_mips.h"
 
-#if HAVE_MMI
-static av_cold void wmv2dsp_init_mmi(WMV2DSPContext *c)
-{
-    c->idct_add  = ff_wmv2_idct_add_mmi;
-    c->idct_put  = ff_wmv2_idct_put_mmi;
-}
-#endif /* HAVE_MMI */
-
 av_cold void ff_wmv2dsp_init_mips(WMV2DSPContext *c)
 {
-#if HAVE_MMI
-    wmv2dsp_init_mmi(c);
-#endif /* HAVE_MMI */
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->idct_add  = ff_wmv2_idct_add_mmi;
+        c->idct_put  = ff_wmv2_idct_put_mmi;
+    }
 }
diff --git a/libavcodec/mips/wmv2dsp_mips.h b/libavcodec/mips/wmv2dsp_mips.h
index 22894c5..c96b3d9 100644
--- a/libavcodec/mips/wmv2dsp_mips.h
+++ b/libavcodec/mips/wmv2dsp_mips.h
@@ -23,7 +23,7 @@
 
 #include "libavcodec/wmv2dsp.h"
 
-void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block);
-void ff_wmv2_idct_put_mmi(uint8_t *dest, int line_size, int16_t *block);
+void ff_wmv2_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_wmv2_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
 
 #endif /* AVCODEC_MIPS_WMV2DSP_MIPS_H */
diff --git a/libavcodec/mips/wmv2dsp_mmi.c b/libavcodec/mips/wmv2dsp_mmi.c
index 1f6ccb2..82e16f9 100644
--- a/libavcodec/mips/wmv2dsp_mmi.c
+++ b/libavcodec/mips/wmv2dsp_mmi.c
@@ -95,7 +95,7 @@ static void wmv2_idct_col_mmi(short * b)
     b[56] = (a0 + a2 - a1 - a5 + 8192) >> 14;
 }
 
-void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block)
+void ff_wmv2_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     int i;
     double ftmp[11];
@@ -212,7 +212,7 @@ void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block)
     );
 }
 
-void ff_wmv2_idct_put_mmi(uint8_t *dest, int line_size, int16_t *block)
+void ff_wmv2_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     int i;
     double ftmp[8];
diff --git a/libavcodec/mips/xvid_idct_mmi.c b/libavcodec/mips/xvid_idct_mmi.c
index d3f9acb..b822b8a 100644
--- a/libavcodec/mips/xvid_idct_mmi.c
+++ b/libavcodec/mips/xvid_idct_mmi.c
@@ -240,13 +240,13 @@ void ff_xvid_idct_mmi(int16_t *block)
     );
 }
 
-void ff_xvid_idct_put_mmi(uint8_t *dest, int32_t line_size, int16_t *block)
+void ff_xvid_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     ff_xvid_idct_mmi(block);
     ff_put_pixels_clamped_mmi(block, dest, line_size);
 }
 
-void ff_xvid_idct_add_mmi(uint8_t *dest, int32_t line_size, int16_t *block)
+void ff_xvid_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     ff_xvid_idct_mmi(block);
     ff_add_pixels_clamped_mmi(block, dest, line_size);
diff --git a/libavcodec/mips/xvididct_init_mips.c b/libavcodec/mips/xvididct_init_mips.c
index c1d82cc..ed545cf 100644
--- a/libavcodec/mips/xvididct_init_mips.c
+++ b/libavcodec/mips/xvididct_init_mips.c
@@ -18,28 +18,23 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "xvididct_mips.h"
 
-#if HAVE_MMI
-static av_cold void xvid_idct_init_mmi(IDCTDSPContext *c, AVCodecContext *avctx,
+av_cold void ff_xvid_idct_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
         unsigned high_bit_depth)
 {
-    if (!high_bit_depth) {
-        if (avctx->idct_algo == FF_IDCT_AUTO ||
-                avctx->idct_algo == FF_IDCT_XVID) {
-            c->idct_put = ff_xvid_idct_put_mmi;
-            c->idct_add = ff_xvid_idct_add_mmi;
-            c->idct = ff_xvid_idct_mmi;
-            c->perm_type = FF_IDCT_PERM_NONE;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (!high_bit_depth) {
+            if (avctx->idct_algo == FF_IDCT_AUTO ||
+                    avctx->idct_algo == FF_IDCT_XVID) {
+                c->idct_put = ff_xvid_idct_put_mmi;
+                c->idct_add = ff_xvid_idct_add_mmi;
+                c->idct = ff_xvid_idct_mmi;
+                c->perm_type = FF_IDCT_PERM_NONE;
+            }
         }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_xvid_idct_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
-        unsigned high_bit_depth)
-{
-#if HAVE_MMI
-    xvid_idct_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
-}
diff --git a/libavcodec/mips/xvididct_mips.h b/libavcodec/mips/xvididct_mips.h
index 0768aaa..bee03c1 100644
--- a/libavcodec/mips/xvididct_mips.h
+++ b/libavcodec/mips/xvididct_mips.h
@@ -24,7 +24,7 @@
 #include "libavcodec/xvididct.h"
 
 void ff_xvid_idct_mmi(int16_t *block);
-void ff_xvid_idct_put_mmi(uint8_t *dest, int32_t line_size, int16_t *block);
-void ff_xvid_idct_add_mmi(uint8_t *dest, int32_t line_size, int16_t *block);
+void ff_xvid_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_xvid_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
 
 #endif /* AVCODEC_MIPS_XVIDIDCT_MIPS_H */
diff --git a/libavcodec/vp3dsp.c b/libavcodec/vp3dsp.c
index fdaa292..cdf7d64 100644
--- a/libavcodec/vp3dsp.c
+++ b/libavcodec/vp3dsp.c
@@ -293,4 +293,6 @@ av_cold void ff_vp3dsp_init(VP3DSPContext *c, int flags)
         ff_vp3dsp_init_ppc(c, flags);
     if (ARCH_X86)
         ff_vp3dsp_init_x86(c, flags);
+    if (ARCH_MIPS)
+        ff_vp3dsp_init_mips(c, flags);
 }
diff --git a/libavcodec/vp3dsp.h b/libavcodec/vp3dsp.h
index 2fdad16..f5f042d 100644
--- a/libavcodec/vp3dsp.h
+++ b/libavcodec/vp3dsp.h
@@ -49,5 +49,6 @@ void ff_vp3dsp_init(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_arm(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_ppc(VP3DSPContext *c, int flags);
 void ff_vp3dsp_init_x86(VP3DSPContext *c, int flags);
+void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags);
 
 #endif /* AVCODEC_VP3DSP_H */
diff --git a/libavutil/cpu.c b/libavutil/cpu.c
index 6548cc3..52f6b9a 100644
--- a/libavutil/cpu.c
+++ b/libavutil/cpu.c
@@ -51,6 +51,8 @@ static atomic_int cpu_flags = ATOMIC_VAR_INIT(-1);
 
 static int get_cpu_flags(void)
 {
+    if (ARCH_MIPS)
+        return ff_get_cpu_flags_mips();
     if (ARCH_AARCH64)
         return ff_get_cpu_flags_aarch64();
     if (ARCH_ARM)
@@ -169,6 +171,9 @@ int av_parse_cpu_flags(const char *s)
         { "armv8",    NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ARMV8    },    .unit = "flags" },
         { "neon",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_NEON     },    .unit = "flags" },
         { "vfp",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_VFP      },    .unit = "flags" },
+#elif ARCH_MIPS
+        { "mmi",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMI      },    .unit = "flags" },
+        { "msa",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MSA      },    .unit = "flags" },
 #endif
         { NULL },
     };
@@ -250,6 +255,9 @@ int av_parse_cpu_caps(unsigned *flags, const char *s)
         { "armv8",    NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ARMV8    },    .unit = "flags" },
         { "neon",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_NEON     },    .unit = "flags" },
         { "vfp",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_VFP      },    .unit = "flags" },
+#elif ARCH_MIPS
+        { "mmi",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMI      },    .unit = "flags" },
+        { "msa",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MSA      },    .unit = "flags" },
 #endif
         { NULL },
     };
@@ -308,6 +316,8 @@ int av_cpu_count(void)
 
 size_t av_cpu_max_align(void)
 {
+    if (ARCH_MIPS)
+        return ff_get_cpu_max_align_mips();
     if (ARCH_AARCH64)
         return ff_get_cpu_max_align_aarch64();
     if (ARCH_ARM)
diff --git a/libavutil/cpu.h b/libavutil/cpu.h
index 8bb9eb6..83099dd 100644
--- a/libavutil/cpu.h
+++ b/libavutil/cpu.h
@@ -71,6 +71,9 @@
 #define AV_CPU_FLAG_VFP_VM       (1 << 7) ///< VFPv2 vector mode, deprecated in ARMv7-A and unavailable in various CPUs implementations
 #define AV_CPU_FLAG_SETEND       (1 <<16)
 
+#define AV_CPU_FLAG_MMI          (1 << 0)
+#define AV_CPU_FLAG_MSA          (1 << 1)
+
 /**
  * Return the flags which specify extensions supported by the CPU.
  * The returned value is affected by av_force_cpu_flags() if that was used
diff --git a/libavutil/cpu_internal.h b/libavutil/cpu_internal.h
index 37122d1..8897643 100644
--- a/libavutil/cpu_internal.h
+++ b/libavutil/cpu_internal.h
@@ -41,11 +41,13 @@
 #define CPUEXT_FAST(flags, cpuext) CPUEXT_SUFFIX_FAST(flags, , cpuext)
 #define CPUEXT_SLOW(flags, cpuext) CPUEXT_SUFFIX_SLOW(flags, , cpuext)
 
+int ff_get_cpu_flags_mips(void);
 int ff_get_cpu_flags_aarch64(void);
 int ff_get_cpu_flags_arm(void);
 int ff_get_cpu_flags_ppc(void);
 int ff_get_cpu_flags_x86(void);
 
+size_t ff_get_cpu_max_align_mips(void);
 size_t ff_get_cpu_max_align_aarch64(void);
 size_t ff_get_cpu_max_align_arm(void);
 size_t ff_get_cpu_max_align_ppc(void);
diff --git a/libavutil/mips/Makefile b/libavutil/mips/Makefile
index dbfa5aa..5f8c9b6 100644
--- a/libavutil/mips/Makefile
+++ b/libavutil/mips/Makefile
@@ -1 +1 @@
-OBJS += mips/float_dsp_mips.o
+OBJS += mips/float_dsp_mips.o mips/cpu.o
diff --git a/libavutil/mips/asmdefs.h b/libavutil/mips/asmdefs.h
index 7481199..76bb2b9 100644
--- a/libavutil/mips/asmdefs.h
+++ b/libavutil/mips/asmdefs.h
@@ -55,4 +55,46 @@
 # define PTR_SLL        "sll "
 #endif
 
+/*
+ * parse_r var, r - Helper assembler macro for parsing register names.
+ *
+ * This converts the register name in $n form provided in \r to the
+ * corresponding register number, which is assigned to the variable \var. It is
+ * needed to allow explicit encoding of instructions in inline assembly where
+ * registers are chosen by the compiler in $n form, allowing us to avoid using
+ * fixed register numbers.
+ *
+ * It also allows newer instructions (not implemented by the assembler) to be
+ * transparently implemented using assembler macros, instead of needing separate
+ * cases depending on toolchain support.
+ *
+ * Simple usage example:
+ * __asm__ __volatile__("parse_r __rt, %0\n\t"
+ *                      ".insn\n\t"
+ *                      "# di    %0\n\t"
+ *                      ".word   (0x41606000 | (__rt << 16))"
+ *                      : "=r" (status);
+ */
+
+/* Match an individual register number and assign to \var */
+#define _IFC_REG(n)                                \
+        ".ifc        \\r, $" #n "\n\t"             \
+        "\\var        = " #n "\n\t"                \
+        ".endif\n\t"
+
+__asm__(".macro        parse_r var r\n\t"
+        "\\var        = -1\n\t"
+        _IFC_REG(0)  _IFC_REG(1)  _IFC_REG(2)  _IFC_REG(3)
+        _IFC_REG(4)  _IFC_REG(5)  _IFC_REG(6)  _IFC_REG(7)
+        _IFC_REG(8)  _IFC_REG(9)  _IFC_REG(10) _IFC_REG(11)
+        _IFC_REG(12) _IFC_REG(13) _IFC_REG(14) _IFC_REG(15)
+        _IFC_REG(16) _IFC_REG(17) _IFC_REG(18) _IFC_REG(19)
+        _IFC_REG(20) _IFC_REG(21) _IFC_REG(22) _IFC_REG(23)
+        _IFC_REG(24) _IFC_REG(25) _IFC_REG(26) _IFC_REG(27)
+        _IFC_REG(28) _IFC_REG(29) _IFC_REG(30) _IFC_REG(31)
+        ".iflt        \\var\n\t"
+        ".error        \"Unable to parse register name \\r\"\n\t"
+        ".endif\n\t"
+        ".endm");
+
 #endif /* AVCODEC_MIPS_ASMDEFS_H */
diff --git a/libavutil/mips/cpu.c b/libavutil/mips/cpu.c
new file mode 100644
index 0000000..59619d5
--- /dev/null
+++ b/libavutil/mips/cpu.c
@@ -0,0 +1,134 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+#include "config.h"
+#if defined __linux__ || defined __ANDROID__
+#include <stdint.h>
+#include <stdio.h>
+#include <string.h>
+#include <sys/auxv.h>
+#include "asmdefs.h"
+#include "libavutil/avstring.h"
+#endif
+
+#if defined __linux__ || defined __ANDROID__
+
+#define HWCAP_LOONGSON_CPUCFG (1 << 14)
+
+static int cpucfg_available(void)
+{
+    return getauxval(AT_HWCAP) & HWCAP_LOONGSON_CPUCFG;
+}
+
+/* Most toolchains have no CPUCFG support yet */
+static uint32_t read_cpucfg(uint32_t reg)
+{
+        uint32_t __res;
+
+        __asm__ __volatile__(
+                "parse_r __res,%0\n\t"
+                "parse_r reg,%1\n\t"
+                ".insn \n\t"
+                ".word (0xc8080118 | (reg << 21) | (__res << 11))\n\t"
+                :"=r"(__res)
+                :"r"(reg)
+                :
+                );
+        return __res;
+}
+
+#define LOONGSON_CFG1 0x1
+
+#define LOONGSON_CFG1_MMI    (1 << 4)
+#define LOONGSON_CFG1_MSA1   (1 << 5)
+
+static int cpu_flags_cpucfg(void)
+{
+    int flags = 0;
+    uint32_t cfg1 = read_cpucfg(LOONGSON_CFG1);
+
+    if (cfg1 & LOONGSON_CFG1_MMI)
+        flags |= AV_CPU_FLAG_MMI;
+
+    if (cfg1 & LOONGSON_CFG1_MSA1)
+        flags |= AV_CPU_FLAG_MSA;
+
+    return flags;
+}
+
+static int cpu_flags_cpuinfo(void)
+{
+    FILE *f = fopen("/proc/cpuinfo", "r");
+    char buf[200];
+    int flags = 0;
+
+    if (!f)
+        return -1;
+
+    while (fgets(buf, sizeof(buf), f)) {
+        /* Legacy kernel may not export MMI in ASEs implemented */
+        if (av_strstart(buf, "cpu model", NULL)) {
+            if (strstr(buf, "Loongson-3 "))
+                flags |= AV_CPU_FLAG_MMI;
+        }
+
+        if (av_strstart(buf, "ASEs implemented", NULL)) {
+            if (strstr(buf, " loongson-mmi"))
+                flags |= AV_CPU_FLAG_MMI;
+            if (strstr(buf, " msa"))
+                flags |= AV_CPU_FLAG_MSA;
+
+            break;
+        }
+    }
+    fclose(f);
+    return flags;
+}
+#endif
+
+int ff_get_cpu_flags_mips(void)
+{
+#if defined __linux__ || defined __ANDROID__
+    if (cpucfg_available())
+        return cpu_flags_cpucfg();
+    else
+        return cpu_flags_cpuinfo();
+#else
+    /* Assume no SIMD ASE supported */
+    return 0;
+#endif
+}
+
+size_t ff_get_cpu_max_align_mips(void)
+{
+    int flags = av_get_cpu_flags();
+
+    if (flags & AV_CPU_FLAG_MSA)
+        return 16;
+
+    /*
+     * MMI itself is 64-bit but quad word load & store
+     * needs 128-bit align.
+     */
+    if (flags & AV_CPU_FLAG_MMI)
+        return 16;
+
+    return 8;
+}
diff --git a/libavutil/mips/cpu.h b/libavutil/mips/cpu.h
new file mode 100644
index 0000000..615dc49
--- /dev/null
+++ b/libavutil/mips/cpu.h
@@ -0,0 +1,28 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_MIPS_CPU_H
+#define AVUTIL_MIPS_CPU_H
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+#define have_mmi(flags) CPUEXT(flags, MMI)
+#define have_msa(flags) CPUEXT(flags, MSA)
+
+#endif /* AVUTIL_MIPS_CPU_H */
diff --git a/libavutil/mips/generic_macros_msa.h b/libavutil/mips/generic_macros_msa.h
index 6a46704..bb25e9f 100644
--- a/libavutil/mips/generic_macros_msa.h
+++ b/libavutil/mips/generic_macros_msa.h
@@ -23,6 +23,11 @@
 
 #include <stdint.h>
 #include <msa.h>
+#include <config.h>
+
+#if HAVE_MSA2
+#include <msa2.h>
+#endif
 
 #define ALIGNMENT           16
 #define ALLOC_ALIGNED(align) __attribute__ ((aligned((align) << 1)))
@@ -106,10 +111,11 @@
         uint32_t val_lw_m;                           \
                                                      \
         __asm__ volatile (                           \
-            "ulw  %[val_lw_m],  %[psrc_lw_m]  \n\t"  \
+            "lwr %[val_lw_m], 0(%[psrc_lw_m]) \n\t"  \
+            "lwl %[val_lw_m], 3(%[psrc_lw_m]) \n\t"  \
                                                      \
-            : [val_lw_m] "=r" (val_lw_m)             \
-            : [psrc_lw_m] "m" (*psrc_lw_m)           \
+            : [val_lw_m] "=&r"(val_lw_m)             \
+            : [psrc_lw_m] "r"(psrc_lw_m)             \
         );                                           \
                                                      \
         val_lw_m;                                    \
@@ -122,10 +128,11 @@
             uint64_t val_ld_m = 0;                       \
                                                          \
             __asm__ volatile (                           \
-                "uld  %[val_ld_m],  %[psrc_ld_m]  \n\t"  \
+                "ldr %[val_ld_m], 0(%[psrc_ld_m]) \n\t"  \
+                "ldl %[val_ld_m], 7(%[psrc_ld_m]) \n\t"  \
                                                          \
-                : [val_ld_m] "=r" (val_ld_m)             \
-                : [psrc_ld_m] "m" (*psrc_ld_m)           \
+                : [val_ld_m] "=&r" (val_ld_m)            \
+                : [psrc_ld_m] "r" (psrc_ld_m)            \
             );                                           \
                                                          \
             val_ld_m;                                    \
@@ -294,6 +301,7 @@
 #define LD_SB4(...) LD_V4(v16i8, __VA_ARGS__)
 #define LD_UH4(...) LD_V4(v8u16, __VA_ARGS__)
 #define LD_SH4(...) LD_V4(v8i16, __VA_ARGS__)
+#define LD_SW4(...) LD_V4(v4i32, __VA_ARGS__)
 
 #define LD_V5(RTYPE, psrc, stride, out0, out1, out2, out3, out4)  \
 {                                                                 \
@@ -332,6 +340,7 @@
 #define LD_SB8(...) LD_V8(v16i8, __VA_ARGS__)
 #define LD_UH8(...) LD_V8(v8u16, __VA_ARGS__)
 #define LD_SH8(...) LD_V8(v8i16, __VA_ARGS__)
+#define LD_SW8(...) LD_V8(v4i32, __VA_ARGS__)
 
 #define LD_V16(RTYPE, psrc, stride,                                   \
                out0, out1, out2, out3, out4, out5, out6, out7,        \
@@ -344,19 +353,6 @@
 }
 #define LD_SH16(...) LD_V16(v8i16, __VA_ARGS__)
 
-/* Description : Load as 4x4 block of signed halfword elements from 1D source
-                 data into 4 vectors (Each vector with 4 signed halfwords)
-   Arguments   : Inputs  - psrc
-                 Outputs - out0, out1, out2, out3
-*/
-#define LD4x4_SH(psrc, out0, out1, out2, out3)                \
-{                                                             \
-    out0 = LD_SH(psrc);                                       \
-    out2 = LD_SH(psrc + 8);                                   \
-    out1 = (v8i16) __msa_ilvl_d((v2i64) out0, (v2i64) out0);  \
-    out3 = (v8i16) __msa_ilvl_d((v2i64) out2, (v2i64) out2);  \
-}
-
 /* Description : Store vectors with stride
    Arguments   : Inputs  - in0, in1, stride
                  Outputs - pdst    (destination pointer to store to)
@@ -400,198 +396,127 @@
 #define ST_SH8(...) ST_V8(v8i16, __VA_ARGS__)
 #define ST_SW8(...) ST_V8(v4i32, __VA_ARGS__)
 
-/* Description : Store as 2x4 byte block to destination memory from input vector
-   Arguments   : Inputs  - in, stidx, pdst, stride
-                 Return Type - unsigned byte
-   Details     : Index stidx halfword element from 'in' vector is copied and
-                 stored on first line
-                 Index stidx+1 halfword element from 'in' vector is copied and
-                 stored on second line
-                 Index stidx+2 halfword element from 'in' vector is copied and
-                 stored on third line
-                 Index stidx+3 halfword element from 'in' vector is copied and
-                 stored on fourth line
-*/
-#define ST2x4_UB(in, stidx, pdst, stride)              \
-{                                                      \
-    uint16_t out0_m, out1_m, out2_m, out3_m;           \
-    uint8_t *pblk_2x4_m = (uint8_t *) (pdst);          \
-                                                       \
-    out0_m = __msa_copy_u_h((v8i16) in, (stidx));      \
-    out1_m = __msa_copy_u_h((v8i16) in, (stidx + 1));  \
-    out2_m = __msa_copy_u_h((v8i16) in, (stidx + 2));  \
-    out3_m = __msa_copy_u_h((v8i16) in, (stidx + 3));  \
-                                                       \
-    SH(out0_m, pblk_2x4_m);                            \
-    SH(out1_m, pblk_2x4_m + stride);                   \
-    SH(out2_m, pblk_2x4_m + 2 * stride);               \
-    SH(out3_m, pblk_2x4_m + 3 * stride);               \
-}
-
-/* Description : Store as 4x2 byte block to destination memory from input vector
-   Arguments   : Inputs  - in, pdst, stride
-                 Return Type - unsigned byte
-   Details     : Index 0 word element from input vector is copied and stored
-                 on first line
-                 Index 1 word element from input vector is copied and stored
-                 on second line
-*/
-#define ST4x2_UB(in, pdst, stride)             \
-{                                              \
-    uint32_t out0_m, out1_m;                   \
-    uint8_t *pblk_4x2_m = (uint8_t *) (pdst);  \
-                                               \
-    out0_m = __msa_copy_u_w((v4i32) in, 0);    \
-    out1_m = __msa_copy_u_w((v4i32) in, 1);    \
-                                               \
-    SW(out0_m, pblk_4x2_m);                    \
-    SW(out1_m, pblk_4x2_m + stride);           \
-}
-
-/* Description : Store as 4x4 byte block to destination memory from input vector
-   Arguments   : Inputs  - in0, in1, pdst, stride
-                 Return Type - unsigned byte
-   Details     : Idx0 word element from input vector 'in0' is copied and stored
-                 on first line
-                 Idx1 word element from input vector 'in0' is copied and stored
-                 on second line
-                 Idx2 word element from input vector 'in1' is copied and stored
-                 on third line
-                 Idx3 word element from input vector 'in1' is copied and stored
-                 on fourth line
-*/
-#define ST4x4_UB(in0, in1, idx0, idx1, idx2, idx3, pdst, stride)  \
-{                                                                 \
-    uint32_t out0_m, out1_m, out2_m, out3_m;                      \
-    uint8_t *pblk_4x4_m = (uint8_t *) (pdst);                     \
-                                                                  \
-    out0_m = __msa_copy_u_w((v4i32) in0, idx0);                   \
-    out1_m = __msa_copy_u_w((v4i32) in0, idx1);                   \
-    out2_m = __msa_copy_u_w((v4i32) in1, idx2);                   \
-    out3_m = __msa_copy_u_w((v4i32) in1, idx3);                   \
-                                                                  \
-    SW4(out0_m, out1_m, out2_m, out3_m, pblk_4x4_m, stride);      \
+/* Description : Store half word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Stores half word 'idx0' from 'in' to (pdst)
+ *               Stores half word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ */
+#define ST_H1(in, idx, pdst)                             \
+{                                                        \
+    uint16_t out0_m;                                     \
+    out0_m = __msa_copy_u_h((v8i16) in, idx);            \
+    SH(out0_m, (pdst));                                  \
 }
-#define ST4x8_UB(in0, in1, pdst, stride)                            \
-{                                                                   \
-    uint8_t *pblk_4x8 = (uint8_t *) (pdst);                         \
-                                                                    \
-    ST4x4_UB(in0, in0, 0, 1, 2, 3, pblk_4x8, stride);               \
-    ST4x4_UB(in1, in1, 0, 1, 2, 3, pblk_4x8 + 4 * stride, stride);  \
+#define ST_H2(in, idx0, idx1, pdst, stride)              \
+{                                                        \
+    uint16_t out0_m, out1_m;                             \
+    out0_m = __msa_copy_u_h((v8i16) in, idx0);           \
+    out1_m = __msa_copy_u_h((v8i16) in, idx1);           \
+    SH(out0_m, (pdst));                                  \
+    SH(out1_m, (pdst) + stride);                         \
+}
+#define ST_H4(in, idx0, idx1, idx2, idx3, pdst, stride)  \
+{                                                        \
+    uint16_t out0_m, out1_m, out2_m, out3_m;             \
+    out0_m = __msa_copy_u_h((v8i16) in, idx0);           \
+    out1_m = __msa_copy_u_h((v8i16) in, idx1);           \
+    out2_m = __msa_copy_u_h((v8i16) in, idx2);           \
+    out3_m = __msa_copy_u_h((v8i16) in, idx3);           \
+    SH(out0_m, (pdst));                                  \
+    SH(out1_m, (pdst) + stride);                         \
+    SH(out2_m, (pdst) + 2 * stride);                     \
+    SH(out3_m, (pdst) + 3 * stride);                     \
+}
+#define ST_H8(in, idx0, idx1, idx2, idx3, idx4, idx5,            \
+              idx6, idx7, pdst, stride)                          \
+{                                                                \
+    ST_H4(in, idx0, idx1, idx2, idx3, pdst, stride)              \
+    ST_H4(in, idx4, idx5, idx6, idx7, (pdst) + 4*stride, stride) \
 }
 
-/* Description : Store as 6x4 byte block to destination memory from input
-                 vectors
-   Arguments   : Inputs  - in0, in1, pdst, stride
-                 Return Type - unsigned byte
-   Details     : Index 0 word element from input vector 'in0' is copied and
-                 stored on first line followed by index 2 halfword element
-                 Index 2 word element from input vector 'in0' is copied and
-                 stored on second line followed by index 2 halfword element
-                 Index 0 word element from input vector 'in1' is copied and
-                 stored on third line followed by index 2 halfword element
-                 Index 2 word element from input vector 'in1' is copied and
-                 stored on fourth line followed by index 2 halfword element
-*/
-#define ST6x4_UB(in0, in1, pdst, stride)       \
+/* Description : Store word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Stores word 'idx0' from 'in' to (pdst)
+ *               Stores word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ */
+#define ST_W1(in, idx, pdst)                             \
+{                                                        \
+    uint32_t out0_m;                                     \
+    out0_m = __msa_copy_u_w((v4i32) in, idx);            \
+    SW(out0_m, (pdst));                                  \
+}
+#define ST_W2(in, idx0, idx1, pdst, stride)              \
+{                                                        \
+    uint32_t out0_m, out1_m;                             \
+    out0_m = __msa_copy_u_w((v4i32) in, idx0);           \
+    out1_m = __msa_copy_u_w((v4i32) in, idx1);           \
+    SW(out0_m, (pdst));                                  \
+    SW(out1_m, (pdst) + stride);                         \
+}
+#define ST_W4(in, idx0, idx1, idx2, idx3, pdst, stride)  \
+{                                                        \
+    uint32_t out0_m, out1_m, out2_m, out3_m;             \
+    out0_m = __msa_copy_u_w((v4i32) in, idx0);           \
+    out1_m = __msa_copy_u_w((v4i32) in, idx1);           \
+    out2_m = __msa_copy_u_w((v4i32) in, idx2);           \
+    out3_m = __msa_copy_u_w((v4i32) in, idx3);           \
+    SW(out0_m, (pdst));                                  \
+    SW(out1_m, (pdst) + stride);                         \
+    SW(out2_m, (pdst) + 2*stride);                       \
+    SW(out3_m, (pdst) + 3*stride);                       \
+}
+#define ST_W8(in0, in1, idx0, idx1, idx2, idx3,                 \
+              idx4, idx5, idx6, idx7, pdst, stride)             \
+{                                                               \
+    ST_W4(in0, idx0, idx1, idx2, idx3, pdst, stride)            \
+    ST_W4(in1, idx4, idx5, idx6, idx7, pdst + 4*stride, stride) \
+}
+
+/* Description : Store double word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Stores double word 'idx0' from 'in' to (pdst)
+ *               Stores double word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ */
+#define ST_D1(in, idx, pdst)                   \
 {                                              \
-    uint32_t out0_m, out1_m, out2_m, out3_m;   \
-    uint16_t out4_m, out5_m, out6_m, out7_m;   \
-    uint8_t *pblk_6x4_m = (uint8_t *) (pdst);  \
-                                               \
-    out0_m = __msa_copy_u_w((v4i32) in0, 0);   \
-    out1_m = __msa_copy_u_w((v4i32) in0, 2);   \
-    out2_m = __msa_copy_u_w((v4i32) in1, 0);   \
-    out3_m = __msa_copy_u_w((v4i32) in1, 2);   \
-                                               \
-    out4_m = __msa_copy_u_h((v8i16) in0, 2);   \
-    out5_m = __msa_copy_u_h((v8i16) in0, 6);   \
-    out6_m = __msa_copy_u_h((v8i16) in1, 2);   \
-    out7_m = __msa_copy_u_h((v8i16) in1, 6);   \
-                                               \
-    SW(out0_m, pblk_6x4_m);                    \
-    SH(out4_m, (pblk_6x4_m + 4));              \
-    pblk_6x4_m += stride;                      \
-    SW(out1_m, pblk_6x4_m);                    \
-    SH(out5_m, (pblk_6x4_m + 4));              \
-    pblk_6x4_m += stride;                      \
-    SW(out2_m, pblk_6x4_m);                    \
-    SH(out6_m, (pblk_6x4_m + 4));              \
-    pblk_6x4_m += stride;                      \
-    SW(out3_m, pblk_6x4_m);                    \
-    SH(out7_m, (pblk_6x4_m + 4));              \
-}
-
-/* Description : Store as 8x1 byte block to destination memory from input vector
-   Arguments   : Inputs  - in, pdst
-   Details     : Index 0 double word element from input vector 'in' is copied
-                 and stored to destination memory at (pdst)
-*/
-#define ST8x1_UB(in, pdst)                   \
-{                                            \
-    uint64_t out0_m;                         \
-    out0_m = __msa_copy_u_d((v2i64) in, 0);  \
-    SD(out0_m, pdst);                        \
-}
-
-/* Description : Store as 8x2 byte block to destination memory from input vector
-   Arguments   : Inputs  - in, pdst, stride
-   Details     : Index 0 double word element from input vector 'in' is copied
-                 and stored to destination memory at (pdst)
-                 Index 1 double word element from input vector 'in' is copied
-                 and stored to destination memory at (pdst + stride)
-*/
-#define ST8x2_UB(in, pdst, stride)             \
+    uint64_t out0_m;                           \
+    out0_m = __msa_copy_u_d((v2i64) in, idx);  \
+    SD(out0_m, (pdst));                        \
+}
+#define ST_D2(in, idx0, idx1, pdst, stride)    \
 {                                              \
     uint64_t out0_m, out1_m;                   \
-    uint8_t *pblk_8x2_m = (uint8_t *) (pdst);  \
-                                               \
-    out0_m = __msa_copy_u_d((v2i64) in, 0);    \
-    out1_m = __msa_copy_u_d((v2i64) in, 1);    \
-                                               \
-    SD(out0_m, pblk_8x2_m);                    \
-    SD(out1_m, pblk_8x2_m + stride);           \
-}
-
-/* Description : Store as 8x4 byte block to destination memory from input
-                 vectors
-   Arguments   : Inputs  - in0, in1, pdst, stride
-   Details     : Index 0 double word element from input vector 'in0' is copied
-                 and stored to destination memory at (pblk_8x4_m)
-                 Index 1 double word element from input vector 'in0' is copied
-                 and stored to destination memory at (pblk_8x4_m + stride)
-                 Index 0 double word element from input vector 'in1' is copied
-                 and stored to destination memory at (pblk_8x4_m + 2 * stride)
-                 Index 1 double word element from input vector 'in1' is copied
-                 and stored to destination memory at (pblk_8x4_m + 3 * stride)
-*/
-#define ST8x4_UB(in0, in1, pdst, stride)                      \
-{                                                             \
-    uint64_t out0_m, out1_m, out2_m, out3_m;                  \
-    uint8_t *pblk_8x4_m = (uint8_t *) (pdst);                 \
-                                                              \
-    out0_m = __msa_copy_u_d((v2i64) in0, 0);                  \
-    out1_m = __msa_copy_u_d((v2i64) in0, 1);                  \
-    out2_m = __msa_copy_u_d((v2i64) in1, 0);                  \
-    out3_m = __msa_copy_u_d((v2i64) in1, 1);                  \
-                                                              \
-    SD4(out0_m, out1_m, out2_m, out3_m, pblk_8x4_m, stride);  \
+    out0_m = __msa_copy_u_d((v2i64) in, idx0); \
+    out1_m = __msa_copy_u_d((v2i64) in, idx1); \
+    SD(out0_m, (pdst));                        \
+    SD(out1_m, (pdst) + stride);               \
 }
-#define ST8x8_UB(in0, in1, in2, in3, pdst, stride)        \
-{                                                         \
-    uint8_t *pblk_8x8_m = (uint8_t *) (pdst);             \
-                                                          \
-    ST8x4_UB(in0, in1, pblk_8x8_m, stride);               \
-    ST8x4_UB(in2, in3, pblk_8x8_m + 4 * stride, stride);  \
-}
-#define ST12x4_UB(in0, in1, in2, pdst, stride)                \
+#define ST_D4(in0, in1, idx0, idx1, idx2, idx3, pdst, stride) \
 {                                                             \
-    uint8_t *pblk_12x4_m = (uint8_t *) (pdst);                \
-                                                              \
-    /* left 8x4 */                                            \
-    ST8x4_UB(in0, in1, pblk_12x4_m, stride);                  \
-    /* right 4x4 */                                           \
-    ST4x4_UB(in2, in2, 0, 1, 2, 3, pblk_12x4_m + 8, stride);  \
+    uint64_t out0_m, out1_m, out2_m, out3_m;                  \
+    out0_m = __msa_copy_u_d((v2i64) in0, idx0);               \
+    out1_m = __msa_copy_u_d((v2i64) in0, idx1);               \
+    out2_m = __msa_copy_u_d((v2i64) in1, idx2);               \
+    out3_m = __msa_copy_u_d((v2i64) in1, idx3);               \
+    SD(out0_m, (pdst));                                       \
+    SD(out1_m, (pdst) + stride);                              \
+    SD(out2_m, (pdst) + 2 * stride);                          \
+    SD(out3_m, (pdst) + 3 * stride);                          \
+}
+#define ST_D8(in0, in1, in2, in3, idx0, idx1, idx2, idx3,              \
+              idx4, idx5, idx6, idx7, pdst, stride)                    \
+{                                                                      \
+    ST_D4(in0, in1, idx0, idx1, idx2, idx3, pdst, stride)              \
+    ST_D4(in2, in3, idx4, idx5, idx6, idx7, pdst + 4 * stride, stride) \
 }
 
 /* Description : Store as 12x8 byte block to destination memory from
@@ -681,67 +606,48 @@
 }
 #define AVER_UB4_UB(...) AVER_UB4(v16u8, __VA_ARGS__)
 
-/* Description : Immediate number of columns to slide with zero
-   Arguments   : Inputs  - in0, in1, slide_val
-                 Outputs - out0, out1
+/* Description : Immediate number of columns to slide
+   Arguments   : Inputs  - s, d, slide_val
+                 Outputs - out
                  Return Type - as per RTYPE
-   Details     : Byte elements from 'zero_m' vector are slide into 'in0' by
+   Details     : Byte elements from 'd' vector are slide into 's' by
                  number of elements specified by 'slide_val'
 */
-#define SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val)                 \
-{                                                                         \
-    v16i8 zero_m = { 0 };                                                 \
-    out0 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in0, slide_val);  \
-    out1 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in1, slide_val);  \
-}
-#define SLDI_B2_0_UB(...) SLDI_B2_0(v16u8, __VA_ARGS__)
-#define SLDI_B2_0_SB(...) SLDI_B2_0(v16i8, __VA_ARGS__)
-#define SLDI_B2_0_SW(...) SLDI_B2_0(v4i32, __VA_ARGS__)
-
-#define SLDI_B3_0(RTYPE, in0, in1, in2, out0, out1, out2,  slide_val)     \
-{                                                                         \
-    v16i8 zero_m = { 0 };                                                 \
-    SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val);                    \
-    out2 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in2, slide_val);  \
-}
-#define SLDI_B3_0_UB(...) SLDI_B3_0(v16u8, __VA_ARGS__)
-#define SLDI_B3_0_SB(...) SLDI_B3_0(v16i8, __VA_ARGS__)
-
-#define SLDI_B4_0(RTYPE, in0, in1, in2, in3,            \
-                  out0, out1, out2, out3, slide_val)    \
-{                                                       \
-    SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val);  \
-    SLDI_B2_0(RTYPE, in2, in3, out2, out3, slide_val);  \
+#define SLDI_B(RTYPE, d, s, slide_val, out)                       \
+{                                                                 \
+    out = (RTYPE) __msa_sldi_b((v16i8) d, (v16i8) s, slide_val);  \
 }
-#define SLDI_B4_0_UB(...) SLDI_B4_0(v16u8, __VA_ARGS__)
-#define SLDI_B4_0_SB(...) SLDI_B4_0(v16i8, __VA_ARGS__)
-#define SLDI_B4_0_SH(...) SLDI_B4_0(v8i16, __VA_ARGS__)
 
-/* Description : Immediate number of columns to slide
-   Arguments   : Inputs  - in0_0, in0_1, in1_0, in1_1, slide_val
-                 Outputs - out0, out1
-                 Return Type - as per RTYPE
-   Details     : Byte elements from 'in0_0' vector are slide into 'in1_0' by
-                 number of elements specified by 'slide_val'
-*/
-#define SLDI_B2(RTYPE, in0_0, in0_1, in1_0, in1_1, out0, out1, slide_val)  \
-{                                                                          \
-    out0 = (RTYPE) __msa_sldi_b((v16i8) in0_0, (v16i8) in1_0, slide_val);  \
-    out1 = (RTYPE) __msa_sldi_b((v16i8) in0_1, (v16i8) in1_1, slide_val);  \
+#define SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+{                                                              \
+    SLDI_B(RTYPE, d0, s0, slide_val, out0)                     \
+    SLDI_B(RTYPE, d1, s1, slide_val, out1)                     \
 }
 #define SLDI_B2_UB(...) SLDI_B2(v16u8, __VA_ARGS__)
 #define SLDI_B2_SB(...) SLDI_B2(v16i8, __VA_ARGS__)
 #define SLDI_B2_SH(...) SLDI_B2(v8i16, __VA_ARGS__)
+#define SLDI_B2_SW(...) SLDI_B2(v4i32, __VA_ARGS__)
 
-#define SLDI_B3(RTYPE, in0_0, in0_1, in0_2, in1_0, in1_1, in1_2,           \
-                out0, out1, out2, slide_val)                               \
-{                                                                          \
-    SLDI_B2(RTYPE, in0_0, in0_1, in1_0, in1_1, out0, out1, slide_val)      \
-    out2 = (RTYPE) __msa_sldi_b((v16i8) in0_2, (v16i8) in1_2, slide_val);  \
+#define SLDI_B3(RTYPE, d0, s0, d1, s1, d2, s2, slide_val,  \
+                out0, out1, out2)                          \
+{                                                          \
+    SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+    SLDI_B(RTYPE, d2, s2, slide_val, out2)                 \
 }
+#define SLDI_B3_UB(...) SLDI_B3(v16u8, __VA_ARGS__)
 #define SLDI_B3_SB(...) SLDI_B3(v16i8, __VA_ARGS__)
 #define SLDI_B3_UH(...) SLDI_B3(v8u16, __VA_ARGS__)
 
+#define SLDI_B4(RTYPE, d0, s0, d1, s1, d2, s2, d3, s3,     \
+                slide_val, out0, out1, out2, out3)         \
+{                                                          \
+    SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+    SLDI_B2(RTYPE, d2, s2, d3, s3, slide_val, out2, out3)  \
+}
+#define SLDI_B4_UB(...) SLDI_B4(v16u8, __VA_ARGS__)
+#define SLDI_B4_SB(...) SLDI_B4(v16i8, __VA_ARGS__)
+#define SLDI_B4_SH(...) SLDI_B4(v8i16, __VA_ARGS__)
+
 /* Description : Shuffle byte vector elements as per mask vector
    Arguments   : Inputs  - in0, in1, in2, in3, mask0, mask1
                  Outputs - out0, out1
@@ -1012,99 +918,78 @@
 
 /* Description : Clips all halfword elements of input vector between min & max
                  out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
-   Arguments   : Inputs  - in       (input vector)
-                         - min      (min threshold)
-                         - max      (max threshold)
-                 Outputs - out_m    (output vector with clipped elements)
+   Arguments   : Inputs  - in    (input vector)
+                         - min   (min threshold)
+                         - max   (max threshold)
+                 Outputs - in    (output vector with clipped elements)
                  Return Type - signed halfword
 */
-#define CLIP_SH(in, min, max)                           \
-( {                                                     \
-    v8i16 out_m;                                        \
-                                                        \
-    out_m = __msa_max_s_h((v8i16) min, (v8i16) in);     \
-    out_m = __msa_min_s_h((v8i16) max, (v8i16) out_m);  \
-    out_m;                                              \
-} )
+#define CLIP_SH(in, min, max)                     \
+{                                                 \
+    in = __msa_max_s_h((v8i16) min, (v8i16) in);  \
+    in = __msa_min_s_h((v8i16) max, (v8i16) in);  \
+}
 
 /* Description : Clips all signed halfword elements of input vector
                  between 0 & 255
-   Arguments   : Inputs  - in       (input vector)
-                 Outputs - out_m    (output vector with clipped elements)
-                 Return Type - signed halfword
+   Arguments   : Inputs  - in    (input vector)
+                 Outputs - in    (output vector with clipped elements)
+                 Return Type - signed halfwords
 */
-#define CLIP_SH_0_255(in)                                 \
-( {                                                       \
-    v8i16 max_m = __msa_ldi_h(255);                       \
-    v8i16 out_m;                                          \
-                                                          \
-    out_m = __msa_maxi_s_h((v8i16) in, 0);                \
-    out_m = __msa_min_s_h((v8i16) max_m, (v8i16) out_m);  \
-    out_m;                                                \
-} )
+#define CLIP_SH_0_255(in)                       \
+{                                               \
+    in = __msa_maxi_s_h((v8i16) in, 0);         \
+    in = (v8i16) __msa_sat_u_h((v8u16) in, 7);  \
+}
+
 #define CLIP_SH2_0_255(in0, in1)  \
 {                                 \
-    in0 = CLIP_SH_0_255(in0);     \
-    in1 = CLIP_SH_0_255(in1);     \
+    CLIP_SH_0_255(in0);           \
+    CLIP_SH_0_255(in1);           \
 }
+
 #define CLIP_SH4_0_255(in0, in1, in2, in3)  \
 {                                           \
     CLIP_SH2_0_255(in0, in1);               \
     CLIP_SH2_0_255(in2, in3);               \
 }
 
-#define CLIP_SH_0_255_MAX_SATU(in)                    \
-( {                                                   \
-    v8i16 out_m;                                      \
-                                                      \
-    out_m = __msa_maxi_s_h((v8i16) in, 0);            \
-    out_m = (v8i16) __msa_sat_u_h((v8u16) out_m, 7);  \
-    out_m;                                            \
-} )
-#define CLIP_SH2_0_255_MAX_SATU(in0, in1)  \
-{                                          \
-    in0 = CLIP_SH_0_255_MAX_SATU(in0);     \
-    in1 = CLIP_SH_0_255_MAX_SATU(in1);     \
-}
-#define CLIP_SH4_0_255_MAX_SATU(in0, in1, in2, in3)  \
-{                                                    \
-    CLIP_SH2_0_255_MAX_SATU(in0, in1);               \
-    CLIP_SH2_0_255_MAX_SATU(in2, in3);               \
+#define CLIP_SH8_0_255(in0, in1, in2, in3,  \
+                       in4, in5, in6, in7)  \
+{                                           \
+    CLIP_SH4_0_255(in0, in1, in2, in3);     \
+    CLIP_SH4_0_255(in4, in5, in6, in7);     \
 }
 
 /* Description : Clips all signed word elements of input vector
                  between 0 & 255
-   Arguments   : Inputs  - in       (input vector)
-                 Outputs - out_m    (output vector with clipped elements)
+   Arguments   : Inputs  - in    (input vector)
+                 Outputs - in    (output vector with clipped elements)
                  Return Type - signed word
 */
-#define CLIP_SW_0_255(in)                                 \
-( {                                                       \
-    v4i32 max_m = __msa_ldi_w(255);                       \
-    v4i32 out_m;                                          \
-                                                          \
-    out_m = __msa_maxi_s_w((v4i32) in, 0);                \
-    out_m = __msa_min_s_w((v4i32) max_m, (v4i32) out_m);  \
-    out_m;                                                \
-} )
+#define CLIP_SW_0_255(in)                       \
+{                                               \
+    in = __msa_maxi_s_w((v4i32) in, 0);         \
+    in = (v4i32) __msa_sat_u_w((v4u32) in, 7);  \
+}
 
-#define CLIP_SW_0_255_MAX_SATU(in)                    \
-( {                                                   \
-    v4i32 out_m;                                      \
-                                                      \
-    out_m = __msa_maxi_s_w((v4i32) in, 0);            \
-    out_m = (v4i32) __msa_sat_u_w((v4u32) out_m, 7);  \
-    out_m;                                            \
-} )
-#define CLIP_SW2_0_255_MAX_SATU(in0, in1)  \
-{                                          \
-    in0 = CLIP_SW_0_255_MAX_SATU(in0);     \
-    in1 = CLIP_SW_0_255_MAX_SATU(in1);     \
+#define CLIP_SW2_0_255(in0, in1)  \
+{                                 \
+    CLIP_SW_0_255(in0);           \
+    CLIP_SW_0_255(in1);           \
 }
-#define CLIP_SW4_0_255_MAX_SATU(in0, in1, in2, in3)  \
-{                                                    \
-    CLIP_SW2_0_255_MAX_SATU(in0, in1);               \
-    CLIP_SW2_0_255_MAX_SATU(in2, in3);               \
+
+#define CLIP_SW4_0_255(in0, in1, in2, in3)  \
+{                                           \
+    CLIP_SW2_0_255(in0, in1);               \
+    CLIP_SW2_0_255(in2, in3);               \
+}
+
+#define CLIP_SW8_0_255(in0, in1, in2, in3,  \
+                       in4, in5, in6, in7)  \
+{                                           \
+    CLIP_SW4_0_255(in0, in1, in2, in3);     \
+    CLIP_SW4_0_255(in4, in5, in6, in7);     \
 }
 
 /* Description : Addition of 4 signed word elements
@@ -1234,6 +1119,15 @@
                  unsigned absolute diff values, even-odd pairs are added
                  together to generate 8 halfword results.
 */
+#if HAVE_MSA2
+#define SAD_UB2_UH(in0, in1, ref0, ref1)                                 \
+( {                                                                      \
+    v8u16 sad_m = { 0 };                                                 \
+    sad_m += __builtin_msa2_sad_adj2_u_w2x_b((v16u8) in0, (v16u8) ref0); \
+    sad_m += __builtin_msa2_sad_adj2_u_w2x_b((v16u8) in1, (v16u8) ref1); \
+    sad_m;                                                               \
+} )
+#else
 #define SAD_UB2_UH(in0, in1, ref0, ref1)                        \
 ( {                                                             \
     v16u8 diff0_m, diff1_m;                                     \
@@ -1247,6 +1141,7 @@
                                                                 \
     sad_m;                                                      \
 } )
+#endif // #if HAVE_MSA2
 
 /* Description : Insert specified word elements from input vectors to 1
                  destination vector
@@ -1491,6 +1386,7 @@
             out4, out5, out6, out7);                              \
 }
 #define ILVR_B8_UH(...) ILVR_B8(v8u16, __VA_ARGS__)
+#define ILVR_B8_SW(...) ILVR_B8(v4i32, __VA_ARGS__)
 
 /* Description : Interleave right half of halfword elements from vectors
    Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
@@ -2287,6 +2183,12 @@
                  extracted and interleaved with same vector 'in0' to generate
                  4 word elements keeping sign intact
 */
+#if HAVE_MSA2
+#define UNPCK_R_SH_SW(in, out)                           \
+{                                                        \
+    out = (v4i32) __builtin_msa2_w2x_lo_s_h((v8i16) in); \
+}
+#else
 #define UNPCK_R_SH_SW(in, out)                       \
 {                                                    \
     v8i16 sign_m;                                    \
@@ -2294,6 +2196,7 @@
     sign_m = __msa_clti_s_h((v8i16) in, 0);          \
     out = (v4i32) __msa_ilvr_h(sign_m, (v8i16) in);  \
 }
+#endif // #if HAVE_MSA2
 
 /* Description : Sign extend byte elements from input vector and return
                  halfword results in pair of vectors
@@ -2306,6 +2209,13 @@
                  Then interleaved left with same vector 'in0' to
                  generate 8 signed halfword elements in 'out1'
 */
+#if HAVE_MSA2
+#define UNPCK_SB_SH(in, out0, out1)                       \
+{                                                         \
+    out0 = (v4i32) __builtin_msa2_w2x_lo_s_b((v16i8) in); \
+    out1 = (v4i32) __builtin_msa2_w2x_hi_s_b((v16i8) in); \
+}
+#else
 #define UNPCK_SB_SH(in, out0, out1)                  \
 {                                                    \
     v16i8 tmp_m;                                     \
@@ -2313,6 +2223,7 @@
     tmp_m = __msa_clti_s_b((v16i8) in, 0);           \
     ILVRL_B2_SH(tmp_m, in, out0, out1);              \
 }
+#endif // #if HAVE_MSA2
 
 /* Description : Zero extend unsigned byte elements to halfword elements
    Arguments   : Inputs  - in           (1 input unsigned byte vector)
@@ -2339,6 +2250,13 @@
                  Then interleaved left with same vector 'in0' to
                  generate 4 signed word elements in 'out1'
 */
+#if HAVE_MSA2
+#define UNPCK_SH_SW(in, out0, out1)                       \
+{                                                         \
+    out0 = (v4i32) __builtin_msa2_w2x_lo_s_h((v8i16) in); \
+    out1 = (v4i32) __builtin_msa2_w2x_hi_s_h((v8i16) in); \
+}
+#else
 #define UNPCK_SH_SW(in, out0, out1)                  \
 {                                                    \
     v8i16 tmp_m;                                     \
@@ -2346,6 +2264,7 @@
     tmp_m = __msa_clti_s_h((v8i16) in, 0);           \
     ILVRL_H2_SW(tmp_m, in, out0, out1);              \
 }
+#endif // #if HAVE_MSA2
 
 /* Description : Swap two variables
    Arguments   : Inputs  - in0, in1
@@ -2479,6 +2398,7 @@
 {                                                                        \
     v16i8 tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                \
     v16i8 tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                \
+    v16i8 zeros = { 0 };                                                 \
                                                                          \
     ILVR_B4_SB(in2, in0, in3, in1, in6, in4, in7, in5,                   \
                tmp0_m, tmp1_m, tmp2_m, tmp3_m);                          \
@@ -2486,8 +2406,8 @@
     ILVRL_B2_SB(tmp3_m, tmp2_m, tmp6_m, tmp7_m);                         \
     ILVRL_W2(RTYPE, tmp6_m, tmp4_m, out0, out2);                         \
     ILVRL_W2(RTYPE, tmp7_m, tmp5_m, out4, out6);                         \
-    SLDI_B2_0(RTYPE, out0, out2, out1, out3, 8);                         \
-    SLDI_B2_0(RTYPE, out4, out6, out5, out7, 8);                         \
+    SLDI_B4(RTYPE, zeros, out0, zeros, out2, zeros, out4, zeros, out6,   \
+            8, out1, out3, out5, out7);                                  \
 }
 #define TRANSPOSE8x8_UB_UB(...) TRANSPOSE8x8_UB(v16u8, __VA_ARGS__)
 #define TRANSPOSE8x8_UB_UH(...) TRANSPOSE8x8_UB(v8u16, __VA_ARGS__)
@@ -2569,8 +2489,6 @@
     out5 = (v16u8) __msa_ilvod_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
                                                                              \
     tmp2_m = (v16u8) __msa_ilvod_h((v8i16) tmp5_m, (v8i16) tmp4_m);          \
-    tmp2_m = (v16u8) __msa_ilvod_h((v8i16) tmp5_m, (v8i16) tmp4_m);          \
-    tmp3_m = (v16u8) __msa_ilvod_h((v8i16) tmp7_m, (v8i16) tmp6_m);          \
     tmp3_m = (v16u8) __msa_ilvod_h((v8i16) tmp7_m, (v8i16) tmp6_m);          \
     out3 = (v16u8) __msa_ilvev_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
     out7 = (v16u8) __msa_ilvod_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
@@ -2850,13 +2768,11 @@
 */
 #define DPADD_SH3_SH(in0, in1, in2, coeff0, coeff1, coeff2)         \
 ( {                                                                 \
-    v8i16 tmp1_m;                                                   \
     v8i16 out0_m;                                                   \
                                                                     \
     out0_m = __msa_dotp_s_h((v16i8) in0, (v16i8) coeff0);           \
     out0_m = __msa_dpadd_s_h(out0_m, (v16i8) in1, (v16i8) coeff1);  \
-    tmp1_m = __msa_dotp_s_h((v16i8) in2, (v16i8) coeff2);           \
-    out0_m = __msa_adds_s_h(out0_m, tmp1_m);                        \
+    out0_m = __msa_dpadd_s_h(out0_m, (v16i8) in2, (v16i8) coeff2);  \
                                                                     \
     out0_m;                                                         \
 } )
@@ -2890,7 +2806,7 @@
     tmp0_m = PCKEV_XORI128_UB(in0, in1);                      \
     tmp1_m = PCKEV_XORI128_UB(in2, in3);                      \
     AVER_UB2_UB(tmp0_m, dst0, tmp1_m, dst1, tmp0_m, tmp1_m);  \
-    ST8x4_UB(tmp0_m, tmp1_m, pdst_m, stride);                 \
+    ST_D4(tmp0_m, tmp1_m, 0, 1, 0, 1, pdst_m, stride);        \
 }
 
 /* Description : Pack even byte elements, extract 0 & 2 index words from pair
diff --git a/libavutil/mips/mmiutils.h b/libavutil/mips/mmiutils.h
index 76b1199..8f692e8 100644
--- a/libavutil/mips/mmiutils.h
+++ b/libavutil/mips/mmiutils.h
@@ -205,7 +205,7 @@
  * backup register
  */
 #define BACKUP_REG \
-  double temp_backup_reg[8];                                    \
+  LOCAL_ALIGNED_16(double, temp_backup_reg, [8]);               \
   if (_MIPS_SIM == _ABI64)                                      \
     __asm__ volatile (                                          \
       "gssqc1       $f25,      $f24,       0x00(%[temp])  \n\t" \
@@ -250,6 +250,15 @@
       : "memory"                                                \
     );
 
+/**
+ * brief: Transpose 2X2 word packaged data.
+ * fr_i0, fr_i1: src
+ * fr_o0, fr_o1: dst
+ */
+#define TRANSPOSE_2W(fr_i0, fr_i1, fr_o0, fr_o1)                          \
+        "punpcklwd  "#fr_o0",   "#fr_i0",   "#fr_i1"                \n\t" \
+        "punpckhwd  "#fr_o1",   "#fr_i0",   "#fr_i1"                \n\t"
+
 /**
  * brief: Transpose 4X4 half word packaged data.
  * fr_i0, fr_i1, fr_i2, fr_i3: src & dst
@@ -336,5 +345,20 @@
         PSRAH_4_MMI(fp1, fp2, fp3, fp4, shift)                            \
         PSRAH_4_MMI(fp5, fp6, fp7, fp8, shift)
 
+/**
+ * brief: (((value) + (1 << ((n) - 1))) >> (n))
+ * fr_i0: src & dst
+ * fr_i1: Operand number
+ * fr_t0, fr_t1: temporary FPR
+ * gr_t0: temporary GPR
+ */
+#define ROUND_POWER_OF_TWO_MMI(fr_i0, fr_i1, fr_t0, fr_t1, gr_t0)         \
+        "li         "#gr_t0",     0x01                              \n\t" \
+        "dmtc1      "#gr_t0",     "#fr_t0"                          \n\t" \
+        "punpcklwd  "#fr_t0",     "#fr_t0",    "#fr_t0"             \n\t" \
+        "psubw      "#fr_t1",     "#fr_i1",    "#fr_t0"             \n\t" \
+        "psllw      "#fr_t1",     "#fr_t0",    "#fr_t1"             \n\t" \
+        "paddw      "#fr_i0",     "#fr_i0",    "#fr_t1"             \n\t" \
+        "psraw      "#fr_i0",     "#fr_i0",    "#fr_i1"             \n\t"
 
 #endif /* AVUTILS_MIPS_MMIUTILS_H */
diff --git a/libavutil/tests/cpu.c b/libavutil/tests/cpu.c
index ce45b71..c853371 100644
--- a/libavutil/tests/cpu.c
+++ b/libavutil/tests/cpu.c
@@ -49,6 +49,9 @@ static const struct {
     { AV_CPU_FLAG_SETEND,    "setend"     },
 #elif ARCH_PPC
     { AV_CPU_FLAG_ALTIVEC,   "altivec"    },
+#elif ARCH_MIPS
+    { AV_CPU_FLAG_MMI,       "mmi"        },
+    { AV_CPU_FLAG_MSA,       "msa"        },
 #elif ARCH_X86
     { AV_CPU_FLAG_MMX,       "mmx"        },
     { AV_CPU_FLAG_MMXEXT,    "mmxext"     },
diff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c
index 721a091..ec00723 100644
--- a/tests/checkasm/checkasm.c
+++ b/tests/checkasm/checkasm.c
@@ -197,6 +197,9 @@ static const struct {
     { "ALTIVEC",  "altivec",  AV_CPU_FLAG_ALTIVEC },
     { "VSX",      "vsx",      AV_CPU_FLAG_VSX },
     { "POWER8",   "power8",   AV_CPU_FLAG_POWER8 },
+#elif ARCH_MIPS
+    { "MMI",      "mmi",      AV_CPU_FLAG_MMI },
+    { "MSA",      "msa",      AV_CPU_FLAG_MSA },
 #elif ARCH_X86
     { "MMX",      "mmx",      AV_CPU_FLAG_MMX|AV_CPU_FLAG_CMOV },
     { "MMXEXT",   "mmxext",   AV_CPU_FLAG_MMXEXT },
-- 
2.20.1

